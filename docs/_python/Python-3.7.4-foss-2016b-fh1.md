---
title: Python-3.7.4-foss-2016b-fh1
date: 2019-10-10
---

### Known Issues
 * None

### Package List
  * [APScheduler-3.6.1](https://github.com/agronholm/apscheduler) .. image:: https://travis-ci.com/agronholm/apscheduler.svg?branch=master
  :target: https://travis-ci.com/agronholm/apscheduler
  :alt: Build Status
.. image:: https://coveralls.io/repos/github/agronholm/apscheduler/badge.svg?branch=master
  :target: https://coveralls.io/github/agronholm/apscheduler?branch=master
  :alt: Code Coverage

Advanced Python Scheduler (APScheduler) is a Python library that lets you schedule your Python code
to be executed later, either just once or periodically. You can add new jobs or remove old ones on
the fly as you please. If you store your jobs in a database, they will also survive scheduler
restarts and maintain their state. When the scheduler is restarted, it will then run all the jobs
it should have run while it was offline [#f1]_.

Among other things, APScheduler can be used as a cross-platform, application specific replacement
to platform specific schedulers, such as the cron daemon or the Windows task scheduler. Please
note, however, that APScheduler is **not** a daemon or service itself, nor does it come with any
command line tools. It is primarily meant to be run inside existing applications. That said,
APScheduler does provide some building blocks for you to build a scheduler service or to run a
dedicated scheduler process.

APScheduler has three built-in scheduling systems you can use:

* Cron-style scheduling (with optional start/end times)
* Interval-based execution (runs jobs on even intervals, with optional start/end times)
* One-off delayed execution (runs jobs once, on a set date/time)

You can mix and match scheduling systems and the backends where the jobs are stored any way you
like. Supported backends for storing jobs include:

* Memory
* `SQLAlchemy <http://www.sqlalchemy.org/>`_ (any RDBMS supported by SQLAlchemy works)
* `MongoDB <http://www.mongodb.org/>`_
* `Redis <http://redis.io/>`_
* `RethinkDB <https://www.rethinkdb.com/>`_
* `ZooKeeper <https://zookeeper.apache.org/>`_

APScheduler also integrates with several common Python frameworks, like:

* `asyncio <http://docs.python.org/3.4/library/asyncio.html>`_ (:pep:`3156`)
* `gevent <http://www.gevent.org/>`_
* `Tornado <http://www.tornadoweb.org/>`_
* `Twisted <http://twistedmatrix.com/>`_
* `Qt <http://qt-project.org/>`_ (using either
  `PyQt <http://www.riverbankcomputing.com/software/pyqt/intro>`_ or
  `PySide <http://qt-project.org/wiki/PySide>`_)

.. [#f1] The cutoff period for this is also configurable.


Documentation
-------------

Documentation can be found `here <http://readthedocs.org/docs/apscheduler/en/latest/>`_.


Source
------

The source can be browsed at `Github <https://github.com/agronholm/apscheduler>`_.


Reporting bugs
--------------

A `bug tracker <https://github.com/agronholm/apscheduler/issues>`_ is provided by Github.


Getting help
------------

If you have problems or other questions, you can either:

* Ask in the `apscheduler <https://gitter.im/apscheduler/Lobby>`_ room on Gitter
* Ask on the `APScheduler Google group <http://groups.google.com/group/apscheduler>`_, or
* Ask on `StackOverflow <http://stackoverflow.com/questions/tagged/apscheduler>`_ and tag your
  question with the ``apscheduler`` tag



  * [AnyQt-0.0.10](https://github.com/ales-erjavec/anyqt) AnyQt
-----

PyQt/PySide compatibility layer.

Features:

* At the top level AnyQt exports a Qt5 compatible module namespace along with
  some minimal renames to better support portability between different
  versions
* Which Qt api/backend is chosen can be controlled by a QT_API env variable
* The api can be chosen/forced programmatically (as long as no
  PyQt4/PyQt5/PySide/PySide2 was already imported)
* provides an optional compatibility import hook, that denys imports from
  conflicting Qt api, or intercepts and fakes a Qt4 api imports, to use a Qt5
  compatible API (some monkey patching is involved).

The documentation is hosted at https://anyqt.readthedocs.io/en/stable/




  * [Babel-2.7.0](http://babel.pocoo.org/) A collection of tools for internationalizing Python applications.



  * [Brotli-1.0.7](https://github.com/google/brotli) 
  * [CacheControl-0.12.5](https://github.com/ionrock/cachecontrol) ==============
 CacheControl
==============

.. image:: https://img.shields.io/pypi/v/cachecontrol.svg
    :target: https://pypi.python.org/pypi/cachecontrol
    :alt: Latest Version

.. image:: https://travis-ci.org/ionrock/cachecontrol.png?branch=master
  :target: https://travis-ci.org/ionrock/cachecontrol

CacheControl is a port of the caching algorithms in httplib2_ for use with
requests_ session object.

It was written because httplib2's better support for caching is often
mitigated by its lack of threadsafety. The same is true of requests in
terms of caching.


Quickstart
==========

.. code-block:: python

  import requests

  from cachecontrol import CacheControl


  sess = requests.session()
  cached_sess = CacheControl(sess)

  response = cached_sess.get('http://google.com')

If the URL contains any caching based headers, it will cache the
result in a simple dictionary.

For more info, check out the docs_

.. _docs: http://cachecontrol.readthedocs.org/en/latest/
.. _httplib2: https://github.com/jcgregorio/httplib2
.. _requests: http://docs.python-requests.org/
  * [ConfigArgParse-0.14.0](https://github.com/bw2/ConfigArgParse) ConfigArgParse
--------------

.. image:: https://img.shields.io/pypi/v/ConfigArgParse.svg?style=flat
    :alt: PyPI version
    :target: https://pypi.python.org/pypi/ConfigArgParse

.. image:: https://img.shields.io/pypi/pyversions/ConfigArgParse.svg
    :alt: Supported Python versions
    :target: https://pypi.python.org/pypi/ConfigArgParse

.. image:: https://travis-ci.org/bw2/ConfigArgParse.svg?branch=master
    :alt: Travis CI build
    :target: https://travis-ci.org/bw2/ConfigArgParse

Overview
~~~~~~~~

Applications with more than a handful of user-settable options are best
configured through a combination of command line args, config files,
hard-coded defaults, and in some cases, environment variables.

Python's command line parsing modules such as argparse have very limited
support for config files and environment variables, so this module
extends argparse to add these features.

Available on PyPI: http://pypi.python.org/pypi/ConfigArgParse

.. image:: https://travis-ci.org/bw2/ConfigArgParse.svg?branch=master
    :target: https://travis-ci.org/bw2/ConfigArgParse

Features
~~~~~~~~

-  command-line, config file, env var, and default settings can now be
   defined, documented, and parsed in one go using a single API (if a
   value is specified in more than one way then: command line >
   environment variables > config file values > defaults)
-  config files can have .ini or .yaml style syntax (eg. key=value or
   key: value)
-  user can provide a config file via a normal-looking command line arg
   (eg. -c path/to/config.txt) rather than the argparse-style @config.txt
-  one or more default config file paths can be specified
   (eg. ['/etc/bla.conf', '~/.my_config'] )
-  all argparse functionality is fully supported, so this module can
   serve as a drop-in replacement (verified by argparse unittests).
-  env vars and config file keys & syntax are automatically documented
   in the -h help message
-  new method :code:`print_values()` can report keys & values and where
   they were set (eg. command line, env var, config file, or default).
-  lite-weight (no 3rd-party library dependencies except (optionally) PyYAML)
-  extensible (:code:`ConfigFileParser` can be subclassed to define a new
   config file format)
-  unittested by running the unittests that came with argparse but on
   configargparse, and using tox to test with Python 2.7 and Python 3+

Example
~~~~~~~

*my_script.py*:

Script that defines 4 options and a positional arg and then parses and prints the values. Also,
it prints out the help message as well as the string produced by :code:`format_values()` to show
what they look like.

.. code:: py

   import configargparse

   p = configargparse.ArgParser(default_config_files=['/etc/app/conf.d/*.conf', '~/.my_settings'])
   p.add('-c', '--my-config', required=True, is_config_file=True, help='config file path')
   p.add('--genome', required=True, help='path to genome file')  # this option can be set in a config file because it starts with '--'
   p.add('-v', help='verbose', action='store_true')
   p.add('-d', '--dbsnp', help='known variants .vcf', env_var='DBSNP_PATH')  # this option can be set in a config file because it starts with '--'
   p.add('vcf', nargs='+', help='variant file(s)')

   options = p.parse_args()

   print(options)
   print("----------")
   print(p.format_help())
   print("----------")
   print(p.format_values())    # useful for logging where different settings came from


*config.txt:*

Since the script above set the config file as required=True, lets create a config file to give it:

.. code:: py

    # settings for my_script.py
    genome = HCMV     # cytomegalovirus genome
    dbsnp = /data/dbsnp/variants.vcf


*command line:*

Now run the script and pass it the config file:

.. code:: bash

    python my_script.py --genome hg19 --my-config config.txt  f1.vcf  f2.vcf

*output:*

Here is the result:

.. code:: bash

    Namespace(dbsnp='/data/dbsnp/variants.vcf', genome='hg19', my_config='config.txt', vcf=['f1.vcf', 'f2.vcf'], verbose=False)
    ----------
    usage: my_script.py [-h] --genome GENOME [-v] -c MY_CONFIG [-d DBSNP]
                        vcf [vcf ...]
    Args that start with '--' (eg. --genome) can also be set in a config file
    (/etc/settings.ini or /home/jeff/.my_settings or provided via -c) by using
    .ini or .yaml-style syntax (eg. genome=value). Command-line values override
    environment variables which override config file values which override
    defaults.

    positional arguments:
      vcf                   variant file
    optional arguments:
      -h, --help            show this help message and exit
      --genome GENOME       path to genome file
      -v                    verbose
      -c MY_CONFIG, --my-config MY_CONFIG
                            config file path
      -d DBSNP, --dbsnp DBSNP
                            known variants .vcf [env var: DBSNP_PATH]
    ----------
    Command Line Args:   --genome hg19 --my-config config.txt f1.vcf f2.vcf
    Config File (config.txt):
      dbsnp:             /data/dbsnp/variants.vcf

Special Values
~~~~~~~~~~~~~~

Under the hood, configargparse handles environment variables and config file
values by converting them to their corresponding command line arg. For
example, "key = value" will be processed as if "--key value" was specified
on the command line.

Also, the following special values (whether in a config file or an environment
variable) are handled in a special way to support booleans and lists:

-  :code:`key = true` is handled as if "--key" was specified on the command line.
   In your python code this key must be defined as a boolean flag
   (eg. action="store_true" or similar).

-  :code:`key = [value1, value2, ...]` is handled as if "--key value1 --key value2"
   etc. was specified on the command line. In your python code this key must
   be defined as a list (eg. action="append").

Config File Syntax
~~~~~~~~~~~~~~~~~~

Only command line args that have a long version (eg. one that starts with '--')
can be set in a config file. For example, "--color" can be set by
putting "color=green" in a config file. The config file syntax depends on the
constuctor arg: :code:`config_file_parser_class` which can be set to one of the
provided classes: :code:`DefaultConfigFileParser` or :code:`YAMLConfigFileParser`,
or to your own subclass of the :code:`ConfigFileParser` abstract class.

*DefaultConfigFileParser*  - the full range of valid syntax is:

.. code:: yaml

        # this is a comment
        ; this is also a comment (.ini style)
        ---            # lines that start with --- are ignored (yaml style)
        -------------------
        [section]      # .ini-style section names are treated as comments

        # how to specify a key-value pair (all of these are equivalent):
        name value     # key is case sensitive: "Name" isn't "name"
        name = value   # (.ini style)  (white space is ignored, so name = value same as name=value)
        name: value    # (yaml style)
        --name value   # (argparse style)

        # how to set a flag arg (eg. arg which has action="store_true")
        --name
        name
        name = True    # "True" and "true" are the same

        # how to specify a list arg (eg. arg which has action="append")
        fruit = [apple, orange, lemon]
        indexes = [1, 12, 35 , 40]


*YAMLConfigFileParser*  - allows a subset of YAML syntax (http://goo.gl/VgT2DU)

.. code:: yaml

        # a comment
        name1: value
        name2: true    # "True" and "true" are the same

        fruit: [apple, orange, lemon]
        indexes: [1, 12, 35, 40]


ArgParser Singletons
~~~~~~~~~~~~~~~~~~~~~~~~~

To make it easier to configure different modules in an application,
configargparse provides globally-available ArgumentParser instances
via configargparse.get_argument_parser('name') (similar to
logging.getLogger('name')).

Here is an example of an application with a utils module that also
defines and retrieves its own command-line args.

*main.py*

.. code:: py

    import configargparse
    import utils

    p = configargparse.get_argument_parser()
    p.add_argument("-x", help="Main module setting")
    p.add_argument("--m-setting", help="Main module setting")
    options = p.parse_known_args()   # using p.parse_args() here may raise errors.

*utils.py*

.. code:: py

    import configargparse
    p = configargparse.get_argument_parser()
    p.add_argument("--utils-setting", help="Config-file-settable option for utils")

    if __name__ == "__main__":
       options = p.parse_known_args()

Help Formatters
~~~~~~~~~~~~~~~

:code:`ArgumentDefaultsRawHelpFormatter` is a new HelpFormatter that both adds
default values AND disables line-wrapping. It can be passed to the constructor:
:code:`ArgParser(.., formatter_class=ArgumentDefaultsRawHelpFormatter)`


Aliases
~~~~~~~

The configargparse.ArgumentParser API inherits its class and method
names from argparse and also provides the following shorter names for
convenience:

-  p = configargparse.get_arg_parser()  # get global singleton instance
-  p = configargparse.get_parser()
-  p = configargparse.ArgParser()  # create a new instance
-  p = configargparse.Parser()
-  p.add_arg(..)
-  p.add(..)
-  options = p.parse(..)

HelpFormatters:

- RawFormatter = RawDescriptionHelpFormatter
- DefaultsFormatter = ArgumentDefaultsHelpFormatter
- DefaultsRawFormatter = ArgumentDefaultsRawHelpFormatter


Design Notes
~~~~~~~~~~~~

Unit tests:

tests/test_configargparse.py contains custom unittests for features
specific to this module (such as config file and env-var support), as
well as a hook to load and run argparse unittests (see the built-in
test.test_argparse module) but on configargparse in place of argparse.
This ensures that configargparse will work as a drop in replacement for
argparse in all usecases.

Previously existing modules (PyPI search keywords: config argparse):

-  argparse (built-in module Python v2.7+)

   -  Good:

      -  fully featured command line parsing
      -  can read args from files using an easy to understand mechanism

   -  Bad:

      -  syntax for specifying config file path is unusual (eg.
         @file.txt)and not described in the user help message.
      -  default config file syntax doesn't support comments and is
         unintuitive (eg. --namevalue)
      -  no support for environment variables

-  ConfArgParse v1.0.15
   (https://pypi.python.org/pypi/ConfArgParse)

   -  Good:

      -  extends argparse with support for config files parsed by
         ConfigParser
      -  clear documentation in README

   -  Bad:

      -  config file values are processed using
         ArgumentParser.set_defaults(..) which means "required" and
         "choices" are not handled as expected. For example, if you
         specify a required value in a config file, you still have to
         specify it again on the command line.
      -  doesn't work with Python 3 yet
      -  no unit tests, code not well documented

-  appsettings v0.5 (https://pypi.python.org/pypi/appsettings)

   -  Good:

      -  supports config file (yaml format) and env_var parsing
      -  supports config-file-only setting for specifying lists and
         dicts

   -  Bad:

      -  passes in config file and env settings via parse_args
         namespace param
      -  tests not finished and don't work with Python 3 (import
         StringIO)

-  argparse_config v0.5.1
   (https://pypi.python.org/pypi/argparse_config)

   -  Good:

      -  similar features to ConfArgParse v1.0.15

   -  Bad:

      -  doesn't work with Python 3 (error during pip install)

-  yconf v0.3.2 - (https://pypi.python.org/pypi/yconf) - features
   and interface not that great
-  hieropt v0.3 - (https://pypi.python.org/pypi/hieropt) - doesn't
   appear to be maintained, couldn't find documentation

-  configurati v0.2.3 - (https://pypi.python.org/pypi/configurati)

   -  Good:

      -  JSON, YAML, or Python configuration files
      -  handles rich data structures such as dictionaries
      -  can group configuration names into sections (like .ini files)

   -  Bad:

      -  doesn't work with Python 3
      -  2+ years since last release to PyPI
      -  apparently unmaintained


Design choices:

1. all options must be settable via command line. Having options that
   can only be set using config files or env. vars adds complexity to
   the API, and is not a useful enough feature since the developer can
   split up options into sections and call a section "config file keys",
   with command line args that are just "--" plus the config key.
2. config file and env. var settings should be processed by appending
   them to the command line (another benefit of #1). This is an
   easy-to-implement solution and implicitly takes care of checking that
   all "required" args are provied, etc., plus the behavior should be
   easy for users to understand.
3. configargparse shouldn't override argparse's
   convert_arg_line_to_args method so that all argparse unit tests
   can be run on configargparse.
4. in terms of what to allow for config file keys, the "dest" value of
   an option can't serve as a valid config key because many options can
   have the same dest. Instead, since multiple options can't use the
   same long arg (eg. "--long-arg-x"), let the config key be either
   "--long-arg-x" or "long-arg-x". This means the developer can allow
   only a subset of the command-line args to be specified via config
   file (eg. short args like -x would be excluded). Also, that way
   config keys are automatically documented whenever the command line
   args are documented in the help message.
5. don't force users to put config file settings in the right .ini
   [sections]. This doesn't have a clear benefit since all options are
   command-line settable, and so have a globally unique key anyway.
   Enforcing sections just makes things harder for the user and adds
   complexity to the implementation.
6. if necessary, config-file-only args can be added later by
   implementing a separate add method and using the namespace arg as in
   appsettings_v0.5

Relevant sites:

-  http://stackoverflow.com/questions/6133517/parse-config-file-environment-and-command-line-arguments-to-get-a-single-coll
-  http://tricksntweaks.blogspot.com/2013_05_01_archive.html
-  http://www.youtube.com/watch?v=vvCwqHgZJc8#t=35


.. |Travis CI Status for bw2/ConfigArgParse| image:: https://travis-ci.org/bw2/ConfigArgParse.svg?branch=master

  * [Cycler-0.10.0](http://github.com/matplotlib/cycler) cycler: composable cycles
=========================

Docs: http://matplotlib.org/cycler/

  * [Cython-0.28.5](http://cython.org/) The Cython language makes writing C extensions for the Python language as
easy as Python itself.  Cython is a source code translator based on Pyrex_,
but supports more cutting edge functionality and optimizations.

The Cython language is a superset of the Python language (almost all Python
code is also valid Cython code), but Cython additionally supports optional
static typing to natively call C functions, operate with C++ classes and
declare fast C types on variables and class attributes.  This allows the
compiler to generate very efficient C code from Cython code.

This makes Cython the ideal language for writing glue code for external
C/C++ libraries, and for fast C modules that speed up the execution of
Python code.

Note that for one-time builds, e.g. for CI/testing, on platforms that are not
covered by one of the wheel packages provided on PyPI, it is substantially faster
than a full source build to install an uncompiled (slower) version of Cython with::

    pip install Cython --install-option="--no-cython-compile"

.. _Pyrex: http://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/



  * [DCA-0.2.3](https://github.com/theislab/dca) 
  * [DataShape-0.5.2](http://datashape.readthedocs.org/en/latest/) =========
DataShape
=========

|Build Status| |PyPI| |Monthly Downloads|

DataShape is a language for describing data. It is an extension of the
NumPy dtype with an emphasis on cross language support.

History
-------

DataShape was originally developed by the Blaze project. The git history
of the blaze.datashape module has been preserved here but for more
complete history see the Blaze project.

Contributing
------------

Anyone wishing to discuss on DataShape should join the
`blaze-dev <https://groups.google.com/a/continuum.io/forum/#!forum/blaze-dev>`__
mailing list at: blaze-dev@continuum.io

License
-------

DataShape development is sponsored by Continuum Analytics.

Released under BSD license. See LICENSE for details.


.. |Build Status| image:: https://travis-ci.org/blaze/datashape.svg?branch=master
   :target: https://travis-ci.org/blaze/datashape

.. |PyPI| image:: https://img.shields.io/pypi/v/datashape.svg
   :target: https://pypi.python.org/pypi/DataShape

.. |Monthly Downloads| image:: https://img.shields.io/pypi/dm/datashape.svg
   :target: https://pypi.python.org/pypi/DataShape
  * [DendroPy-4.4.0](http://packages.python.org/DendroPy/) .. image:: https://raw.githubusercontent.com/jeetsukumaran/DendroPy/DendroPy4/doc/source/_static/dendropy_logo.png
   :align: right
   :alt: DendroPy

DendroPy is a Python library for phylogenetic computing.
It provides classes and functions for the simulation, processing, and
manipulation of phylogenetic trees and character matrices, and supports the
reading and writing of phylogenetic data in a range of formats, such as NEXUS,
NEWICK, NeXML, Phylip, FASTA, etc.  Application scripts for performing some
useful phylogenetic operations, such as data conversion and tree posterior
distribution summarization, are also distributed and installed as part of the
libary.  DendroPy can thus function as a stand-alone library for phylogenetics,
a component of more complex multi-library phyloinformatic pipelines, or as a
scripting "glue" that assembles and drives such pipelines.

The primary home page for DendroPy, with detailed tutorials and documentation, is at:

    http://dendropy.org/

DendroPy is also hosted in the official Python repository:

    http://packages.python.org/DendroPy/

Requirements and Installation
=============================

DendroPy 4.x runs under Python 3 (all versions > 3.1) and Python 2 (Python 2.7 only).

You can install DendroPy by running::

    $ sudo pip install dendropy

More information is available here:

    http://dendropy.org/downloading.html

Documentation
=============

Full documentation is available here:

    http://dendropy.org/

This includes:

    -   `A comprehensive "getting started" primer <http://dendropy.org/primer/index.html>`_ .
    -   `API documentation <http://dendropy.org/library/index.html>`_ .
    -   `Descriptions of data formats supported for reading/writing <http://dendropy.org/schemas/index.html>`_ .

and more.

License and Warranty
====================

Please see the file "LICENSE.rst" for details.

Current Release
===============

The current release of DendroPy is version 4.4.0.


  * [Deprecated-1.2.6](https://github.com/tantale/deprecated) 
Deprecated Library
------------------

Deprecated is Easy to Use
`````````````````````````

If you need to mark a function or a method as deprecated,
you can use the ``@deprecated`` decorator:

Save in a hello.py:

.. code:: python

    from deprecated import deprecated


    @deprecated(version='1.2.1', reason="You should use another function")
    def some_old_function(x, y):
        return x + y


    class SomeClass(object):
        @deprecated(version='1.3.0', reason="This method is deprecated")
        def some_old_method(self, x, y):
            return x + y


    some_old_function(12, 34)
    obj = SomeClass()
    obj.some_old_method(5, 8)


And Easy to Setup
`````````````````

And run it:

.. code:: bash

    $ pip install Deprecated
    $ python hello.py
    hello.py:15: DeprecationWarning: Call to deprecated function (or staticmethod) some_old_function.
    (You should use another function) -- Deprecated since version 1.2.0.
      some_old_function(12, 34)
    hello.py:17: DeprecationWarning: Call to deprecated method some_old_method.
    (This method is deprecated) -- Deprecated since version 1.3.0.
      obj.some_old_method(5, 8)


You can document your code
``````````````````````````

Have you ever wonder how to document that some functions, classes, methods, etc. are deprecated?
This is now possible with the integrated Sphinx directives:

For instance, in hello_sphinx.py:

.. code:: python

    from deprecated.sphinx import deprecated
    from deprecated.sphinx import versionadded
    from deprecated.sphinx import versionchanged


    @versionadded(version='1.0', reason="This function is new")
    def function_one():
        '''This is the function one'''


    @versionchanged(version='1.0', reason="This function is modified")
    def function_two():
        '''This is the function two'''


    @deprecated(version='1.0', reason="This function will be removed soon")
    def function_three():
        '''This is the function three'''


    function_one()
    function_two()
    function_three()  # warns

    help(function_one)
    help(function_two)
    help(function_three)


The result it immediate
```````````````````````

Run it:

.. code:: bash

    $ python hello_sphinx.py

    hello_sphinx.py:23: DeprecationWarning: Call to deprecated function (or staticmethod) function_three.
    (This function will be removed soon) -- Deprecated since version 1.0.
      function_three()  # warns

    Help on function function_one in module __main__:

    function_one()
        This is the function one

        .. versionadded:: 1.0
           This function is new

    Help on function function_two in module __main__:

    function_two()
        This is the function two

        .. versionchanged:: 1.0
           This function is modified

    Help on function function_three in module __main__:

    function_three()
        This is the function three

        .. deprecated:: 1.0
           This function will be removed soon


Links
`````

* `Python package index (PyPi) <https://pypi.python.org/pypi/deprecated>`_
* `GitHub website <https://github.com/tantale/deprecated>`_
* `Read The Docs <https://readthedocs.org/projects/deprecated>`_
* `EBook on Lulu.com <http://www.lulu.com/commerce/index.php?fBuyContent=21305117>`_
* `StackOverFlow Q&A <https://stackoverflow.com/a/40301488/1513933>`_
* `Development version
  <https://github.com/tantale/deprecated/zipball/master#egg=Deprecated-dev>`_




  * [Django-2.2.4](https://www.djangoproject.com/) Django is a high-level Python Web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.

All documentation is in the "``docs``" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

* First, read ``docs/intro/install.txt`` for instructions on installing Django.

* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,
  ``docs/intro/tutorial02.txt``, etc.).

* If you want to set up an actual deployment server, read
  ``docs/howto/deployment/index.txt`` for instructions.

* You'll probably want to read through the topical guides (in ``docs/topics``)
  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific
  problems, and check out the reference (``docs/ref``) for gory details.

* See ``docs/README`` for instructions on building an HTML version of the docs.

Docs are updated rigorously. If you find any problems in the docs, or think
they should be clarified in any way, please take 30 seconds to fill out a
ticket here: https://code.djangoproject.com/newticket

To get more help:

* Join the ``#django`` channel on irc.freenode.net. Lots of helpful people hang
  out there. See https://en.wikipedia.org/wiki/Wikipedia:IRC/Tutorial if you're
  new to IRC.

* Join the django-users mailing list, or read the archives, at
  https://groups.google.com/group/django-users.

To contribute to Django:

* Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for
  information about getting involved.

To run Django's test suite:

* Follow the instructions in the "Unit tests" section of
  ``docs/internals/contributing/writing-code/unit-tests.txt``, published online at
  https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests



  * [Flask-1.1.1](https://palletsprojects.com/p/flask/) Flask
=====

Flask is a lightweight `WSGI`_ web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around `Werkzeug`_
and `Jinja`_ and has become one of the most popular Python web
application frameworks.

Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    pip install -U Flask


A Simple Example
----------------

.. code-block:: python

    from flask import Flask

    app = Flask(__name__)

    @app.route("/")
    def hello():
        return "Hello, World!"

.. code-block:: text

    $ env FLASK_APP=hello.py flask run
     * Serving Flask app "hello"
     * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)


Contributing
------------

For guidance on setting up a development environment and how to make a
contribution to Flask, see the `contributing guidelines`_.

.. _contributing guidelines: https://github.com/pallets/flask/blob/master/CONTRIBUTING.rst


Donate
------

The Pallets organization develops and supports Flask and the libraries
it uses. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, `please
donate today`_.

.. _please donate today: https://psfmember.org/civicrm/contribute/transact?reset=1&id=20


Links
-----

* Website: https://palletsprojects.com/p/flask/
* Documentation: https://flask.palletsprojects.com/
* Releases: https://pypi.org/project/Flask/
* Code: https://github.com/pallets/flask
* Issue tracker: https://github.com/pallets/flask/issues
* Test status: https://dev.azure.com/pallets/flask/_build
* Official chat: https://discord.gg/t6rrQZH

.. _WSGI: https://wsgi.readthedocs.io
.. _Werkzeug: https://www.palletsprojects.com/p/werkzeug/
.. _Jinja: https://www.palletsprojects.com/p/jinja/
.. _pip: https://pip.pypa.io/en/stable/quickstart/



  * [Flask-Bootstrap-3.3.7.1](http://github.com/mbr/flask-bootstrap) ===============
Flask-Bootstrap
===============

.. image:: https://travis-ci.org/mbr/flask-bootstrap.png?branch=master
   :target: https://travis-ci.org/mbr/flask-bootstrap

Flask-Bootstrap packages `Bootstrap
<http://getbootstrap.com>`_ into an extension that mostly consists
of a blueprint named 'bootstrap'. It can also create links to serve Bootstrap
from a CDN and works with no boilerplate code in your application.

Usage
-----

Here is an example::

  from flask_bootstrap import Bootstrap

  [...]

  Bootstrap(app)

This makes some new templates available, containing blank pages that include all
bootstrap resources, and have predefined blocks where you can put your content.

As of version 3, Flask-Bootstrap has a `proper documentation
<http://pythonhosted.org /Flask-Bootstrap>`_, which you can check for more
details.
  * [Flask-Cors-3.0.8](https://github.com/corydolphin/flask-cors) Flask-CORS
==========

|Build Status| |Latest Version| |Supported Python versions|
|License|

A Flask extension for handling Cross Origin Resource Sharing (CORS),
making cross-origin AJAX possible.

This package has a simple philosophy, when you want to enable CORS, you
wish to enable it for all use cases on a domain. This means no mucking
around with different allowed headers, methods, etc. By default,
submission of cookies across domains is disabled due to the security
implications, please see the documentation for how to enable
credential'ed requests, and please make sure you add some sort of
`CSRF <http://en.wikipedia.org/wiki/Cross-site_request_forgery>`__
protection before doing so!

Installation
------------

Install the extension with using pip, or easy\_install.

.. code:: bash

    $ pip install -U flask-cors

Usage
-----

This package exposes a Flask extension which by default enables CORS support on all routes, for all origins and methods. It allows parameterization of all CORS headers on a per-resource level. The package also contains a decorator, for those who prefer this approach.

Simple Usage
~~~~~~~~~~~~

In the simplest case, initialize the Flask-Cors extension with default
arguments in order to allow CORS for all domains on all routes. See the
full list of options in the `documentation <https://flask-cors.corydolphin.com/en/latest/api.html#extension>`__.

.. code:: python


    from flask import Flask
    from flask_cors import CORS

    app = Flask(__name__)
    CORS(app)

    @app.route("/")
    def helloWorld():
      return "Hello, cross-origin-world!"

Resource specific CORS
^^^^^^^^^^^^^^^^^^^^^^

Alternatively, you can specify CORS options on a resource and origin
level of granularity by passing a dictionary as the `resources` option,
mapping paths to a set of options. See the
full list of options in the `documentation <https://flask-cors.corydolphin.com/en/latest/api.html#extension>`__.

.. code:: python

    app = Flask(__name__)
    cors = CORS(app, resources={r"/api/*": {"origins": "*"}})

    @app.route("/api/v1/users")
    def list_users():
      return "user example"

Route specific CORS via decorator
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This extension also exposes a simple decorator to decorate flask routes
with. Simply add ``@cross_origin()`` below a call to Flask's
``@app.route(..)`` to allow CORS on a given route. See the
full list of options in the `decorator documentation <https://flask-cors.corydolphin.com/en/latest/api.html#decorator>`__.

.. code:: python

    @app.route("/")
    @cross_origin()
    def helloWorld():
      return "Hello, cross-origin-world!"

Documentation
-------------

For a full list of options, please see the full
`documentation <https://flask-cors.corydolphin.com/en/latest/api.html>`__

Troubleshooting
---------------

If things aren't working as you expect, enable logging to help understand
what is going on under the hood, and why.

.. code:: python

    logging.getLogger('flask_cors').level = logging.DEBUG


Tests
-----

A simple set of tests is included in ``test/``. To run, install nose,
and simply invoke ``nosetests`` or ``python setup.py test`` to exercise
the tests.

Contributing
------------

Questions, comments or improvements? Please create an issue on
`Github <https://github.com/corydolphin/flask-cors>`__, tweet at
`@corydolphin <https://twitter.com/corydolphin>`__ or send me an email.
I do my best to include every contribution proposed in any way that I
can.

Credits
-------

This Flask extension is based upon the `Decorator for the HTTP Access
Control <http://flask.pocoo.org/snippets/56/>`__ written by Armin
Ronacher.

.. |Build Status| image:: https://api.travis-ci.org/corydolphin/flask-cors.svg?branch=master
   :target: https://travis-ci.org/corydolphin/flask-cors
.. |Latest Version| image:: https://img.shields.io/pypi/v/Flask-Cors.svg
   :target: https://pypi.python.org/pypi/Flask-Cors/
.. |Supported Python versions| image:: https://img.shields.io/pypi/pyversions/Flask-Cors.svg
   :target: https://img.shields.io/pypi/pyversions/Flask-Cors.svg
.. |License| image:: http://img.shields.io/:license-mit-blue.svg
   :target: https://pypi.python.org/pypi/Flask-Cors/



  * [Flask-Debug-0.4.3](http://github.com/mbr/flask-debug) Flask-Debug
===========

Flask-Debug is an extension for Flask_ that displays various debugging insights
during development.

.. code-block:: python

    from flask import Flask
    from flask_debug import Debug

    app = Flask(__name__)
    Debug(app)
    app.run(debug=True)


It can be manually added but also (using Flask-Appconfig_) be
automatically instantiated only during development and invisible in production.

Opening http://localhost:5000/_debug will show some information about the
application, such as a list of registered views, url maps or configuration
values.

See the `documentation <http://pythonhosted.org/Flask-Debug>`_ for more
details.

.. _Flask-Appconfig: https://pypi.python.org/pypi/flask-appconfig
.. _Flask: http://flask.pocoo.org
  * [Flask-Migrate-2.5.2](http://github.com/miguelgrinberg/flask-migrate/) 
Flask-Migrate
--------------

SQLAlchemy database migrations for Flask applications using Alembic.



  * [Flask-SQLAlchemy-2.4.0](https://github.com/pallets/flask-sqlalchemy) Flask-SQLAlchemy
================

Flask-SQLAlchemy is an extension for `Flask`_ that adds support for
`SQLAlchemy`_ to your application. It aims to simplify using SQLAlchemy
with Flask by providing useful defaults and extra helpers that make it
easier to accomplish common tasks.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

  $ pip install -U Flask-SQLAlchemy


A Simple Example
----------------

.. code-block:: python

    from flask import Flask
    from flask_sqlalchemy import SQLAlchemy

    app = Flask(__name__)
    app.config["SQLALCHEMY_DATABASE_URI"] = "sqlite:///example.sqlite"
    db = SQLAlchemy(app)


    class User(db.Model):
        id = db.Column(db.Integer, primary_key=True)
        username = db.Column(db.String, unique=True, nullable=False)
        email = db.Column(db.String, unique=True, nullable=False)


    db.session.add(User(name="Flask", email="example@example.com"))
    db.session.commit()

    users = User.query.all()


Links
-----

-   Documentation: https://flask-sqlalchemy.palletsprojects.com/
-   Releases: https://pypi.org/project/Flask-SQLAlchemy/
-   Code: https://github.com/pallets/flask-sqlalchemy
-   Issue tracker: https://github.com/pallets/flask-sqlalchemy/issues
-   Test status: https://travis-ci.org/pallets/flask-sqlalchemy
-   Test coverage: https://codecov.io/gh/pallets/flask-sqlalchemy

.. _Flask: https://palletsprojects.com/p/flask/
.. _SQLAlchemy: https://www.sqlalchemy.org
.. _pip: https://pip.pypa.io/en/stable/quickstart/



  * [Flask-Script-2.0.6](http://github.com/smurfix/flask-script) 
Flask-Script
--------------

Flask support for writing external scripts.

Links
`````

* `documentation <http://flask-script.readthedocs.org>`_



  * [Flask-WTF-0.14.2](https://github.com/lepture/flask-wtf) Flask-WTF
=========

.. image:: https://travis-ci.org/lepture/flask-wtf.svg?branch=master
   :target: https://travis-ci.org/lepture/flask-wtf
   :alt: Travis CI Status
.. image:: https://coveralls.io/repos/lepture/flask-wtf/badge.svg?branch=master
   :target: https://coveralls.io/r/lepture/flask-wtf
   :alt: Coverage Status

Simple integration of Flask and WTForms, including CSRF, file upload,
and reCAPTCHA.

Links
-----

* `Documentation <https://flask-wtf.readthedocs.io>`_
* `PyPI <https://pypi.python.org/pypi/Flask-WTF>`_
* `GitHub <https://github.com/lepture/flask-wtf>`_



  * [Genshi-0.7.3](http://genshi.edgewall.org/) Genshi is a Python library that provides an integrated set of
components for parsing, generating, and processing HTML, XML or
other textual content for output generation on the web. The major
feature is a template language, which is heavily inspired by Kid.


  * [GitPython-2.1.13](https://github.com/gitpython-developers/GitPython) GitPython is a python library used to interact with Git repositories



  * [HTSeq-0.11.2](https://github.com/simon-anders/htseq) A framework to process and analyze data from high-throughput sequencing
(HTS) assays.

Development: https://github.com/simon-anders/htseq

Documentation: http://htseq.readthedocs.io


  * [HeapDict-1.0.0](http://stutzbachenterprises.com/) heapdict: a heap with decreased-key and increase-key operations

heapdict implements the MutableMapping ABC, meaning it works pretty
much like a regular Python dict.  Its designed to be used as a
priority queue, where items are added and consumed.

  * [Jinja2-2.10.1](https://palletsprojects.com/p/jinja/) Jinja

Jinja is a fast, expressive, extensible templating engine. Special
placeholders in the template allow writing code similar to Python
syntax. 

  * [Kivy-1.11.1](http://kivy.org) Kivy
====

<img align="right" height="256" src="https://raw.githubusercontent.com/kivy/kivy/master/kivy/data/logo/kivy-icon-256.png"/>

Innovative user interfaces made easy.

Kivy is an open source, cross-platform [Python](https://www.python.org)
framework for the development of applications that make use of innovative,
multi-touch user interfaces.
The aim is to allow for quick and easy interaction design and rapid prototyping
whilst making your code reusable and deployable.

  * [Mako-1.1.0](https://www.makotemplates.org/) Mako Templates for Python

Mako is a template library written in Python. It provides a familiar, non-XML 
syntax which compiles into Python modules for maximum performance. Makos 
syntax and API borrows from the best ideas of many others, including Django
templates, Cheetah, Myghty, and Genshi. Conceptually, Mako is an embedded 
Python (i.e. Python Server Page) language, which refines the familiar ideas
of componentized layout and inheritance to produce one of the most 
straightforward and flexible models available, while also maintaining close 
ties to Python calling and scoping semantics.

  * [MonkeyType-19.5.0](https://github.com/instagram/MonkeyType) MonkeyType

MonkeyType collects runtime types of function arguments and return values, and
can automatically generate stub files or even add draft type annotations
directly to your Python code based on the types collected at runtime.

  * [Morfessor-2.0.6](http://morpho.aalto.fi) 
  * [NanoComp-1.9.0](https://github.com/wdecoster/NanoComp) NanoComp


Compare multiple runs of long read sequencing data and alignments. Creates violin plots or box plots of length, quality and percent identity and creates dynamic, overlaying read length histograms and a cumulative yield plot.

  * [NanoLyse-1.1.0](https://github.com/wdecoster/nanolyse)
#### NanoLyse
Remove reads mapping to the lambda phage genome from a fastq file.  

  * [NanoPlot-1.26.3](https://github.com/wdecoster/NanoPlot) # NanoPlot

  * [NanoStat-1.1.2](https://github.com/wdecoster/nanostat) # NanoStat

  * [Nuitka-0.6.5](http://nuitka.net) 
  * [Parsley-1.3](http://launchpad.net/parsley) .. -*- mode: rst -*-

  * [Paste-3.1.0](https://pythonpaste.readthedocs.io/) 
Paste is in maintenance mode and recently moved from bitbucket to github.
Patches are accepted to keep it on life support, but for the most part, please
consider using other options.

  internally, in ``paste.recursive``

Web Application
---------------

* Run CGI programs as WSGI applications in ``paste.cgiapp``

* Traverse files and load WSGI applications from ``.py`` files (or
  static files), in ``paste.urlparser``

* Serve static directories of files, also in ``paste.urlparser``; also
  in that module serving from Egg resources using ``pkg_resources``.

Tools
-----

* Catch HTTP-related exceptions (e.g., ``HTTPNotFound``) and turn them
  into proper responses in ``paste.httpexceptions``

* Several authentication techniques, including HTTP (Basic and
  Digest), signed cookies, and CAS single-signon, in the
  ``paste.auth`` package.

* Create sessions in ``paste.session`` and ``paste.flup_session``

* Gzip responses in ``paste.gzip``

* A wide variety of routines for manipulating WSGI requests and
  producing responses, in ``paste.request``, ``paste.response`` and
  ``paste.wsgilib``

Debugging Filters
-----------------

* Catch (optionally email) errors with extended tracebacks (using
  Zope/ZPT conventions) in ``paste.exceptions``

* Catch errors presenting a `cgitb
  <http://docs.python.org/2/library/cgitb.html>`_-based
  output, in ``paste.cgitb_catcher``.

* Profile each request and append profiling information to the HTML,
  in ``paste.debug.profile``

* Capture ``print`` output and present it in the browser for
  debugging, in ``paste.debug.prints``

* Validate all HTML output from applications using the `WDG Validator
  <http://www.htmlhelp.com/tools/validator/>`_, appending any errors
  or warnings to the page, in ``paste.debug.wdg_validator``

Other Tools
-----------

* A file monitor to allow restarting the server when files have been
  updated (for automatic restarting when editing code) in
  ``paste.reloader``

* A class for generating and traversing URLs, and creating associated
  HTML code, in ``paste.url``

The official development repo is at https://github.com/cdent/paste.



  * [PasteDeploy-2.0.1](https://pylonsproject.org/) This tool provides code to load WSGI applications and servers from
URIs. These URIs can refer to Python eggs for INI-style configuration
files.  `Paste Script <https://github.com/cdent/pastescript>`_ provides
commands to serve applications based on this configuration file.

The latest version is available on `GitHub
<https://github.com/Pylons/pastedeploy/>`_ (or download a wheel or tarball from
`PyPI <https://pypi.org/project/PasteDeploy/#files>`_).

For the latest changes see the `news file
<https://docs.pylonsproject.org/projects/pastedeploy/en/latest/news.html>`_.



  * [Pillow-6.1.0](http://python-pillow.org) Pillow
======

Python Imaging Library (Fork)
-----------------------------

Pillow is the friendly PIL fork by `Alex Clark and Contributors <https://github.com/python-pillow/Pillow/graphs/contributors>`_. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. As of 2019, Pillow development is `supported by Tidelift <https://tidelift.com/subscription/pkg/pypi-pillow>`_.

.. start-badges

.. list-table::
    :stub-columns: 1

    * - docs
      - |docs|
    * - tests
      - |linux| |macos| |windows| |coverage|
    * - package
      - |zenodo| |tidelift| |version| |downloads|
    * - social
      - |gitter| |twitter|

.. end-badges

More Information
----------------

- `Documentation <https://pillow.readthedocs.io/>`_

  - `Installation <https://pillow.readthedocs.io/en/latest/installation.html>`_
  - `Handbook <https://pillow.readthedocs.io/en/latest/handbook/index.html>`_

- `Contribute <https://github.com/python-pillow/Pillow/blob/master/.github/CONTRIBUTING.md>`_

  - `Issues <https://github.com/python-pillow/Pillow/issues>`_
  - `Pull requests <https://github.com/python-pillow/Pillow/pulls>`_

- `Changelog <https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst>`_

  - `Pre-fork <https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst#pre-fork>`_

Report a Vulnerability
----------------------

To report a security vulnerability, please follow the procedure described in the `Tidelift security policy <https://tidelift.com/docs/security>`_.

.. |docs| image:: https://readthedocs.org/projects/pillow/badge/?version=latest
   :target: https://pillow.readthedocs.io/?badge=latest
   :alt: Documentation Status

.. |linux| image:: https://img.shields.io/travis/python-pillow/Pillow/master.svg?label=Linux%20build
   :target: https://travis-ci.org/python-pillow/Pillow
   :alt: Travis CI build status (Linux)

.. |macos| image:: https://img.shields.io/travis/python-pillow/pillow-wheels/master.svg?label=macOS%20build
   :target: https://travis-ci.org/python-pillow/pillow-wheels
   :alt: Travis CI build status (macOS)

.. |windows| image:: https://img.shields.io/appveyor/ci/python-pillow/Pillow/master.svg?label=Windows%20build
   :target: https://ci.appveyor.com/project/python-pillow/Pillow
   :alt: AppVeyor CI build status (Windows)

.. |coverage| image:: https://codecov.io/gh/python-pillow/Pillow/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/python-pillow/Pillow
   :alt: Code coverage

.. |zenodo| image:: https://zenodo.org/badge/17549/python-pillow/Pillow.svg
   :target: https://zenodo.org/badge/latestdoi/17549/python-pillow/Pillow

.. |tidelift| image:: https://tidelift.com/badges/package/pypi/Pillow?style=flat
   :target: https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=referral&utm_campaign=readme

.. |version| image:: https://img.shields.io/pypi/v/pillow.svg
   :target: https://pypi.org/project/Pillow/
   :alt: Latest PyPI version

.. |downloads| image:: https://img.shields.io/pypi/dm/pillow.svg
   :target: https://pypi.org/project/Pillow/
   :alt: Number of PyPI downloads

.. |gitter| image:: https://badges.gitter.im/python-pillow/Pillow.svg
   :target: https://gitter.im/python-pillow/Pillow?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
   :alt: Join the chat at https://gitter.im/python-pillow/Pillow

.. |twitter| image:: https://img.shields.io/badge/tweet-on%20Twitter-00aced.svg
   :target: https://twitter.com/PythonPillow
   :alt: Follow on https://twitter.com/PythonPillow



  * [PrettyTable-0.7.2](http://code.google.com/p/prettytable/) PrettyTable is a simple Python library designed to make it quick and easy to
represent tabular data in visually appealing ASCII tables. It was inspired by
the ASCII tables used in the PostgreSQL shell psql. PrettyTable allows for
selection of which columns are to be printed, independent alignment of columns
(left or right justified or centred) and printing of "sub-tables" by specifying
a row range.
  * [PyChef-0.3.0](http://github.com/coderanger/pychef) PyChef
======

.. image:: https://secure.travis-ci.org/coderanger/pychef.png?branch=master
    :target: http://travis-ci.org/coderanger/pychef

A Python API for interacting with a Chef server.

Example
-------

::

    from chef import autoconfigure, Node
    
    api = autoconfigure()
    n = Node('web1')
    print n['fqdn']
    n['myapp']['version'] = '1.0'
    n.save()

Further Reading
---------------

For more information check out http://pychef.readthedocs.org/en/latest/index.html
  * PyClone 0.13.1
  * [PyDP-0.2.4](http://www.yanjingang.com/docs/pydp/dp.html) # pydp
python常用基础库

# 安装
    pip install pydp

# 使用
[说明文档](http://www.yanjingang.com/docs/pydp/dp.html)



  * [PyGithub-1.43.8](http://pygithub.readthedocs.io/en/latest/) (Very short) Tutorial
=====================

First create a Github instance::

    from github import Github

    # using username and password
    g = Github("user", "password")

    # or using an access token
    g = Github("access_token")

Then play with your Github objects::

    for repo in g.get_user().get_repos():
        print(repo.name)
        repo.edit(has_wiki=False)

Reference documentation
=======================

See http://pygithub.readthedocs.io/en/latest/
  * [PyGreSQL-5.1](http://www.pygresql.org) PyGreSQL is an open-source Python module that interfaces to a
PostgreSQL database. It embeds the PostgreSQL query library to allow
easy use of the powerful PostgreSQL features from a Python script.


  * [PyICU-2.3.1](https://github.com/ovalhub/pyicu) # README file for PyICU

## Welcome

Welcome to PyICU, a Python extension wrapping the ICU C++ libraries.

ICU stands for "International Components for Unicode".
These are the i18n libraries of the Unicode Consortium.
They implement much of the Unicode Standard,
many of its companion Unicode Technical Standards,
and much of Unicode CLDR.

The PyICU source code is hosted on GitHub at https://github.com/ovalhub/pyicu.

The ICU homepage is http://site.icu-project.org/

See also the CLDR homepage at http://cldr.unicode.org/

## Building PyICU

Before building PyICU the ICU libraries must be built and installed. Refer
to each system's instructions for more information.

PyICU is built with distutils or setuptools:

   - verify that the icu-config program is available or that the ``INCLUDES``,
     ``LFLAGS``, ``CFLAGS`` and ``LIBRARIES`` dictionaries in ``setup.py``
     contain correct values for your platform. Starting with ICU 60, -std=c++11
     must appear in your CFLAGS.
   - ``python setup.py build``
   - ``sudo python setup.py install``


## Running PyICU

  - Mac OS X
    Make sure that ``DYLD_LIBRARY_PATH`` contains paths to the directory(ies)
    containing the ICU libs.

  - Linux & Solaris
    Make sure that ``LD_LIBRARY_PATH`` contains paths to the directory(ies)
    containing the ICU libs or that you added the corresponding ``-rpath``
    argument to ``LFLAGS``.

  - Windows
    Make sure that ``PATH`` contains paths to the directory(ies)
    containing the ICU DLLs.


## What's available

See the ``CHANGES`` file for an up to date log of changes and additions.


## API Documentation

There is no API documentation for PyICU. The API for ICU is documented at
http://icu-project.org/apiref/icu4c/ and the following patterns can be
used to translate from the C++ APIs to the corresponding Python APIs.

### strings

The ICU string type, ``UnicodeString``, is a type pointing at a mutable
array of ``UChar`` Unicode 16-bit wide characters. The Python unicode type
is an immutable string of 16-bit or 32-bit wide Unicode characters.

Because of these differences, ``UnicodeString`` and Python's ``unicode``
type are not merged into the same type when crossing the C++ boundary.
ICU APIs taking ``UnicodeString`` arguments have been overloaded to also
accept Python str or unicode type arguments. In the case of ``str``
objects, the ``utf-8`` encoding is assumed when converting them to
``UnicodeString`` objects.

To convert a Python ``str`` encoded in an encoding other than ``utf-8`` to
an ICU ``UnicodeString`` use the ``UnicodeString(str, encodingName)``
constructor.

ICU's C++ APIs accept and return ``UnicodeString`` arguments in several
ways: by value, by pointer or by reference.
When an ICU C++ API is documented to accept a ``UnicodeString`` reference
parameter, it is safe to assume that there are several corresponding
PyICU python APIs making it accessible in simpler ways:

For example, the
``'UnicodeString &Locale::getDisplayName(UnicodeString &)'`` API,
documented at
http://icu-project.org/apiref/icu4c/classLocale.html
can be invoked from Python in several ways:

1. The ICU way

        >>> from icu import UnicodeString, Locale
        >>> locale = Locale('pt_BR')
        >>> string = UnicodeString()
        >>> name = locale.getDisplayName(string)
        >>> name
        <UnicodeString: Portuguese (Brazil)>
        >>> name is string
        True                  <-- string arg was returned, modified in place

2. The Python way

        >>> from icu import Locale
        >>> locale = Locale('pt_BR')
        >>> name = locale.getDisplayName()
        >>> name
        u'Portuguese (Brazil)'

    A ``UnicodeString`` object was allocated and converted to a Python
    ``unicode`` object.

A UnicodeString can be coerced to a Python unicode string with Python's
``unicode()`` constructor. The usual ``len()``, ``str()``, comparison,
``[]`` and ``[:]`` operators are all available, with the additional
twists that slicing is not read-only and that ``+=`` is also available
since a UnicodeString is mutable. For example:

    >>> name = locale.getDisplayName()
    u'Portuguese (Brazil)'
    >>> name = UnicodeString(name)
    >>> name
    <UnicodeString: Portuguese (Brazil)>
    >>> unicode(name)
    u'Portuguese (Brazil)'
    >>> len(name)
    19
    >>> str(name)           <-- works when chars fit with default encoding
    'Portuguese (Brazil)'
    >>> name[3]
    u't'
    >>> name[12:18]
    <UnicodeString: Brazil>
    >>> name[12:18] = 'the country of Brasil'
    >>> name
    <UnicodeString: Portuguese (the country of Brasil)>
    >>> name += ' oh joy'
    >>> name
    <UnicodeString: Portuguese (the country of Brasil) oh joy>

### error reporting

The C++ ICU library does not use C++ exceptions to report errors. ICU
C++ APIs return errors via a ``UErrorCode`` reference argument. All such
APIs are wrapped by Python APIs that omit this argument and throw an
``ICUError`` Python exception instead. The same is true for ICU APIs
taking both a ``ParseError`` and a ``UErrorCode``, they are both to be
omitted.

For example, the ``'UnicodeString &DateFormat::format(const Formattable &,
UnicodeString &, UErrorCode &)'`` API, documented at
http://icu-project.org/apiref/icu4c/classDateFormat.html
is invoked from Python with:

    >>> from icu import DateFormat, Formattable
    >>> df = DateFormat.createInstance()
    >>> df
    <SimpleDateFormat: M/d/yy h:mm a>
    >>> f = Formattable(940284258.0, Formattable.kIsDate)
    >>> df.format(f)
    u'10/18/99 3:04 PM'

Of course, the simpler ``'UnicodeString &DateFormat::format(UDate,
UnicodeString &)'`` documented here:
http://icu-project.org/apiref/icu4c/classDateFormat.html
can be used too:

    >>> from icu import DateFormat
    >>> df = DateFormat.createInstance()
    >>> df
    <SimpleDateFormat: M/d/yy h:mm a>
    >>> df.format(940284258.0)
    u'10/18/99 3:04 PM'

### dates

ICU uses a double floating point type called ``UDate`` that represents the
number of milliseconds elapsed since 1970-jan-01 UTC for dates.

In Python, the value returned by the ``time`` module's ``time()``
function is the number of seconds since 1970-jan-01 UTC. Because of this
difference, floating point values are multiplied by 1000 when passed to
APIs taking ``UDate`` and divided by 1000 when returned as ``UDate``.

Python's ``datetime`` objects, with or without timezone information, can
also be used with APIs taking ``UDate`` arguments. The ``datetime``
objects get converted to ``UDate`` when crossing into the C++ layer.

### arrays

Many ICU API take array arguments. A list of elements of the array
element types is to be passed from Python.

### StringEnumeration

An ICU ``StringEnumeration`` has three ``next`` methods: ``next()`` which
returns a ``str`` objects, ``unext()`` which returns ``unicode`` objects
and ``snext()`` which returns ``UnicodeString`` objects.
Any of these methods can be used as an iterator, using the Python
built-in ``iter`` function.

For example, let ``e`` be a ``StringEnumeration`` instance::

```python
[s for s in e] is a list of 'str' objects
[s for s in iter(e.unext, None)] is a list of 'unicode' objects
[s for s in iter(e.snext, None)] is a list of 'UnicodeString' objects
```

### timezones

The ICU ``TimeZone`` type may be wrapped with an ``ICUtzinfo`` type for
usage with Python's ``datetime`` type. For example::

```python
tz = ICUtzinfo(TimeZone.createTimeZone('US/Mountain'))
datetime.now(tz)
```

or, even simpler::

```python
tz = ICUtzinfo.getInstance('Pacific/Fiji')
datetime.now(tz)
```

To get the default time zone use::

```python
defaultTZ = ICUtzinfo.getDefault()
```

To get the time zone's id, use the ``tzid`` attribute or coerce the time
zone to a string::

```python
ICUtzinfo.getInstance('Pacific/Fiji').tzid -> 'Pacific/Fiji'
str(ICUtzinfo.getInstance('Pacific/Fiji')) -> 'Pacific/Fiji'
```

  * [PyInstaller-3.5](http://www.pyinstaller.org) PyInstaller Overview
====================

PyInstaller bundles a Python application and all its dependencies into a single
package. The user can run the packaged app without installing a Python
interpreter or any modules.


**Help keeping PyInstaller alive:**
Maintaining PyInstaller is a huge amount of work.
PyInstaller development can only continue
if users and companies provide sustainable funding. See
http://www.pyinstaller.org/funding.html for how to support PyInstaller.


:Documentation: https://pyinstaller.readthedocs.io/
:Website:       http://www.pyinstaller.org/
:Code:          https://github.com/pyinstaller/pyinstaller
:Donate:        | https://www.bountysource.com/teams/pyinstaller
                | Bitcoin: 1JUFjawzWDR7Tc8z9TKXstVFdjkDY9FbtK
                | `more ways to donate … <http://www.pyinstaller.org/donate.html>`_


PyInstaller reads a Python script written by you. It analyzes your code
to discover every other module and library your script needs in order to
execute. Then it collects copies of all those files -- including the active
Python interpreter! -- and puts them with your script in a single folder, or
optionally in a single executable file.


PyInstaller is tested against Windows, Mac OS X, and GNU/Linux.
However, it is not a cross-compiler:
to make a Windows app you run PyInstaller in Windows; to make
a GNU/Linux app you run it in GNU/Linux, etc.
PyInstaller has been used successfully
with AIX, Solaris, and FreeBSD, but is not tested against them.


Main Advantages
---------------

- Works out-of-the-box with any Python version 2.7 / 3.4-3.7
  (although Python 3.4 is not longer tested by the test-suite).
- Fully multi-platform, and uses the OS support to load the dynamic libraries,
  thus ensuring full compatibility.
- Correctly bundles the major Python packages such as numpy, PyQt4, PyQt5,
  PySide, Django, wxPython, matplotlib and others out-of-the-box.
- Compatible with many 3rd-party packages out-of-the-box. (All the required
  tricks to make external packages work are already integrated.)
- Libraries like PyQt5, PyQt4, PySide, wxPython, matplotlib or Django are fully
  supported, without having to handle plugins or external data files manually.
- Working code signing on OS X.
- Bundles MS Visual C++ DLLs on Windows.


Installation
------------

PyInstaller is available on PyPI. You can install it through `pip`::

      pip install pyinstaller


Requirements and Tested Platforms
------------------------------------

- Python: 

 - 2.7 or 3.4-3.7
   (although Python 3.4 is not longer tested by the test-suite)
 - PyCrypto_ 2.4+ (only if using bytecode encryption)

- Windows (32bit/64bit):

 - Windows XP or newer.
    
- GNU/Linux (32bit/64bit)

 - ldd: Console application to print the shared libraries required
   by each program or shared library. This typically can be found in
   the distribution-package `glibc` or `libc-bin`.
 - objdump: Console application to display information from 
   object files. This typically can be found in the
   distribution-package `binutils`.
 - objcopy: Console application to copy and translate object files.
   This typically can be found in the distribution-package `binutils`,
   too.

- Mac OS X (64bit):

 - Mac OS X 10.7 (Lion) or newer.


Usage
-----

Basic usage is very simple, just run it against your main script::

      pyinstaller /path/to/yourscript.py

For more details, see the `manual`_.


Untested Platforms
---------------------

The following platforms have been contributed and any feedback or
enhancements on these are welcome.

- FreeBSD

 - ldd

- Solaris

 - ldd
 - objdump

- AIX

 - AIX 6.1 or newer. PyInstaller will not work with statically
   linked Python libraries.
 - ldd

- PowerPC GNU/Linux (Debian)


Before using any contributed platform, you need to build the PyInstaller
bootloader, as we do not ship binary packages. Download PyInstaller
source, and build the bootloader::
     
        cd bootloader
        python ./waf distclean all

Then install PyInstaller::

        python setup.py install
        
or simply use it directly from the source (pyinstaller.py).


Support
---------------------

See http://www.pyinstaller.org/support.html for how to find help as well as
for commercial support.


Funding
---------------------

Maintaining PyInstaller is a huge amount of work.
PyInstaller development can only continue
if users and companies provide sustainable funding. See
http://www.pyinstaller.org/funding.html for how to support PyInstaller.



.. _PyCrypto: https://www.dlitz.net/software/pycrypto/
.. _`manual`: https://pyinstaller.readthedocs.io/en/v3.5/



.. Define some roles so they can be used in the README.

.. role:: ref
.. role:: program
.. role:: pep
.. role:: issue


Changelog for PyInstaller
=========================

.. NOTE:

   You should *NOT* be adding new change log entries to this file, this
   file is managed by towncrier. You *may* edit previous change logs to
   fix problems like typo corrections or such.

   To add a new change log entry, please see
   https://pyinstaller.readthedocs.io/en/latest/development/changelog-entries.html

.. towncrier release notes start

3.5 (2019-07-09)
----------------

Features
~~~~~~~~

* (Windows) Force ``--windowed`` option if first script is a ``.pyw`` file.
  This might still be overwritten in the spec-file. (:issue:`#4001`)
* Add support for relative paths for icon-files, resource-files and
  version-resource-files. (:issue:`#3333`, :issue:`#3444`)
* Add support for the RedHat Software Collections (SCL) Python 3.x.
  (:issue:`#3536`, :issue:`#3881`)
* Install platform-specific dependencies only on that platform.
  (:issue:`#4166`, :issue:`#4173`)
* New command-line option ``--upx-exclude``, which allows the user to prevent
  binaries from being compressed with UPX. (:issue:`#3821`)


Bugfix
~~~~~~

* (conda) Fix detection of conda/anaconda platform.
* (GNU/Linux) Fix Anaconda Python library search. (:issue:`#3885`,
  :issue:`#4015`)
* (Windows) Fix UAC in one-file mode by embedding the manifest.
  (:issue:`#1729`, :issue:`#3746`)
* (Windows\\Py3.7) Now able to locate pylib when VERSION.dll is listed in
  python.exe PE Header rather than pythonXY.dll (:issue:`#3942`,
  :issue:`#3956`)
* Avoid errors if PyQt5 or PySide2 is referenced by the modulegraph but isn't
  importable. (:issue:`#3997`)
* Correctly parse the ``--debug=import``, ``--debug=bootloader``, and
  ``--debug=noarchive`` command-line options. (:issue:`#3808`)
* Don't treat PyQt5 and PySide2 files as resources in an OS X windowed build.
  Doing so causes the resulting frozen app to fail under Qt 5.12.
  (:issue:`#4237`)
* Explicitly specify an encoding of UTF-8 when opening *all* text files.
  (:issue:`#3605`)
* Fix appending the content of ``datas`` in a `spec` files to ``binaries``
  instead of the internal ``datas``. (:issue:`#2326`, :issue:`#3694`)
* Fix crash when changing from ``--onefile`` to ``--onedir`` on consecutive
  runs. (:issue:`#3662`)
* Fix discovery of Qt paths on Anaconda. (:issue:`#3740`)
* Fix encoding error raised when reading a XML manifest file which includes
  non-ASCII characters. This error inhibited building an executable which
  has non-ASCII characters in the filename. (:issue:`#3478`)
* Fix inputs to ``QCoreApplication`` constructor in ``Qt5LibraryInfo``. Now the
  core application's initialization and finalization in addition to system-wide
  and application-wide settings is safer. (:issue:`#4121`)
* Fix installation with pip 19.0. (:issue:`#4003`)
* Fixes PE-file corruption during version update. (:issue:`#3142`,
  :issue:`#3572`)
* In the fake ´site` module set `USER_BASE` to empty string instead of None
  as Jupyter Notebook requires it to be a 'str'. (:issue:`#3945`)
* Query PyQt5 to determine if SSL is supported, only adding SSL DLLs if so. In
  addition, search the path for SSL DLLs, instead of looking in Qt's
  ``BinariesPath``. (:issue:`#4048`)
* Require ``pywin32-ctypes`` version 0.2.0, the minimum version which supports
  Python 3.7. (:issue:`#3763`)
* Use pkgutil instead of filesystem operations for interacting with the
  modules. (:issue:`#4181`)


Incompatible Changes
~~~~~~~~~~~~~~~~~~~~

* PyInstaller is no longer tested against Python 3.4, which is end-of-live.
* Functions ``compat.architecture()``, ``compat.system()`` and
  ``compat.machine()`` have been replace by variables of the same name. This
  avoids evaluating the save several times.
* Require an option for the ``--debug`` argument, rather than assuming a
  default of ``all``. (:issue:`#3737`)


Hooks
~~~~~

* Added hooks for
  `aliyunsdkcore <https://pypi.org/project/aliyun-python-sdk-core/>`_ (:issue:`#4228`),
  astropy (:issue:`#4274`),
  `BTrees <https://pypi.org/project/BTrees/>`_ (:issue:`#4239`),
  dateparser.utils.strptime (:issue:`#3790`),
  `faker <https://faker.readthedocs.io>`_ (:issue:`#3989`, :issue:`#4133`),
  gooey (:issue:`#3773`),
  GtkSourceView (:issue:`#3893`),
  imageio_ffmpeg (:issue:`#4051`),
  importlib_metadata and importlib_resources (:issue:`#4095`),
  jsonpath_rw_ext (:issue:`#3841`),
  jupyterlab (:issue:`#3951`),
  lz4 (:issue:`#3710`),
  `magic <https://pypi.org/project/python-magic-bin>`_ (:issue:`#4267`),
  nanite (:issue:`#3860`),
  nbconvert (:issue:`#3947`),
  nbdime (:issue:`#3949`),
  nbformat (:issue:`#3946`),
  notebook (:issue:`#3950`),
  pendulum (:issue:`#3906`),
  pysoundfile (:issue:`#3844`),
  python-docx (:issue:`#2574`, :issue:`#3848`),
  python-wavefile (:issue:`#3785`),
  pytzdata (:issue:`#3906`),
  `PyWavelets pywt <https://github.com/PyWavelets/pywt>`_ (:issue:`#4120`),
  pywebview (:issue:`#3771`),
  radicale (:issue:`#4109`),
  rdflib (:issue:`#3708`),
  resampy (:issue:`#3702`),
  `sqlalchemy-migrate <https://github.com/openstack/sqlalchemy-migrate>`_ (:issue:`#4250`),
  `textdistance <https://pypi.org/project/textdistance/>`_ (:issue:`#4239`),
  tcod (:issue:`#3622`),
  ttkthemes (:issue:`#4105`), and
  `umap-learn <https://umap-learn.readthedocs.io/en/latest/>`_ (:issue:`#4165`).
  
* Add runtime hook for certifi. (:issue:`#3952`)
* Updated hook for 'notebook' to look in all Jupyter paths reported by
  jupyter_core. (:issue:`#4270`)
* Fixed hook for 'notebook' to only include directories that actually exist.
  (:issue:`#4270`)
  
* Fixed pre-safe-import-module hook for `setuptools.extern.six`. (:issue:`#3806`)
* Fixed QtWebEngine hook on OS X. (:issue:`#3661`)
* Fixed the QtWebEngine hook on distributions which don't have a NSS subdir
  (such as Archlinux) (:issue:`#3758`)
* Include dynamically-imported backends in the ``eth_hash`` package.
  (:issue:`#3681`)
* Install platform-specific dependencies only on that platform.
  (:issue:`#4168`)
* Skip packaging PyQt5 QML files if the QML directory doesn't exist.
  (:issue:`#3864`)
* Support ECC in PyCryptodome. (:issue:`#4212`, :issue:`#4229`)
* Updated PySide2 hooks to follow PyQt5 approach. (:issue:`#3655`,
  :issue:`#3689`, :issue:`#3724`, :issue:`#4040`, :issue:`#4103`,
  :issue:`#4136`, :issue:`#4175`, :issue:`#4177`, :issue:`#4198`,
  :issue:`#4206`)
* Updated the jsonschema hook for v3.0+. (:issue:`#4100`)
* Updated the Sphinx hook to correctly package Sphinx 1.8.


Bootloader
~~~~~~~~~~

* Update bundled zlib library to 1.2.11 address vulnerabilities.
  (:issue:`#3742`)


Documentation
~~~~~~~~~~~~~

* Update the text produced by ``--help`` to state that the ``--debug`` argument
  requires an option. Correctly format this argument in the Sphinx build
  process. (:issue:`#3737`)


Project & Process
~~~~~~~~~~~~~~~~~

* Remove the PEP-518 "build-system" table from ``pyproject.toml`` to fix
  installation with pip 19.0.


PyInstaller Core
~~~~~~~~~~~~~~~~

* Add support for folders in `COLLECT` and `BUNDLE`. (:issue:`#3653`)
* Completely remove `pywin32` dependency, which has erratic releases and
  the version on pypi may no longer have future releases.
  Require `pywin32-ctypes` instead which is pure python. (:issue:`#3728`,
  :issue:`#3729`)
* modulegraph: Align with upstream version 0.17.
* Now prints a more descriptive error when running a tool fails (instead of
  dumping a trace-back). (:issue:`#3772`)
* Suppress warnings about missing UCRT dependencies on Win 10. (:issue:`#1566`,
  :issue:`#3736`)


Test-suite and Continuous Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Fix Appveyor failures of ``test_stderr_encoding()`` and
  ``test_stdout_encoding()`` on Windows Python 3.7 x64. (:issue:`#4144`)
* November update of packages used in testing. Prevent pyup from touching
  ``test/requirements-tools.txt``. (:issue:`#3845`)
* Rewrite code to avoid a ``RemovedInPytest4Warning: Applying marks directly to
  parameters is deprecated, please use pytest.param(..., marks=...) instead.``
* Run Travis tests under Xenial; remove the deprecated ``sudo: false`` tag.
  (:issue:`#4140`)
* Update the Markdown test to comply with `Markdown 3.0 changes
  <https://python-markdown.github.io/change_log/release-3.0/#positional-arguments-deprecated>`_
  by using correct syntax for `extensions
  <https://python-markdown.github.io/reference/#extensions>`_.


3.4 (2018-09-09)
----------------

Features
~~~~~~~~

* Add support for Python 3.7 (:issue:`#2760`, :issue:`#3007`, :issue:`#3076`,
  :issue:`#3399`, :issue:`#3656`), implemented by Hartmut Goebel.
* Improved support for Qt5-based applications (:issue:`#3439`).
  By emulating much of the Qt deployment tools' behavior
  most PyQt5 variants are supported.
  However, Anaconda's PyQt5 packages are not supported
  because its ``QlibraryInfo`` implementation reports incorrect values.
  CI tests currently run on PyQt5 5.11.2. Many thanks to Bryan A. Jones for
  taking this struggle.
* ``--debug`` now allows more debugging to be activated more easily. This
  includes bootloader messages, Python's "verbose imports" and store collected
  Python files in the output directory instead of freezing. See ``pyinstaller
  –-help`` for details. (:issue:`#3546`, :issue:`#3585`, :issue:`#3587`)
* Hint users to install development package for missing `pyconfig.h`.
  (:issue:`#3348`)
* In ``setup.py`` specify Python versions this distribution is compatible with.
* Make ``base_library.zip`` reproducible: Set time-stamp of files. (:issue:`#2952`,
  :issue:`#2990`)
* New command-line option ``--bootloader-ignore-signals`` to make the
  bootloader forward all signals to the bundle application. (:issue:`#208`,
  :issue:`#3515`)
* (OS X) Python standard library module ``plistlib`` is now used for generating
  the ``Info.plist`` file. This allows passing complex and nested data in
  ``info_plist``. (:issue:`#3532`, :issue:`#3541`)


Bugfix
~~~~~~

* Add missing ``warnings`` module to ``base_library.zip``. (:issue:`#3397`,
  :issue:`#3400`)
* Fix and simplify search for libpython on Windows, msys2, cygwin.
  (:issue:`#3167`, :issue:`#3168`)
* Fix incompatibility with `pycryptodome` (a replacement for the apparently
  abandoned `pycrypto` library) when using encrypted PYZ-archives.
  (:issue:`#3537`)
* Fix race condition caused by the bootloader parent process terminating before
  the child is finished. This might happen e.g. when the child process itself
  plays with ``switch_root``. (:issue:`#2966`)
* Fix wrong security alert if a filename contains ``..``. (:issue:`#2641`,
  :issue:`#3491`)
* Only update resources of cached files when necessary to keep signature valid.
  (:issue:`#2526`)
* (OS X) Fix: App icon appears in the dock, even if ``LSUIElement=True``.
  (:issue:`#1917`, :issue:`#2075`, :issue:`#3566`)
* (Windows) Fix crash when trying to add resources to Windows executable using
  the ``--resource`` option. (:issue:`#2675`, :issue:`#3423`)
* (Windows) Only update resources when necessary to keep signature valid
  (:issue:`#3323`)
* (Windows) Use UTF-8 when reading XML manifest file. (:issue:`#3476`)
* (Windows) utils/win32: trap invalid ``--icon`` arguments and terminate with a
  message. (:issue:`#3126`)


Incompatible Changes
~~~~~~~~~~~~~~~~~~~~

* Drop support for Python 3.3 (:issue:`#3288`), Thanks to Hugo and xoviat.
* ``--debug`` now expects an (optional) argument. Thus using ``… --debug
  script.py`` will break. Use ``… script.py --debug`` or ``… --debug=all
  script.py`` instead. Also ``--debug=all`` (which is the default if no
  argument is given) includes ``noarchive``, which will store all collected
  Python files in the output directory instead of freezing them. Use
  ``--debug=bootloader`` to get the former behavior. (:issue:`#3546`,
  :issue:`#3585`, :issue:`#3587`)
* (minor) Change naming of intermediate build files and the `warn` file. This
  only effects 3rd-party tools (if any exists) relying on the names of these
  files.
* (minor) The destination path for ``--add-data`` and ``--add-binary`` must no
  longer be empty, use ``.`` instead. (:issue:`#3066`)
* (minor) Use standard path, not dotted path, for C extensions (Python 3 only).


Hooks
~~~~~

* New hooks for bokeh visualization library (:issue:`#3607`),
  Champlain, Clutter (:issue:`#3443`) dynaconf (:issue:`#3641`), flex
  (:issue:`#3401`), FMPy (:issue:`#3589`), gi.repository.xlib
  (:issue:`#2634`, :issue:`#3396`) google-cloud-translate,
  google-api-core (:issue:`#3658`), jedi (:issue:`#3535`,
  :issue:`#3612`), nltk (:issue:`#3705`), pandas (:issue:`#2978`,
  :issue:`#2998`, :issue:`#2999`, :issue:`#3015`, :issue:`#3063`,
  :issue:`#3079`), phonenumbers (:issue:`#3381`, :issue:`#3558`),
  pinyin (:issue:`#2822`), PySide.phonon, PySide.QtSql
  (:issue:`#2859`), pytorch (:issue:`#3657`), scipy (:issue:`#2987`,
  :issue:`#3048`), uvloop (:issue:`#2898`), web3, eth_account,
  eth_keyfile (:issue:`#3365`, :issue:`#3373`).
* Updated hooks for Cryptodome 3.4.8, Django 2.1, gevent 1.3.
  Crypto (support for PyCryptodome) (:issue:`#3424`),
  Gst and GdkPixbuf (to work on msys2, :issue:`#3257`, :issue:`#3387`),
  sphinx 1.7.1, setuptools 39.0.
* Updated hooks for PyQt5 (:issue:`#1930`, :issue:`#1988`, :issue:`#2141`,
  :issue:`#2156`, :issue:`#2220`, :issue:`#2518`, :issue:`#2566`,
  :issue:`#2573`, :issue:`#2577`, :issue:`#2857`, :issue:`#2924`,
  :issue:`#2976`, :issue:`#3175`, :issue:`#3211`, :issue:`#3233`,
  :issue:`#3308`, :issue:`#3338`, :issue:`#3417`, :issue:`#3439`,
  :issue:`#3458`, :issue:`#3505`), among others:

  - All QML is now loaded by ``QtQml.QQmlEngine``.
  - Improve error reporting when determining the PyQt5 library location.
  - Improved method for finding ``qt.conf``.
  - Include OpenGL fallback DLLs for PyQt5. (:issue:`#3568`).
  - Place PyQt5 DLLs in the correct location (:issue:`#3583`).
* Fix hooks for cryptodome (:issue:`#3405`),
  PySide2 (style mismatch) (:issue:`#3374`, :issue:`#3578`)
* Fix missing SSL libraries on Windows with ``PyQt5.QtNetwork``. (:issue:`#3511`,
  :issue:`#3520`)
* Fix zmq on Windows Python 2.7. (:issue:`#2147`)
* (GNU/Linux) Fix hook usb: Resolve library name reported by usb.backend.
  (:issue:`#2633`, :issue:`#2831`, :issue:`#3269`)
* Clean up the USB hook logic.


Bootloader
~~~~~~~~~~

* Forward all signals to the child process if option
  ``pyi-bootloader-ignore-signals`` to be set in the archive. (:issue:`#208`,
  :issue:`#3515`)
* Use ``waitpid`` instead of ``wait`` to avoid the bootloder parent process gets
  signaled. (:issue:`#2966`)
* (OS X) Don't make the application a GUI app by default, even in
  ``--windowed`` mode. Not enforcing this programmatically in the bootloader
  allows to control behavior using ``Info.plist`` options - which can by set in
  PyInstaller itself or in the `.spec`-file. (:issue:`#1917`, :issue:`#2075`,
  :issue:`#3566`)
* (Windows) Show respectivly print utf-8 debug messages ungarbled.
  (:issue:`#3477`)
* Fix ``setenv()`` call when ``HAVE_UNSETENV`` is not defined. (:issue:`#3722`,
  :issue:`#3723`)


Module Loader
~~~~~~~~~~~~~

* Improved error message in case importing an extension module fails.
  (:issue:`#3017`)


Documentation
~~~~~~~~~~~~~

* Fix typos, smaller errors and formatting errors in documentation.
  (:issue:`#3442`, :issue:`#3521`, :issue:`#3561`, :issue:`#3638`)
* Make clear that ``--windowed`` is independent of ``--onedir``.
  (:issue:`#3383`)
* Mention imports using imports ``imp.find_module()`` are not detected.
* Reflect actual behavior regarding ``LD_LIBRARY_PATH``. (:issue:`#3236`)
* (OS X) Revise section on ``info_plist`` for ``plistlib`` functionality and
  use an example more aligned with real world usage. (:issue:`#3532`,
  :issue:`#3540`, :issue:`#3541`)
* (developers) Overhaul guidelines for commit and commit-messages.
  (:issue:`#3466`)
* (developers) Rework developer’s quick-start guide.


Project & Process
~~~~~~~~~~~~~~~~~

* Add a pip ``requirements.txt`` file.
* Let `pyup` update package requirements for “Test – Libraries” every month
  only.
* Use `towncrier` to manage the change log entries. (:issue:`#2756`,
  :issue:`#2837`, :issue:`#3698`)


PyInstaller Core
~~~~~~~~~~~~~~~~

* Add ``requirements_for_package()`` and ``collect_all()`` helper functions for
  hooks.
* Add a explanatory header to the warn-file, hopefully reducing the number of
  those posting the file to the issue tracker.
* Add module ``enum`` to base_library.zip, required for module ``re`` in
  Python 3.6 (and ``re`` is required by ``warnings``).
* Always write the `warn` file.
* Apply ``format_binaries_and_datas()`` (which converts hook-style tuples into
  ``TOC``-style tuples) to binaries and datas added through the hook api.
* Avoid printing a useless exceptions in the ``get_module_file_attribute()``
  helper function..
* Don't gather Python extensions in ``collect_dynamic_libc()``.
* Fix several ResourceWarnings and DeprecationWarnings (:issue:`#3677`)
* Hint users to install necessary development packages if, in
  ``format_binaries_and_datas()``, the file not found is ``pyconfig.h``.
  (:issue:`#1539`, :issue:`#3348`)
* Hook helper function ``is_module_satisfies()`` returns ``False`` for packages
  not found. (:issue:`#3428`, :issue:`#3481`)
* Read data for cache digest in chunks. (:issue:`#3281`)
* Select correct file extension for C-extension file-names like
  ``libzmq.cp36-win_amd64.pyd``.
* State type of import (conditional, delayed, etc.) in the *warn* file again.
* (modulegraph) Unbundle `altgraph` library, use from upstream.
  (:issue:`#3058`)
* (OS X) In ``--console`` mode set ``LSBackgroundOnly=True`` in``Info.plist`` to
  hide the app-icon in the dock. This can still be overruled by passing
  ``info_plist`` in the `.spec`-file. (:issue:`#1917`, :issue:`#3566`)
* (OS X) Use the python standard library ``plistlib`` for generating the
  ``Info.plist`` file. (:issue:`#3532`, :issue:`#3541`)
* (Windows) Completely remove `pywin32` dependency, which has erratic releases
  and the version on pypi may no longer have future releases. Require
  `pywin32-ctypes` instead, which is pure python. (:issue:`#3141`)
* (Windows) Encode manifest before updating resource. (:issue:`#3423`)
* (Windows) Make import compatible with python.net, which uses an incompatible
  signature for ``__import__``. (:issue:`#3574`)


Test-suite and Continuous Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Add script and dockerfile for running tests in docker. (Contributed, not
  maintained) (:issue:`#3519`)
* Avoid log messages to be written (and captured) twice.
* Fix decorator ``skipif_no_compiler``.
* Fix the test for the "W" run-time Python option to verify module *warnings*
  can actually be imported. (:issue:`#3402`, :issue:`#3406`)
* Fix unicode errors when not capturing output by pytest.
* Run ``pyinstaller -h`` to verify it works.
* ``test_setuptools_nspkg`` no longer modifies source files.
* Appveyor:

  - Add documentation for Appveyor variables used to ``appveyor.yml``.
  - Significantly clean-up appveyor.yml (:issue:`#3107`)
  - Additional tests produce > 1 hour runs. Split each job into two
    jobs.
  - Appveyor tests run on 2 cores; therefore, run 2 jobs in parallel.
  - Reduce disk usage.
  - Split Python 2.7 tests into two jobs to avoid the 1 hour limit.
  - Update to use Windows Server 2016. (:issue:`#3563`)
* Travis

  - Use build-stages.
  - Clean-up travis.yml (:issue:`#3108`)
  - Fix Python installation on OS X. (:issue:`#3361`)
  - Start a X11 server for the "Test - Libraries" stage only.
  - Use target python interpreter to compile bootloader to check if the
    build tool can be used with that this Python version.


Bootloader build
~~~~~~~~~~~~~~~~

* Print invoking python version when compiling.
* Update `waf` build-tool to 2.0.9 and fix our ``wscript`` for `waf` 2.0.
* (GNU/Linux) When building with ``--debug`` turn of FORTIFY_SOURCE to ease
  debugging.


.. _v3.4 known issues:

Known Issues
~~~~~~~~~~~~~~~~~~

* Anaconda's PyQt5 packages are not supported
  because its ``QlibraryInfo`` implementation reports incorrect values.
* All scripts frozen into the package, as well as all run-time hooks, share
  the same global variables. This issue exists since v3.2 but was discovered
  only lately, see :issue:`3037`. This may lead to leaking global variables
  from run-time hooks into the script and from one script to subsequent ones.
  It should have effects in rare cases only, though.
* Data-files from wheels, unzipped eggs or not ad egg at all are not included
  automatically. This can be worked around using a hook-file, but may not
  suffice when using ``--onefile`` and something like `python-daemon`.

* The multipackage (MERGE) feature (:issue:`1527`) is currently broken.
* (OSX) Support for OpenDocument events (:issue:`1309`) is broken.
* (Windows) With Python 2.7 the frozen application may not run if the
  user-name (more specifically ``%TEMPDIR%``) includes some Unicode
  characters. This does not happen with all Unicode characters, but only some
  and seems to be a windows bug. As a work-around please upgrade to Python 3
  (:issue:`2754`, :issue:`2767`).
* (Windows) For Python >= 3.5 targeting *Windows < 10*, the developer needs to
  take special care to include the Visual C++ run-time .dlls. Please see the
  section :ref:`Platform-specific Notes <Platform-specific Notes - Windows>`
  in the manual. (:issue:`1566`)


3.3.1 (2017-12-13)
------------------

Hooks
~~~~~~~~~~

* Fix imports in hooks accessible_output and sound_lib (#2860).
* Fix ImportError for sysconfig for 3.5.4 Conda (#3105, #3106).
* Fix shapely hook for conda environments on Windows (#2838).
* Add hook for unidecode.

Bootloader
~~~~~~~~~~~~~~

* (Windows) Pre-build bootloaders (and custom-build ones using MSVC) can be
  used on Windows XP again. Set minimum target OS to XP (#2974).

Bootloader build
~~~~~~~~~~~~~~~~~~~

* Fix build for FreeBSD (#2861, #2862).

PyInstaller Core
~~~~~~~~~~~~~~~~~~~~~~~

* Usage: Add help-message clarifying use of options when a spec-file is
  provided (#3039).

* Add printing infos on UnicodeDecodeError in exec_command(_all).
* (win32) Issue an error message on errors loading the icon file (#2039).
* (aarch64) Use correct bootloader for 64-bit ARM (#2873).
* (OS X) Fix replacement of run-time search path keywords (``@…`` ) (#3100).

* Modulegraph

  * Fix recursion too deep errors cause by reimporting SWIG-like modules
    (#2911, #3040, #3061).
  * Keep order of imported identifiers.


Test-suite and Continuous Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* In Continuous Integration tests: Enable flake8-diff linting. This will
  refuse all changed lines not following PEP 8.

* Enable parallel testing on Windows,
* Update requirements.
* Add more test cases for modulegraph.
* Fix a test-case for order of module import.

* Add test-cases to check scripts do not share the same global vars (see
  :ref:`v3.3.1 known issues`).

Documentation
~~~~~~~~~~~~~~~~~~~

* Add clarification about treatment of options when a spec-file is provided
  (#3039).
* Add docs for running PyInstaller with Python optimizations (#2905).

* Add notes about limitations of Cython support.
* Add information how to handle undetected ctypes libraries.
* Add notes about requirements and restrictions of SWIG support.
* Add note to clarify what `binary files` are.

* Add a Development Guide.
* Extend "How to Contribute".
* Add "Running the Test Suite".

* Remove badges from the Readme (#2853).

* Update outdated sections in man-pages and otehr enhancements to the
  man-page.


.. _v3.3.1 known issues:

Known Issues
~~~~~~~~~~~~~~~~~~

* All scripts frozen into the package, as well as all run-time hooks, share
  the same global variables. This issue exists since v3.2 but was discovered
  only lately, see :issue:`3037`. This may lead to leaking global variables
  from run-time hooks into the script and from one script to subsequent ones.
  It should have effects in rare cases only, though.

* Further see the :ref:`Known Issues for release 3.3 <v3.3 known issues>`.


3.3 (2017-09-21)
----------------

* **Add Support for Python 3.6!** Many thanks to xiovat! (#2331, #2341)

* New command line options for adding data files (``--datas``, #1990) and
  binaries (``--binaries``, #703)

* Add command line option '--runtime-tmpdir'.

* Bootloaders for Windows are now build using MSVC and statically linked with
  the run-time-library (CRT). This solved a lot of issues related to .dlls
  being incompatible with the ones required by ``python.dll``.

* Bootloaders for GNU/Linux are now officially no LSB binaries. This was
  already the case since release 3.1, but documented the other way round. Also
  the build defaults to non-LSB binaries now. (#2369)

* We improved and stabilized both building the bootloaders and the continuous
  integration tests. See below for details. Many thanks to all who worked on
  this.

* To ease solving issues with packages included wrongly, the html-file with a
  cross-reference is now always generated. It's visual appearance has been
  modernized (#2765).

Incompatible changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Command-line option obsoleted several version ago are not longer handled
  gracefully but raise an error (#2413)

* Installation: PyInstaller removed some internal copies of 3rd-party
  packages. These are now taken from their official releases at PyPI (#2589).
  This results in PyInstaller to no longer can be used from just an unpacked
  archive, but needs to be installed like any Python package. This should
  effect only a few people, e.g. the developers.

* Following :pep:`527`, we only release one source archive now and decided to
  use `.tar.gz` (#2754).

Hooks
~~~~~~~~~~

* New and Updated hooks: accessible_output2 (#2266), ADIOS (#2096), CherryPy
  (#2112), PySide2 (#2471, #2744) (#2472), Sphinx (#2612, 2708) (#2708),
  appdir (#2478), clr (#2048), cryptodome (#2125), cryptography (#2013), dclab
  (#2657), django (#2037), django migrations (#1795), django.contrib (#2336),
  google.cloud, google.cloud.storage, gstreamer (#2603), imageio (#2696),
  langcodes (#2682), libaudioverse (#2709), mpl_toolkits (#2400), numba,
  llvmlite (#2113), openpyxl (#2066), pylint, pymssql, pyopencl, pyproj
  (#2677), pytest (#2119), qtawesome (#2617), redmine, requests (#2334),
  setuptools, setuptools (#2565), shapely (#2569), sound_lib (#2267),
  sysconfig, uniseg (#2683), urllib3, wx.rc (#2295),

  * numpy: Look for .dylib libraries, too ( (#2544), support numpy MKL builds
    (#1881, #2111)

  * osgeo: Add conda specific places to check for auxiliary data (#2401)

  * QT and related

    - Add hooks for PySide2
    - Eliminate run-time hook by placing files in the correct directory
    - Fix path in homebrew for searching for qmake (#2354)
    - Repair Qt dll location  (#2403)
    - Bundle PyQT 5.7 DLLs (#2152)
    - PyQt5: Return qml plugin path including subdirectory (#2694)
    - Fix hooks for PyQt5.QtQuick (#2743)
    - PyQt5.QtWebEngineWidgets: Include files needed by QWebEngine

  * GKT+ and related

    - Fix Gir file path on windows.
    - Fix unnecessary file search & generation when GI's typelib is exists
    - gi: change gir search path when running from a virtualenv
    - gi: package gdk-pixbuf in osx codesign agnostic dir
    - gi: rewrite the GdkPixbuf loader cache at runtime on Linux
    - gi: support onefile mode for GdkPixbuf
    - gi: support using gdk-pixbuf-query-loaders-64 when present
    - gi: GIR files are only required on OSX
    - gio: copy the mime.cache also
    - Fix hooks for PyGObject on windows platform (#2306)

* Fixed hooks: botocore (#2384), clr (#1801), gstreamer (#2417), h5py
  (#2686), pylint, Tix data files (#1660), usb.core (#2088), win32com on
  non-windows-systems (#2479)

* Fix ``multiprocess`` spawn mode on POSIX OSs (#2322, #2505, #2759, #2795).

Bootloader
~~~~~~~~~~~~~~

* Add `tempdir` option to control where bootloader will extract files (#2221)
* (Windows) in releases posted on PyPI requires msvcr*.dll (#2343)
* Fix unsafe string manipulation, resource and memory leaks. Thanks to Vito
  Kortbeek (#2489, #2502, #2503)
* Remove a left-over use of ``getenv()``
* Set proper LISTEN_PID (set by `systemd`) in child process (#2345)
* Adds PID to bootloader log messages (#2466, #2480)

* (Windows) Use _wputenv_s() instead of ``SetEnvironmentVariableW()``
* (Windows) Enhance error messages (#1431)
* (Windows) Add workaround for a Python 3 issue
  http://bugs.python.org/issue29778 (#2496, #2844)

* (OS X): Use single process for --onedir mode (#2616, #2618)

* (GNU/Linux) Compile bootloaders with --no-lsb by default (#2369)
* (GNU/Linux) Fix: linux64 bootloader requires glibc 2.14 (#2160)
* (GNU/Linux) set_dynamic_library_path change breaks plugin library use
  (#625)

Bootloader build
~~~~~~~~~~~~~~~~~~~

The bootloader build was largely overhauled. In the wscript, the build no
longer depends on the Python interpreter's bit-size, but on the compiler. We
have a machine for building bootloaders for Windows and cross-building for
OS X. Thus all mainteriner are now able to build the bootloaders for all
supported platforms.

* Add "official" build-script.

* (GNU/Linux) Make --no-lsb the default, add option --lsb.

* Largely overhauled Vagrantfile:

    - Make Darwin bootloaders build in OS X box (unused)
    - Make Windows bootloaders build using MSVC
    - Allow specifying cross-target on linux64.
    - Enable cross-building for OS X.
    - Enable cross-building for Windows (unused)
    - Add box for building osxcross.

* Largely overhauled wscript:

    - Remove options --target-cpu.
    - Use compiler's target arch, not Python's.
    - Major overhaul of the script
    - Build zlib if required, not if "on windows".
    - Remove obsolete warnings.
    - Update Solaris, AIX and HPUX support.
    - Add flags for 'strip' tool in AIX platform.
    - Don't set POSIX / SUS version defines.

* (GNU/Linux) for 64-bit arm/aarch ignore the :program:`gcc` flag ``-m64``
  (#2801).

Module loader
~~~~~~~~~~~~~~~~~~~~~~

* Implement PEP-451 ModuleSpec type import system (#2377)
* Fix: Import not thread-save? (#2010, #2371)

PyInstaller Core
~~~~~~~~~~~~~~~~~~~~~~~

* Analyze: Check Python version when testing whether to rebuild.
* Analyze: Don't fail on syntax error in modules, simply ignore them.
* Better error message when `datas` are not found. (#2308)
* Building: OSX: Use unicode literals when creating Info.plist XML
* Building: Don't fail if "datas" filename contain glob special characters.
  (#2314)
* Building: Read runtime-tmpdir from .spec-file.
* Building: Update a comment.
* building: warn users if bincache gets corrupted. (#2614)
* Cli-utils: Remove graceful handling of obsolete command line options.
* Configure: Create new parent-dir when moving old cache-dir. (#2679)
* Depend: Include vcruntime140.dll on Windows. (#2487)
* Depend: print nice error message if analyzed script has syntax error.
* Depend: When scanning for ctypes libs remove non-basename binaries.
* Enhance run-time error message on ctypes import error.
* Fix #2585: py2 non-unicode sys.path been tempted by os.path.abspath().
  (#2585)
* Fix crash if extension module has hidden import to ctypes. (#2492)
* Fix handling of obsolete command line options. (#2411)
* Fix versioninfo.py breakage on Python 3.x (#2623)
* Fix: "Unicode-objects must be encoded before hashing" (#2124)
* Fix: UnicodeDecodeError - collect_data_files does not return filenames as
  unicode (#1604)
* Remove graceful handling of obsolete command line options. (#2413)
* Make grab version more polite on non-windows (#2054)
* Make utils/win32/versioninfo.py round trip the version info correctly.
* Makespec: Fix version number processing for PyCrypto. (#2476)
* Optimizations and refactoring to modulegraph and scanning for ctypes
  dependencies.
* pyinstaller should not crash when hitting an encoding error in source code
  (#2212)
* Remove destination for COLLECT and EXE prior to copying it (#2701)
* Remove uninformative traceback when adding not found data files (#2346)
* threading bug while processing imports (#2010)
* utils/hooks: Add logging to collect_data_files.

* (win32) Support using pypiwin32 or pywin32-ctypes (#2602)
* (win32) Use os.path.normpath to ensure that system libs are excluded.
* (win32) Look for libpython%.%.dll in Windows MSYS2 (#2571)
* (win32) Make versioninfo.py round trip the version info correctly (#2599)
* (win32) Ensure that pywin32 isn't imported before check_requirements is
  called

* (win32) pyi-grab_version and --version-file not working? (#1347)
* (win32) Close PE() object to avoid mmap memory leak (#2026)
* (win32) Fix: ProductVersion in windows version info doesn't show in some
  cases (#846)
* (win32) Fix multi-byte path bootloader import issue with python2 (#2585)
* (win32) Forward DYLD_LIBRARY_PATH through `arch` command. (#2035)
* (win32) Add ``vcruntime140.dll`` to_win_includes for Python 3.5 an 3.6
  (#2487)

* (OS X) Add libpython%d.%dm.dylib to Darwin (is_darwin) PYDYLIB_NAMES.
  (#1971)
* (OS X) macOS bundle Info.plist should be in UTF-8 (#2615)
* (OS X) multiprocessing spawn in python 3 does not work on macOS (#2322)
* (OS X) Pyinstaller not able to find path (@rpath) of dynamic library (#1514)

* Modulegraph

  - Align with upstream version 0.13.
  - Add the upstream test-suite
  - Warn on syntax error and unicode error. (#2430)
  - Implement ``enumerate_instructions()`` (#2720)
  - Switch byte-code analysis to use `Instruction` (like dis3 does) (#2423)
  - Log warning on unicode error instead of only a debug message (#2418)
  - Use standard logging for messages. (#2433)
  - Fix to reimport failed SWIG C modules (1522, #2578).

* Included 3rd-party libraries

  - Remove bundled ``pefile`` and ``macholib``, use the releases from PyPI.
    (#1920, #2689)
  - altgraph: Update to altgraph 0.13, add upstream test-suite.

Utilities
~~~~~~~~~~~~~~~

* :program:`grab_version.py`: Display a friendly error message when utility
  fails (#859, #2792).


Test-suite and Continuous Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Rearrange requirements files.
* Pin required versions – now updated using pyup (#2745)
* Hide useless trace-backs of helper-functions.
* Add a test for PyQt5.QtQuick.
* Add functional tests for PySide2
* Add test for new feature --runtime-tmpdir.
* Fix regression-test for #2492.
* unit: Add test-cases for PyiModuleGraph.
* unit/altgraph: Bringing in upstream altgraph test-suite.
* unit/modulegraph: Bringing in the modulegraph test-suite.

* Continuous Integration

  - Lots of enhancements to the CI tests to make them more stabile and
    reliable.
  - Pin required versions – now updated using pyup (#2745)
  - OS X is now tested along with GNU/Linux at Travis CI (#2508)
  - Travis: Use stages (#2753)
  - appveyor: Save cache on failure (#2690)
  - appveyor: Verify built bootloaders have the expected arch.

Documentation
~~~~~~~~~~~~~~~~~~~

* Add information how to donate (#2755, #2772).
* Add how to install the development version using pip.
* Fix installation instructions for development version. (#2761)
* Better examples for hidden imports.
* Clarify and fix "Adding Data Files" and "Adding Binary Files". (#2482)
* Document new command line option '--runtime-tmpdir'.
* pyinstaller works on powerpc linux, big endian arch (#2000)
* Largely rewrite section "Building the Bootloader", update from the wiki
  page.
* Describe building LSB-compliant bootloader as (now) special case.
* help2rst: Add cross-reference labels for option-headers.
* Enable sphinx.ext.intersphinx and links to our website.
* Sphinx should not "adjust" display of command line documentation (#2217)

.. _v3.3 known issues:

Known Issues
~~~~~~~~~~~~~~~~~~

* Data-files from wheels, unzipped eggs or not ad egg at all are not included
  automatically. This can be worked around using a hook-file, but may not
  suffice when using ``--onefile`` and something like `python-daemon`.

* The multipackage (MERGE) feature (#1527) is currently broken.

* (OSX) Support for OpenDocument events (#1309) is broken.

* (Windows) With Python 2.7 the frozen application may not run if the
  user-name (more specifically ``%TEMPDIR%``) includes some Unicode
  characters. This does not happen with all Unicode characters, but only some
  and seems to be a windows bug. As a work-around please upgrade to Python 3
  (#2754, #2767).

* (Windows) For Python >= 3.5 targeting *Windows < 10*, the developer needs to
  take special care to include the Visual C++ run-time .dlls. Please see the
  section :ref:`Platform-specific Notes <Platform-specific Notes - Windows>`
  in the manual. (#1566)

* For Python 3.3, imports are not thread-safe (#2371#). Since Python 3.3 is
  end of live at 2017-09-29, we are not going to fix this.
  * [PyJWT-1.7.1](http://github.com/jpadilla/pyjwt) PyJWT
=====

.. image:: https://travis-ci.com/jpadilla/pyjwt.svg?branch=master
   :target: http://travis-ci.com/jpadilla/pyjwt?branch=master

.. image:: https://ci.appveyor.com/api/projects/status/h8nt70aqtwhht39t?svg=true
   :target: https://ci.appveyor.com/project/jpadilla/pyjwt

.. image:: https://img.shields.io/pypi/v/pyjwt.svg
   :target: https://pypi.python.org/pypi/pyjwt

.. image:: https://coveralls.io/repos/jpadilla/pyjwt/badge.svg?branch=master
   :target: https://coveralls.io/r/jpadilla/pyjwt?branch=master

.. image:: https://readthedocs.org/projects/pyjwt/badge/?version=latest
   :target: https://pyjwt.readthedocs.io

A Python implementation of `RFC 7519 <https://tools.ietf.org/html/rfc7519>`_. Original implementation was written by `@progrium <https://github.com/progrium>`_.

Sponsor
-------

+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| |auth0-logo| | If you want to quickly add secure token-based authentication to Python projects, feel free to check Auth0's Python SDK and free plan at `auth0.com/overview <https://auth0.com/overview?utm_source=GHsponsor&utm_medium=GHsponsor&utm_campaign=pyjwt&utm_content=auth>`_. |
+--------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

.. |auth0-logo| image:: https://user-images.githubusercontent.com/83319/31722733-de95bbde-b3ea-11e7-96bf-4f4e8f915588.png

Installing
----------

Install with **pip**:

.. code-block:: sh

    $ pip install PyJWT


Usage
-----

.. code:: python

    >>> import jwt
    >>> encoded = jwt.encode({'some': 'payload'}, 'secret', algorithm='HS256')
    'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb21lIjoicGF5bG9hZCJ9.4twFt5NiznN84AWoo1d7KO1T_yoc0Z6XOpOVswacPZg'

    >>> jwt.decode(encoded, 'secret', algorithms=['HS256'])
    {'some': 'payload'}


Command line
------------

Usage::

    pyjwt [options] INPUT

Decoding examples::

    pyjwt --key=secret decode TOKEN
    pyjwt decode --no-verify TOKEN

See more options executing ``pyjwt --help``.


Documentation
-------------

View the full docs online at https://pyjwt.readthedocs.io/en/latest/


Tests
-----

You can run tests from the project root after cloning with:

.. code-block:: sh

    $ python setup.py test



  * [PyMySQL-0.9.3](https://github.com/PyMySQL/PyMySQL/) .. image:: https://readthedocs.org/projects/pymysql/badge/?version=latest
    :target: https://pymysql.readthedocs.io/
    :alt: Documentation Status

.. image:: https://badge.fury.io/py/PyMySQL.svg
    :target: https://badge.fury.io/py/PyMySQL

.. image:: https://travis-ci.org/PyMySQL/PyMySQL.svg?branch=master
    :target: https://travis-ci.org/PyMySQL/PyMySQL

.. image:: https://coveralls.io/repos/PyMySQL/PyMySQL/badge.svg?branch=master&service=github
    :target: https://coveralls.io/github/PyMySQL/PyMySQL?branch=master

.. image:: https://img.shields.io/badge/license-MIT-blue.svg
    :target: https://github.com/PyMySQL/PyMySQL/blob/master/LICENSE


PyMySQL
=======

.. contents:: Table of Contents
   :local:

This package contains a pure-Python MySQL client library, based on `PEP 249`_.

Most public APIs are compatible with mysqlclient and MySQLdb.

NOTE: PyMySQL doesn't support low level APIs `_mysql` provides like `data_seek`,
`store_result`, and `use_result`. You should use high level APIs defined in `PEP 249`_.
But some APIs like `autocommit` and `ping` are supported because `PEP 249`_ doesn't cover
their usecase.

.. _`PEP 249`: https://www.python.org/dev/peps/pep-0249/


Requirements
-------------

* Python -- one of the following:

  - CPython_ : 2.7 and >= 3.4
  - PyPy_ : Latest version

* MySQL Server -- one of the following:

  - MySQL_ >= 5.5
  - MariaDB_ >= 5.5

.. _CPython: https://www.python.org/
.. _PyPy: https://pypy.org/
.. _MySQL: https://www.mysql.com/
.. _MariaDB: https://mariadb.org/


Installation
------------

Package is uploaded on `PyPI <https://pypi.org/project/PyMySQL>`_.

You can install it with pip::

    $ python3 -m pip install PyMySQL

To use "sha256_password" or "caching_sha2_password" for authenticate,
you need to install additional dependency::

   $ python3 -m pip install PyMySQL[rsa]


Documentation
-------------

Documentation is available online: https://pymysql.readthedocs.io/

For support, please refer to the `StackOverflow
<https://stackoverflow.com/questions/tagged/pymysql>`_.

Example
-------

The following examples make use of a simple table

.. code:: sql

   CREATE TABLE `users` (
       `id` int(11) NOT NULL AUTO_INCREMENT,
       `email` varchar(255) COLLATE utf8_bin NOT NULL,
       `password` varchar(255) COLLATE utf8_bin NOT NULL,
       PRIMARY KEY (`id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin
   AUTO_INCREMENT=1 ;


.. code:: python

    import pymysql.cursors

    # Connect to the database
    connection = pymysql.connect(host='localhost',
                                 user='user',
                                 password='passwd',
                                 db='db',
                                 charset='utf8mb4',
                                 cursorclass=pymysql.cursors.DictCursor)

    try:
        with connection.cursor() as cursor:
            # Create a new record
            sql = "INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)"
            cursor.execute(sql, ('webmaster@python.org', 'very-secret'))

        # connection is not autocommit by default. So you must commit to save
        # your changes.
        connection.commit()

        with connection.cursor() as cursor:
            # Read a single record
            sql = "SELECT `id`, `password` FROM `users` WHERE `email`=%s"
            cursor.execute(sql, ('webmaster@python.org',))
            result = cursor.fetchone()
            print(result)
    finally:
        connection.close()

This example will print:

.. code:: python

    {'password': 'very-secret', 'id': 1}


Resources
---------

* DB-API 2.0: https://www.python.org/dev/peps/pep-0249/

* MySQL Reference Manuals: https://dev.mysql.com/doc/

* MySQL client/server protocol:
  https://dev.mysql.com/doc/internals/en/client-server-protocol.html

* "Connector" channel in MySQL Community Slack:
  https://lefred.be/mysql-community-on-slack/

* PyMySQL mailing list: https://groups.google.com/forum/#!forum/pymysql-users

License
-------

PyMySQL is released under the MIT License. See LICENSE for more information.
  * [PyNaCl-1.2.1](https://github.com/pyca/pynacl/) ===============================================
PyNaCl: Python binding to the libsodium library
===============================================

.. image:: https://img.shields.io/pypi/v/pynacl.svg
    :target: https://pypi.org/project/PyNaCl/
    :alt: Latest Version

.. image:: https://travis-ci.org/pyca/pynacl.svg?branch=master
    :target: https://travis-ci.org/pyca/pynacl

.. image:: https://codecov.io/github/pyca/pynacl/coverage.svg?branch=master
    :target: https://codecov.io/github/pyca/pynacl?branch=master

PyNaCl is a Python binding to `libsodium`_, which is a fork of the
`Networking and Cryptography library`_. These libraries have a stated goal of
improving usability, security and speed. It supports Python 2.7 and 3.4+ as
well as PyPy 2.6+.

.. _libsodium: https://github.com/jedisct1/libsodium
.. _Networking and Cryptography library: https://nacl.cr.yp.to/

Features
--------

* Digital signatures
* Secret-key encryption
* Public-key encryption
* Hashing and message authentication
* Password based key derivation and password hashing

Installation
============

Binary wheel install
--------------------

PyNaCl ships as a binary wheel on OS X, Windows and Linux ``manylinux1`` [#many]_ ,
so all dependencies are included. Make sure you have an up-to-date pip
and run:

.. code-block:: console

    $ pip install pynacl

Linux source build
------------------

PyNaCl relies on `libsodium`_, a portable C library. A copy is bundled
with PyNaCl so to install you can run:

.. code-block:: console

    $ pip install pynacl

If you'd prefer to use the version of ``libsodium`` provided by your
distribution, you can disable the bundled copy during install by running:

.. code-block:: console

    $ SODIUM_INSTALL=system pip install pynacl

.. warning:: Usage of the legacy ``easy_install`` command provided by setuptools
   is generally discouraged, and is completely unsupported in PyNaCl's case.

.. _libsodium: https://github.com/jedisct1/libsodium

.. [#many] `manylinux1 wheels <https://www.python.org/dev/peps/pep-0513/>`_
    are built on a baseline linux environment based on Centos 5.11
    and should work on most x86 and x86_64 glibc based linux environments.

Changelog
=========

1.3.0 2018-09-26
----------------

* Added support for Python 3.7.
* Update ``libsodium`` to 1.0.16.
* Run and test all code examples in PyNaCl docs through sphinx's
  doctest builder.
* Add low-level bindings for chacha20-poly1305 AEAD constructions.
* Add low-level bindings for the chacha20-poly1305 secretstream constructions.
* Add low-level bindings for ed25519ph pre-hashed signing construction.
* Add low-level bindings for constant-time increment and addition
  on fixed-precision big integers represented as little-endian
  byte sequences.
* Add low-level bindings for the ISO/IEC 7816-4 compatible padding API.
* Add low-level bindings for libsodium's crypto_kx... key exchange
  construction.
* Set hypothesis deadline to None in tests/test_pwhash.py to avoid
  incorrect test failures on slower processor architectures.  GitHub
  issue #370

1.2.1 - 2017-12-04
------------------

* Update hypothesis minimum allowed version.
* Infrastructure: add proper configuration for readthedocs builder
  runtime environment.

1.2.0 - 2017-11-01
------------------

* Update ``libsodium`` to 1.0.15.
* Infrastructure: add jenkins support for automatic build of
  ``manylinux1`` binary wheels
* Added support for ``SealedBox`` construction.
* Added support for ``argon2i`` and ``argon2id`` password hashing constructs
  and restructured high-level password hashing implementation to expose
  the same interface for all hashers.
* Added support for 128 bit ``siphashx24`` variant of ``siphash24``.
* Added support for ``from_seed`` APIs for X25519 keypair generation.
* Dropped support for Python 3.3.

1.1.2 - 2017-03-31
------------------

* reorder link time library search path when using bundled
  libsodium

1.1.1 - 2017-03-15
------------------

* Fixed a circular import bug in ``nacl.utils``.

1.1.0 - 2017-03-14
------------------

* Dropped support for Python 2.6.
* Added ``shared_key()`` method on ``Box``.
* You can now pass ``None`` to ``nonce`` when encrypting with ``Box`` or
  ``SecretBox`` and it will automatically generate a random nonce.
* Added support for ``siphash24``.
* Added support for ``blake2b``.
* Added support for ``scrypt``.
* Update ``libsodium`` to 1.0.11.
* Default to the bundled ``libsodium`` when compiling.
* All raised exceptions are defined mixing-in
  ``nacl.exceptions.CryptoError``

1.0.1 - 2016-01-24
------------------

* Fix an issue with absolute paths that prevented the creation of wheels.

1.0 - 2016-01-23
----------------

* PyNaCl has been ported to use the new APIs available in cffi 1.0+.
  Due to this change we no longer support PyPy releases older than 2.6.
* Python 3.2 support has been dropped.
* Functions to convert between Ed25519 and Curve25519 keys have been added.

0.3.0 - 2015-03-04
------------------

* The low-level API (`nacl.c.*`) has been changed to match the
  upstream NaCl C/C++ conventions (as well as those of other NaCl bindings).
  The order of arguments and return values has changed significantly. To
  avoid silent failures, `nacl.c` has been removed, and replaced with
  `nacl.bindings` (with the new argument ordering). If you have code which
  calls these functions (e.g. `nacl.c.crypto_box_keypair()`), you must review
  the new docstrings and update your code/imports to match the new
  conventions.
  * [PyOpenGL-3.1.0](http://pyopengl.sourceforge.net) 
  * [PyQt5-5.13.0](https://www.riverbankcomputing.com/software/pyqt/) PyQt5 - Comprehensive Python Bindings for Qt v5
===============================================

Qt is set of cross-platform C++ libraries that implement high-level APIs for
accessing many aspects of modern desktop and mobile systems.  These include
location and positioning services, multimedia, NFC and Bluetooth connectivity,
a Chromium based web browser, as well as traditional UI development.

PyQt5 is a comprehensive set of Python bindings for Qt v5.  It is implemented
as more than 35 extension modules and enables Python to be used as an
alternative application development language to C++ on all supported platforms
including iOS and Android.

PyQt5 may also be embedded in C++ based applications to allow users of those
applications to configure or enhance the functionality of those applications.


Author
------

PyQt5 is copyright (c) Riverbank Computing Limited.  Its homepage is
https://www.riverbankcomputing.com/software/pyqt/.

Support may be obtained from the PyQt mailing list at
https://www.riverbankcomputing.com/mailman/listinfo/pyqt/.


License
-------

PyQt5 is released under the GPL v3 license and under a commercial license that
allows for the development of proprietary applications.


Documentation
-------------

The documentation for the latest release can be found
`here <https://www.riverbankcomputing.com/static/Docs/PyQt5/>`__.


Installation
------------

The GPL version of PyQt5 can be installed from PyPI::

    pip install PyQt5

The wheels include a copy of the required parts of the LGPL version of Qt.

``pip`` will also build and install the bindings from the sdist package but
Qt's ``qmake`` tool must be on ``PATH``.

The ``sip-install`` tool will also install the bindings from the sdist package
but will allow you to configure many aspects of the installation.

  * [PyQt5-sip-4.19.18](https://www.riverbankcomputing.com/software/sip/) sip Extension Module
====================

The sip extension module provides support for the PyQt5 package.



  * [PyVCF-0.6.8](https://github.com/jamescasbon/PyVCF) A VCFv4.0 and 4.1 parser for Python.

Online version of PyVCF documentation is available at http://pyvcf.rtfd.org/

The intent of this module is to mimic the ``csv`` module in the Python stdlib,
as opposed to more flexible serialization formats like JSON or YAML.  ``vcf``
will attempt to parse the content of each record based on the data types
specified in the meta-information lines --  specifically the ##INFO and
##FORMAT lines.  If these lines are missing or incomplete, it will check
against the reserved types mentioned in the spec.  Failing that, it will just
return strings.

There main interface is the class: ``Reader``.  It takes a file-like
object and acts as a reader::

    >>> import vcf
    >>> vcf_reader = vcf.Reader(open('vcf/test/example-4.0.vcf', 'r'))
    >>> for record in vcf_reader:
    ...     print record
    Record(CHROM=20, POS=14370, REF=G, ALT=[A])
    Record(CHROM=20, POS=17330, REF=T, ALT=[A])
    Record(CHROM=20, POS=1110696, REF=A, ALT=[G, T])
    Record(CHROM=20, POS=1230237, REF=T, ALT=[None])
    Record(CHROM=20, POS=1234567, REF=GTCT, ALT=[G, GTACT])


This produces a great deal of information, but it is conveniently accessed.
The attributes of a Record are the 8 fixed fields from the VCF spec::

    * ``Record.CHROM``
    * ``Record.POS``
    * ``Record.ID``
    * ``Record.REF``
    * ``Record.ALT``
    * ``Record.QUAL``
    * ``Record.FILTER``
    * ``Record.INFO``

plus attributes to handle genotype information:

    * ``Record.FORMAT``
    * ``Record.samples``
    * ``Record.genotype``

``samples`` and ``genotype``, not being the title of any column, are left lowercase.  The format
of the fixed fields is from the spec.  Comma-separated lists in the VCF are
converted to lists.  In particular, one-entry VCF lists are converted to
one-entry Python lists (see, e.g., ``Record.ALT``).  Semicolon-delimited lists
of key=value pairs are converted to Python dictionaries, with flags being given
a ``True`` value. Integers and floats are handled exactly as you'd expect::

    >>> vcf_reader = vcf.Reader(open('vcf/test/example-4.0.vcf', 'r'))
    >>> record = next(vcf_reader)
    >>> print record.POS
    14370
    >>> print record.ALT
    [A]
    >>> print record.INFO['AF']
    [0.5]

There are a number of convenience methods and properties for each ``Record`` allowing you to
examine properties of interest::

    >>> print record.num_called, record.call_rate, record.num_unknown
    3 1.0 0
    >>> print record.num_hom_ref, record.num_het, record.num_hom_alt
    1 1 1
    >>> print record.nucl_diversity, record.aaf, record.heterozygosity
    0.6 [0.5] 0.5
    >>> print record.get_hets()
    [Call(sample=NA00002, CallData(GT=1|0, GQ=48, DP=8, HQ=[51, 51]))]
    >>> print record.is_snp, record.is_indel, record.is_transition, record.is_deletion
    True False True False
    >>> print record.var_type, record.var_subtype
    snp ts
    >>> print record.is_monomorphic
    False

``record.FORMAT`` will be a string specifying the format of the genotype
fields.  In case the FORMAT column does not exist, ``record.FORMAT`` is
``None``.  Finally, ``record.samples`` is a list of dictionaries containing the
parsed sample column and ``record.genotype`` is a way of looking up genotypes
by sample name::

    >>> record = next(vcf_reader)
    >>> for sample in record.samples:
    ...     print sample['GT']
    0|0
    0|1
    0/0
    >>> print record.genotype('NA00001')['GT']
    0|0

The genotypes are represented by ``Call`` objects, which have three attributes: the
corresponding Record ``site``, the sample name in ``sample`` and a dictionary of
call data in ``data``::

     >>> call = record.genotype('NA00001')
     >>> print call.site
     Record(CHROM=20, POS=17330, REF=T, ALT=[A])
     >>> print call.sample
     NA00001
     >>> print call.data
     CallData(GT=0|0, GQ=49, DP=3, HQ=[58, 50])

Please note that as of release 0.4.0, attributes known to have single values (such as
``DP`` and ``GQ`` above) are returned as values.  Other attributes are returned
as lists (such as ``HQ`` above).

There are also a number of methods::

    >>> print call.called, call.gt_type, call.gt_bases, call.phased
    True 0 T|T True

Metadata regarding the VCF file itself can be investigated through the
following attributes:

    * ``Reader.metadata``
    * ``Reader.infos``
    * ``Reader.filters``
    * ``Reader.formats``
    * ``Reader.samples``

For example::

    >>> vcf_reader.metadata['fileDate']
    '20090805'
    >>> vcf_reader.samples
    ['NA00001', 'NA00002', 'NA00003']
    >>> vcf_reader.filters
    OrderedDict([('q10', Filter(id='q10', desc='Quality below 10')), ('s50', Filter(id='s50', desc='Less than 50% of samples have data'))])
    >>> vcf_reader.infos['AA'].desc
    'Ancestral Allele'

ALT records are actually classes, so that you can interrogate them::

    >>> reader = vcf.Reader(open('vcf/test/example-4.1-bnd.vcf'))
    >>> _ = next(reader); row = next(reader)
    >>> print row
    Record(CHROM=1, POS=2, REF=T, ALT=[T[2:3[])
    >>> bnd = row.ALT[0]
    >>> print bnd.withinMainAssembly, bnd.orientation, bnd.remoteOrientation, bnd.connectingSequence
    True False True T

The Reader supports retrieval of records within designated regions for files
with tabix indexes via the fetch method. This requires the pysam module as a
dependency. Pass in a chromosome, and, optionally, start and end coordinates,
for the regions of interest::

    >>> vcf_reader = vcf.Reader(filename='vcf/test/tb.vcf.gz')
    >>> # fetch all records on chromosome 20 from base 1110696 through 1230237
    >>> for record in vcf_reader.fetch('20', 1110695, 1230237):  # doctest: +SKIP
    ...     print record
    Record(CHROM=20, POS=1110696, REF=A, ALT=[G, T])
    Record(CHROM=20, POS=1230237, REF=T, ALT=[None])

Note that the start and end coordinates are in the zero-based, half-open
coordinate system, similar to ``_Record.start`` and ``_Record.end``. The very
first base of a chromosome is index 0, and the the region includes bases up
to, but not including the base at the end coordinate. For example::

    >>> # fetch all records on chromosome 4 from base 11 through 20
    >>> vcf_reader.fetch('4', 10, 20)   # doctest: +SKIP

would include all records overlapping a 10 base pair region from the 11th base
of through the 20th base (which is at index 19) of chromosome 4. It would not
include the 21st base (at index 20). (See
http://genomewiki.ucsc.edu/index.php/Coordinate_Transforms for more
information on the zero-based, half-open coordinate system.)

The ``Writer`` class provides a way of writing a VCF file.  Currently, you must specify a
template ``Reader`` which provides the metadata::

    >>> vcf_reader = vcf.Reader(filename='vcf/test/tb.vcf.gz')
    >>> vcf_writer = vcf.Writer(open('/dev/null', 'w'), vcf_reader)
    >>> for record in vcf_reader:
    ...     vcf_writer.write_record(record)

An extensible script is available to filter vcf files in vcf_filter.py.  VCF filters
declared by other packages will be available for use in this script.  Please
see :doc:`FILTERS` for full description.

  * [PyWavelets-1.0.3](https://github.com/PyWavelets/pywt) PyWavelets is a Python wavelet transforms module that includes:

* nD Forward and Inverse Discrete Wavelet Transform (DWT and IDWT)
* 1D and 2D Forward and Inverse Stationary Wavelet Transform (Undecimated Wavelet Transform)
* 1D and 2D Wavelet Packet decomposition and reconstruction
* 1D Continuous Wavelet Tranfsorm
* Computing Approximations of wavelet and scaling functions
* Over 100 built-in wavelet filters and support for custom wavelets
* Single and double precision calculations
* Real and complex calculations
* Results compatible with Matlab Wavelet Toolbox (TM)



  * [PyYAML-5.1.2](https://github.com/yaml/pyyaml) YAML is a data serialization format designed for human readability
and interaction with scripting languages.  PyYAML is a YAML parser
and emitter for Python.

PyYAML features a complete YAML 1.1 parser, Unicode support, pickle
support, capable extension API, and sensible error messages.  PyYAML
supports standard YAML tags and provides Python-specific tags that
allow to represent an arbitrary Python object.

PyYAML is applicable for a broad range of tasks from complex
configuration files to object serialization and persistence.


  * [Pygments-2.4.2](http://pygments.org/) 
Pygments
~~~~~~~~

Pygments is a syntax highlighting package written in Python.

It is a generic syntax highlighter suitable for use in code hosting, forums,
wikis or other applications that need to prettify source code.  Highlights
are:

* a wide range of over 300 languages and other text formats is supported
* special attention is paid to details, increasing quality by a fair amount
* support for new languages and formats are added easily
* a number of output formats, presently HTML, LaTeX, RTF, SVG, all image     formats that PIL supports and ANSI sequences
* it is usable as a command-line tool and as a library

:copyright: Copyright 2006-2019 by the Pygments team, see AUTHORS.
:license: BSD, see LICENSE for details.



  * [Pyro4-4.76](http://pyro4.readthedocs.io) Pyro means PYthon Remote Objects.
It is a library that enables you to build applications in which
objects can talk to eachother over the network, with minimal programming effort.
You can just use normal Python method calls, with almost every possible parameter
and return value type, and Pyro takes care of locating the right object on the right
computer to execute the method. It is designed to be very easy to use, and to
generally stay out of your way. But it also provides a set of powerful features that
enables you to build distributed applications rapidly and effortlessly.
Pyro is a pure Python library and runs on many different platforms and Python versions.

The source code repository is on Github: https://github.com/irmen/Pyro4

The documentation can be found here: http://pyro4.readthedocs.io

  * [QtPy-1.9.0](https://github.com/spyder-ide/qtpy) # QtPy: Abstraction layer for PyQt5/PyQt4/PySide2/PySide

[![license](https://img.shields.io/pypi/l/qtpy.svg)](./LICENSE)
[![pypi version](https://img.shields.io/pypi/v/qtpy.svg)](https://pypi.org/project/QtPy/)
[![conda version](https://img.shields.io/conda/vn/conda-forge/qtpy.svg)](https://www.anaconda.com/download/)
[![download count](https://img.shields.io/conda/dn/conda-forge/qtpy.svg)](https://www.anaconda.com/download/)
[![OpenCollective Backers](https://opencollective.com/spyder/backers/badge.svg?color=blue)](#backers)
[![Join the chat at https://gitter.im/spyder-ide/public](https://badges.gitter.im/spyder-ide/spyder.svg)](https://gitter.im/spyder-ide/public)<br>
[![PyPI status](https://img.shields.io/pypi/status/qtpy.svg)](https://github.com/spyder-ide/qtpy)
[![Build status](https://ci.appveyor.com/api/projects/status/62y6i02vhn4hefg0/branch/master?svg=true)](https://ci.appveyor.com/project/spyder-ide/qtpy/branch/master)
[![CircleCI](https://circleci.com/gh/spyder-ide/qtpy.svg?style=shield)](https://circleci.com/gh/spyder-ide/qtpy)
[![Coverage Status](https://coveralls.io/repos/github/spyder-ide/qtpy/badge.svg?branch=master)](https://coveralls.io/github/spyder-ide/qtpy?branch=master)

*Copyright © 2009–2019 The Spyder Development Team*


## Description

**QtPy** is a small abstraction layer that lets you
write applications using a single API call to either PyQt or PySide.

It provides support for PyQt5, PyQt4, PySide2 and PySide using the Qt5 layout
(where the QtGui module has been split into QtGui and QtWidgets).

Basically, you can write your code as if you were using PySide2
but import Qt modules from `qtpy` instead of `PySide2` (or `PyQt5`)


### Attribution and acknowledgments

This project is based on the [pyqode.qt](https://github.com/pyQode/pyqode.qt)
project and the [spyderlib.qt](https://github.com/spyder-ide/spyder/tree/2.3/spyderlib/qt)
module from the [Spyder](https://github.com/spyder-ide/spyder) project, and
also includes contributions adapted from
[qt-helpers](https://github.com/glue-viz/qt-helpers), developed as part of the
[glue](http://glueviz.org) project.

Unlike `pyqode.qt` this is not a namespace package, so it is not tied
to a particular project or namespace.


### License

This project is released under the MIT license.


### Requirements

You need PyQt5, PyQt4, PySide2 or PySide installed in your system to make use
of QtPy. If several of these packages are found, PyQt5 is used by
default unless you set the `QT_API` environment variable.

`QT_API` can take the following values:

* `pyqt5` (to use PyQt5).
* `pyqt` or `pyqt4` (to use PyQt4).
* `pyside2` (to use PySide2)
* `pyside` (to use PySide).


### Installation

```bash
pip install qtpy
```

or

```bash
conda install qtpy
```


## Contributing

Everyone is welcome to contribute!


## Sponsors

Become a sponsor to get your logo on our README on Github.

[![Sponsors](https://opencollective.com/spyder/sponsors.svg)](https://opencollective.com/spyder#support)



  * [Routes-2.4.1](https://routes.readthedocs.io/) Routes is a Python re-implementation of the Rails routes system for mapping
URL's to Controllers/Actions and generating URL's. Routes makes it easy to
create pretty and concise URL's that are RESTful with little effort.

Speedy and dynamic URL generation means you get a URL with minimal cruft
(no big dangling query args). Shortcut features like Named Routes cut down
on repetitive typing.

See `the documentation for installation and usage of Routes <http://readthedocs.org/docs/routes/en/latest/>`_.

.. image:: https://secure.travis-ci.org/bbangert/routes.png?branch=master
   :alt: Build Status
   :target: https://secure.travis-ci.org/bbangert/routes


Routes Changelog
%%%%%%%%%%%%%%%%

Release 2.4.0 (January 1, 2017)
===============================

* Release as a universal wheel. PR #75.
* Convert readthedocs links for their .org -> .io migration for hosted projects. PR #67.


Release 2.3.1 (March 30, 2016)
==============================
* Backwards compatability fix - connect should work with mandatory
  routename and optional path. Patch by Davanum Srinivas (PR #65).

Release 2.3 (March 28, 2016)
============================
* Fix sub_domain equivalence check. Patch by Nikita Uvarov
* Add support for protocol-relative URLs generation (i.e. starting with double
  slash ``//``). PR #60. Patch by Sviatoslav Sydorenko.
* Add support for the ``middleware`` extra requirement, making possible to
  depend on ``webob`` optionally. PR #59. Patch by Sviatoslav Sydorenko.
* Fix matching of an empty string route, which led to exception in earlier
  versions. PR #58. Patch by Sviatoslav Sydorenko.
* Add support for the ``requirements`` option when using
  mapper.resource to create routes. PR #57. Patch by Sean Dague.
* Concatenation fix when using submappers with path prefixes. Multiple
  submappers combined the path prefix inside the controller argument in
  non-obvious ways. The controller argument will now be properly carried
  through when using submappers. PR #28.

Release 2.2 (July 21, 2015)
===========================
* Fix Python 3 support. Patch by Victor Stinner.

Release 2.1 (January 17, 2015)
==============================
* Fix 3 other route matching groups in route.py to use anonymous groups for
  optional sections to avoid exceeding regex limits. Fixes #15.
* Printing a mapper now includes the Controller/action parameters from the
  route. Fixes #11.
* Fix regression that didn't allow passing in params 'host', 'protocol', or
  'anchor'. They can now be passed in with a trailing '_' as was possible
  before commit d1d1742903fa5ca24ef848a6ae895303f2661b2a. Fixes #7.
* URL generation with/without SCRIPT_NAME was resulting in the URL cache
  failing to return the appropriate cached URL generation. The URL cache
  should always include the SCRIPT_NAME, even if its empty, in the cache
  to avoid this, and now does. Fixes #6.
* Extract Route creation into separate method in Mapper.  Subclasses of Route
  can be created by Mappers now.
* Use the first X_FORWARDED_FOR value if there are multiple proxies in the
  path. Fixes #5.

Release 2.0 (November 17, 2013)
===============================
* Python 3.2/3.3 Support. Fixes Issue #2. Thanks to Alejandro Sánchez for
  the pull request!

Release 1.13 (March 12, 2012)
=============================
* Fix bug with dots forcing extension by default. The portion with the dot can
  now be recognized. Patch by Michael Basnight.

Release 1.12.3 (June 5, 2010)
=============================
* Fix bug with URLGenerator not properly including SCRIPT_NAME when generating
  URL's and the singleton is not present.

Release 1.12.2 (May 5, 2010)
============================
* Fix bug with routes URLGenerator not properly including SCRIPT_NAME when
  generating qualified URL's.

Release 1.12.1 (March 11, 2010)
===============================
* Fix bug with routes not generating URL's with callables in defaults.
* Fix bug with routes not handling sub-domain defaults during generation.

Release 1.12 (February 28, 2010)
================================
* Split up the Routes docs.
* Fix bug with relative URL's using qualified merging host and URL without
  including the appropriate slash. Fixes #13.
* Fix bug with mapper.extend and Routes modifying their original args.
  Fixes #24.
* Fix url.current() not returning current args when explicit is True.
* Added explicit way to directly use the Mapper to match with environ.
* Fix bug with improper len placement for submapper.
* Adding regular expression builder for entire regexp for faster rejection
  in a single regexp match should none of the routes match.
* Give Mapper a tabular string representation.
* Make SubMapper objects nestable and add route-generation helpers.
* Add SubMapper-based collections.
* Make the deprecated Mapper.minimization False (disabled) by default.
* Make the mapper explicit (true) by default.

Release 1.11 (September 28, 2009)
=================================
* Extensive documentation rewrite.
* Added Mapper.extend function that allows one to add lists of Routes objects
  to the mapper in one batch, optionally with a path_prefix.
* Added Mapper.submapper function that returns a SubMapper object to enable
  easier declaration of routes that have multiple keyword argument options
  in common.
* Mapper controller_scan argument now handles None, and lists of controller
  names in addition to a callable.
* Route object now takes a name parameter, which is the name it responds to.
  This name is automatically added when called by using Mapper's connect
  class method.
* Added optional LRU object for use with Routes when URL's change too often
  for the Routes urlcache dict to be a viable option.

Release 1.10.3 (February 8, 2009)
=================================
* Tweak to use WebOb Request rather than Paste.
* Performance tweaks for URL recognition.
* Bugfix for routes.middleware not re.escaping the path_info before moving it
  to the script name.

Release 1.10.2 (January 11, 2009)
=================================
* Bugfix for unicode encoding problems with non-minimized Route generation.
  Spotted by Wichert Akkerman.
* Bugfix for when environ is {} in unit tests.

Release 1.10.1 (September 27, 2008)
===================================
* Removing LRU cache due to performance and threading issues. Cache does hit
  a max-size for the given routes.

Release 1.10 (September 24, 2008)
=================================
* Adding LRU cache instead of just dict for caching generated routes. This
  avoids slow memory leakage over long-running and non-existent route
  generation.
* Adding URLGenerator object.
* Adding redirect routes.
* Static routes can now interpolate variable parts in the path if using {}
  variable part syntax.
* Added sub_domain condition option to accept False or None, to require that
  there be no sub-domain provided for the route to match.

Release 1.9.2 (July 8, 2008)
============================
* Fixed bug in url_for which caused it to return a literal when it shouldn't
  have.

Release 1.9.1 (June 28, 2008)
=============================
* Fixed bug in formatted route recognition with formatting being absorbed
  into the id.

Release 1.9 (June 12, 2008)
===========================
* Fix undefined arg bug in url_for.
* Fixed bug with url_for not working properly outside of a request when
  sub-domains are active. Thanks Pavel Skvazh.
* Add non-minimization option to Routes and the Mapper for generation and
  recognition.
* Add Routes 2.0 style syntax for making routes and regexp. For example, this
  route will now work: '{controller}/{action}/{id}'.
* Fixed Routes to not use quote_plus when making URL's.
* WARNING: Mapper now comes with hardcode_names set to True by default. This
  means routes generated by name must work for the URL.
* Actually respect having urlcache disabled.
* WARNING: Calling url_for with a set of args that returns None now throws an
  exception. Code that previously checked to see if a url could be made must
  be updated accordingly.
* Updated url_for to return url in a literal for use in templating that may
  try to escape it again.
* Added option to use X_FORWARDED_PROTO for proxying behind https to work
  easier.
* Fixed map.resource to be less restrictive on id than just spaces.
* Fixed Mapper.create_regs not being thread safe, particularly when
  always_scan=True.

Release 1.8 (March 28, 2008)
============================
* Fixed bug of map.resource not allowing spaces in id.
* Fixed url generation to properly handle unicode defaults in addition to
  unicode arguments.
* Fixed url_for to handle lists as keyword args when generating query
  parameters.
* WARNING: Changed map.resource to not use ';', for actions, but the
  normal '/'. This means that formatted URL's will also now have the format
  come AFTER the action. Ie: /messsages/4.xml;rss -> /messages/4/rss.xml

Release 1.7.3 (May 28th, 2008)
==============================
* Fixed triple escaping bug, since WSGI servers are responsible for basic
  unescaping.

Release 1.7.2 (Feb. 27th, 2008)
===============================
* Fixed bug with keyword args not being coerced to raw string properly.

Release 1.7.1 (Nov. 16th, 2007)
===============================
* Fixed bug with sub-domains from route defaults getting encoded to unicode
  resulting in a unicode route which then caused url_for to throw an
  exception.
* Removed duplicate assignment in map.resource. Patch by Mike Naberezny.
* Applied test patch fix for path checking. Thanks Mike Naberezny.
* Added additional checking of remaining URL, to properly swallow periods in
  the appropriate context. Fixes #57.
* Added mapper.hardcode_names option which restricts url generation to the
  named route during generation rather than using the routes default options
  during generation.
* Fixed the special '_method' attribute not being recognized during POST
  requests of Content-Type 'multipart/form-data'.

Release 1.7 (June 8th, 2007)
============================
* Fixed url_unquoting to only apply for strings.
* Added _encoding option to individual routes to toggle decoding/encoding on a
  per route basis.
* Fixed route matching so that '.' and other special chars are only part of the
  match should they not be followed by that character. Fixed regexp creation so
  that route parts with '.' in them aren't matched properly. Fixes #48.
* Fixed Unicode decoding/encoding so that the URL decoding and encoding can be
  set on the mapper with mapper.encoding. Fixes #40.
* Don't assume environ['CONTENT_TYPE'] always exists: it may be omitted
  according to the WSGI PEP.
* Fixed Unicode decode/encoding of path_info dynamic/wildcard parts so that
  PATH_INFO will stay a raw string as it should. Fixes #51.
* Fixed url_for (thus redirect_to) to throw an exception if a Unicode
  string is returned as that's an invalid URL. Fixes #46.
* Fixed Routes middleware to only parse POST's if the content type is
  application/x-www-form-urlencoded for a HTML form. This properly avoids
  parsing wsgi.input when it doesn't need to be.

Release 1.6.3 (April 10th, 2007)
================================
* Fixed matching so that an attempt to match an empty path raises a
  RouteException. Fixes #44.
* Added ability to use characters in URL's such as '-' and '_' in
  map.resource. Patch by Wyatt Baldwin. Fixes #45.
* Updated Mapper.resource handling with name_prefix and path_prefix checking
  to specify defaults. Also ensures that should either of them be set, they
  override the prefixes should parent_resource be specified. Patch by Wyatt
  Baldwin. Fixes #42.
* Added utf-8 decoding of incoming path arguments, with fallback to ignoring
  them in the very rare cases a malformed request URL is sent. Patch from
  David Smith.
* Fixed treatment of '#' character as something that can be left off and
  used in route paths. Found by Mike Orr.
* Added ability to specify parent resource to map.resource command. Patch from
  Wyatt Baldwin.
* Fixed formatted route issue with map.resource when additional collection
  methods are specified. Added unit tests to verify the collection methods
  work properly.
* Updated URL parsing to properly use HTTP_HOST for hostname + port info before
  falling back to SERVER_PORT and SERVER_NAME. Fixes #43.
* Added member_name and collection_name setting to Route object when made with
  map.resource.
* Updated routes.middleware to make the Routes matched accessible as
  environ['routes.route'].
* Updating mapper object to use thread local for request data (such as
  environ) and middleware now deletes environ references at the end of the
  request.
* Added explicit option to Routes and Mapper. Routes _explicit setting will
  prevent the Route defaults from being implicitly set, while setting Mapper
  to explicit will prevent Route implicit defaults and stop url_for from using
  Route memory. Fixes #38.
* Updated config object so that the route is attached if possible.
* Adding standard logging usage with debug messages.
* Added additional test for normal '.' match and fixed new special matching to
  match it properly. Thanks David Smith.
* Fixed hanging special char issue with 'special' URL chars at the end of a URL
  that are missing the variable afterwards.
* Changed Routes generation and recognition to handle other 'special' URL chars
  , . and ; as if they were /. This lets them be optionally left out of the
  resulting generated URL. Feature requested by David Smith.
* Fixed lookahead assertion in regexp builder to properly handle two grouped
  patterns in a row.
* Applied patch to generation and matching to handle Unicode characters
  properly. Reported with patch by David Smith.

Release 1.6.2 (Jan. 5, 2007)
============================
* Fixed issue with method checking not properly handling different letter
  cases in REQUEST_METHOD. Reported by Sean Davis.
* redirect_to now supports config.redirect returning a redirect, not just
  raising one.

Release 1.6.1 (Dec. 29, 2006)
=============================
* Fixed zipsafe flag to be False.

Release 1.6 (Dec. 14th, 2006)
=============================
* Fixed append_slash to take effect in the route generation itself instead of
  relying on url_for function. Reported by ToddG.
* Added additional url_for tests to ensure map.resource generates proper named
  routes.
* WARNING: Changed map.resource initialization to accept individual member and
  collection names to generate proper singular and plural route names. Those
  using map.resource will need to update their routes and url_for statements
  accordingly.
* Added additional map.resource recognition tests.
* Added WSGI middleware that does route resolving using new `WSGI.org Routing
  Vars Spec <http://wsgi.org/wsgi/Specifications/routing_args>`_.
* Added _absolute keyword option route connect to ignore SCRIPT_NAME settings.
  Suggested by Ian Bicking.

Release 1.5.2 (Oct. 16th, 2006)
===============================
* Fixed qualified keyword to keep host port names when used, unless a host
  is specifically passed in. Reported by Jon Rosebaugh.
* Added qualified keyword option to url_for to have it generate a full
  URL. Resolves #29.
* Fixed examples in url_for doc strings so they'll be accurate.

Release 1.5.1 (Oct. 4th, 2006)
==============================
* Fixed bug with escaping part names in the regular expression, reported by
  James Taylor.

Release 1.5 (Sept. 19th, 2006)
==============================
* Significant updates to map.resource and unit tests that comb it thoroughly
  to ensure its creating all the proper routes (it now is). Increased unit
  testing coverage to 95%.
* Added unit tests to ensure controller_scan works properly with nested
  controller files and appropriately scans the directory structure. This
  brings the Routes util module up to full code coverage.
* Fixed url_for so that when the protocol is changed, port information is
  removed from the host.
* Added more thorough testing to _RequestConfig object and the ability to
  set your own object. This increases testing coverage of the __init__ module
  to 100%.
* Fixed bug with sub_domain not maintaining port information in url_for and
  added unit tests. Reported by Jonathan Rosebaugh.
* Added unit tests to ensure sub_domain option works with named routes, cleaned
  up url_for memory argument filtering. Fixed bug with named routes and sub_domain
  option not working together, reported by Jonathan Rosebaugh.
* Changed order in which sub-domain is added to match-dict so it can be used
  in a conditions function.

Release 1.4.1 (Sept. 6th, 2006)
===============================
* Added sub_domains option to mapper, along with sub_domains_ignore list for
  subdomains that are considered equivilant to the main domain. When sub_domains
  is active, url_for will now take a sub_domain option that can alter the host
  the route will go to.
* Added ability for filter functions to provide a _host, _protocol, _anchor arg
  which is then used to create the URL with the appropriate host/protocol/anchor
  destination.
* Patch applied from Ticket #28. Resolves issue with Mapper's controller_scan
  function requiring a valid directory argument. Submitted by Zoran Isailovski.

Release 1.4 (July 21, 2006)
===========================
* Fixed bug with map.resource related to member methods, found in Rails version.
* Fixed bug with map.resource member methods not requiring a member id.
* Fixed bug related to handling keyword argument controller.
* Added map.resource command which can automatically generate a batch of routes intended
  to be used in a REST-ful manner by a web framework.
* Added URL generation handling for a 'method' argument. If 'method' is specified, it
  is not dropped and will be changed to '_method' for use by the framework.
* Added conditions option to map.connect. Accepts a dict with optional keyword args
  'method' or 'function'. Method is a list of HTTP methods that are valid for the route.
  Function is a function that will be called with environ, matchdict where matchdict is
  the dict created by the URL match.
* Fixed redirect_to function for using absolute URL's. redirect_to now passes all args to
  url_for, then passes the resulting URL to the redirect function. Reported by climbus.

Release 1.3.2 (April 30th, 2006)
================================
* Fixed _filter bug with inclusion in match dict during matching, reported by David Creemer.
* Fixed improper url quoting by using urllib.encode, patch by Jason Culverhouse.

Release 1.3.1 (April 4th, 2006)
===============================
* Mapper has an optional attribute ``append_slash``. When set to ``True``, any URL's
  generated will have a slash appended to the end.
* Fixed prefix option so that if the PATH_INFO is empty after prefix regexp, its set to
  '/' so the match proceeds ok.
* Fixed prefix bug that caused routes after the initial one to not see the proper url
  for matching. Caught by Jochen Kupperschmidt.

Release 1.3 (Feb. 25th, 2006)
=============================
* url_for keyword filters:
  Named routes can now have a _filter argument that should specify a function that takes
  a dict as its sole argument. The dict will contain the full set of keywords passed to
  url_for, which the function can then modify as it pleases. The new dict will then be
  used as if it was the original set of keyword args given to url_for.
* Fixed Python 2.3 incompatibility due to using keyword arg for a sort statement
  when using the built-in controller scanner.

Release 1.2 (Feb. 17th, 2006)
=============================
* If a named route doesn't exist, and a url_for call is used, instead of using the
  keyword arguments to generate a URL, they will be used as query args for the raw
  URL supplied. (Backwards Incompatible)
* If Mapper has debug=True, using match will return two additional values, the route
  that matched, if one did match. And a list of routes that were tried, and information
  about why they didn't pass.
* url_for enhancements:
  Can now be used with 'raw' URL's to generate proper url's for static content that
  will then automatically include SCRIPT_NAME if necessary
  Static named routes can now be used to shortcut common path information as desired.
* Controller Scanner will now sort controller names so that the longest one is first. This
  ensures that the deepest nested controller is executed first before more shallow ones to
  increase predictability.
* Controller Scanner now scans directories properly, the version in 1.1 left off the
  directory prefix when created the list of controllers.
  (Thanks to Justin for drawing my attention to it)

Release 1.1 (Jan. 13th, 2006)
=============================
* Routes Mapper additions:
  Now takes several optional arguments that determine how it will
  generate the regexp's.
  Can now hold a function for use when determining what the available
  controllers are. Comes with a default directory scanner
  Given a directory for the default scanner or a function, the Mapper
  will now automatically run it to get the controller list when needed
* Syntax available for splitting routes to allow more complex route paths, such
  as ':controller/:(action)-:(id).html'
* Easier setup/integration with Routes per request. Setting the environ in a
  WSGI environ will run match, and setup everything needed for url_for/etc.

Release 1.0.2 (Dec. 30th, 2005)
===============================
* Routes where a default was present but None were filling in improper values.
* Passing a 0 would evaluate to None during generation, resulting in missing
  URL parts

Release 1.0.1 (Dec. 18th, 2005)
===============================
* Request Local Callable - You can now designate your own callable function that
  should then be used to store the request_config data. This is most useful for
  environments where its possible multiple requests might be running in a single
  thread. The callable should return a request specific object for attributes to
  be attached. See routes.__init__.py for more information.

Release 1.0 (Nov. 21st, 2005)
=============================
* routes.__init__ will now load the common symbols most people will
  want to actually use.
  Thus, you can either::

       from routes import *

  Or::

       from routes import request_config, Mapper

  The following names are available for importing from routes::

      request_config, Mapper, url_for, redirect_to

* Route Names - You can now name a route, which will save a copy of the defaults
  defined for later use by url_for or redirect_to.
  Thus, a route and url_for looking like this::

       m.connect('home', controller='blog', action='splash')
       url_for(controller='blog', action='splash')   # => /home

  Can now be used with a name::

       m.connect('home_url','home', controller='blog', action='splash')
       url_for('home_url')  # => /home

  Additional keywords can still be added to url_for and will override defaults in
  the named route.
* Trailing / - Route recognition earlier failed on trailing slashes, not really a bug,
  not really a feature I guess. Anyways, trailing slashes are o.k. now as in the Rails
  version.
* redirect_to now has two sets of tests to ensure it works properly
  * [SIP-4.19.8](https://www.riverbankcomputing.com/software/sip/) SIP - A Python Bindings Generator for C and C++ Libraries
=========================================================

What is SIP?
------------

One of the features of Python that makes it so powerful is the ability to take
existing libraries, written in C or C++, and make them available as Python
extension modules.  Such extension modules are often called bindings for the
library.

SIP is a collection of tools that makes it very easy to create Python bindings
for C and C++ libraries.  It was originally developed in 1998 to create PyQt,
the Python bindings for the Qt toolkit, but can be used to create bindings for
any C or C++ library.  For example it is also used to generate wxPython, the
Python bindings for wxWidgets.

SIP comprises a set of build tools and a sip module. The build tools process a
set of specification files and generates C or C++ code which is then compiled
to create the bindings extension module.  Several extension modules may be
installed in the same Python package.  Extension modules can be built so that
they are are independent of the version of Python being used.  In other words a
wheel created from them can be installed with any version of Python starting
with v3.5.

The specification files contain a description of the interface of the C or C++
library, i.e. the classes, methods, functions and variables.  The format of a
specification file is almost identical to a C or C++ header file, so much so
that the easiest way of creating a specification file is to edit a copy of the
corresponding header file.

The sip module provides support functions to the automatically generated
code.  The sip module is installed as part of the same Python package as the
generated extension modules.  Unlike the extension modules the sip module is
specific to a particular version of Python (e.g. v3.5, v3.6, v3.7, v3.8).

SIP makes it easy to exploit existing C or C++ libraries in a productive
interpretive programming environment.  SIP also makes it easy to take a Python
application (maybe a prototype) and selectively implement parts of the
application (maybe for performance reasons) in C or C++.


Author
------

SIP is copyright (c) Riverbank Computing Limited.  Its homepage is
https://www.riverbankcomputing.com/software/sip/.

Support may be obtained from the PyQt mailing list at
https://www.riverbankcomputing.com/mailman/listinfo/pyqt/.


License
-------

SIP is released under the GPL v2, GPL v3 licenses, and under a license similar
to the BSD license.


Installation
------------

SIP can be installed from PyPI::

    pip install sip


Documentation
-------------

The documentation for the latest release can be found
`here <https://www.riverbankcomputing.com/static/Docs/sip/>`__.
  * [SPARQLWrapper-1.8.4](http://rdflib.github.io/sparqlwrapper) This is a wrapper around a SPARQL service. It helps in creating the query URI and, possibly, convert the result into a more manageable format.



  * [SQLAlchemy-1.3.6](http://www.sqlalchemy.org) SQLAlchemy
==========

The Python SQL Toolkit and Object Relational Mapper

Introduction
-------------

SQLAlchemy is the Python SQL toolkit and Object Relational Mapper
that gives application developers the full power and
flexibility of SQL. SQLAlchemy provides a full suite
of well known enterprise-level persistence patterns,
designed for efficient and high-performing database
access, adapted into a simple and Pythonic domain
language.

Major SQLAlchemy features include:

* An industrial strength ORM, built
  from the core on the identity map, unit of work,
  and data mapper patterns.   These patterns
  allow transparent persistence of objects
  using a declarative configuration system.
  Domain models
  can be constructed and manipulated naturally,
  and changes are synchronized with the
  current transaction automatically.
* A relationally-oriented query system, exposing
  the full range of SQL's capabilities
  explicitly, including joins, subqueries,
  correlation, and most everything else,
  in terms of the object model.
  Writing queries with the ORM uses the same
  techniques of relational composition you use
  when writing SQL.  While you can drop into
  literal SQL at any time, it's virtually never
  needed.
* A comprehensive and flexible system
  of eager loading for related collections and objects.
  Collections are cached within a session,
  and can be loaded on individual access, all
  at once using joins, or by query per collection
  across the full result set.
* A Core SQL construction system and DBAPI
  interaction layer.  The SQLAlchemy Core is
  separate from the ORM and is a full database
  abstraction layer in its own right, and includes
  an extensible Python-based SQL expression
  language, schema metadata, connection pooling,
  type coercion, and custom types.
* All primary and foreign key constraints are
  assumed to be composite and natural.  Surrogate
  integer primary keys are of course still the
  norm, but SQLAlchemy never assumes or hardcodes
  to this model.
* Database introspection and generation.  Database
  schemas can be "reflected" in one step into
  Python structures representing database metadata;
  those same structures can then generate
  CREATE statements right back out - all within
  the Core, independent of the ORM.

SQLAlchemy's philosophy:

* SQL databases behave less and less like object
  collections the more size and performance start to
  matter; object collections behave less and less like
  tables and rows the more abstraction starts to matter.
  SQLAlchemy aims to accommodate both of these
  principles.
* An ORM doesn't need to hide the "R".   A relational
  database provides rich, set-based functionality
  that should be fully exposed.   SQLAlchemy's
  ORM provides an open-ended set of patterns
  that allow a developer to construct a custom
  mediation layer between a domain model and
  a relational schema, turning the so-called
  "object relational impedance" issue into
  a distant memory.
* The developer, in all cases, makes all decisions
  regarding the design, structure, and naming conventions
  of both the object model as well as the relational
  schema.   SQLAlchemy only provides the means
  to automate the execution of these decisions.
* With SQLAlchemy, there's no such thing as
  "the ORM generated a bad query" - you
  retain full control over the structure of
  queries, including how joins are organized,
  how subqueries and correlation is used, what
  columns are requested.  Everything SQLAlchemy
  does is ultimately the result of a developer-
  initiated decision.
* Don't use an ORM if the problem doesn't need one.
  SQLAlchemy consists of a Core and separate ORM
  component.   The Core offers a full SQL expression
  language that allows Pythonic construction
  of SQL constructs that render directly to SQL
  strings for a target database, returning
  result sets that are essentially enhanced DBAPI
  cursors.
* Transactions should be the norm.  With SQLAlchemy's
  ORM, nothing goes to permanent storage until
  commit() is called.  SQLAlchemy encourages applications
  to create a consistent means of delineating
  the start and end of a series of operations.
* Never render a literal value in a SQL statement.
  Bound parameters are used to the greatest degree
  possible, allowing query optimizers to cache
  query plans effectively and making SQL injection
  attacks a non-issue.

Documentation
-------------

Latest documentation is at:

http://www.sqlalchemy.org/docs/

Installation / Requirements
---------------------------

Full documentation for installation is at
`Installation <http://www.sqlalchemy.org/docs/intro.html#installation>`_.

Getting Help / Development / Bug reporting
------------------------------------------

Please refer to the `SQLAlchemy Community Guide <http://www.sqlalchemy.org/support.html>`_.

Code of Conduct
---------------

Above all, SQLAlchemy places great emphasis on polite, thoughtful, and
constructive communication between users and developers.
Please see our current Code of Conduct at
`Code of Conduct <http://www.sqlalchemy.org/codeofconduct.html>`_.

License
-------

SQLAlchemy is distributed under the `MIT license
<http://www.opensource.org/licenses/mit-license.php>`_.
  * [SecretStorage-3.1.1](https://github.com/mitya57/secretstorage) .. image:: https://api.travis-ci.org/mitya57/secretstorage.svg
   :target: https://travis-ci.org/mitya57/secretstorage
   :alt: Travis CI status
.. image:: https://codecov.io/gh/mitya57/secretstorage/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/mitya57/secretstorage
   :alt: Coverage status
.. image:: https://readthedocs.org/projects/secretstorage/badge/?version=latest
   :target: https://secretstorage.readthedocs.io/en/latest/
   :alt: ReadTheDocs status

Module description
==================

This module provides a way for securely storing passwords and other secrets.

It uses D-Bus `Secret Service`_ API that is supported by GNOME Keyring
(since version 2.30) and KSecretsService.

The main classes provided are ``secretstorage.Item``, representing a secret
item (that has a *label*, a *secret* and some *attributes*) and
``secretstorage.Collection``, a place items are stored in.

SecretStorage supports most of the functions provided by Secret Service,
including creating and deleting items and collections, editing items,
locking and unlocking collections (asynchronous unlocking is also supported).

The documentation can be found on `secretstorage.readthedocs.io`_.

.. _`Secret Service`: https://specifications.freedesktop.org/secret-service/
.. _`secretstorage.readthedocs.io`: https://secretstorage.readthedocs.io/en/latest/

Building the module
===================

.. note::
   SecretStorage 3.x supports Python 3.5 and newer versions.
   If you have an older version of Python, install SecretStorage 2.x::

      pip install "SecretStorage < 3"

SecretStorage requires these packages to work:

* Jeepney_
* `python-cryptography`_

To build SecretStorage, use this command::

   python3 setup.py build

If you have Sphinx_ installed, you can also build the documentation::

   python3 setup.py build_sphinx

.. _Jeepney: https://pypi.org/project/jeepney/
.. _`python-cryptography`: https://pypi.org/project/cryptography/
.. _Sphinx: http://sphinx-doc.org/

Testing the module
==================

First, make sure that you have the Secret Service daemon installed.
The `GNOME Keyring`_ is the reference server-side implementation for the
Secret Service specification.

.. _`GNOME Keyring`: https://download.gnome.org/sources/gnome-keyring/

Then, start the daemon and unlock the ``default`` collection, if needed.
The testsuite will fail to run if the ``default`` collection exists and is
locked. If it does not exist, the testsuite can also use the temporary
``session`` collection, as provided by the GNOME Keyring.

Then, run the Python unittest module::

   python3 -m unittest discover -s tests

If you want to run the tests in an isolated or headless environment, run
this command in a D-Bus session::

   dbus-run-session -- python3 -m unittest discover -s tests

Get the code
============

SecretStorage is available under BSD license. The source code can be found
on GitHub_.

.. _GitHub: https://github.com/mitya57/secretstorage



  * [Send2Trash-1.5.0](https://github.com/hsoft/send2trash) ==================================================
Send2Trash -- Send files to trash on all platforms
==================================================

Send2Trash is a small package that sends files to the Trash (or Recycle Bin) *natively* and on
*all platforms*. On OS X, it uses native ``FSMoveObjectToTrashSync`` Cocoa calls, on Windows, it
uses native (and ugly) ``SHFileOperation`` win32 calls. On other platforms, if `PyGObject`_ and
`GIO`_ are available, it will use this.  Otherwise, it will fallback to its own implementation
of the `trash specifications from freedesktop.org`_.

``ctypes`` is used to access native libraries, so no compilation is necessary.

Send2Trash supports Python 2.7 and up (Python 3 is supported).

Installation
------------

You can download it with pip::

    pip install Send2Trash

or you can download the source from http://github.com/hsoft/send2trash and install it with::

    >>> python setup.py install

Usage
-----

>>> from send2trash import send2trash
>>> send2trash('some_file')

On Freedesktop platforms (Linux, BSD, etc.), you may not be able to efficiently
trash some files. In these cases, an exception ``send2trash.TrashPermissionError``
is raised, so that the application can handle this case. This inherits from
``PermissionError`` (``OSError`` on Python 2). Specifically, this affects
files on a different device to the user's home directory, where the root of the
device does not have a ``.Trash`` directory, and we don't have permission to
create a ``.Trash-$UID`` directory.

For any other problem, ``OSError`` is raised.

.. _PyGObject: https://wiki.gnome.org/PyGObject
.. _GIO: https://developer.gnome.org/gio/
.. _trash specifications from freedesktop.org: http://freedesktop.org/wiki/Specifications/trash-spec/


Changes
=======

Version 1.5.0 -- 2018/02/16
---------------------------

* More specific error when failing to create XDG fallback trash directory (#20)
* Windows: Workaround for long paths (#23)

Version 1.4.2 -- 2017/11/17
---------------------------

* Fix incompatibility with Python 3.6 on Windows. (#18)

Version 1.4.1 -- 2017/08/07
---------------------------

* Fix crash on Windows introduced in v1.4.0. Oops... (#14)

Version 1.4.0 -- 2017/08/07
---------------------------

* Use ``bytes`` instead of ``str`` for internal path handling in ``plat_other``. (#13)

Version 1.3.1 -- 2017/07/31
---------------------------

* Throw ``WindowsError`` instead of ``OSError`` in ``plat_win``. (#7)
* Fix ``TypeError`` on python 2 in ``plat_other``. (#12)

Version 1.3.0 -- 2013/07/19
---------------------------

* Added support for Gnome's GIO.
* Merged Python 3 and Python 2 versions in a single codebase.

Version 1.2.0 -- 2011/03/16
---------------------------

* Improved ``plat_other`` to follow freedesktop.org trash specification.

Version 1.1.0 -- 2010/10/18
---------------------------

* Converted compiled modules to ctypes so that cross-platform compilation isn't necessary anymore.

Version 1.0.2 -- 2010/07/10
---------------------------

* Fixed bugs with external volumes in plat_other.

Version 1.0.1 -- 2010/04/19
---------------------------

* Fixed memory leak in OS X module.

Version 1.0.0 -- 2010/04/07
---------------------------

* Initial Release

  * [Sphinx-2.1.2](http://sphinx-doc.org/) ========
 Sphinx
========

.. image:: https://img.shields.io/pypi/v/sphinx.svg
   :target: https://pypi.org/project/Sphinx/
   :alt: Package on PyPI

.. image:: https://readthedocs.org/projects/sphinx/badge/?version=master
   :target: http://www.sphinx-doc.org/
   :alt: Documentation Status

.. image:: https://travis-ci.org/sphinx-doc/sphinx.svg?branch=master
   :target: https://travis-ci.org/sphinx-doc/sphinx
   :alt: Build Status (Travis CI)

.. image:: https://ci.appveyor.com/api/projects/status/github/sphinx-doc/sphinx?branch=master&svg=true
   :target: https://ci.appveyor.com/project/sphinxdoc/sphinx
   :alt: Build Status (AppVeyor)

.. image:: https://circleci.com/gh/sphinx-doc/sphinx.svg?style=shield
   :target: https://circleci.com/gh/sphinx-doc/sphinx
   :alt: Build Status (CircleCI)

.. image:: https://codecov.io/gh/sphinx-doc/sphinx/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/sphinx-doc/sphinx
   :alt: Code Coverage Status (Codecov)

Sphinx is a tool that makes it easy to create intelligent and beautiful
documentation for Python projects (or other documents consisting of multiple
reStructuredText sources), written by Georg Brandl.  It was originally created
for the new Python documentation, and has excellent facilities for Python
project documentation, but C/C++ is supported as well, and more languages are
planned.

Sphinx uses reStructuredText as its markup language, and many of its strengths
come from the power and straightforwardness of reStructuredText and its parsing
and translating suite, the Docutils.

Among its features are the following:

* Output formats: HTML (including derivative formats such as HTML Help, Epub
  and Qt Help), plain text, manual pages and LaTeX or direct PDF output
  using rst2pdf
* Extensive cross-references: semantic markup and automatic links
  for functions, classes, glossary terms and similar pieces of information
* Hierarchical structure: easy definition of a document tree, with automatic
  links to siblings, parents and children
* Automatic indices: general index as well as a module index
* Code handling: automatic highlighting using the Pygments highlighter
* Flexible HTML output using the Jinja 2 templating engine
* Various extensions are available, e.g. for automatic testing of snippets
  and inclusion of appropriately formatted docstrings
* Setuptools integration

For more information, refer to the `the documentation`__.

.. __: http://www.sphinx-doc.org/

Installation
============

Sphinx is published on `PyPI`__ and can be installed from there::

   pip install -U sphinx

We also publish beta releases::

   pip install -U --pre sphinx

If you wish to install `Sphinx` for development purposes, refer to `the
contributors guide`__.

__ https://pypi.org/project/Sphinx/
__ http://www.sphinx-doc.org/en/master/devguide.html

Documentation
=============

Documentation is available from `sphinx-doc.org`__.

__ http://www.sphinx-doc.org/

Get in touch
============

- Report bugs, suggest features or view the source code `on GitHub`_.
- For less well defined questions or ideas, use the `mailing list`_.

.. _on GitHub: https://github.com/sphinx-doc/sphinx
.. _mailing list: https://groups.google.com/forum/#!forum/sphinx-users

Testing
=======

Continuous testing is provided by `Travis`__ (for unit tests and style checks
on Linux), `AppVeyor`__ (for unit tests on Windows), and `CircleCI`__ (for
large processes like TeX compilation).

For information on running tests locally, refer to `the contributors guide`__.

__ https://travis-ci.org/sphinx-doc/sphinx
__ https://ci.appveyor.com/project/sphinxdoc/sphinx
__ https://circleci.com/gh/sphinx-doc/sphinx
__ http://www.sphinx-doc.org/en/master/devguide.html

Contributing
============

Refer to `the contributors guide`__.

__ http://www.sphinx-doc.org/en/master/devguide.html

Release signatures
==================

Releases are signed with following keys:

* `498D6B9E <https://pgp.mit.edu/pks/lookup?op=vindex&search=0x102C2C17498D6B9E>`_
* `5EBA0E07 <https://pgp.mit.edu/pks/lookup?op=vindex&search=0x1425F8CE5EBA0E07>`_



  * [Tempita-0.5.2](http://pythonpaste.org/tempita/) Tempita is a small templating language for text substitution.

This isn't meant to be the Next Big Thing in templating; it's just a
handy little templating language for when your project outgrows
``string.Template`` or ``%`` substitution.  It's small, it embeds
Python in strings, and it doesn't do much else.

You can read about the `language
<http://pythonpaste.org/tempita/#the-language>`_, the `interface
<http://pythonpaste.org/tempita/#the-interface>`_, and there's nothing
more to learn about it.
  * [Theano-1.0.4](http://deeplearning.net/software/theano/) Theano is a Python library that allows you to define, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional arrays. It is built on top of NumPy_. Theano features:

 * **tight integration with NumPy:** a similar interface to NumPy's. numpy.ndarrays are also used internally in Theano-compiled functions.
 * **transparent use of a GPU:** perform data-intensive computations up to 140x faster than on a CPU (support for float32 only).
 * **efficient symbolic differentiation:** Theano can compute derivatives for functions of one or many inputs.
 * **speed and stability optimizations:** avoid nasty bugs when computing expressions such as log(1 + exp(x)) for large values of x.
 * **dynamic C code generation:** evaluate expressions faster.
 * **extensive unit-testing and self-verification:** includes tools for detecting and diagnosing bugs and/or potential problems.

Theano has been powering large-scale computationally intensive scientific
research since 2007, but it is also approachable enough to be used in the
classroom (IFT6266 at the University of Montreal).

.. _NumPy: http://numpy.scipy.org/


=============
Release Notes
=============

Theano 1.0.4 (16th of January 2019)
=====================================

This is a maintenance release of Theano, version ``1.0.4``, with no
new features, but some important bug fixes.

We recommend that everybody update to this version.

Highlights (since 1.0.3):

 - Theano is now compatible with NumPy 1.16.

A total of 10 people contributed to this release since ``1.0.3``:

 - wonghang
 - Frederic Bastien
 - Arnaud Bergeron
 - Duc Nguyen
 - Andrew Nelson
 - Björn Linse
 - Luis Mario Domenzain
 - Rebecca N. Palmer
 - Luciano Paz
 - Dan Foreman-Mackey
  * [Twisted-19.2.1](https://twistedmatrix.com/) Twisted
=======

|pypi|_
|travis|_
|circleci|_

For information on what's new in Twisted 19.2.0, see the `NEWS <https://github.com/twisted/twisted/blob/trunk/NEWS.rst>`_ file that comes with the distribution.


What is this?
-------------

Twisted is an event-based framework for internet applications, supporting Python 2.7 and Python 3.5+.
It includes modules for many different purposes, including the following:

- ``twisted.web``: HTTP clients and servers, HTML templating, and a WSGI server
- ``twisted.conch``: SSHv2 and Telnet clients and servers and terminal emulators
- ``twisted.words``: Clients and servers for IRC, XMPP, and other IM protocols
- ``twisted.mail``: IMAPv4, POP3, SMTP clients and servers
- ``twisted.positioning``: Tools for communicating with NMEA-compatible GPS receivers
- ``twisted.names``: DNS client and tools for making your own DNS servers
- ``twisted.trial``: A unit testing framework that integrates well with Twisted-based code.

Twisted supports all major system event loops -- ``select`` (all platforms), ``poll`` (most POSIX platforms), ``epoll`` (Linux), ``kqueue`` (FreeBSD, macOS), IOCP (Windows), and various GUI event loops (GTK+2/3, Qt, wxWidgets).
Third-party reactors can plug into Twisted, and provide support for additional event loops.


Installing
----------

To install the latest version of Twisted using pip::

  $ pip install twisted

Additional instructions for installing this software are in `the installation instructions <https://github.com/twisted/twisted/blob/trunk/INSTALL.rst>`_.


Documentation and Support
-------------------------

Twisted's documentation is available from the `Twisted Matrix website <https://twistedmatrix.com/documents/current/>`_.
This documentation contains how-tos, code examples, and an API reference.

Help is also available on the `Twisted mailing list <https://twistedmatrix.com/cgi-bin/mailman/listinfo/twisted-python>`_.

There is also a pair of very lively IRC channels, ``#twisted`` (for general Twisted questions) and ``#twisted.web`` (for Twisted Web), on ``chat.freenode.net``.


Unit Tests
----------

Twisted has a comprehensive test suite, which can be run by ``tox``::

  $ tox -l            # to view all test environments
  $ tox -e py27-tests # to run the tests for Python 2.7
  $ tox -e py35-tests # to run the tests for Python 3.5


You can test running the test suite under the different reactors with the ``TWISTED_REACTOR`` environment variable::

  $ env TWISTED_REACTOR=epoll tox -e py27-tests


Some of these tests may fail if you:

* don't have the dependencies required for a particular subsystem installed,
* have a firewall blocking some ports (or things like Multicast, which Linux NAT has shown itself to do), or
* run them as root.


Copyright
---------

All of the code in this distribution is Copyright (c) 2001-2019 Twisted Matrix Laboratories.

Twisted is made available under the MIT license.
The included `LICENSE <https://github.com/twisted/twisted/blob/trunk/LICENSE>`_ file describes this in detail.


Warranty
--------

  THIS SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
  EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
  TO THE USE OF THIS SOFTWARE IS WITH YOU.

  IN NO EVENT WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
  AND/OR REDISTRIBUTE THE LIBRARY, BE LIABLE TO YOU FOR ANY DAMAGES, EVEN IF
  SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
  DAMAGES.

Again, see the included `LICENSE <https://github.com/twisted/twisted/blob/trunk/LICENSE>`_ file for specific legal details.


.. |pypi| image:: http://img.shields.io/pypi/v/twisted.svg
.. _pypi: https://pypi.python.org/pypi/twisted

.. |travis| image:: https://travis-ci.org/twisted/twisted.svg?branch=trunk
.. _travis: https://travis-ci.org/twisted/twisted

.. |circleci| image:: https://circleci.com/gh/twisted/twisted.svg?style=svg
.. _circleci: https://circleci.com/gh/twisted/twisted
  * [WTForms-2.2.1](https://wtforms.readthedocs.io/) WTForms
=======

WTForms is a flexible forms validation and rendering library for Python
web development. It is `framework agnostic`_ and can work with whatever
web framework and template engine you choose. There are various
community libraries that provide closer integration with popular
frameworks.

To get started using WTForms, we recommend reading the `crash course`_
in the docs.

.. _crash course: https://wtforms.readthedocs.io/en/stable/crash_course.html
.. _framework agnostic: https://wtforms.readthedocs.io/en/stable/faq.html#does-wtforms-work-with-library-here


Installation
------------

Install and update using pip::

    pip install -U WTForms


Third-Party Library Integrations
--------------------------------

WTForms is designed to work with any web framework and template engine.
There are a number of community-provided libraries that make integrating
with frameworks even better.

-   `Flask-WTF`_ integrates with the Flask framework. It can
    automatically load data from the request, uses Flask-Babel to
    translate based on user-selected locale, provides full-application
    CSRF, and more.
-   `WTForms-Alchemy`_ provides rich support for generating forms from
    SQLAlchemy models, including an expanded set of fields and
    validators.
-   `WTForms-SQLAlchemy`_ provides ORM-backed fields and form generation
    from SQLAlchemy models.
-   `WTForms-AppEngine`_ provides ORM-backed fields and form generation
    from AppEnding db/ndb schema
-   `WTForms-AppEngine`_ provides ORM-backed fields and form generation
    from Django models, as well as integration with Django's I18N
    support.

.. _Flask-WTF: https://flask-wtf.readthedocs.io/
.. _WTForms-Alchemy: https://wtforms-alchemy.readthedocs.io/
.. _WTForms-SQLAlchemy: https://github.com/wtforms/wtforms-sqlalchemy
.. _WTForms-AppEngine: https://github.com/wtforms/wtforms-appengine
.. _WTForms-Django: https://github.com/wtforms/wtforms-django


Links
-----

-   Documentation: https://wtforms.readthedocs.io/
-   License: `BSD <https://github.com/wtforms/wtforms/blob/master/LICENSE>`_
-   Releases: https://pypi.org/project/WTForms/
-   Code: https://github.com/wtforms/wtforms
-   Issue tracker: https://github.com/wtforms/wtforms/issues
-   Test status:

    -   Linux: https://travis-ci.org/wtforms/wtforms

-   Test coverage: https://coveralls.io/github/wtforms/wtforms



  * [WebOb-1.8.5](http://webob.org/) WebOb
=====

.. image:: https://travis-ci.org/Pylons/webob.png?branch=master
        :target: https://travis-ci.org/Pylons/webob

.. image:: https://readthedocs.org/projects/webob/badge/?version=stable
        :target: https://docs.pylonsproject.org/projects/webob/en/stable/
        :alt: Documentation Status

WebOb provides objects for HTTP requests and responses.  Specifically
it does this by wrapping the `WSGI <http://wsgi.readthedocs.io/en/latest/>`_ request
environment and response status/headers/app_iter(body).

The request and response objects provide many conveniences for parsing
HTTP request and forming HTTP responses.  Both objects are read/write:
as a result, WebOb is also a nice way to create HTTP requests and
parse HTTP responses.

Support and Documentation
-------------------------

See the `WebOb Documentation website <https://docs.pylonsproject.org/projects/webob/en/stable/>`_ to view
documentation, report bugs, and obtain support.

License
-------

WebOb is offered under the `MIT-license
<https://docs.pylonsproject.org/projects/webob/en/stable/license.html>`_.

Authors
-------

WebOb was authored by Ian Bicking and is currently maintained by the `Pylons
Project <https://pylonsproject.org/>`_ and a team of contributors.

1.8.5 (2019-01-03)
------------------

Warnings
~~~~~~~~

- Fixed one last remaining invalid escape sequence in a docstring.

1.8.4 (2018-11-11)
------------------

Bugfix
~~~~~~

- Response.content_type now accepts unicode strings on Python 2 and encodes
  them to latin-1. See https://github.com/Pylons/webob/pull/389 and
  https://github.com/Pylons/webob/issues/388

- Accept header classes now support a .copy() function that may be used to
  create a copy. This allows ``create_accept_header`` and other like functions
  to accept an pre-existing Accept header. See
  https://github.com/Pylons/webob/pull/386 and
  https://github.com/Pylons/webob/issues/385

Warnings
~~~~~~~~

- Some backslashes introduced with the new accept handling code were causing
  DeprecationWarnings upon compiling the source to pyc files, all of the
  backslashes have been reigned in as appropriate, and users should no longer
  see DeprecationWarnings for invalid escape sequence. See
  https://github.com/Pylons/webob/issues/384

1.8.3 (2018-10-14)
------------------

Bugfix
~~~~~~

- ``acceptparse.AcceptValidHeader``, ``acceptparse.AcceptInvalidHeader``, and
  ``acceptparse.AcceptNoHeader`` will now always ignore offers that do not
  match the required media type grammar when calling ``.acceptable_offers()``.
  Previous versions raised a ``ValueError`` for invalid offers in
  ``AcceptValidHeader`` and returned them as acceptable in the others.
  See https://github.com/Pylons/webob/pull/372

Feature
~~~~~~~

- Add Request.remote_host, exposing REMOTE_HOST environment variable.

- Added ``acceptparse.Accept.parse_offer`` to codify what types of offers
  are compatible with ``acceptparse.AcceptValidHeader.acceptable_offers``,
  ``acceptparse.AcceptMissingHeader.acceptable_offers``, and
  ``acceptparse.AcceptInvalidHeader.acceptable_offers``. This API also
  normalizes the offer with lowercased type/subtype and parameter names.
  See https://github.com/Pylons/webob/pull/376 and
  https://github.com/Pylons/webob/pull/379

1.8.2 (2018-06-05)
------------------

Bugfix
~~~~~~

- SameSite may now be passed as str or bytes to `Response.set_cookie` and
  `cookies.make_cookie`. This was an oversight as all other arguments would be
  correctly coerced before being serialized. See
  https://github.com/Pylons/webob/issues/361 and
  https://github.com/Pylons/webob/pull/362


1.8.1 (2018-04-10)
------------------

Bugfix
~~~~~~

- acceptparse.MIMEAccept which is deprecated in WebOb 1.8.0 made a backwards
  incompatible change that led to it raising on an invalid Accept header. This
  behaviour has now been reversed, as well as some other fixes to allow
  MIMEAccept to behave more like the old version. See
  https://github.com/Pylons/webob/pull/356

1.8.0 (2018-04-04)
------------------

Feature
~~~~~~~

- ``request.POST`` now supports any requests with the appropriate
  Content-Type. Allowing any HTTP method to access form encoded content,
  including DELETE, PUT, and others. See
  https://github.com/Pylons/webob/pull/352

Compatibility
~~~~~~~~~~~~~

- WebOb is no longer officially supported on Python 3.3 which was EOL'ed on
  2017-09-29.

Backwards Incompatibilities
~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Many changes have been made to the way WebOb does Accept handling, not just
  for the Accept header itself, but also for Accept-Charset, Accept-Encoding
  and Accept-Language. This was a `Google Summer of Code
  <https://developers.google.com/open-source/gsoc/>`_ project completed by
  Whiteroses (https://github.com/whiteroses). Many thanks to Google for running
  GSoC, the Python Software Foundation for organising and a huge thanks to Ira
  for completing the work. See https://github.com/Pylons/webob/pull/338 and
  https://github.com/Pylons/webob/pull/335. Documentation is available at
  https://docs.pylonsproject.org/projects/webob/en/master/api/webob.html

- When calling a ``@wsgify`` decorated function, the default arguments passed
  to ``@wsgify`` are now used when called with the request, and not as a
  `start_response`

  .. code::

     def hello(req, name):
         return "Hello, %s!" % name
     app = wsgify(hello, args=("Fred",))

     req = Request.blank('/')
     resp = req.get_response(app)  # => "Hello, Fred"
     resp2 = app(req) # => "Hello, Fred"

  Previously the ``resp2`` line would have failed with a ``TypeError``. With
  this change there is no way to override the default arguments with no
  arguments. See https://github.com/Pylons/webob/pull/203

- When setting ``app_iter`` on a ``Response`` object the ``content_md5`` header
  is no longer cleared. This behaviour is odd and disallows setting the
  ``content_md5`` and then returning an iterator for chunked content encoded
  responses. See https://github.com/Pylons/webob/issues/86

Experimental Features
~~~~~~~~~~~~~~~~~~~~~

These features are experimental and may change at any point in the future.

- The cookie APIs now have the ability to set the SameSite attribute on a
  cookie in both ``webob.cookies.make_cookie`` and
  ``webob.cookies.CookieProfile``. See https://github.com/Pylons/webob/pull/255

Bugfix
~~~~~~

- Exceptions now use string.Template.safe_substitute rather than
  string.Template.substitute. The latter would raise for missing mappings, the
  former will simply not substitute the missing variable. This is safer in case
  the WSGI environ does not contain the keys necessary for the body template.
  See https://github.com/Pylons/webob/issues/345.

- Request.host_url, Request.host_port, Request.domain correctly parse IPv6 Host
  headers as provided by a browser. See
  https://github.com/Pylons/webob/pull/332

- Request.authorization would raise ValueError for unusual or malformed header
  values. See https://github.com/Pylons/webob/issues/231

- Allow unnamed fields in form data to be properly transcoded when calling
  request.decode with an alternate encoding. See
  https://github.com/Pylons/webob/pull/309

- ``Response.__init__`` would discard ``app_iter`` when a ``Response`` had no
  body, this would cause issues when ``app_iter`` was an object that was tied
  to the life-cycle of a web application and had to be properly closed.
  ``app_iter`` is more advanced API for ``Response`` and thus even if it
  contains a body and is thus against the HTTP RFC's, we should let the users
  shoot themselves by returning a body. See
  https://github.com/Pylons/webob/issues/305



  * [Werkzeug-0.15.5](https://palletsprojects.com/p/werkzeug/) Werkzeug
========

*werkzeug* German noun: "tool". Etymology: *werk* ("work"), *zeug* ("stuff")

Werkzeug is a comprehensive `WSGI`_ web application library. It began as
a simple collection of various utilities for WSGI applications and has
become one of the most advanced WSGI utility libraries.

It includes:

-   An interactive debugger that allows inspecting stack traces and
    source code in the browser with an interactive interpreter for any
    frame in the stack.
-   A full-featured request object with objects to interact with
    headers, query args, form data, files, and cookies.
-   A response object that can wrap other WSGI applications and handle
    streaming data.
-   A routing system for matching URLs to endpoints and generating URLs
    for endpoints, with an extensible system for capturing variables
    from URLs.
-   HTTP utilities to handle entity tags, cache control, dates, user
    agents, cookies, files, and more.
-   A threaded WSGI server for use while developing applications
    locally.
-   A test client for simulating HTTP requests during testing without
    requiring running a server.

Werkzeug is Unicode aware and doesn't enforce any dependencies. It is up
to the developer to choose a template engine, database adapter, and even
how to handle requests. It can be used to build all sorts of end user
applications such as blogs, wikis, or bulletin boards.

`Flask`_ wraps Werkzeug, using it to handle the details of WSGI while
providing more structure and patterns for defining powerful
applications.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    pip install -U Werkzeug


A Simple Example
----------------

.. code-block:: python

    from werkzeug.wrappers import Request, Response

    @Request.application
    def application(request):
        return Response('Hello, World!')

    if __name__ == '__main__':
        from werkzeug.serving import run_simple
        run_simple('localhost', 4000, application)


Links
-----

-   Website: https://palletsprojects.com/p/werkzeug/
-   Documentation: https://werkzeug.palletsprojects.com/
-   Releases: https://pypi.org/project/Werkzeug/
-   Code: https://github.com/pallets/werkzeug
-   Issue tracker: https://github.com/pallets/werkzeug/issues
-   Test status: https://dev.azure.com/pallets/werkzeug/_build
-   Official chat: https://discord.gg/t6rrQZH

.. _WSGI: https://wsgi.readthedocs.io/en/latest/
.. _Flask: https://www.palletsprojects.com/p/flask/
.. _pip: https://pip.pypa.io/en/stable/quickstart/



  * [XlsxWriter-1.1.8](https://github.com/jmcnamara/XlsxWriter) XlsxWriter
==========

**XlsxWriter** is a Python module for writing files in the Excel 2007+ XLSX
file format.

XlsxWriter can be used to write text, numbers, formulas and hyperlinks to
multiple worksheets and it supports features such as formatting and many more,
including:

* 100% compatible Excel XLSX files.
* Full formatting.
* Merged cells.
* Defined names.
* Charts.
* Autofilters.
* Data validation and drop down lists.
* Conditional formatting.
* Worksheet PNG/JPEG/BMP/WMF/EMF images.
* Rich multi-format strings.
* Cell comments.
* Integration with Pandas.
* Textboxes.
* Support for adding Macros.
* Memory optimization mode for writing large files.

It supports Python 2.7, 3.4+, Jython and PyPy and uses standard libraries only.

Here is a simple example:

.. code-block:: python

   import xlsxwriter


   # Create an new Excel file and add a worksheet.
   workbook = xlsxwriter.Workbook('demo.xlsx')
   worksheet = workbook.add_worksheet()

   # Widen the first column to make the text clearer.
   worksheet.set_column('A:A', 20)

   # Add a bold format to use to highlight cells.
   bold = workbook.add_format({'bold': True})

   # Write some simple text.
   worksheet.write('A1', 'Hello')

   # Text with formatting.
   worksheet.write('A2', 'World', bold)

   # Write some numbers, with row/column notation.
   worksheet.write(2, 0, 123)
   worksheet.write(3, 0, 123.456)

   # Insert an image.
   worksheet.insert_image('B5', 'logo.png')

   workbook.close()

.. image:: https://raw.github.com/jmcnamara/XlsxWriter/master/dev/docs/source/_images/demo.png

See the full documentation at: https://xlsxwriter.readthedocs.io

Release notes: https://xlsxwriter.readthedocs.io/changes.html




  * [ZConfig-3.5.0](https://github.com/zopefoundation/ZConfig/) ZConfig: Schema-driven configuration
====================================

.. image:: https://img.shields.io/pypi/v/ZConfig.svg
        :target: https://pypi.python.org/pypi/ZConfig/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/ZConfig.svg
        :target: https://pypi.org/project/ZConfig/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/ZConfig.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/ZConfig

.. image:: https://coveralls.io/repos/github/zopefoundation/ZConfig/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/ZConfig?branch=master

.. image:: https://readthedocs.org/projects/zconfig/badge/?version=latest
        :target: http://zconfig.readthedocs.org/en/latest/
        :alt: Documentation Status

ZConfig is a configuration library intended for general use.  It
supports a hierarchical schema-driven configuration model that allows
a schema to specify data conversion routines written in Python.
ZConfig's model is very different from the model supported by the
ConfigParser module found in Python's standard library, and is more
suitable to configuration-intensive applications.

ZConfig schema are written in an XML-based language and are able to
"import" schema components provided by Python packages.  Since
components are able to bind to conversion functions provided by Python
code in the package (or elsewhere), configuration objects can be
arbitrarily complex, with values that have been verified against
arbitrary constraints.  This makes it easy for applications to
separate configuration support from configuration loading even with
configuration data being defined and consumed by a wide range of
separate packages.

ZConfig is licensed under the Zope Public License, version 2.1.  See
the file LICENSE.txt in the distribution for the full license text.

Reference documentation is available at https://zconfig.readthedocs.io.

Information on the latest released version of the ZConfig package is
available at

  https://pypi.python.org/pypi/ZConfig/

You may either create an RPM and install this, or install directly from
the source distribution.

There is a mailing list for discussions and questions about ZConfig;
more information on the list is available at

  http://mail.zope.org/mailman/listinfo/zconfig/


Configuring Logging
-------------------

One common use of ZConfig is to configure the Python logging
framework. This is extremely simple to do as the following example
demonstrates:

    >>> from ZConfig import configureLoggers
    >>> configureLoggers('''
    ... <logger>
    ...    level INFO
    ...    <logfile>
    ...       PATH STDOUT
    ...       format %(levelname)s %(name)s %(message)s
    ...    </logfile>
    ... </logger>
    ... ''')

The above configures the root logger to output messages logged at INFO
or above to the console, as we can see in the following example:

    >>> from logging import getLogger
    >>> logger = getLogger()
    >>> logger.info('An info message')
    INFO root An info message
    >>> logger.debug('A debug message')

A more common configuration would see STDOUT replaced with a path to
the file into which log entries would be written.

For more information, see the `the documentation <https://zconfig.readthedocs.io>`_.


Installing from the source distribution
---------------------------------------

For a simple installation::

  python setup.py install


To install to a user's home-dir::

  python setup.py install --home=<dir>


To install to another prefix (for example, /usr/local)::

  python setup.py install --prefix=/usr/local


If you need to force the python interpreter to (for example) python2::

  python2 setup.py install


For more information on installing packages, please refer to the
`Python Packaging User Guide <https://packaging.python.org/>`__.


============================
 Change History for ZConfig
============================

3.5.0 (2019-06-24)
==================

- Add support for documenting schema files contained in packages to
  the Sphinx extension. See `issue 59
  <https://github.com/zopefoundation/ZConfig/issues/59>`_.

3.4.0 (2019-01-02)
==================

Many changes have been made in the support for logging configurations:

- The log handler section types defined by the
  ``ZConfig.components.logger`` package support additional, optional
  parameters:

  ``style``
      Used to configure alternate format styles as found in the Python 3
      standard library.  Four ``style`` values are supported:
      ``classic`` (the default), ``format`` (equivalent to ``style='{'``
      in Python 3), ``template`` (equivalent to ``style='$'``), and
      ``safe-template`` (similar to ``template``, but using the
      ``string.Template`` method ``safe_substitute`` method).  A
      best-effort implementation is provided for Python 2.

  ``arbitrary-fields``
      A Boolean defauting to ``False`` for backward compatibility,
      allows arbitrary replacement field names to be accepted in the
      format string (regardless of the ``style`` setting).  This
      supports applications where log records are known to be generated
      with additional string or numeric fields, at least for some
      loggers.  (An exception is still raised at format time if the
      additional fields are not provided, unless the ``style`` value
      ``safe-template`` is used.)

- The ``logfile`` section type defined by the ``ZConfig.components.logger``
  package supports the optional ``delay`` and ``encoding`` parameters.
  These can only be used for regular files, not the special ``STDOUT``
  and ``STDERR`` streams.

- More validation on the parameters to the ``logfile`` and
  ``email-notifier`` sections is performed early (at the construction of
  the factory, rather than at creation of the ``logging`` handler).
  This allows more checking of parameter combinations before any log
  files are opened.

- The ``ZConfig.components.logger.handlers.log_format`` data type
  function now supports formats that include numeric formatting for
  ``levelno``, and accept ``funcName`` as a valid log record field
  (added in Python 2.6 and 3.1).


3.3.0 (2018-10-04)
==================

- Drop support for Python 3.3.

- Add support for Python 3.7.

- Drop support for 'python setup.py test'. See `issue 38
  <https://github.com/zopefoundation/ZConfig/issues/38>`_.

- Add support for ``example`` in ``section`` and ``multisection``, and
  include those examples in generated documentation. See
  https://github.com/zopefoundation/ZConfig/pull/5.

- Fix configuration loaders to decode byte data using UTF-8 instead of
  the default encoding (usually ASCII). See `issue 37
  <https://github.com/zopefoundation/ZConfig/issues/37>`_.

3.2.0 (2017-06-22)
==================

- Drop support for Python 2.6 and 3.2 and add support for Python 3.6.

- Run tests with pypy and pypy3 as well.

- Host docs at https://zconfig.readthedocs.io

- BaseLoader is now an abstract class that cannot be instantiated.

- Allow ``nan``, ``inf`` and ``-inf`` values for floats in
  configurations. See
  https://github.com/zopefoundation/ZConfig/issues/16.

- Scripts ``zconfig`` (for schema validation) and
  ``zconfig_schema2html`` are ported to Python 3.

- A new ``ZConfig.sphinx`` `Sphinx extension
  <https://zconfig.readthedocs.io/en/latest/documenting-components.html#documenting-components>`_
  facilitates automatically documenting ZConfig components using their
  description and examples in Sphinx documentation. See
  https://github.com/zopefoundation/ZConfig/pull/25.

- Simplify internal schema processing of max and min occurrence
  values. See https://github.com/zopefoundation/ZConfig/issues/15.

- Almost all uses of ``type`` as a parameter name have been replaced
  with ``type_`` to avoid shadowing a builtin. These were typically
  not public APIs and weren't expected to be called with keyword
  arguments so there should not be any user-visible changes. See
  https://github.com/zopefoundation/ZConfig/issues/17

3.1.0 (2015-10-17)
==================

- Add ability to do variable substitution from environment variables using
  $() syntax.

3.0.4 (2014-03-20)
==================

- Added Python 3.4 support.


3.0.3 (2013-03-02)
==================

- Added Python 3.2 support.


3.0.2 (2013-02-14)
==================

- Fixed ResourceWarning in BaseLoader.openResource().


3.0.1 (2013-02-13)
==================

- Removed an accidentally left `pdb` statement from the code.

- Fix a bug in Python 3 with the custom string `repr()` function.


3.0.0 (2013-02-13)
==================

- Added Python 3.3 support.

- Dropped Python 2.4 and 2.5 support.


2.9.3 (2012-06-25)
==================

- Fixed: port values of 0 weren't allowed.  Port 0 is used to request
  an ephemeral port.


2.9.2 (2012-02-11)
==================

- Adjust test classes to avoid base classes being considered separate
  test cases by (at least) the "nose" test runner.


2.9.1 (2012-02-11)
==================

- Make FileHandler.reopen thread safe.


2.9.0 (2011-03-22)
==================

- Allow identical redefinition of ``%define`` names.
- Added support for IPv6 addresses.


2.8.0 (2010-04-13)
==================

- Fix relative path recognition.
  https://bugs.launchpad.net/zconfig/+bug/405687

- Added SMTP authentication support for email logger on Python 2.6.


2.7.1 (2009-06-13)
==================

- Improved documentation

- Fixed tests failures on windows.


2.7.0 (2009-06-11)
==================

- Added a convenience function, ``ZConfig.configureLoggers(text)`` for
  configuring loggers.

- Relaxed the requirement for a logger name in logger sections,
  allowing the logger section to be used for both root and non-root
  loggers.


2.6.1 (2008-12-05)
==================

- Fixed support for schema descriptions that override descriptions from a base
  schema.  If multiple base schema provide descriptions but the derived schema
  does not, the first base mentioned that provides a description wins.
  https://bugs.launchpad.net/zconfig/+bug/259475

- Fixed compatibility bug with Python 2.5.0.

- No longer trigger deprecation warnings under Python 2.6.


2.6.0 (2008-09-03)
==================

- Added support for file rotation by time by specifying when and
  interval, rather than max-size, for log files.

- Removed dependency on setuptools from the setup.py.


2.5.1 (2007-12-24)
==================

- Made it possible to run unit tests via 'python setup.py test' (requires
  setuptools on sys.path).

- Added better error messages to test failure assertions.


2.5 (2007-08-31)
================

*A note on the version number:*

Information discovered in the revision control system suggests that some
past revision has been called "2.4", though it is not clear that any
actual release was made with that version number.  We're going to skip
revision 2.4 entirely to avoid potential issues with anyone using
something claiming to be ZConfig 2.4, and go straight to version 2.5.

- Add support for importing schema components from ZIP archives (including
  eggs).

- Added a 'formatter' configuration option in the logging handler sections
  to allow specifying a constructor for the formatter.

- Documented the package: URL scheme that can be used in extending schema.

- Added support for reopening all log files opened via configurations using
  the ZConfig.components.logger package.  For Zope, this is usable via the
  ``zc.signalhandler`` package.  ``zc.signalhandler`` is not required for
  ZConfig.

- Added support for rotating log files internally by size.

- Added a minimal implementation of schema-less parsing; this is mostly
  intended for applications that want to read several fragments of ZConfig
  configuration files and assemble a combined configuration.  Used in some
  ``zc.buildout`` recipes.

- Converted to using ``zc.buildout`` and the standard test runner from
  ``zope.testing``.

- Added more tests.


2.3.1 (2005-08-21)
==================

- Isolated some of the case-normalization code so it will at least be
  easier to override.  This remains non-trivial.


2.3 (2005-05-18)
================

- Added "inet-binding-address" and "inet-connection-address" to the
  set of standard datatypes.  These are similar to the "inet-address"
  type, but the default hostname is more sensible.  The datatype used
  should reflect how the value will be used.

- Alternate rotating logfile handler for Windows, to avoid platform
  limitations on renaming open files.  Contributed by Sidnei da Silva.

- For <section> and <multisection>, if the name attribute is omitted,
  assume name="*", since this is what is used most often.


2.2 (2004-04-21)
================

- More documentation has been written.

- Added a timedelta datatype function; the input is the same as for
  the time-interval datatype, but the resulting value is a
  datetime.timedelta object.

- Make sure keys specified as attributes of the <default> element are
  converted by the appropriate key type, and are re-checked for
  derived sections.

- Refactored the ZConfig.components.logger schema components so that a
  schema can import just one of the "eventlog" or "logger" sections if
  desired.  This can be helpful to avoid naming conflicts.

- Added a reopen() method to the logger factories.

- Always use an absolute pathname when opening a FileHandler.

- A fix to the logger 'format' key to allow the %(process)d expansion variable
  that the logging package supports.

- A new timedelta built-in datatype was added.  Similar to time-interval
  except that it returns a datetime.timedelta object instead.


2.1 (2004-04-12)
================

- Removed compatibility with Python 2.1 and 2.2.

- Schema components must really be in Python packages; the directory
  search has been modified to perform an import to locate the package
  rather than incorrectly implementing the search algorithm.

- The default objects use for section values now provide a method
  getSectionAttributes(); this returns a list of all the attributes of
  the section object which store configuration-defined data (including
  information derived from the schema).

- Default information can now be included in a schema for <key
  name="+"> and <multikey name="+"> by using <default key="...">.

- More documentation has been added to discuss schema extension.

- Support for a Unicode-free Python has been fixed.

- Derived section types now inherit the datatype of the base type if
  no datatype is identified explicitly.

- Derived section types can now override the keytype instead of always
  inheriting from their base type.

- <import package='...'/> makes use of the current prefix if the
  package name begins witha dot.

- Added two standard datatypes:  dotted-name and dotted-suffix.

- Added two standard schema components: ZConfig.components.basic and
  ZConfig.components.logger.


2.0 (2003-10-27)
================

- Configurations can import additional schema components using a new
  "%import" directive; this can be used to integrate 3rd-party
  components into an application.

- Schemas may be extended using a new "extends" attribute on the
  <schema> element.

- Better error messages when elements in a schema definition are
  improperly nested.

- The "zconfig" script can now simply verify that a schema definition
  is valid, if that's all that's needed.


1.0 (2003-03-25)
================

- Initial release.



  * [absl-py-0.7.1](https://github.com/abseil/abseil-py) # Abseil Python Common Libraries

This repository is a collection of Python library code for building Python
applications. The code is collected from Google's own Python code base, and has
been extensively tested and used in production.

## Features

* Simple application startup
* Distributed commandline flags system
* Custom logging module with additional features
* Testing utilities

## Getting Started

### Installation

To install the package, simply run:

```bash
pip install absl-py
```

Or install from source:

```bash
python setup.py install
```

### Running Tests

To run Abseil tests, you can clone the git repo and run
[bazel](https://bazel.build/):

```bash
git clone https://github.com/abseil/abseil-py.git
cd abseil-py
bazel test absl/...
```

### Example Code

Please refer to [smoke_tests/sample_app.py](smoke_tests/sample_app.py) as an
example to get started.

## Documentation

See the [Abseil Python Developer Guide](https://abseil.io/docs/python/).

## Future Releases

The current repository includes an initial set of libraries for early adoption.
More components and interoperability with Abseil C++ Common Libraries
will come in future releases.

## License

The Abseil Python library is licensed under the terms of the Apache
license. See [LICENSE](LICENSE) for more information.
  * [abstract_rendering-0.5.1](https://github.com/JosephCottam/AbstractRendering) UNKNOWN
  * [adal-1.2.2](https://github.com/AzureAD/azure-activedirectory-library-for-python) 
  * [aenum-2.2.1](https://bitbucket.org/stoneleaf/aenum) Advanced Enumerations (compatible with Python's stdlib Enum), NamedTuples, and NamedConstants

aenum includes a Python stdlib Enum-compatible data type, as well as a metaclass-based NamedTuple implementation and a NamedConstant class.

An Enum is a set of symbolic names (members) bound to unique, constant values. Within an enumeration, the members can be compared by identity, and the enumeration itself can be iterated over.  Support exists for unique values, multiple values, auto-numbering, and suspension of aliasing (members with the same value are not identical), plus the ability to have values automatically bound to attributes.

A NamedTuple is a class-based, fixed-length tuple with a name for each possible position accessible using attribute-access notation as well as the standard index notation.

A NamedConstant is a class whose members cannot be rebound; it lacks all other Enum capabilities, however.

Enum classes:

- Enum: Base class for creating enumerated constants.

- IntEnum: Base class for creating enumerated constants that are also
           subclasses of int.

- Flag: Base class for creating enumerated constants that can be combined
        using the bitwise operations without losing their Flag membership.

- IntFlag: Base class for creating enumerated constants that can be combined
           using the bitwise operators without losing their IntFlag membership.
           IntFlag members are also subclasses of int.

- AutoNumberEnum: Derived class that automatically assigns an int value to each
                  member.

- OrderedEnum: Derived class that adds <, <=, >=, and > methods to an Enum.

- UniqueEnum: Derived class that ensures only one name is bound to any one
              value.

Utility functions include:

- convert: helper to convert target global variables into an Enum

- constant: helper class for creating constant members

- enum: helper class for creating members with keywords

- export: helper to insert Enum members into a namespace (usually globals())

- extend_enum: add new members to enumerations after creation

- module: inserts NamedConstant and Enum classes into sys.modules
          where it will appear to be a module whose top-level names
          cannot be rebound

- skip: class that prevents attributes from being converted to a
        constant or enum member

- unique: decorator that ensures no duplicate members



  * [agfusion-1.251](https://github.com/murphycj/AGFusion) # Annotate Gene Fusion (AGFusion)
AGFusion is a python package for annotating gene fusions from the human or mouse genomes. AGFusion simply needs the reference genome, the two gene partners, and the fusion junction coordinates as input, and outputs the following:

* FASTA files of cDNA, CDS, and protein sequences.
* Visualizes the protein domain and exon architectures of the fusion transcripts.
* Saves tables listing the coordinates of protein features and exons included in the fusion.
* Optional exon structure and protein domain visualization of the wild-type  version of the fusion gene partners.

Some other things to know:

* AGFusion automatically predicts the functional effect of the gene fusion (e.g. in-frame, out-of-frame, etc.).
* Annotation is by default done only for canonical gene isoforms, but there is the option to annotate all gene non-canonical isoform combinations.
* All gene and protein annotation is from Ensembl
* Supports up to Ensembl release 95


## Table of Contents

- [Examples](#examples)
  * [Basic Usage](#basic-usage)
  * [Plotting wild-type protein and exon structure](#plotting-wild-type-protein-and-exon-structure)
  * [Canonical gene isoforms](#canonical-gene-isoforms)
  * [Input from fusion-finding algorithms](#input-from-fusion-finding-algorithms)
  * [Graphical parameters](#graphical-parameters)
- [Installation](#installation)
- [Dependencies](#dependencies)
- [Troubleshooting](#troubleshooting)
- [License](#license)
- [Citing AGFusion](#citing-agfusion)


## Examples

### Basic Usage

You just need to provide the two fusion gene partners (gene symbol, Ensembl ID, or Entrez gene ID), their predicted fusion junctions in genomic coordinates, and the genome build. You can also specify certain transcripts with Ensembl transcript ID or RefSeq ID

Example usage from the command line:

```
agfusion annotate \
  --gene5prime DLG1 \
  --gene3prime BRAF \
  --junction5prime 31684294 \
  --junction3prime 39648486 \
  -db agfusion.mus_musculus.87.db \
  -o DLG1-BRAF
```

The protein domain structure of the DLG1-BRAF fusion:

![alt tag](https://github.com/murphycj/AGFusion/blob/master/doc/ENSMUST00000064477-ENSMUST00000002487.png)

The exon structure of the DLG1-BRAF fusion:

![alt tag](https://github.com/murphycj/AGFusion/blob/master/doc/ENSMUST00000064477-ENSMUST00000002487.exon.png)

### Plotting wild-type protein and exon structure

You can additionally plot the wild-type proteins and exon structures for each gene with --WT flag.

```
agfusion annotate \
   -g5 ENSMUSG00000022770 \
   -g3 ENSMUSG00000002413 \
   -j5 31684294 \
   -j3 39648486 \
   -db agfusion.mus_musculus.87.db \
   -o DLG1-BRAF \
   --WT
```

### Canonical gene isoforms

By default AGFusion only plots the [canonical](http://useast.ensembl.org/Help/Glossary?id=346) gene isoforms, but you can tell AGFusion to include non-canonical isoform with the --noncanonical flag.

```
agfusion annotate \
  -g5 ENSMUSG00000022770 \
  -g3 ENSMUSG00000002413 \
  -j5 31684294 \
  -j3 39648486 \
  -db agfusion.mus_musculus.87.db \
  -o DLG1-BRAF \
  --noncanonical
```

### Input from fusion-finding algorithms

You can provide as input output files from fusion-finding algorithms. Currently supported algorithms are:

* Bellerophontes
* BreakFusion
* ChimeraScan
* ChimeRScope
* deFuse
* EricScript
* FusionCatcher
* FusionHunter
* FusionMap
* InFusion
* JAFFA
* MapSplice (only if --gene-gtf specified)
* STAR-Fusion
* TopHat-Fusion



Below is an example for FusionCatcher.

```
agfusion batch \
  -f final-list_candidate-fusion-genes.txt \
  -a fusioncatcher \
  -o test \
  -db agfusion.mus_musculus.87.db
```

### Graphical parameters

You can change domain names and colors:

```
agfusion annotate \
  -g5 ENSMUSG00000022770 \
  -g3 ENSMUSG00000002413 \
  -j5 31684294 \
  -j3 39648486 \
  -db agfusion.mus_musculus.87.db \
  -o DLG1-BRAF \
  --recolor "Pkinase_Tyr;red" --recolor "L27_1;blue" \
  --rename "Pkinase_Tyr;Kinase" --rename "L27_1;L27"
```

![alt tag](https://github.com/murphycj/AGFusion/blob/master/doc/ENSMUST00000064477-ENSMUST00000002487-recolorRename.png)

You can rescale the protein length so that images of two different fusions have appropriate relative lengths when plotted side by side:

```
agfusion annotate \
  -g5 ENSMUSG00000022770 \
  -g3 ENSMUSG00000002413 \
  -j5 31684294 \
  -j3 39648486 \
  -db agfusion.mus_musculus.87.db \
  -o DLG1-BRAF \
  --recolor "Pkinase_Tyr;red" --recolor "L27_1;blue" \
  --rename "Pkinase_Tyr;Kinase" --rename "L27_1;L27" \
  --scale 2000
agfusion annotate \
  -g5 FGFR2 \
  -g3 DNM3 \
  -j5 130167703 \
  -j3 162019992 \
  -db agfusion.mus_musculus.87.db \
  -o FGFR2-DNM3 \
  --recolor "Pkinase_Tyr;red" \
  --rename "Pkinase_Tyr;Kinase" \
  --scale 2000
```

![alt tag](https://github.com/murphycj/AGFusion/blob/master/doc/ENSMUST00000064477-ENSMUST00000002487-rescale.png)
![alt tag](https://github.com/murphycj/AGFusion/blob/master/doc/ENSMUST00000122054-ENSMUST00000070330-rescale.png)

# Installation

First you need to install pyensembl (and the other dependencies listed at the bottom) and download the reference genome you will use by running one of the following.

```
For GRCh38/hg38:
pyensembl install --species homo_sapiens --release 87

For GRCh37/hg19:
pyensembl install --species homo_sapiens --release 75

For GRCm38/mm10:
pyensembl install --species mus_musculus --release 87
```

Then you can install AGFusion:

```
pip install agfusion
```

Finally, download the AGFusion database for your reference genome (downloaded from [here](https://github.com/murphycj/AGFusionDB)).

```
For GRCh38/hg38:
agfusion download -g hg38

For GRCh37/hg19:
agfusion download -g hg19

For GRCm38/mm10:
agfusion download -g mm10
```

You can view all supported species and ensembl releases with ```agfusion download -a```. Due to limitations in pyensembl, the maximum supported Ensembl release is 87.

# Dependencies

- python 2.7, 3.5
- matplotlib>=1.5.0
- pandas>=0.18.1
- biopython>=1.67
- future>=0.16.0
- pyensembl>=1.1.0

# Troubleshooting

**Problem:** I get a warning message like the following:


> 2017-08-28 15:02:51,377 - AGFusion - WARNING - No cDNA sequence available for AC073283.4! Will not print cDNA sequence for the AC073283.4-MSH2 fusion. You might be working with an outdated pyensembl. Update the package and rerun 'pyensembl install'

**Solution:** Run the following to update pyensembl package and database:

```
git clone git@github.com:hammerlab/pyensembl.git
cd pyensembl
sudo pip install .
pyensembl install --release (your-release) --species (your-species)
```

# License

MIT license

# Citing AGFusion

You can cite bioRxiv: http://dx.doi.org/10.1101/080903
  * [aiodns-2.0.0](http://github.com/saghul/aiodns) ===============================
Simple DNS resolver for asyncio
===============================

.. image:: https://secure.travis-ci.org/saghul/aiodns.png?branch=master
    :target: http://travis-ci.org/saghul/aiodns

aiodns provides a simple way for doing asynchronous DNS resolutions using `pycares <https://github.com/saghul/pycares>`_.


Example
=======

::

    import asyncio
    import aiodns

    loop = asyncio.get_event_loop()
    resolver = aiodns.DNSResolver(loop=loop)

    async def query(name, query_type):
        return await resolver.query(name, query_type)

    coro = query('google.com', 'A')
    result = loop.run_until_complete(coro)


The following query types are supported: A, AAAA, ANY, CNAME, MX, NAPTR, NS, PTR, SOA, SRV, TXT.


API
===

The API is pretty simple, three functions are provided in the ``DNSResolver`` class:

* ``query(host, type)``: Do a DNS resolution of the given type for the given hostname. It returns an
  instance of ``asyncio.Future``. The actual result of the DNS query is taken directly from pycares.
  As of version 1.0.0 of aiodns (and pycares, for that matter) results are always namedtuple-like
  objects with different attributes. Please check `the documentation <http://pycares.readthedocs.org/en/latest/channel.html#pycares.Channel.query>`_
  for the result fields.
* ``gethostbyname(host, socket_family)``: Do a DNS resolution for the given
  hostname and the desired type of address family (i.e. ``socket.AF_INET``).
  While ``query()`` always performs a request to a DNS server,
  ``gethostbyname()`` first looks into ``/etc/hosts`` and thus can resolve
  local hostnames (such as ``localhost``).  Please check `the documentation
  <http://pycares.readthedocs.io/en/latest/channel.html#pycares.Channel.gethostbyname>`_
  for the result fields. The actual result of the call is a ``asyncio.Future``.
* ``gethostbyaddr(name)``: Make a reverse lookup for an address.
* ``cancel()``: Cancel all pending DNS queries. All futures will get ``DNSError`` exception set, with
  ``ARES_ECANCELLED`` errno.


Running the test suite
======================

To run the test suite: ``python tests.py``


Author
======

Saúl Ibarra Corretgé <s@saghul.net>


License
=======

aiodns uses the MIT license, check LICENSE file.


Python versions
===============

Python >= 3.5 are supported.


Contributing
============

If you'd like to contribute, fork the project, make a patch and send a pull
request. Have a look at the surrounding code and please, make yours look
alike :-)




  * [airr-1.2.1](http://docs.airr-community.org) AIRR Python Reference Library
===============================================================================

**Installation**

Install in the usual manner from PyPI::

    > pip3 install airr --user

Or from the `downloaded <https://github.com/airr-community/airr-standards>`__
source code directory::

    > python3 setup.py install --user

**Reading AIRR formatted files**

The ``airr`` package contains functions to read and write AIRR data files
as either iterables or pandas data frames. The usage is straightforward,
as the file format is a typical tab delimited file, but the package
performs some additional validation and type conversion beyond using a
standard CSV reader.

.. code-block:: python

    import airr

    # Create an iteratable that returns a dictionary for each row
    reader = airr.read_rearrangement('input.tsv')

    # Load the entire file into a pandas data frame
    df = airr.load_rearrangement('input.tsv')

**Writing AIRR formatted files**

Similar to the read operations, write functions are provided for either creating
a writer class to perform row-wise output or writing the entire contents of
a pandas data frame to a file. Again, usage is straightforward with the `airr`
output functions simply performing some type conversion and field ordering
operations.

.. code-block:: python

    import airr

    # Create a writer class for iterative row output
    writer = airr.create_rearrangement('output.tsv')
    for row in reader:  writer.write(row)

    # Write an entire pandas data frame to a file
    airr.dump_rearrangement(df, 'file.tsv')
  * [alabaster-0.7.12](https://alabaster.readthedocs.io) What is Alabaster?
==================

Alabaster is a visually (c)lean, responsive, configurable theme for the `Sphinx
<http://sphinx-doc.org>`_ documentation system. It is Python 2+3 compatible.

It began as a third-party theme, and is still maintained separately, but as of
Sphinx 1.3, Alabaster is an install-time dependency of Sphinx and is selected
as the default theme.

Live examples of this theme can be seen on `this project's own website
<http://alabaster.readthedocs.io>`_, `paramiko.org <http://paramiko.org>`_,
`fabfile.org <http://fabfile.org>`_ and `pyinvoke.org <http://pyinvoke.org>`_.

For more documentation, please see http://alabaster.readthedocs.io.

.. note::
    You can install the development version via ``pip install -e
    git+https://github.com/bitprophet/alabaster/#egg=alabaster``.



  * [alembic-1.0.11](https://alembic.sqlalchemy.org) Alembic is a database migrations tool written by the author
of `SQLAlchemy <http://www.sqlalchemy.org>`_.  A migrations tool
offers the following functionality:

* Can emit ALTER statements to a database in order to change
  the structure of tables and other constructs
* Provides a system whereby "migration scripts" may be constructed;
  each script indicates a particular series of steps that can "upgrade" a
  target database to a new version, and optionally a series of steps that can
  "downgrade" similarly, doing the same steps in reverse.
* Allows the scripts to execute in some sequential manner.

The goals of Alembic are:

* Very open ended and transparent configuration and operation.   A new
  Alembic environment is generated from a set of templates which is selected
  among a set of options when setup first occurs. The templates then deposit a
  series of scripts that define fully how database connectivity is established
  and how migration scripts are invoked; the migration scripts themselves are
  generated from a template within that series of scripts. The scripts can
  then be further customized to define exactly how databases will be
  interacted with and what structure new migration files should take.
* Full support for transactional DDL.   The default scripts ensure that all
  migrations occur within a transaction - for those databases which support
  this (Postgresql, Microsoft SQL Server), migrations can be tested with no
  need to manually undo changes upon failure.
* Minimalist script construction.  Basic operations like renaming
  tables/columns, adding/removing columns, changing column attributes can be
  performed through one line commands like alter_column(), rename_table(),
  add_constraint(). There is no need to recreate full SQLAlchemy Table
  structures for simple operations like these - the functions themselves
  generate minimalist schema structures behind the scenes to achieve the given
  DDL sequence.
* "auto generation" of migrations. While real world migrations are far more
  complex than what can be automatically determined, Alembic can still
  eliminate the initial grunt work in generating new migration directives
  from an altered schema.  The ``--autogenerate`` feature will inspect the
  current status of a database using SQLAlchemy's schema inspection
  capabilities, compare it to the current state of the database model as
  specified in Python, and generate a series of "candidate" migrations,
  rendering them into a new migration script as Python directives. The
  developer then edits the new file, adding additional directives and data
  migrations as needed, to produce a finished migration. Table and column
  level changes can be detected, with constraints and indexes to follow as
  well.
* Full support for migrations generated as SQL scripts.   Those of us who
  work in corporate environments know that direct access to DDL commands on a
  production database is a rare privilege, and DBAs want textual SQL scripts.
  Alembic's usage model and commands are oriented towards being able to run a
  series of migrations into a textual output file as easily as it runs them
  directly to a database. Care must be taken in this mode to not invoke other
  operations that rely upon in-memory SELECTs of rows - Alembic tries to
  provide helper constructs like bulk_insert() to help with data-oriented
  operations that are compatible with script-based DDL.
* Non-linear, dependency-graph versioning.   Scripts are given UUID
  identifiers similarly to a DVCS, and the linkage of one script to the next
  is achieved via human-editable markers within the scripts themselves.
  The structure of a set of migration files is considered as a
  directed-acyclic graph, meaning any migration file can be dependent
  on any other arbitrary set of migration files, or none at
  all.  Through this open-ended system, migration files can be organized
  into branches, multiple roots, and mergepoints, without restriction.
  Commands are provided to produce new branches, roots, and merges of
  branches automatically.
* Provide a library of ALTER constructs that can be used by any SQLAlchemy
  application. The DDL constructs build upon SQLAlchemy's own DDLElement base
  and can be used standalone by any application or script.
* At long last, bring SQLite and its inablity to ALTER things into the fold,
  but in such a way that SQLite's very special workflow needs are accommodated
  in an explicit way that makes the most of a bad situation, through the
  concept of a "batch" migration, where multiple changes to a table can
  be batched together to form a series of instructions for a single, subsequent
  "move-and-copy" workflow.   You can even use "move-and-copy" workflow for
  other databases, if you want to recreate a table in the background
  on a busy system.

Documentation and status of Alembic is at https://alembic.sqlalchemy.org/
  * [altair-3.1.0](http://altair-viz.github.io) # Altair <a href="https://altair-viz.github.io/"><img align="right" src="https://altair-viz.github.io/_static/altair-logo-light.png" height="50"></img></a>

[![build status](http://img.shields.io/travis/altair-viz/altair/master.svg?style=flat)](https://travis-ci.org/altair-viz/altair)
[![JOSS Paper](http://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](http://joss.theoj.org/papers/10.21105/joss.01057)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/altair-viz/altair_notebooks/master?urlpath=lab/tree/notebooks/Index.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altair-viz/altair_notebooks/blob/master/notebooks/Index.ipynb)

[http://altair-viz.github.io](http://altair-viz.github.io)

**Altair** is a declarative statistical visualization library for Python. With Altair, you can spend more time understanding your data and its meaning. Altair's
API is simple, friendly and consistent and built on top of the powerful
[Vega-Lite](https://github.com/vega/vega-lite) JSON specification. This elegant
simplicity produces beautiful and effective visualizations with a minimal amount of code. *Altair is developed by [Jake Vanderplas](https://github.com/jakevdp) and [Brian
Granger](https://github.com/ellisonbg) in close collaboration with the [UW
Interactive Data Lab](http://idl.cs.washington.edu/).*

## Altair Documentation

See [Altair's Documentation Site](http://altair-viz.github.io),
as well as Altair's [Tutorial Notebooks](http://github.com/altair-viz/altair_notebooks).

## Example

Here is an example using Altair to quickly visualize and display a dataset with the native Vega-Lite renderer in the JupyterLab:

```python
import altair as alt

# to use with Jupyter notebook (not JupyterLab) run the following
# alt.renderers.enable('notebook')

# load a simple dataset as a pandas DataFrame
from vega_datasets import data
cars = data.cars()

alt.Chart(cars).mark_point().encode(
    x='Horsepower',
    y='Miles_per_Gallon',
    color='Origin',
)
```

![Altair Visualization](https://raw.githubusercontent.com/altair-viz/altair/master/images/cars.png)

One of the unique features of Altair, inherited from Vega-Lite, is a declarative grammar of not just visualization, but _interaction_. 
With a few modifications to the example above we can created a linked histogram which is filtered based on a selection of the scatter plot.

```python 
import altair as alt
from vega_datasets import data

source = data.cars()

brush = alt.selection(type='interval')

points = alt.Chart(source).mark_point().encode(
    x='Horsepower',
    y='Miles_per_Gallon',
    color=alt.condition(brush, 'Origin', alt.value('lightgray'))
).add_selection(
    brush
)

bars = alt.Chart(source).mark_bar().encode(
    y='Origin',
    color='Origin',
    x='count(Origin)'
).transform_filter(
    brush
)

points & bars
```

![Altair Visualization Gif](https://raw.githubusercontent.com/altair-viz/altair/master/images/cars_scatter_bar.gif)


## Getting your Questions Answered

If you have a question that is not addressed in the documentation, there are several ways to ask:

- open a [Github Issue](https://github.com/altair-viz/altair/issues)
- post a [StackOverflow Question](https://stackoverflow.com/questions/tagged/altair) (be sure to use the `altair` tag)
- ask on the [Altair Google Group](https://groups.google.com/forum/#!forum/altair-viz)

We'll do our best to get your question answered

## A Python API for statistical visualizations

Altair provides a Python API for building statistical visualizations in a declarative
manner. By statistical visualization we mean:

* The **data source** is a `DataFrame` that consists of columns of different data types (quantitative, ordinal, nominal and date/time).
* The `DataFrame` is in a [tidy format](http://vita.had.co.nz/papers/tidy-data.pdf)
  where the rows correspond to samples and the columns correspond to the observed variables.
* The data is mapped to the **visual properties** (position, color, size, shape,
  faceting, etc.) using the group-by data transformation.

The Altair API contains no actual visualization rendering code but instead
emits JSON data structures following the
[Vega-Lite](https://github.com/vega/vega-lite) specification. The resulting
Vega-Lite JSON data can be rendered in the following user-interfaces:

* [Jupyter Notebook](https://github.com/jupyter/notebook) (by installing [ipyvega](https://github.com/vega/ipyvega)).
* [JupyterLab](https://github.com/jupyterlab/jupyterlab) (no additional dependencies needed).
* [nteract](https://github.com/nteract/nteract) (no additional dependencies needed).

## Features

* Carefully-designed, declarative Python API based on
  [traitlets](https://github.com/ipython/traitlets).
* Auto-generated internal Python API that guarantees visualizations are type-checked and
  in full conformance with the [Vega-Lite](https://github.com/vega/vega-lite)
  specification.
* Auto-generate Altair Python code from a Vega-Lite JSON spec.
* Display visualizations in the live Jupyter Notebook, JupyterLab, nteract, on GitHub and
  [nbviewer](http://nbviewer.jupyter.org/).
* Export visualizations to PNG/SVG images, stand-alone HTML pages and the
[Online Vega-Lite Editor](https://vega.github.io/editor/#/).
* Serialize visualizations as JSON files.
* Explore Altair with dozens of examples in the [Example Gallery](https://altair-viz.github.io/gallery/index.html)

## Installation

To use Altair for visualization, you need to install two sets of tools

1. The core Altair Package and its dependencies

2. The renderer for the frontend you wish to use (i.e. `Jupyter Notebook`,
   `JupyterLab`, or `nteract`)

Altair can be installed with either ``pip`` or with ``conda``.
For full installation instructions, please see
https://altair-viz.github.io/getting_started/installation.html

## Example and tutorial notebooks

We maintain a separate Github repository of Jupyter Notebooks that contain an
interactive tutorial and examples:

https://github.com/altair-viz/altair_notebooks

To launch a live notebook server with those notebook using [binder](https://mybinder.org/) or
[Colab](http://colab.research.google.com), click on one of the following badges:

[![Binder](https://beta.mybinder.org/badge.svg)](https://beta.mybinder.org/v2/gh/altair-viz/altair_notebooks/master)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altair-viz/altair_notebooks/blob/master/notebooks/Index.ipynb)

## Project philosophy

Many excellent plotting libraries exist in Python, including the main ones:

* [Matplotlib](http://matplotlib.org/)
* [Bokeh](http://bokeh.pydata.org/en/latest/)
* [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/#)
* [Lightning](http://lightning-viz.org/)
* [Plotly](https://plot.ly/)
* [Pandas built-in plotting](http://pandas.pydata.org/pandas-docs/stable/visualization.html)
* [HoloViews](http://holoviews.org)
* [VisPy](http://vispy.org/)
* [pygg](http://www.github.com/sirrice/pygg)

Each library does a particular set of things well.

### User challenges

However, such a proliferation of options creates great difficulty for users
as they have to wade through all of these APIs to find which of them is the
best for the task at hand. None of these libraries are optimized for
high-level statistical visualization, so users have to assemble their own
using a mishmash of APIs. For individuals just learning data science, this
forces them to focus on learning APIs rather than exploring their data.

Another challenge is current plotting APIs require the user to write code,
even for incidental details of a visualization. This results in unfortunate
and unnecessary cognitive burden as the visualization type (histogram,
scatterplot, etc.) can often be inferred using basic information such as the
columns of interest and the data types of those columns.

For example, if you are interested in a visualization of two numerical
columns, a scatterplot is almost certainly a good starting point. If you add
a categorical column to that, you probably want to encode that column using
colors or facets. If inferring the visualization proves difficult at times, a
simple user interface can construct a visualization without any coding.
[Tableau](http://www.tableau.com/) and the [Interactive Data
Lab's](http://idl.cs.washington.edu/)
[Polestar](https://github.com/vega/polestar) and
[Voyager](https://github.com/vega/voyager) are excellent examples of such UIs.

### Design approach and solution

We believe that these challenges can be addressed without the creation of yet
another visualization library that has a programmatic API and built-in
rendering. Altair's approach to building visualizations uses a layered design
that leverages the full capabilities of existing visualization libraries:

1. Create a constrained, simple Python API (Altair) that is purely declarative
2. Use the API (Altair) to emit JSON output that follows the Vega-Lite spec
3. Render that spec using existing visualization libraries

This approach enables users to perform exploratory visualizations with a much
simpler API initially, pick an appropriate renderer for their usage case, and
then leverage the full capabilities of that renderer for more advanced plot
customization.

We realize that a declarative API will necessarily be limited compared to the
full programmatic APIs of Matplotlib, Bokeh, etc. That is a deliberate design
choice we feel is needed to simplify the user experience of exploratory
visualization.

## Development install

Altair requires the following dependencies:

* [pandas](http://pandas.pydata.org/)
* [traitlets](https://github.com/ipython/traitlets)
* [IPython](https://github.com/ipython/ipython)

If you have cloned the repository, run the following command from the root of the repository:

```
pip install -e .[dev]
```

If you do not wish to clone the repository, you can install using:

```
pip install git+https://github.com/altair-viz/altair
```

## Testing

To run the test suite you must have [py.test](http://pytest.org/latest/) installed.
To run the tests, use

```
py.test --pyargs altair
```
(you can omit the `--pyargs` flag if you are running the tests from a source checkout).

## Feedback and Contribution

See [`CONTRIBUTING.md`](https://github.com/altair-viz/altair/blob/master/CONTRIBUTING.md)

## Citing Altair

[![JOSS Paper](http://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](http://joss.theoj.org/papers/10.21105/joss.01057)

If you use Altair in an academic work, please consider citing http://joss.theoj.org/papers/10.21105/joss.01057 as

```bib
@article{Altair2018,
  doi = {10.21105/joss.01057},
  url = {https://doi.org/10.21105/joss.01057},
  year  = {2018},
  month = {dec},
  publisher = {The Open Journal},
  author = {Jacob VanderPlas and Brian Granger and Jeffrey Heer and Dominik Moritz and Kanit Wongsuphasawat and Arvind Satyanarayan and Eitan Lees and Ilia Timofeev and Ben Welsh and Scott Sievert},
  title = {Altair: Interactive Statistical Visualizations for Python},
  journal = {Journal of Open Source Software}
}
```

## Whence Altair?

Altair is the [brightest star](https://en.wikipedia.org/wiki/Altair) in the constellation Aquila, and along with Deneb and Vega forms the northern-hemisphere asterism known as the [Summer Triangle](https://en.wikipedia.org/wiki/Summer_Triangle).



  * [altgraph-0.16.1](https://altgraph.readthedocs.io) altgraph is a fork of graphlib: a graph (network) package for constructing
graphs, BFS and DFS traversals, topological sort, shortest paths, etc. with
graphviz output.

altgraph includes some additional usage of Python 2.6+ features and
enhancements related to modulegraph and macholib.

Project links
-------------

* `Documentation <https://altgraph.readthedocs.io/en/latest/>`_

* `Issue Tracker <https://bitbucket.org/ronaldoussoren/altgraph/issues?status=new&status=open>`_

* `Repository <https://bitbucket.org/ronaldoussoren/altgraph/>`_


Release history
===============

0.16.1
------

Explicitly mark Python 3.7 as supported in wheel metadata.

0.16
----

* Add LICENSE file

0.15
----

* ``ObjectGraph.get_edges``, ``ObjectGraph.getEdgeData`` and ``ObjectGraph.updateEdgeData``
  accept *None* as the node to get and treat this as an alias for *self* (as other
  methods already did).

0.14
----

- Issue #7: Remove use of ``iteritems`` in altgraph.GraphAlgo code

0.13
----

- Issue #4: Graph._bfs_subgraph and back_bfs_subgraph return subgraphs with reversed edges

  Fix by "pombredanne" on bitbucket.


0.12
----

- Added ``ObjectGraph.edgeData`` to retrieve the edge data
  from a specific edge.

- Added ``AltGraph.update_edge_data`` and ``ObjectGraph.updateEdgeData``
  to update the data associated with a graph edge.

0.11
----

- Stabilize the order of elements in dot file exports,
  patch from bitbucket user 'pombredanne'.

- Tweak setup.py file to remove dependency on distribute (but
  keep the dependency on setuptools)


0.10.2
------

- There where no classifiers in the package metadata due to a bug
  in setup.py

0.10.1
------

This is a bugfix release

Bug fixes:

- Issue #3: The source archive contains a README.txt
  while the setup file refers to ReadMe.txt.

  This is caused by a misfeature in distutils, as a
  workaround I've renamed ReadMe.txt to README.txt
  in the source tree and setup file.


0.10
-----

This is a minor feature release

Features:

- Do not use "2to3" to support Python 3.

  As a side effect of this altgraph now supports
  Python 2.6 and later, and no longer supports
  earlier releases of Python.

- The order of attributes in the Dot output
  is now always alphabetical.

  With this change the output will be consistent
  between runs and Python versions.

0.9
---

This is a minor bugfix release

Features:

- Added ``altgraph.ObjectGraph.ObjectGraph.nodes``, a method
  yielding all nodes in an object graph.

Bugfixes:

- The 0.8 release didn't work with py2app when using
  python 3.x.


0.8
-----

This is a minor feature release. The major new feature
is a extensive set of unittests, which explains almost
all other changes in this release.

Bugfixes:

- Installing failed with Python 2.5 due to using a distutils
  class that isn't available in that version of Python
  (issue #1 on the issue tracker)

- ``altgraph.GraphStat.degree_dist`` now actually works

- ``altgraph.Graph.add_edge(a, b, create_nodes=False)`` will
  no longer create the edge when one of the nodes doesn't
  exist.

- ``altgraph.Graph.forw_topo_sort`` failed for some sparse graphs.

- ``altgraph.Graph.back_topo_sort`` was completely broken in
  previous releases.

- ``altgraph.Graph.forw_bfs_subgraph`` now actually works.

- ``altgraph.Graph.back_bfs_subgraph`` now actually works.

- ``altgraph.Graph.iterdfs`` now returns the correct result
  when the ``forward`` argument is ``False``.

- ``altgraph.Graph.iterdata`` now returns the correct result
  when the ``forward`` argument is ``False``.


Features:

- The ``altgraph.Graph`` constructor now accepts an argument
  that contains 2- and 3-tuples instead of requireing that
  all items have the same size. The (optional) argument can now
  also be any iterator.

- ``altgraph.Graph.Graph.add_node`` has no effect when you
  add a hidden node.

- The private method ``altgraph.Graph._bfs`` is no longer
  present.

- The private method ``altgraph.Graph._dfs`` is no longer
  present.

- ``altgraph.ObjectGraph`` now has a ``__contains__`` methods,
  which means you can use the ``in`` operator to check if a
  node is part of a graph.

- ``altgraph.GraphUtil.generate_random_graph`` will raise
  ``GraphError`` instead of looping forever when it is
  impossible to create the requested graph.

- ``altgraph.Dot.edge_style`` raises ``GraphError`` when
  one of the nodes is not present in the graph. The method
  silently added the tail in the past, but without ensuring
  a consistent graph state.

- ``altgraph.Dot.save_img`` now works when the mode is
  ``"neato"``.

0.7.2
-----

This is a minor bugfix release

Bugfixes:

- distutils didn't include the documentation subtree

0.7.1
-----

This is a minor feature release

Features:

- Documentation is now generated using `sphinx <http://pypi.python.org/pypi/sphinx>`_
  and can be viewed at <http://packages.python.org/altgraph>.

- The repository has moved to bitbucket

- ``altgraph.GraphStat.avg_hops`` is no longer present, the function had no
  implementation and no specified behaviour.

- the module ``altgraph.compat`` is gone, which means altgraph will no
  longer work with Python 2.3.


0.7.0
-----

This is a minor feature release.

Features:

- Support for Python 3

- It is now possible to run tests using 'python setup.py test'

  (The actual testsuite is still very minimal though)



  * [amqp-2.5.0](http://github.com/celery/py-amqp) =====================================================================
 Python AMQP 0.9.1 client library
=====================================================================

|build-status| |coverage| |license| |wheel| |pyversion| |pyimp|

:Version: 2.5.2
:Web: https://amqp.readthedocs.io/
:Download: https://pypi.org/project/amqp/
:Source: http://github.com/celery/py-amqp/
:Keywords: amqp, rabbitmq

About
=====

This is a fork of amqplib_ which was originally written by Barry Pederson.
It is maintained by the Celery_ project, and used by `kombu`_ as a pure python
alternative when `librabbitmq`_ is not available.

This library should be API compatible with `librabbitmq`_.

.. _amqplib: https://pypi.org/project/amqplib/
.. _Celery: http://celeryproject.org/
.. _kombu: https://kombu.readthedocs.io/
.. _librabbitmq: https://pypi.org/project/librabbitmq/

Differences from `amqplib`_
===========================

- Supports draining events from multiple channels (``Connection.drain_events``)
- Support for timeouts
- Channels are restored after channel error, instead of having to close the
  connection.
- Support for heartbeats

    - ``Connection.heartbeat_tick(rate=2)`` must called at regular intervals
      (half of the heartbeat value if rate is 2).
    - Or some other scheme by using ``Connection.send_heartbeat``.
- Supports RabbitMQ extensions:
    - Consumer Cancel Notifications
        - by default a cancel results in ``ChannelError`` being raised
        - but not if a ``on_cancel`` callback is passed to ``basic_consume``.
    - Publisher confirms
        - ``Channel.confirm_select()`` enables publisher confirms.
        - ``Channel.events['basic_ack'].append(my_callback)`` adds a callback
          to be called when a message is confirmed. This callback is then
          called with the signature ``(delivery_tag, multiple)``.
    - Exchange-to-exchange bindings: ``exchange_bind`` / ``exchange_unbind``.
        - ``Channel.confirm_select()`` enables publisher confirms.
        - ``Channel.events['basic_ack'].append(my_callback)`` adds a callback
          to be called when a message is confirmed. This callback is then
          called with the signature ``(delivery_tag, multiple)``.
    - Authentication Failure Notifications
        Instead of just closing the connection abruptly on invalid
        credentials, py-amqp will raise an ``AccessRefused`` error
        when connected to rabbitmq-server 3.2.0 or greater.
- Support for ``basic_return``
- Uses AMQP 0-9-1 instead of 0-8.
    - ``Channel.access_request`` and ``ticket`` arguments to methods
      **removed**.
    - Supports the ``arguments`` argument to ``basic_consume``.
    - ``internal`` argument to ``exchange_declare`` removed.
    - ``auto_delete`` argument to ``exchange_declare`` deprecated
    - ``insist`` argument to ``Connection`` removed.
    - ``Channel.alerts`` has been removed.
    - Support for ``Channel.basic_recover_async``.
    - ``Channel.basic_recover`` deprecated.
- Exceptions renamed to have idiomatic names:
    - ``AMQPException`` -> ``AMQPError``
    - ``AMQPConnectionException`` -> ConnectionError``
    - ``AMQPChannelException`` -> ChannelError``
    - ``Connection.known_hosts`` removed.
    - ``Connection`` no longer supports redirects.
    - ``exchange`` argument to ``queue_bind`` can now be empty
      to use the "default exchange".
- Adds ``Connection.is_alive`` that tries to detect
  whether the connection can still be used.
- Adds ``Connection.connection_errors`` and ``.channel_errors``,
  a list of recoverable errors.
- Exposes the underlying socket as ``Connection.sock``.
- Adds ``Channel.no_ack_consumers`` to keep track of consumer tags
  that set the no_ack flag.
- Slightly better at error recovery

Further
=======

- Differences between AMQP 0.8 and 0.9.1

    http://www.rabbitmq.com/amqp-0-8-to-0-9-1.html

- AMQP 0.9.1 Quick Reference

    http://www.rabbitmq.com/amqp-0-9-1-quickref.html

- RabbitMQ Extensions

    http://www.rabbitmq.com/extensions.html

- For more information about AMQP, visit

    http://www.amqp.org

- For other Python client libraries see:

    http://www.rabbitmq.com/devtools.html#python-dev

.. |build-status| image:: https://secure.travis-ci.org/celery/py-amqp.png?branch=master
    :alt: Build status
    :target: https://travis-ci.org/celery/py-amqp

.. |coverage| image:: https://codecov.io/github/celery/py-amqp/coverage.svg?branch=master
    :target: https://codecov.io/github/celery/py-amqp?branch=master

.. |license| image:: https://img.shields.io/pypi/l/amqp.svg
    :alt: BSD License
    :target: https://opensource.org/licenses/BSD-3-Clause

.. |wheel| image:: https://img.shields.io/pypi/wheel/amqp.svg
    :alt: Python AMQP can be installed via wheel
    :target: https://pypi.org/project/amqp/

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/amqp.svg
    :alt: Supported Python versions.
    :target: https://pypi.org/project/amqp/

.. |pyimp| image:: https://img.shields.io/pypi/implementation/amqp.svg
    :alt: Support Python implementations.
    :target: https://pypi.org/project/amqp/




  * [anndata-0.6.22.post1](http://github.com/theislab/anndata) |PyPI| |Docs| |Build Status| |Coverage|

.. |PyPI| image:: https://img.shields.io/pypi/v/anndata.svg
   :target: https://pypi.org/project/anndata
.. |Docs| image:: https://readthedocs.com/projects/icb-anndata/badge/?version=latest
   :target: https://anndata.readthedocs.io
.. |Build Status| image:: https://travis-ci.org/theislab/anndata.svg?branch=master
   :target: https://travis-ci.org/theislab/anndata
.. |Coverage| image:: https://codecov.io/gh/theislab/anndata/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/theislab/anndata

anndata - Annotated Data
========================

Install via ``pip install anndata`` or ``conda install anndata -c bioconda``.

Read the `documentation <https://anndata.readthedocs.io>`_.

.. would be nice to have http://falexwolf.de/img/scanpy/anndata.svg also on GitHub, but it's much too wide there;
.. GitHub doesn't plan to resolve scaling images: https://github.com/github/markup/issues/295
  * [annoy-1.16.0](https://github.com/spotify/annoy) .. note::

   For the latest source, discussion, etc, please visit the
   `GitHub repository <https://github.com/spotify/annoy>`_



.. image:: https://img.shields.io/github/stars/spotify/annoy.svg
    :target: https://github.com/spotify/annoy

Annoy
-----



.. figure:: https://raw.github.com/spotify/annoy/master/ann.png
   :alt: Annoy example
   :align: center

.. image:: https://img.shields.io/travis/spotify/annoy/master.svg?style=flat
    :target: https://travis-ci.org/spotify/annoy

.. image:: https://ci.appveyor.com/api/projects/status/github/spotify/annoy?svg=true&pendingText=windows%20-%20Pending&passingText=windows%20-%20OK&failingText=windows%20-%20Failing
    :target: https://ci.appveyor.com/project/erikbern/annoy

.. image:: https://img.shields.io/pypi/v/annoy.svg?style=flat
   :target: https://pypi.python.org/pypi/annoy

Annoy (`Approximate Nearest Neighbors <http://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor>`__ Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are `mmapped <https://en.wikipedia.org/wiki/Mmap>`__ into memory so that many processes may share the same data.

Install
-------

To install, simply do ``pip install --user annoy`` to pull down the latest version from `PyPI <https://pypi.python.org/pypi/annoy>`_.

For the C++ version, just clone the repo and ``#include "annoylib.h"``.

Background
----------

There are some other libraries to do nearest neighbor search. Annoy is almost as fast as the fastest libraries, (see below), but there is actually another feature that really sets Annoy apart: it has the ability to **use static files as indexes**. In particular, this means you can **share index across processes**. Annoy also decouples creating indexes from loading them, so you can pass around indexes as files and map them into memory quickly. Another nice thing of Annoy is that it tries to minimize memory footprint so the indexes are quite small.

Why is this useful? If you want to find nearest neighbors and you have many CPU's, you only need to build the index once. You can also pass around and distribute static files to use in production environment, in Hadoop jobs, etc. Any process will be able to load (mmap) the index into memory and will be able to do lookups immediately.

We use it at `Spotify <http://www.spotify.com/>`__ for music recommendations. After running matrix factorization algorithms, every user/item can be represented as a vector in f-dimensional space. This library helps us search for similar users/items. We have many millions of tracks in a high-dimensional space, so memory usage is a prime concern.

Annoy was built by `Erik Bernhardsson <http://www.erikbern.com>`__ in a couple of afternoons during `Hack Week <http://labs.spotify.com/2013/02/15/organizing-a-hack-week/>`__.

Summary of features
-------------------

* `Euclidean distance <https://en.wikipedia.org/wiki/Euclidean_distance>`__, `Manhattan distance <https://en.wikipedia.org/wiki/Taxicab_geometry>`__, `cosine distance <https://en.wikipedia.org/wiki/Cosine_similarity>`__, `Hamming distance <https://en.wikipedia.org/wiki/Hamming_distance>`__, or `Dot (Inner) Product distance <https://en.wikipedia.org/wiki/Dot_product>`__
* Cosine distance is equivalent to Euclidean distance of normalized vectors = sqrt(2-2*cos(u, v))
* Works better if you don't have too many dimensions (like <100) but seems to perform surprisingly well even up to 1,000 dimensions
* Small memory usage
* Lets you share memory between multiple processes
* Index creation is separate from lookup (in particular you can not add more items once the tree has been created)
* Native Python support, tested with 2.7, 3.6, and 3.7.
* Build index on disk to enable indexing big datasets that won't fit into memory (contributed by `Rene Hollander <https://github.com/ReneHollander>`__)

Python code example
-------------------

.. code-block:: python

  from annoy import AnnoyIndex
  import random

  f = 40
  t = AnnoyIndex(f)  # Length of item vector that will be indexed
  for i in xrange(1000):
      v = [random.gauss(0, 1) for z in xrange(f)]
      t.add_item(i, v)

  t.build(10) # 10 trees
  t.save('test.ann')

  # ...

  u = AnnoyIndex(f)
  u.load('test.ann') # super fast, will just mmap the file
  print(u.get_nns_by_item(0, 1000)) # will find the 1000 nearest neighbors

Right now it only accepts integers as identifiers for items. Note that it will allocate memory for max(id)+1 items because it assumes your items are numbered 0 … n-1. If you need other id's, you will have to keep track of a map yourself.

Full Python API
---------------

* ``AnnoyIndex(f, metric='angular')`` returns a new index that's read-write and stores vector of ``f`` dimensions. Metric can be ``"angular"``, ``"euclidean"``, ``"manhattan"``, ``"hamming"``, or ``"dot"``.
* ``a.add_item(i, v)`` adds item ``i`` (any nonnegative integer) with vector ``v``. Note that it will allocate memory for ``max(i)+1`` items.
* ``a.build(n_trees)`` builds a forest of ``n_trees`` trees. More trees gives higher precision when querying. After calling ``build``, no more items can be added.
* ``a.save(fn, prefault=False)`` saves the index to disk and loads it (see next function). After saving, no more items can be added.
* ``a.load(fn, prefault=False)`` loads (mmaps) an index from disk. If `prefault` is set to `True`, it will pre-read the entire file into memory (using mmap with `MAP_POPULATE`). Default is `False`.
* ``a.unload()`` unloads.
* ``a.get_nns_by_item(i, n, search_k=-1, include_distances=False)`` returns the ``n`` closest items. During the query it will inspect up to ``search_k`` nodes which defaults to ``n_trees * n`` if not provided. ``search_k`` gives you a run-time tradeoff between better accuracy and speed. If you set ``include_distances`` to ``True``, it will return a 2 element tuple with two lists in it: the second one containing all corresponding distances.
* ``a.get_nns_by_vector(v, n, search_k=-1, include_distances=False)`` same but query by vector ``v``.
* ``a.get_item_vector(i)`` returns the vector for item ``i`` that was previously added.
* ``a.get_distance(i, j)`` returns the distance between items ``i`` and ``j``. NOTE: this used to return the *squared* distance, but has been changed as of Aug 2016.
* ``a.get_n_items()`` returns the number of items in the index.
* ``a.get_n_trees()`` returns the number of trees in the index.
* ``a.on_disk_build(fn)`` prepares annoy to build the index in the specified file instead of RAM (execute before adding items, no need to save after build)

Notes:

* There's no bounds checking performed on the values so be careful.
* Annoy uses Euclidean distance of normalized vectors for its angular distance, which for two vectors u,v is equal to ``sqrt(2(1-cos(u,v)))``


The C++ API is very similar: just ``#include "annoylib.h"`` to get access to it.

Tradeoffs
---------

There are just two main parameters needed to tune Annoy: the number of trees ``n_trees`` and the number of nodes to inspect during searching ``search_k``.

* ``n_trees`` is provided during build time and affects the build time and the index size. A larger value will give more accurate results, but larger indexes.
* ``search_k`` is provided in runtime and affects the search performance. A larger value will give more accurate results, but will take longer time to return.

If ``search_k`` is not provided, it will default to ``n * n_trees * D`` where ``n`` is the number of approximate nearest neighbors and ``D`` is a constant depending on the metric. Otherwise, ``search_k`` and ``n_trees`` are roughly independent, i.e. a the value of ``n_trees`` will not affect search time if ``search_k`` is held constant and vice versa. Basically it's recommended to set ``n_trees`` as large as possible given the amount of memory you can afford, and it's recommended to set ``search_k`` as large as possible given the time constraints you have for the queries.

You can also accept slower search times in favour of reduced loading times, memory usage, and disk IO. On supported platforms the index is prefaulted during ``load`` and ``save``, causing the file to be pre-emptively read from disk into memory. If you set ``prefault`` to ``False``, pages of the mmapped index are instead read from disk and cached in memory on-demand, as necessary for a search to complete. This can significantly increase early search times but may be better suited for systems with low memory compared to index size, when few queries are executed against a loaded index, and/or when large areas of the index are unlikely to be relevant to search queries.


How does it work
----------------

Using `random projections <http://en.wikipedia.org/wiki/Locality-sensitive_hashing#Random_projection>`__ and by building up a tree. At every intermediate node in the tree, a random hyperplane is chosen, which divides the space into two subspaces. This hyperplane is chosen by sampling two points from the subset and taking the hyperplane equidistant from them.

We do this k times so that we get a forest of trees. k has to be tuned to your need, by looking at what tradeoff you have between precision and performance.

Hamming distance (contributed by `Martin Aumüller <https://github.com/maumueller>`__) packs the data into 64-bit integers under the hood and uses built-in bit count primitives so it could be quite fast. All splits are axis-aligned.

Dot Product distance (contributed by `Peter Sobot <https://github.com/psobot>`__) reduces the provided vectors from dot (or "inner-product") space to a more query-friendly cosine space using `a method by Bachrach et al., at Microsoft Research, published in 2014 <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/XboxInnerProduct.pdf>`__.



More info
---------

* `Dirk Eddelbuettel <https://github.com/eddelbuettel>`__ provides an `R version of Annoy <http://dirk.eddelbuettel.com/code/rcpp.annoy.html>`__.
* `Andy Sloane <https://github.com/a1k0n>`__ provides a `Java version of Annoy <https://github.com/spotify/annoy-java>`__ although currently limited to cosine and read-only.
* `Pishen Tsai <https://github.com/pishen>`__ provides a `Scala wrapper of Annoy <https://github.com/pishen/annoy4s>`__ which uses JNA to call the C++ library of Annoy.
* There is `experimental support for Go <https://github.com/spotify/annoy/blob/master/README_GO.rst>`__ provided by `Taneli Leppä <https://github.com/rosmo>`__.
* `Boris Nagaev <https://github.com/starius>`__ wrote `Lua bindings <https://github.com/spotify/annoy/blob/master/README_Lua.md>`__.
* During part of Spotify Hack Week 2016 (and a bit afterward), `Jim Kang <https://github.com/jimkang>`__ wrote `Node bindings <https://github.com/jimkang/annoy-node>`__ for Annoy.
* `Min-Seok Kim <https://github.com/mskimm>`__ built a `Scala version <https://github.com/mskimm/ann4s>`__ of Annoy.
* `Presentation from New York Machine Learning meetup <http://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup>`__ about Annoy
* Radim Řehůřek's blog posts comparing Annoy to a couple of other similar Python libraries: `Intro <http://radimrehurek.com/2013/11/performance-shootout-of-nearest-neighbours-intro/>`__, `Contestants <http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/>`__, `Querying <http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/>`__
* `ann-benchmarks <https://github.com/erikbern/ann-benchmarks>`__ is a benchmark for several approximate nearest neighbor libraries. Annoy seems to be fairly competitive, especially at higher precisions:

.. figure:: https://github.com/erikbern/ann-benchmarks/raw/master/results/glove-100-angular.png
   :alt: ANN benchmarks
   :align: center
   :target: https://github.com/erikbern/ann-benchmarks

Source code
-----------

It's all written in C++ with a handful of ugly optimizations for performance and memory usage. You have been warned :)

The code should support Windows, thanks to `Qiang Kou <https://github.com/thirdwing>`__ and `Timothy Riley <https://github.com/tjrileywisc>`__.

To run the tests, execute `python setup.py nosetests`. The test suite includes a big real world dataset that is downloaded from the internet, so it will take a few minutes to execute.

Discuss
-------

Feel free to post any questions or comments to the `annoy-user <https://groups.google.com/group/annoy-user>`__ group. I'm `@fulhack <https://twitter.com/fulhack>`__ on Twitter.



  * [ansible-2.8.4](https://ansible.com/) |PyPI version| |Docs badge| |Chat badge| |Build Status| |Code Of Conduct| |Mailing Lists| |License|

*******
Ansible
*******

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on `the Ansible website <https://ansible.com/>`_.

Design Principles
=================

*  Have a dead simple setup process and a minimal learning curve.
*  Manage machines very quickly and in parallel.
*  Avoid custom-agents and additional open ports, be agentless by
   leveraging the existing SSH daemon.
*  Describe infrastructure in a language that is both machine and human
   friendly.
*  Focus on security and easy auditability/review/rewriting of content.
*  Manage new remote machines instantly, without bootstrapping any
   software.
*  Allow module development in any dynamic language, not just Python.
*  Be usable as non-root.
*  Be the easiest IT automation system to use, ever.

Use Ansible
===========

You can install a released version of Ansible via ``pip``, a package manager, or
our `release repository <https://releases.ansible.com/ansible/>`_. See our
`installation guide <https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html>`_ for details on installing Ansible
on a variety of platforms.

Red Hat offers supported builds of `Ansible Engine <https://www.ansible.com/ansible-engine>`_.

Power users and developers can run the ``devel`` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the ``devel`` branch. We recommend getting involved
in the Ansible community if you want to run the ``devel`` branch.

Get Involved
============

*  Read `Community
   Information <https://docs.ansible.com/ansible/latest/community>`_ for all
   kinds of ways to contribute to and interact with the project,
   including mailing list information and how to submit bug reports and
   code to Ansible.
*  Join a `Working Group
   <https://github.com/ansible/community/wiki>`_, an organized community devoted to a specific technology domain or platform.
*  Submit proposed a code update through a pull request to the ``devel`` branch.
*  Talk to us before making larger changes
   to avoid duplicate efforts. This not only helps everyone
   know what is going on, it also helps save time and effort if we decide
   some changes are needed.
*  For a list of email lists, IRC channels and Working Groups, see the
   `Communication page <https://docs.ansible.com/ansible/latest/community/communication.html>`_

Branch Info
===========

*  The ``devel`` branch corresponds to the release actively under development.
*  The ``stable-2.X`` branches correspond to stable releases.
*  For information about the active branches see the
   `Ansible release and maintenance <https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html>`_ page.

Roadmap
=======

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The `Ansible Roadmap page <https://docs.ansible.com/ansible/devel/roadmap/>`_ details what is planned and how to influence the roadmap.

Authors
=======

Ansible was created by `Michael DeHaan <https://github.com/mpdehaan>`_
and has contributions from over 4000 users (and growing). Thanks everyone!

`Ansible <https://www.ansible.com>`_ is sponsored by `Red Hat, Inc.
<https://www.redhat.com>`_

License
=======

GNU General Public License v3.0

See `COPYING <COPYING>`_ to see the full text.

.. |PyPI version| image:: https://img.shields.io/pypi/v/ansible.svg
   :target: https://pypi.org/project/ansible
.. |Docs badge| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg
   :target: https://docs.ansible.com/ansible/latest/
.. |Build Status| image:: https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel
   :target: https://app.shippable.com/projects/573f79d02a8192902e20e34b
.. |Chat badge| image:: https://img.shields.io/badge/chat-IRC-brightgreen.svg
   :target: https://docs.ansible.com/ansible/latest/community/communication.html
.. |Code Of Conduct| image:: https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg
   :target: https://docs.ansible.com/ansible/latest/community/code_of_conduct.html
   :alt: Ansible Code of Conduct
.. |Mailing Lists| image:: https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg
   :target: https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information
   :alt: Ansible mailing lists
.. |License| image:: https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg
   :target: COPYING
   :alt: Repository License

  * [apipkg-1.5](https://github.com/pytest-dev/apipkg) Welcome to apipkg!
------------------------

With apipkg you can control the exported namespace of a Python package and
greatly reduce the number of imports for your users.
It is a `small pure Python module`_ that works on CPython 2.7 and 3.4+,
Jython and PyPy. It cooperates well with Python's ``help()`` system,
custom importers (PEP302) and common command-line completion tools.

Usage is very simple: you can require 'apipkg' as a dependency or you
can copy paste the ~200 lines of code into your project.


Tutorial example
-------------------

Here is a simple ``mypkg`` package that specifies one namespace
and exports two objects imported from different modules::

    # mypkg/__init__.py
    import apipkg
    apipkg.initpkg(__name__, {
        'path': {
            'Class1': "_mypkg.somemodule:Class1",
            'clsattr': "_mypkg.othermodule:Class2.attr",
        }
    }

The package is initialized with a dictionary as namespace.

You need to create a ``_mypkg`` package with a ``somemodule.py``
and ``othermodule.py`` containing the respective classes.
The ``_mypkg`` is not special - it's a completely
regular Python package.

Namespace dictionaries contain ``name: value`` mappings
where the value may be another namespace dictionary or
a string specifying an import location.  On accessing
an namespace attribute an import will be performed::

    >>> import mypkg
    >>> mypkg.path
    <ApiModule 'mypkg.path'>
    >>> mypkg.path.Class1   # '_mypkg.somemodule' gets imported now
    <class _mypkg.somemodule.Class1 at 0xb7d428fc>
    >>> mypkg.path.clsattr  # '_mypkg.othermodule' gets imported now
    4 # the value of _mypkg.othermodule.Class2.attr

The ``mypkg.path`` namespace and its two entries are
loaded when they are accessed.   This means:

* lazy loading - only what is actually needed is ever loaded

* only the root "mypkg" ever needs to be imported to get
  access to the complete functionality

* the underlying modules are also accessible, for example::

    from mypkg.sub import Class1


Including apipkg in your package
--------------------------------------

If you don't want to add an ``apipkg`` dependency to your package you
can copy the `apipkg.py`_ file somewhere to your own package,
for example ``_mypkg/apipkg.py`` in the above example.  You
then import the ``initpkg`` function from that new place and
are good to go.

.. _`small pure Python module`:
.. _`apipkg.py`: https://github.com/pytest-dev/apipkg/blob/master/src/apipkg/__init__.py

Feedback?
-----------------------

If you have questions you are welcome to

* join the #pylib channel on irc.freenode.net
* create an issue on https://github.com/pytest-dev/apipkg/issues

have fun,
holger krekel



  * [apiwrapper-0.1.8](https://github.com/ardydedase/apiwrapper) =================================
API Wrapper
=================================

.. image:: https://api.travis-ci.org/ardydedase/apiwrapper.svg
        :target: https://travis-ci.org/ardydedase/apiwrapper

.. image:: https://img.shields.io/pypi/v/apiwrapper.svg
        :target: https://pypi.python.org/pypi/apiwrapper

.. image:: https://readthedocs.org/projects/apiwrapper/badge/?version=latest
        :target: https://readthedocs.org/projects/apiwrapper/?badge=latest
        :alt: Documentation Status

Simple API Wrapper

* Free software: BSD license
* Documentation: https://apiwrapper.readthedocs.org.

Overview
--------

Recently noticed a pattern and repeated pieces of code in Python API wrappers for simple requests and polling. A separate Python package will minimize code duplication and encourage de-coupling of logic from the API request functions.

Installation
------------

At the command line::

    $ easy_install apiwrapper

Or, if you have virtualenvwrapper installed::

    $ mkvirtualenv apiwrapper
    $ pip install apiwrapper

Getting started with a simple request
-------------------------------------

.. code:: python

    # as a helper class
    from apiwrapper import APIWrapper

    my_api = APIWrapper()
    url = 'https://api.github.com/users/ardydedase/repos'
    resp = my_api.make_request(url=url)
    print(resp)

    # as a parent class
    class GithubAPI(APIWrapper):
        def get_repos(self, username):
            """
            Uses `make_request` method              
            """
            url = "https://api.github.com/users/{username}/repos".format(username=username)
            return self.make_request(url, method='get', headers=None, data=None, callback=None)

More features including polling
-------------------------------

Read the docs: https://apiwrapper.readthedocs.org/en/latest/usage.html

Or use `apiwrapper/apiwrapper.py` as a reference. Implementation is straightforward.




History
-------

0.1.0 (2015-01-11)
---------------------

* First release on PyPI.
  * [appdirs-1.4.3](http://github.com/ActiveState/appdirs) 
.. image:: https://secure.travis-ci.org/ActiveState/appdirs.png
    :target: http://travis-ci.org/ActiveState/appdirs

the problem
===========

What directory should your app use for storing user data? If running on Mac OS X, you
should use::

    ~/Library/Application Support/<AppName>

If on Windows (at least English Win XP) that should be::

    C:\Documents and Settings\<User>\Application Data\Local Settings\<AppAuthor>\<AppName>

or possibly::

    C:\Documents and Settings\<User>\Application Data\<AppAuthor>\<AppName>

for `roaming profiles <http://bit.ly/9yl3b6>`_ but that is another story.

On Linux (and other Unices) the dir, according to the `XDG
spec <http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html>`_, is::

    ~/.local/share/<AppName>


``appdirs`` to the rescue
=========================

This kind of thing is what the ``appdirs`` module is for. ``appdirs`` will
help you choose an appropriate:

- user data dir (``user_data_dir``)
- user config dir (``user_config_dir``)
- user cache dir (``user_cache_dir``)
- site data dir (``site_data_dir``)
- site config dir (``site_config_dir``)
- user log dir (``user_log_dir``)

and also:

- is a single module so other Python packages can include their own private copy
- is slightly opinionated on the directory names used. Look for "OPINION" in
  documentation and code for when an opinion is being applied.


some example output
===================

On Mac OS X::

    >>> from appdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    '/Users/trentm/Library/Application Support/SuperApp'
    >>> site_data_dir(appname, appauthor)
    '/Library/Application Support/SuperApp'
    >>> user_cache_dir(appname, appauthor)
    '/Users/trentm/Library/Caches/SuperApp'
    >>> user_log_dir(appname, appauthor)
    '/Users/trentm/Library/Logs/SuperApp'

On Windows 7::

    >>> from appdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    'C:\\Users\\trentm\\AppData\\Local\\Acme\\SuperApp'
    >>> user_data_dir(appname, appauthor, roaming=True)
    'C:\\Users\\trentm\\AppData\\Roaming\\Acme\\SuperApp'
    >>> user_cache_dir(appname, appauthor)
    'C:\\Users\\trentm\\AppData\\Local\\Acme\\SuperApp\\Cache'
    >>> user_log_dir(appname, appauthor)
    'C:\\Users\\trentm\\AppData\\Local\\Acme\\SuperApp\\Logs'

On Linux::

    >>> from appdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    '/home/trentm/.local/share/SuperApp
    >>> site_data_dir(appname, appauthor)
    '/usr/local/share/SuperApp'
    >>> site_data_dir(appname, appauthor, multipath=True)
    '/usr/local/share/SuperApp:/usr/share/SuperApp'
    >>> user_cache_dir(appname, appauthor)
    '/home/trentm/.cache/SuperApp'
    >>> user_log_dir(appname, appauthor)
    '/home/trentm/.cache/SuperApp/log'
    >>> user_config_dir(appname)
    '/home/trentm/.config/SuperApp'
    >>> site_config_dir(appname)
    '/etc/xdg/SuperApp'
    >>> os.environ['XDG_CONFIG_DIRS'] = '/etc:/usr/local/etc'
    >>> site_config_dir(appname, multipath=True)
    '/etc/SuperApp:/usr/local/etc/SuperApp'


``AppDirs`` for convenience
===========================

::

    >>> from appdirs import AppDirs
    >>> dirs = AppDirs("SuperApp", "Acme")
    >>> dirs.user_data_dir
    '/Users/trentm/Library/Application Support/SuperApp'
    >>> dirs.site_data_dir
    '/Library/Application Support/SuperApp'
    >>> dirs.user_cache_dir
    '/Users/trentm/Library/Caches/SuperApp'
    >>> dirs.user_log_dir
    '/Users/trentm/Library/Logs/SuperApp'



Per-version isolation
=====================

If you have multiple versions of your app in use that you want to be
able to run side-by-side, then you may want version-isolation for these
dirs::

    >>> from appdirs import AppDirs
    >>> dirs = AppDirs("SuperApp", "Acme", version="1.0")
    >>> dirs.user_data_dir
    '/Users/trentm/Library/Application Support/SuperApp/1.0'
    >>> dirs.site_data_dir
    '/Library/Application Support/SuperApp/1.0'
    >>> dirs.user_cache_dir
    '/Users/trentm/Library/Caches/SuperApp/1.0'
    >>> dirs.user_log_dir
    '/Users/trentm/Library/Logs/SuperApp/1.0'



appdirs Changelog
=================

appdirs 1.4.3
-------------
- [PR #76] Python 3.6 invalid escape sequence deprecation fixes
- Fix for Python 3.6 support

appdirs 1.4.2
-------------
- [PR #84] Allow installing without setuptools
- [PR #86] Fix string delimiters in setup.py description
- Add Python 3.6 support

appdirs 1.4.1
-------------
- [issue #38] Fix _winreg import on Windows Py3
- [issue #55] Make appname optional

appdirs 1.4.0
-------------
- [PR #42] AppAuthor is now optional on Windows
- [issue 41] Support Jython on Windows, Mac, and Unix-like platforms. Windows
  support requires `JNA <https://github.com/twall/jna>`_.
- [PR #44] Fix incorrect behaviour of the site_config_dir method

appdirs 1.3.0
-------------
- [Unix, issue 16] Conform to XDG standard, instead of breaking it for
  everybody
- [Unix] Removes gratuitous case mangling of the case, since \*nix-es are
  usually case sensitive, so mangling is not wise
- [Unix] Fixes the utterly wrong behaviour in ``site_data_dir``, return result
  based on XDG_DATA_DIRS and make room for respecting the standard which
  specifies XDG_DATA_DIRS is a multiple-value variable
- [Issue 6] Add ``*_config_dir`` which are distinct on nix-es, according to
  XDG specs; on Windows and Mac return the corresponding ``*_data_dir``

appdirs 1.2.0
-------------

- [Unix] Put ``user_log_dir`` under the *cache* dir on Unix. Seems to be more
  typical.
- [issue 9] Make ``unicode`` work on py3k.

appdirs 1.1.0
-------------

- [issue 4] Add ``AppDirs.user_log_dir``.
- [Unix, issue 2, issue 7] appdirs now conforms to `XDG base directory spec
  <http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html>`_.
- [Mac, issue 5] Fix ``site_data_dir()`` on Mac.
- [Mac] Drop use of 'Carbon' module in favour of hardcoded paths; supports
  Python3 now.
- [Windows] Append "Cache" to ``user_cache_dir`` on Windows by default. Use
  ``opinion=False`` option to disable this.
- Add ``appdirs.AppDirs`` convenience class. Usage:

        >>> dirs = AppDirs("SuperApp", "Acme", version="1.0")
        >>> dirs.user_data_dir
        '/Users/trentm/Library/Application Support/SuperApp/1.0'

- [Windows] Cherry-pick Komodo's change to downgrade paths to the Windows short
  paths if there are high bit chars.
- [Linux] Change default ``user_cache_dir()`` on Linux to be singular, e.g.
  "~/.superapp/cache".
- [Windows] Add ``roaming`` option to ``user_data_dir()`` (for use on Windows only)
  and change the default ``user_data_dir`` behaviour to use a *non*-roaming
  profile dir (``CSIDL_LOCAL_APPDATA`` instead of ``CSIDL_APPDATA``). Why? Because
  a large roaming profile can cause login speed issues. The "only syncs on
  logout" behaviour can cause surprises in appdata info.


appdirs 1.0.1 (never released)
------------------------------

Started this changelog 27 July 2010. Before that this module originated in the
`Komodo <http://www.activestate.com/komodo>`_ product as ``applib.py`` and then
as `applib/location.py
<http://github.com/ActiveState/applib/blob/master/applib/location.py>`_ (used by
`PyPM <http://code.activestate.com/pypm/>`_ in `ActivePython
<http://www.activestate.com/activepython>`_). This is basically a fork of
applib.py 1.0.1 and applib/location.py 1.0.1.




  * [argcomplete-1.10.0](https://github.com/kislyuk/argcomplete) argcomplete - Bash tab completion for argparse
==============================================
*Tab complete all the things!*

Argcomplete provides easy, extensible command line tab completion of arguments for your Python script.

It makes two assumptions:

* You're using bash as your shell (limited support for zsh and tcsh is available)
* You're using `argparse <http://docs.python.org/3/library/argparse.html>`_ to manage your command line arguments/options

Argcomplete is particularly useful if your program has lots of options or subparsers, and if your program can
dynamically suggest completions for your argument/option values (for example, if the user is browsing resources over
the network).

Installation
------------
::

    pip install argcomplete
    activate-global-python-argcomplete

See `Activating global completion`_ below for details about the second step (or if it reports an error).

Refresh your bash environment (start a new shell or ``source /etc/profile``).

Synopsis
--------
Python code (e.g. ``my-awesome-script``):

.. code-block:: python

    #!/usr/bin/env python
    # PYTHON_ARGCOMPLETE_OK
    import argcomplete, argparse
    parser = argparse.ArgumentParser()
    ...
    argcomplete.autocomplete(parser)
    args = parser.parse_args()
    ...

Shellcode (only necessary if global completion is not activated - see `Global completion`_ below), to be put in e.g. ``.bashrc``::

    eval "$(register-python-argcomplete my-awesome-script)"

argcomplete.autocomplete(*parser*)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This method is the entry point to the module. It must be called **after** ArgumentParser construction is complete, but
**before** the ``ArgumentParser.parse_args()`` method is called. The method looks for an environment variable that the
completion hook shellcode sets, and if it's there, collects completions, prints them to the output stream (fd 8 by
default), and exits. Otherwise, it returns to the caller immediately.

.. admonition:: Side effects

 Argcomplete gets completions by running your program. It intercepts the execution flow at the moment
 ``argcomplete.autocomplete()`` is called. After sending completions, it exits using ``exit_method`` (``os._exit``
 by default). This means if your program has any side effects that happen before ``argcomplete`` is called, those
 side effects will happen every time the user presses ``<TAB>`` (although anything your program prints to stdout or
 stderr will be suppressed). For this reason it's best to construct the argument parser and call
 ``argcomplete.autocomplete()`` as early as possible in your execution flow.

.. admonition:: Performance

 If the program takes a long time to get to the point where ``argcomplete.autocomplete()`` is called, the tab completion
 process will feel sluggish, and the user may lose confidence in it. So it's also important to minimize the startup time
 of the program up to that point (for example, by deferring initialization or importing of large modules until after
 parsing options).

Specifying completers
---------------------
You can specify custom completion functions for your options and arguments. Two styles are supported: callable and
readline-style. Callable completers are simpler. They are called with the following keyword arguments:

* ``prefix``: The prefix text of the last word before the cursor on the command line.
  For dynamic completers, this can be used to reduce the work required to generate possible completions.
* ``action``: The ``argparse.Action`` instance that this completer was called for.
* ``parser``: The ``argparse.ArgumentParser`` instance that the action was taken by.
* ``parsed_args``: The result of argument parsing so far (the ``argparse.Namespace`` args object normally returned by
  ``ArgumentParser.parse_args()``).

Completers should return their completions as a list of strings. An example completer for names of environment
variables might look like this:

.. code-block:: python

    def EnvironCompleter(**kwargs):
        return os.environ

To specify a completer for an argument or option, set the ``completer`` attribute of its associated action. An easy
way to do this at definition time is:

.. code-block:: python

    from argcomplete.completers import EnvironCompleter

    parser = argparse.ArgumentParser()
    parser.add_argument("--env-var1").completer = EnvironCompleter
    parser.add_argument("--env-var2").completer = EnvironCompleter
    argcomplete.autocomplete(parser)

If you specify the ``choices`` keyword for an argparse option or argument (and don't specify a completer), it will be
used for completions.

A completer that is initialized with a set of all possible choices of values for its action might look like this:

.. code-block:: python

    class ChoicesCompleter(object):
        def __init__(self, choices):
            self.choices = choices

        def __call__(self, **kwargs):
            return self.choices

The following two ways to specify a static set of choices are equivalent for completion purposes:

.. code-block:: python

    from argcomplete.completers import ChoicesCompleter

    parser.add_argument("--protocol", choices=('http', 'https', 'ssh', 'rsync', 'wss'))
    parser.add_argument("--proto").completer=ChoicesCompleter(('http', 'https', 'ssh', 'rsync', 'wss'))

Note that if you use the ``choices=<completions>`` option, argparse will show
all these choices in the ``--help`` output by default. To prevent this, set
``metavar`` (like ``parser.add_argument("--protocol", metavar="PROTOCOL",
choices=('http', 'https', 'ssh', 'rsync', 'wss'))``).

The following `script <https://raw.github.com/kislyuk/argcomplete/master/docs/examples/describe_github_user.py>`_ uses
``parsed_args`` and `Requests <http://python-requests.org/>`_ to query GitHub for publicly known members of an
organization and complete their names, then prints the member description:

.. code-block:: python

    #!/usr/bin/env python
    # PYTHON_ARGCOMPLETE_OK
    import argcomplete, argparse, requests, pprint

    def github_org_members(prefix, parsed_args, **kwargs):
        resource = "https://api.github.com/orgs/{org}/members".format(org=parsed_args.organization)
        return (member['login'] for member in requests.get(resource).json() if member['login'].startswith(prefix))

    parser = argparse.ArgumentParser()
    parser.add_argument("--organization", help="GitHub organization")
    parser.add_argument("--member", help="GitHub member").completer = github_org_members

    argcomplete.autocomplete(parser)
    args = parser.parse_args()

    pprint.pprint(requests.get("https://api.github.com/users/{m}".format(m=args.member)).json())

`Try it <https://raw.github.com/kislyuk/argcomplete/master/docs/examples/describe_github_user.py>`_ like this::

    ./describe_github_user.py --organization heroku --member <TAB>

If you have a useful completer to add to the `completer library
<https://github.com/kislyuk/argcomplete/blob/master/argcomplete/completers.py>`_, send a pull request!

Readline-style completers
~~~~~~~~~~~~~~~~~~~~~~~~~
The readline_ module defines a completer protocol in rlcompleter_. Readline-style completers are also supported by
argcomplete, so you can use the same completer object both in an interactive readline-powered shell and on the bash
command line. For example, you can use the readline-style completer provided by IPython_ to get introspective
completions like you would get in the IPython shell:

.. _readline: http://docs.python.org/3/library/readline.html
.. _rlcompleter: http://docs.python.org/3/library/rlcompleter.html#completer-objects
.. _IPython: http://ipython.org/

.. code-block:: python

    import IPython
    parser.add_argument("--python-name").completer = IPython.core.completer.Completer()

You can also use `argcomplete.CompletionFinder.rl_complete <https://argcomplete.readthedocs.org/en/latest/#argcomplete.CompletionFinder.rl_complete>`_
to plug your entire argparse parser as a readline completer.

Printing warnings in completers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Normal stdout/stderr output is suspended when argcomplete runs. Sometimes, though, when the user presses ``<TAB>``, it's
appropriate to print information about why completions generation failed. To do this, use ``warn``:

.. code-block:: python

    from argcomplete import warn

    def AwesomeWebServiceCompleter(prefix, **kwargs):
        if login_failed:
            warn("Please log in to Awesome Web Service to use autocompletion")
        return completions

Using a custom completion validator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
By default, argcomplete validates your completions by checking if they start with the prefix given to the completer. You
can override this validation check by supplying the ``validator`` keyword to ``argcomplete.autocomplete()``:

.. code-block:: python

    def my_validator(current_input, keyword_to_check_against):
        # Pass through ALL options even if they don't all start with 'current_input'
        return True

    argcomplete.autocomplete(parser, validator=my_validator)

Global completion
-----------------
In global completion mode, you don't have to register each argcomplete-capable executable separately. Instead, bash
will look for the string **PYTHON_ARGCOMPLETE_OK** in the first 1024 bytes of any executable that it's running
completion for, and if it's found, follow the rest of the argcomplete protocol as described above.

Additionally, completion is activated for scripts run as ``python <script>`` and ``python -m <module>``.
This also works for alternate Python versions (e.g. ``python3`` and ``pypy``), as long as that version of Python has
argcomplete installed.

.. admonition:: Bash version compatibility

 Global completion requires bash support for ``complete -D``, which was introduced in bash 4.2. On OS X or older Linux
 systems, you will need to update bash to use this feature. Check the version of the running copy of bash with
 ``echo $BASH_VERSION``. On OS X, install bash via `Homebrew <http://brew.sh/>`_ (``brew install bash``), add
 ``/usr/local/bin/bash`` to ``/etc/shells``, and run ``chsh`` to change your shell.

 Global completion is not currently compatible with zsh.

.. note:: If you use setuptools/distribute ``scripts`` or ``entry_points`` directives to package your module,
 argcomplete will follow the wrapper scripts to their destination and look for ``PYTHON_ARGCOMPLETE_OK`` in the
 destination code.

If you choose not to use global completion, or ship a bash completion module that depends on argcomplete, you must
register your script explicitly using ``eval "$(register-python-argcomplete my-awesome-script)"``. Standard bash
completion registration roules apply: namely, the script name is passed directly to ``complete``, meaning it is only tab
completed when invoked exactly as it was registered. In the above example, ``my-awesome-script`` must be on the path,
and the user must be attempting to complete it by that name. The above line alone would **not** allow you to complete
``./my-awesome-script``, or ``/path/to/my-awesome-script``.


Activating global completion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The script ``activate-global-python-argcomplete`` will try to install the file
``bash_completion.d/python-argcomplete.sh`` (`see on GitHub`_) into an appropriate location on your system
(``/etc/bash_completion.d/`` or ``~/.bash_completion.d/``). If it
fails, but you know the correct location of your bash completion scripts directory, you can specify it with ``--dest``::

    activate-global-python-argcomplete --dest=/path/to/bash_completion.d

Otherwise, you can redirect its shellcode output into a file::

    activate-global-python-argcomplete --dest=- > file

The file's contents should then be sourced in e.g. ``~/.bashrc``.

.. _`see on GitHub`: https://github.com/kislyuk/argcomplete/blob/master/argcomplete/bash_completion.d/python-argcomplete.sh

Zsh Support
------------
To activate completions for zsh you need to have ``bashcompinit`` enabled in zsh::

    autoload -U bashcompinit
    bashcompinit

Afterwards you can enable completion for your scripts with ``register-python-argcomplete``::

    eval "$(register-python-argcomplete my-awesome-script)"

Tcsh Support
------------
To activate completions for tcsh use::

    eval `register-python-argcomplete --shell tcsh my-awesome-script`

The ``python-argcomplete-tcsh`` script provides completions for tcsh.
The following is an example of the tcsh completion syntax for
``my-awesome-script`` emitted by ``register-python-argcomplete``::

    complete my-awesome-script 'p@*@`python-argcomplete-tcsh my-awesome-script`@'

Fish Support
------------
To activate completions for fish use::

    register-python-argcomplete --shell fish my-awesome-script | .

or create new completion file, e.g::

    register-python-argcomplete --shell fish ~/.config/fish/completions/my-awesome-script.fish

Python Support
--------------
Argcomplete requires Python 2.7 or 3.3+.

Common Problems
---------------
If global completion is not completing your script, bash may have registered a
default completion function::

    $ complete | grep my-awesome-script
    complete -F _minimal my-awesome-script

You can fix this by restarting your shell, or by running
``complete -r my-awesome-script``.

Debugging
---------
Set the ``_ARC_DEBUG`` variable in your shell to enable verbose debug output every time argcomplete runs. This will
disrupt the command line composition state of your terminal, but make it possible to see the internal state of the
completer if it encounters problems.

Acknowledgments
---------------
Inspired and informed by the optcomplete_ module by Martin Blais.

.. _optcomplete: http://pypi.python.org/pypi/optcomplete

Links
-----
* `Project home page (GitHub) <https://github.com/kislyuk/argcomplete>`_
* `Documentation (Read the Docs) <https://argcomplete.readthedocs.io/en/latest/>`_
* `Package distribution (PyPI) <https://pypi.python.org/pypi/argcomplete>`_
* `Change log <https://github.com/kislyuk/argcomplete/blob/master/Changes.rst>`_

Bugs
~~~~
Please report bugs, issues, feature requests, etc. on `GitHub <https://github.com/kislyuk/argcomplete/issues>`_.

License
-------
Licensed under the terms of the `Apache License, Version 2.0 <http://www.apache.org/licenses/LICENSE-2.0>`_.

.. image:: https://travis-ci.org/kislyuk/argcomplete.png
        :target: https://travis-ci.org/kislyuk/argcomplete
.. image:: https://codecov.io/github/kislyuk/argcomplete/coverage.svg?branch=master
        :target: https://codecov.io/github/kislyuk/argcomplete?branch=master
.. image:: https://img.shields.io/pypi/v/argcomplete.svg
        :target: https://pypi.python.org/pypi/argcomplete
.. image:: https://img.shields.io/pypi/l/argcomplete.svg
        :target: https://pypi.python.org/pypi/argcomplete
.. image:: https://readthedocs.org/projects/argcomplete/badge/?version=latest
        :target: https://argcomplete.readthedocs.org/



  * [argh-0.26.2](http://github.com/neithere/argh/) Argh: The Natural CLI
=====================

.. image:: https://img.shields.io/coveralls/neithere/argh.svg
    :target: https://coveralls.io/r/neithere/argh

.. image:: https://img.shields.io/travis/neithere/argh.svg
    :target: https://travis-ci.org/neithere/argh

.. image:: https://img.shields.io/pypi/format/argh.svg
    :target: https://pypi.python.org/pypi/argh

.. image:: https://img.shields.io/pypi/status/argh.svg
    :target: https://pypi.python.org/pypi/argh

.. image:: https://img.shields.io/pypi/v/argh.svg
    :target: https://pypi.python.org/pypi/argh

.. image:: https://img.shields.io/pypi/pyversions/argh.svg
    :target: https://pypi.python.org/pypi/argh

.. image:: https://img.shields.io/pypi/dd/argh.svg
    :target: https://pypi.python.org/pypi/argh

.. image:: https://readthedocs.org/projects/argh/badge/?version=stable
    :target: http://argh.readthedocs.org/en/stable/

.. image:: https://readthedocs.org/projects/argh/badge/?version=latest
    :target: http://argh.readthedocs.org/en/latest/

Building a command-line interface?  Found yourself uttering "argh!" while
struggling with the API of `argparse`?  Don't like the complexity but need
the power?

.. epigraph::

    Everything should be made as simple as possible, but no simpler.

    -- Albert Einstein (probably)

`Argh` is a smart wrapper for `argparse`.  `Argparse` is a very powerful tool;
`Argh` just makes it easy to use.

In a nutshell
-------------

`Argh`-powered applications are *simple* but *flexible*:

:Modular:
    Declaration of commands can be decoupled from assembling and dispatching;

:Pythonic:
    Commands are declared naturally, no complex API calls in most cases;

:Reusable:
    Commands are plain functions, can be used directly outside of CLI context;

:Layered:
    The complexity of code raises with requirements;

:Transparent:
    The full power of argparse is available whenever needed;

:Namespaced:
    Nested commands are a piece of cake, no messing with subparsers (though
    they are of course used under the hood);

:Term-Friendly:
    Command output is processed with respect to stream encoding;

:Unobtrusive:
    `Argh` can dispatch a subset of pure-`argparse` code, and pure-`argparse`
    code can update and dispatch a parser assembled with `Argh`;

:DRY:
    The amount of boilerplate code is minimal; among other things, `Argh` will:

    * infer command name from function name;
    * infer arguments from function signature;
    * infer argument type from the default value;
    * infer argument action from the default value (for booleans);
    * add an alias root command ``help`` for the ``--help`` argument.

:NIH free:
    `Argh` supports *completion*, *progress bars* and everything else by being
    friendly to excellent 3rd-party libraries.  No need to reinvent the wheel.

Sounds good?  Check the tutorial!

Relation to argparse
--------------------

`Argh` is fully compatible with `argparse`.  You can mix `Argh`-agnostic and
`Argh`-aware code.  Just keep in mind that the dispatcher does some extra work
that a custom dispatcher may not do.

Installation
------------

Using pip::

    $ pip install argh

Arch Linux (AUR)::

    $ yaourt python-argh

Examples
--------

A very simple application with one command:

.. code-block:: python

    import argh

    def main():
        return 'Hello world'

    argh.dispatch_command(main)

Run it:

.. code-block:: bash

    $ ./app.py
    Hello world

A potentially modular application with multiple commands:

.. code-block:: python

    import argh

    # declaring:

    def echo(text):
        "Returns given word as is."
        return text

    def greet(name, greeting='Hello'):
        "Greets the user with given name. The greeting is customizable."
        return greeting + ', ' + name

    # assembling:

    parser = argh.ArghParser()
    parser.add_commands([echo, greet])

    # dispatching:

    if __name__ == '__main__':
        parser.dispatch()

Of course it works:

.. code-block:: bash

    $ ./app.py greet Andy
    Hello, Andy

    $ ./app.py greet Andy -g Arrrgh
    Arrrgh, Andy

Here's the auto-generated help for this application (note how the docstrings
are reused)::

    $ ./app.py help

    usage: app.py {echo,greet} ...

    positional arguments:
        echo        Returns given word as is.
        greet       Greets the user with given name. The greeting is customizable.

...and for a specific command (an ordinary function signature is converted
to CLI arguments)::

    $ ./app.py help greet

    usage: app.py greet [-g GREETING] name

    Greets the user with given name. The greeting is customizable.

    positional arguments:
      name

    optional arguments:
      -g GREETING, --greeting GREETING   'Hello'

(The help messages have been simplified a bit for brevity.)

`Argh` easily maps plain Python functions to CLI.  Sometimes this is not
enough; in these cases the powerful API of `argparse` is also available:

.. code-block:: python

    @arg('text', default='hello world', nargs='+', help='The message')
    def echo(text):
        print text

The approaches can be safely combined even up to this level:

.. code-block:: python

    # adding help to `foo` which is in the function signature:
    @arg('foo', help='blah')
    # these are not in the signature so they go to **kwargs:
    @arg('baz')
    @arg('-q', '--quux')
    # the function itself:
    def cmd(foo, bar=1, *args, **kwargs):
        yield foo
        yield bar
        yield ', '.join(args)
        yield kwargs['baz']
        yield kwargs['quux']

Links
-----

* `Project home page`_ (GitHub)
* `Documentation`_ (Read the Docs)
* `Package distribution`_ (PyPI)
* Questions, requests, bug reports, etc.:

  * `Issue tracker`_ (GitHub)
  * `Mailing list`_ (subscribe to get important announcements)
  * Direct e-mail (neithere at gmail com)

.. _project home page: http://github.com/neithere/argh/
.. _documentation: http://argh.readthedocs.org
.. _package distribution: http://pypi.python.org/pypi/argh
.. _issue tracker: http://github.com/neithere/argh/issues/
.. _mailing list: http://groups.google.com/group/argh-users

Author
------

Developed by Andrey Mikhaylenko since 2010.

See file `AUTHORS` for a complete list of contributors to this library.

Support
-------

The fastest way to improve this project is to submit tested and documented
patches or detailed bug reports.

Otherwise you can "flattr" me: |FlattrLink|_

.. _FlattrLink: https://flattr.com/submit/auto?user_id=neithere&url=http%3A%2F%2Fpypi.python.org%2Fpypi%2Fargh
.. |FlattrLink| image:: https://api.flattr.com/button/flattr-badge-large.png
   :alt: Flattr the Argh project

Licensing
---------

Argh is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Argh is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with Argh.  If not, see <http://gnu.org/licenses/>.
  * [argon2-cffi-19.1.0](https://argon2-cffi.readthedocs.io/) =====================================
CFFI-based Argon2 Bindings for Python
=====================================

.. image:: https://img.shields.io/pypi/v/argon2_cffi.svg
   :target: https://pypi.org/project/argon2_cffi/
   :alt: PyPI

.. image:: https://readthedocs.org/projects/argon2-cffi/badge/?version=stable
   :target: http://argon2-cffi.readthedocs.io/en/stable/?badge=stable
   :alt: Documentation Status

.. image:: https://travis-ci.org/hynek/argon2_cffi.svg?branch=master
   :target: https://travis-ci.org/hynek/argon2_cffi
   :alt: Travis CI status

.. image:: https://ci.appveyor.com/api/projects/status/3faufu7qgwc8nv2v/branch/master?svg=true
   :target: https://ci.appveyor.com/project/hynek/argon2-cffi
   :alt: AppVeyor CI Status

.. image:: https://codecov.io/github/hynek/argon2_cffi/branch/master/graph/badge.svg
   :target: https://codecov.io/github/hynek/argon2_cffi
   :alt: Test Coverage

.. image:: https://www.irccloud.com/invite-svg?channel=%23cryptography-dev&amp;hostname=irc.freenode.net&amp;port=6697&amp;ssl=1
   :target: https://www.irccloud.com/invite?channel=%23cryptography-dev&amp;hostname=irc.freenode.net&amp;port=6697&amp;ssl=1
   :alt: IRC

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
   :alt: Code style: black

.. teaser-begin

`Argon2 <https://github.com/p-h-c/phc-winner-argon2>`_ won the `Password Hashing Competition <https://password-hashing.net/>`_ and ``argon2_cffi`` is the simplest way to use it in Python and PyPy:

.. code-block:: pycon

  >>> from argon2 import PasswordHasher
  >>> ph = PasswordHasher()
  >>> hash = ph.hash("s3kr3tp4ssw0rd")
  >>> hash  # doctest: +SKIP
  '$argon2id$v=19$m=102400,t=2,p=8$tSm+JOWigOgPZx/g44K5fQ$WDyus6py50bVFIPkjA28lQ'
  >>> ph.verify(hash, "s3kr3tp4ssw0rd")
  True
  >>> ph.check_needs_rehash(hash)
  False
  >>> ph.verify(hash, "t0t411ywr0ng")
  Traceback (most recent call last):
    ...
  argon2.exceptions.VerifyMismatchError: The password does not match the supplied hash


.. note::
   `passlib <https://pypi.org/project/passlib/>`_ 1.7.0 and later offers `Argon2 support <https://passlib.readthedocs.io/en/stable/lib/passlib.hash.argon2.html>`_ using this library too.

``argon2_cffi``\ ’s documentation lives at `Read the Docs <https://argon2-cffi.readthedocs.io/>`_, the code on `GitHub <https://github.com/hynek/argon2_cffi>`_.
It’s rigorously tested on Python 2.7, 3.4+, and PyPy.


Release Information
===================

19.1.0 (2019-01-17)
-------------------

Vendoring Argon2 @ `670229c <https://github.com/P-H-C/phc-winner-argon2/tree/670229c849b9fe882583688b74eb7dfdc846f9f6>`_ (20171227)


Backward-incompatible changes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

*none*


Deprecations:
^^^^^^^^^^^^^

*none*


Changes:
^^^^^^^^

- Added support for Argon2 v1.2 hashes in ``argon2.extract_parameters()``.

`Full changelog <https://argon2-cffi.readthedocs.io/en/stable/changelog.html>`_.

Credits & License
=================

``argon2_cffi`` is maintained by Hynek Schlawack and released under the `MIT license <https://github.com/hynek/argon2_cffi/blob/master/LICENSE>`_.

The development is kindly supported by `Variomedia AG <https://www.variomedia.de/>`_.

A full list of contributors can be found in GitHub's `overview <https://github.com/hynek/argon2_cffi/graphs/contributors>`_.


Vendored Code
-------------

Argon2
^^^^^^

The original Argon2 repo can be found at https://github.com/P-H-C/phc-winner-argon2/.

Except for the components listed below, the Argon2 code in this repository is copyright (c) 2015 Daniel Dinu, Dmitry Khovratovich (main authors), Jean-Philippe Aumasson and Samuel Neves, and under CC0_ license.

The string encoding routines in src/encoding.c are copyright (c) 2015 Thomas Pornin, and under CC0_ license.

The `BLAKE2 <https://blake2.net>`_ code in ``src/blake2/`` is copyright (c) Samuel Neves, 2013-2015, and under CC0_ license.

The authors of Argon2 also were very helpful to get the library to compile on ancient versions of Visual Studio for ancient versions of Python.

The documentation also quotes frequently from the Argon2 paper_ to avoid mistakes by rephrasing.

.. _CC0: https://creativecommons.org/publicdomain/zero/1.0/
.. _paper: https://password-hashing.net/argon2-specs.pdf

msinttypes
^^^^^^^^^^

In order to be able to compile on Visual Studio 2008 and Visual Studio 2010 which are required for Python 2.7 and 3.4 respectively, we also ship two C headers with integer types.
They are from the `msinttypes project <https://code.google.com/archive/p/msinttypes>`_ (`auto-import on GitHub <https://github.com/chemeris/msinttypes>`_) and licensed under New BSD:

Copyright (c) 2006-2013 Alexander Chemeris

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  1. Redistributions of source code must retain the above copyright notice,
     this list of conditions and the following disclaimer.
  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation and/or other materials provided with the distribution.
  3. Neither the name of the product nor the names of its contributors may
     be used to endorse or promote products derived from this software
     without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR ''AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  * [asciitree-0.3.3](http://github.com/mbr/asciitree) ASCII Trees
===========

.. code:: console

  asciitree
   +-- sometimes
   |   +-- you
   +-- just
   |   +-- want
   |       +-- to
   |       +-- draw
   +-- trees
   +-- in
       +-- your
           +-- terminal


.. code:: python

  from asciitree import LeftAligned
  from collections import OrderedDict as OD

  tree = {
      'asciitree': OD([
          ('sometimes',
              {'you': {}}),
          ('just',
              {'want': OD([
                  ('to', {}),
                  ('draw', {}),
              ])}),
          ('trees', {}),
          ('in', {
              'your': {
                  'terminal': {}
              }
          })
      ])
  }

  tr = LeftAligned()
  print(tr(tree))


Read the documentation at http://pythonhosted.org/asciitree
  * [asn1crypto-0.24.0](https://github.com/wbond/asn1crypto) # asn1crypto

A fast, pure Python library for parsing and serializing ASN.1 structures.

 - [Features](#features)
 - [Why Another Python ASN.1 Library?](#why-another-python-asn1-library)
 - [Related Crypto Libraries](#related-crypto-libraries)
 - [Current Release](#current-release)
 - [Dependencies](#dependencies)
 - [Installation](#installation)
 - [License](#license)
 - [Documentation](#documentation)
 - [Continuous Integration](#continuous-integration)
 - [Testing](#testing)
 - [Development](#development)
 - [CI Tasks](#ci-tasks)

[![GitHub Actions CI](https://github.com/wbond/asn1crypto/workflows/CI/badge.svg)](https://github.com/wbond/asn1crypto/actions?workflow=CI)
[![Travis CI](https://api.travis-ci.org/wbond/asn1crypto.svg?branch=master)](https://travis-ci.org/wbond/asn1crypto)
[![AppVeyor](https://ci.appveyor.com/api/projects/status/github/wbond/asn1crypto?branch=master&svg=true)](https://ci.appveyor.com/project/wbond/asn1crypto)
[![CircleCI](https://circleci.com/gh/wbond/asn1crypto.svg?style=shield)](https://circleci.com/gh/wbond/asn1crypto)
[![Codecov](https://codecov.io/gh/wbond/asn1crypto/branch/master/graph/badge.svg)](https://codecov.io/gh/wbond/asn1crypto)
[![PyPI](https://img.shields.io/pypi/v/asn1crypto.svg)](https://pypi.org/project/asn1crypto/)

## Features

In addition to an ASN.1 BER/DER decoder and DER serializer, the project includes
a bunch of ASN.1 structures for use with various common cryptography standards:

| Standard               | Module                                      | Source                                                                                                                 |
| ---------------------- | ------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| X.509                  | [`asn1crypto.x509`](asn1crypto/x509.py)     | [RFC 5280](https://tools.ietf.org/html/rfc5280)                                                                        |
| CRL                    | [`asn1crypto.crl`](asn1crypto/crl.py)       | [RFC 5280](https://tools.ietf.org/html/rfc5280)                                                                        |
| CSR                    | [`asn1crypto.csr`](asn1crypto/csr.py)       | [RFC 2986](https://tools.ietf.org/html/rfc2986), [RFC 2985](https://tools.ietf.org/html/rfc2985)                       |
| OCSP                   | [`asn1crypto.ocsp`](asn1crypto/ocsp.py)     | [RFC 6960](https://tools.ietf.org/html/rfc6960)                                                                        |
| PKCS#12                | [`asn1crypto.pkcs12`](asn1crypto/pkcs12.py) | [RFC 7292](https://tools.ietf.org/html/rfc7292)                                                                        |
| PKCS#8                 | [`asn1crypto.keys`](asn1crypto/keys.py)     | [RFC 5208](https://tools.ietf.org/html/rfc5208)                                                                        |
| PKCS#1 v2.1 (RSA keys) | [`asn1crypto.keys`](asn1crypto/keys.py)     | [RFC 3447](https://tools.ietf.org/html/rfc3447)                                                                        |
| DSA keys               | [`asn1crypto.keys`](asn1crypto/keys.py)     | [RFC 3279](https://tools.ietf.org/html/rfc3279)                                                                        |
| Elliptic curve keys    | [`asn1crypto.keys`](asn1crypto/keys.py)     | [SECG SEC1 V2](http://www.secg.org/sec1-v2.pdf)                                                                        |
| PKCS#3 v1.4            | [`asn1crypto.algos`](asn1crypto/algos.py)   | [PKCS#3 v1.4](ftp://ftp.rsasecurity.com/pub/pkcs/ascii/pkcs-3.asc)                                                        |
| PKCS#5 v2.1            | [`asn1crypto.algos`](asn1crypto/algos.py)   | [PKCS#5 v2.1](http://www.emc.com/collateral/white-papers/h11302-pkcs5v2-1-password-based-cryptography-standard-wp.pdf) |
| CMS (and PKCS#7)       | [`asn1crypto.cms`](asn1crypto/cms.py)       | [RFC 5652](https://tools.ietf.org/html/rfc5652), [RFC 2315](https://tools.ietf.org/html/rfc2315)                       |
| TSP                    | [`asn1crypto.tsp`](asn1crypto/tsp.py)       | [RFC 3161](https://tools.ietf.org/html/rfc3161)                                                                        |
| PDF signatures         | [`asn1crypto.pdf`](asn1crypto/pdf.py)       | [PDF 1.7](http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/pdf/pdfs/PDF32000_2008.pdf)                           |

## Why Another Python ASN.1 Library?

Python has long had the [pyasn1](https://pypi.org/project/pyasn1/) and
[pyasn1_modules](https://pypi.org/project/pyasn1-modules/) available for
parsing and serializing ASN.1 structures. While the project does include a
comprehensive set of tools for parsing and serializing, the performance of the
library can be very poor, especially when dealing with bit fields and parsing
large structures such as CRLs.

After spending extensive time using *pyasn1*, the following issues were
identified:

 1. Poor performance
 2. Verbose, non-pythonic API
 3. Out-dated and incomplete definitions in *pyasn1-modules*
 4. No simple way to map data to native Python data structures
 5. No mechanism for overridden universal ASN.1 types

The *pyasn1* API is largely method driven, and uses extensive configuration
objects and lowerCamelCase names. There were no consistent options for
converting types of native Python data structures. Since the project supports
out-dated versions of Python, many newer language features are unavailable
for use.

Time was spent trying to profile issues with the performance, however the
architecture made it hard to pin down the primary source of the poor
performance. Attempts were made to improve performance by utilizing unreleased
patches and delaying parsing using the `Any` type. Even with such changes, the
performance was still unacceptably slow.

Finally, a number of structures in the cryptographic space use universal data
types such as `BitString` and `OctetString`, but interpret the data as other
types. For instance, signatures are really byte strings, but are encoded as
`BitString`. Elliptic curve keys use both `BitString` and `OctetString` to
represent integers. Parsing these structures as the base universal types and
then re-interpreting them wastes computation.

*asn1crypto* uses the following techniques to improve performance, especially
when extracting one or two fields from large, complex structures:

 - Delayed parsing of byte string values
 - Persistence of original ASN.1 encoded data until a value is changed
 - Lazy loading of child fields
 - Utilization of high-level Python stdlib modules

While there is no extensive performance test suite, the
`CRLTests.test_parse_crl` test case was used to parse a 21MB CRL file on a
late 2013 rMBP. *asn1crypto* parsed the certificate serial numbers in just
under 8 seconds. With *pyasn1*, using definitions from *pyasn1-modules*, the
same parsing took over 4,100 seconds.

For smaller structures the performance difference can range from a few times
faster to an order of magnitude or more.

## Related Crypto Libraries

*asn1crypto* is part of the modularcrypto family of Python packages:

 - [asn1crypto](https://github.com/wbond/asn1crypto)
 - [oscrypto](https://github.com/wbond/oscrypto)
 - [csrbuilder](https://github.com/wbond/csrbuilder)
 - [certbuilder](https://github.com/wbond/certbuilder)
 - [crlbuilder](https://github.com/wbond/crlbuilder)
 - [ocspbuilder](https://github.com/wbond/ocspbuilder)
 - [certvalidator](https://github.com/wbond/certvalidator)

## Current Release

1.0.1 - [changelog](changelog.md)

## Dependencies

Python 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8 or pypy. *No third-party
packages required.*

## Installation

```bash
pip install asn1crypto
```

## License

*asn1crypto* is licensed under the terms of the MIT license. See the
[LICENSE](LICENSE) file for the exact license text.

## Documentation

The documentation for *asn1crypto* is composed of tutorials on basic usage and
links to the source for the various pre-defined type classes.

### Tutorials

 - [Universal Types with BER/DER Decoder and DER Encoder](docs/universal_types.md)
 - [PEM Encoder and Decoder](docs/pem.md)

### Reference

 - [Universal types](asn1crypto/core.py), `asn1crypto.core`
 - [Digest, HMAC, signed digest and encryption algorithms](asn1crypto/algos.py), `asn1crypto.algos`
 - [Private and public keys](asn1crypto/keys.py), `asn1crypto.keys`
 - [X509 certificates](asn1crypto/x509.py), `asn1crypto.x509`
 - [Certificate revocation lists (CRLs)](asn1crypto/crl.py), `asn1crypto.crl`
 - [Online certificate status protocol (OCSP)](asn1crypto/ocsp.py), `asn1crypto.ocsp`
 - [Certificate signing requests (CSRs)](asn1crypto/csr.py), `asn1crypto.csr`
 - [Private key/certificate containers (PKCS#12)](asn1crypto/pkcs12.py), `asn1crypto.pkcs12`
 - [Cryptographic message syntax (CMS, PKCS#7)](asn1crypto/cms.py), `asn1crypto.cms`
 - [Time stamp protocol (TSP)](asn1crypto/tsp.py), `asn1crypto.tsp`
 - [PDF signatures](asn1crypto/pdf.py), `asn1crypto.pdf`

## Continuous Integration

 - [Windows](https://ci.appveyor.com/project/wbond/asn1crypto/history) via AppVeyor
 - [OS X](https://circleci.com/gh/wbond/asn1crypto) via CircleCI
 - [Linux](https://travis-ci.org/wbond/asn1crypto/builds) via Travis CI
 - [Test Coverage](https://codecov.io/gh/wbond/asn1crypto/commits) via Codecov

## Testing

Tests are written using `unittest` and require no third-party packages.

Depending on what type of source is available for the package, the following
commands can be used to run the test suite.

### Git Repository

When working within a Git working copy, or an archive of the Git repository,
the full test suite is run via:

```bash
python run.py tests
```

To run only some tests, pass a regular expression as a parameter to `tests`.

```bash
python run.py tests ocsp
```

### PyPi Source Distribution

When working within an extracted source distribution (aka `.tar.gz`) from
PyPi, the full test suite is run via:

```bash
python setup.py test
```

### Package

When the package has been installed via pip (or another method), the package
`asn1crypto_tests` may be installed and invoked to run the full test suite:

```bash
pip install asn1crypto_tests
python -m asn1crypto_tests
```

## Development

To install the package used for linting, execute:

```bash
pip install --user -r requires/lint
```

The following command will run the linter:

```bash
python run.py lint
```

Support for code coverage can be installed via:

```bash
pip install --user -r requires/coverage
```

Coverage is measured by running:

```bash
python run.py coverage
```

To change the version number of the package, run:

```bash
python run.py version {pep440_version}
```

To install the necessary packages for releasing a new version on PyPI, run:

```bash
pip install --user -r requires/release
```

Releases are created by:

 - Making a git tag in [PEP 440](https://www.python.org/dev/peps/pep-0440/#examples-of-compliant-version-schemes) format
 - Running the command:

   ```bash
   python run.py release
   ```

Existing releases can be found at https://pypi.org/project/asn1crypto/.

## CI Tasks

A task named `deps` exists to download and stage all necessary testing
dependencies. On posix platforms, `curl` is used for downloads and on Windows
PowerShell with `Net.WebClient` is used. This configuration sidesteps issues
related to getting pip to work properly and messing with `site-packages` for
the version of Python being used.

The `ci` task runs `lint` (if flake8 is available for the version of Python) and
`coverage` (or `tests` if coverage is not available for the version of Python).
If the current directory is a clean git working copy, the coverage data is
submitted to codecov.io.

```bash
python run.py deps
python run.py ci
```



  * [astor-0.8.0](https://github.com/berkerpeksag/astor) =============================
astor -- AST observe/rewrite
=============================

:PyPI: https://pypi.org/project/astor/
:Documentation: https://astor.readthedocs.io
:Source: https://github.com/berkerpeksag/astor
:License: 3-clause BSD
:Build status:
    .. image:: https://secure.travis-ci.org/berkerpeksag/astor.svg
        :alt: Travis CI
        :target: https://travis-ci.org/berkerpeksag/astor/

astor is designed to allow easy manipulation of Python source via the AST.

There are some other similar libraries, but astor focuses on the following areas:

- Round-trip an AST back to Python [1]_:

  - Modified AST doesn't need linenumbers, ctx, etc. or otherwise
    be directly compileable for the round-trip to work.
  - Easy to read generated code as, well, code
  - Can round-trip two different source trees to compare for functional
    differences, using the astor.rtrip tool (for example, after PEP8 edits).

- Dump pretty-printing of AST

  - Harder to read than round-tripped code, but more accurate to figure out what
    is going on.

  - Easier to read than dump from built-in AST module

- Non-recursive treewalk

  - Sometimes you want a recursive treewalk (and astor supports that, starting
    at any node on the tree), but sometimes you don't need to do that.  astor
    doesn't require you to explicitly visit sub-nodes unless you want to:

  - You can add code that executes before a node's children are visited, and/or
  - You can add code that executes after a node's children are visited, and/or
  - You can add code that executes and keeps the node's children from being
    visited (and optionally visit them yourself via a recursive call)

  - Write functions to access the tree based on object names and/or attribute names
  - Enjoy easy access to parent node(s) for tree rewriting

.. [1]
    The decompilation back to Python is based on code originally written
    by Armin Ronacher.  Armin's code was well-structured, but failed on
    some obscure corner cases of the Python language (and even more corner
    cases when the AST changed on different versions of Python), and its
    output arguably had cosmetic issues -- for example, it produced
    parentheses even in some cases where they were not needed, to
    avoid having to reason about precedence.

    Other derivatives of Armin's code are floating around, and typically
    have fixes for a few corner cases that happened to be noticed by the
    maintainers, but most of them have not been tested as thoroughly as
    astor.  One exception may be the version of codegen
    `maintained at github by CensoredUsername`__.  This has been tested
    to work properly on Python 2.7 using astor's test suite, and, as it
    is a single source file, it may be easier to drop into some applications
    that do not require astor's other features or Python 3.x compatibility.

__ https://github.com/CensoredUsername/codegen



  * [astroid-2.2.5](https://github.com/PyCQA/astroid) Astroid
=======

.. image:: https://travis-ci.org/PyCQA/astroid.svg?branch=master
    :target: https://travis-ci.org/PyCQA/astroid

.. image:: https://ci.appveyor.com/api/projects/status/co3u42kunguhbh6l/branch/master?svg=true
    :alt: AppVeyor Build Status
    :target: https://ci.appveyor.com/project/PCManticore/astroid

.. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=master
    :target: https://coveralls.io/github/PyCQA/astroid?branch=master

.. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
    :target: http://astroid.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

.. |tideliftlogo| image:: doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White_small.png
   :width: 75
   :height: 60
   :alt: Tidelift

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for astroid is available as part of the `Tidelift
       Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme



What's this?
------------

The aim of this module is to provide a common base representation of
python source code. It is currently the library powering pylint's capabilities.

It provides a compatible representation which comes from the `_ast`
module.  It rebuilds the tree generated by the builtin _ast module by
recursively walking down the AST and building an extended ast. The new
node classes have additional methods and attributes for different
usages. They include some support for static inference and local name
scopes. Furthermore, astroid can also build partial trees by inspecting living
objects.


Installation
------------

Extract the tarball, jump into the created directory and run::

	pip install .


If you want to do an editable installation, you can run::

    pip install -e .


If you have any questions, please mail the code-quality@python.org
mailing list for support. See
http://mail.python.org/mailman/listinfo/code-quality for subscription
information and archives.

Documentation
-------------
http://astroid.readthedocs.io/en/latest/


Python Versions
---------------

astroid 2.0 is currently available for Python 3 only. If you want Python 2
support, older versions of astroid will still supported until 2020.

Test
----

Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
either `tox` or `pytest`::

  tox
  pytest astroid



  * [astropy-3.2.1](http://astropy.org) =======
Astropy
=======

|Travis Status| |CircleCI Status| |Coverage Status| |PyPI Status| |Documentation Status|

The Astropy Project (http://astropy.org/) is a community effort to develop a
single core package for Astronomy in Python and foster interoperability between
Python astronomy packages. This repository contains the core package which is
intended to contain much of the core functionality and some common tools needed
for performing astronomy and astrophysics with Python.

Releases are `registered on PyPI <http://pypi.python.org/pypi/astropy>`_,
and development is occurring at the
`project's GitHub page <http://github.com/astropy/astropy>`_.

For installation instructions, see the `online documentation <http://docs.astropy.org/>`_
or  `docs/install.rst <docs/install.rst>`_ in this source distribution.

Contributing Code, Documentation, or Feedback
---------------------------------------------

The Astropy Project is made both by and for its users, so we welcome and
encourage contributions of many kinds. Our goal is to keep this a positive,
inclusive, successful, and growing community by abiding with the
`Astropy Community Code of Conduct <http://www.astropy.org/about.html#codeofconduct>`_.

More detailed information on contributing to the project or submitting feedback
can be found on the `contributions <http://www.astropy.org/contribute.html>`_
page. A `summary of contribution guidelines <CONTRIBUTING.md>`_ can also be
used as a quick reference when you are ready to start writing or validating
code for submission.

Supporting the Project
----------------------

|NumFOCUS| |Donate|

The Astropy Project is sponsored by NumFOCUS, a 501(c)(3) nonprofit in the
United States. You can donate to the project by using the link above, and this
donation will support our mission to promote sustainable, high-level code base
for the astronomy community, open code development, educational materials, and
reproducible scientific research.

License
-------

Astropy is licensed under a 3-clause BSD style license - see the
`LICENSE.rst <LICENSE.rst>`_ file.

Notes for Package Managers
--------------------------

For system packagers: Please install `astropy` with the command::

    $ python setup.py --offline install

This will prevent the astropy_helpers bootstrap script from attempting to
reach out to PyPI.

.. |Travis Status| image:: https://travis-ci.org/astropy/astropy.svg
    :target: https://travis-ci.org/astropy/astropy
    :alt: Astropy's Travis CI Status

.. |CircleCI Status| image:: https://circleci.com/gh/astropy/astropy.svg?style=svg
    :target: https://circleci.com/gh/astropy/astropy
    :alt: Astropy's CircleCI Status

.. |Coverage Status| image:: https://codecov.io/gh/astropy/astropy/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/astropy/astropy
    :alt: Astropy's Coverage Status

.. |PyPI Status| image:: https://img.shields.io/pypi/v/astropy.svg
    :target: https://pypi.python.org/pypi/astropy
    :alt: Astropy's PyPI Status

.. |Documentation Status| image:: https://readthedocs.org/projects/astropy/badge/?version=stable
    :target: http://docs.astropy.org/en/stable/?badge=stable
    :alt: Documentation Status

.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
    :target: http://numfocus.org
    :alt: Powered by NumFOCUS

.. |Donate| image:: https://img.shields.io/badge/Donate-to%20Astropy-brightgreen.svg
    :target: https://numfocus.salsalabs.org/donate-to-astropy/index.html



  * [async_generator-1.10](https://github.com/python-trio/async_generator) .. image:: https://img.shields.io/badge/chat-join%20now-blue.svg
   :target: https://gitter.im/python-trio/general
   :alt: Join chatroom

.. image:: https://img.shields.io/badge/docs-read%20now-blue.svg
   :target: https://async-generator.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status

.. image:: https://travis-ci.org/python-trio/async_generator.svg?branch=master
   :target: https://travis-ci.org/python-trio/async_generator
   :alt: Automated test status

.. image:: https://ci.appveyor.com/api/projects/status/af4eyed8o8tc3t0r/branch/master?svg=true
   :target: https://ci.appveyor.com/project/python-trio/trio/history
   :alt: Automated test status (Windows)

.. image:: https://codecov.io/gh/python-trio/async_generator/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/python-trio/async_generator
   :alt: Test coverage

The async_generator library
===========================

Python 3.6 added `async generators
<https://www.python.org/dev/peps/pep-0525/>`__. (What's an async
generator? `Check out my 5-minute lightning talk demo from PyCon 2016
<https://youtu.be/PulzIT8KYLk?t=24m30s>`__.) Python 3.7 adds some more
tools to make them usable, like ``contextlib.asynccontextmanager``.

This library gives you all that back to Python 3.5.

For example, this code only works in Python 3.6+:

.. code-block:: python3

   async def load_json_lines(stream_reader):
       async for line in stream_reader:
           yield json.loads(line)

But this code does the same thing, and works on Python 3.5+:

.. code-block:: python3

   from async_generator import async_generator, yield_

   @async_generator
   async def load_json_lines(stream_reader):
       async for line in stream_reader:
           await yield_(json.loads(line))

Or in Python 3.7, you can write:

.. code-block:: python3

   from contextlib import asynccontextmanager

   @asynccontextmanager
   async def background_server():
       async with trio.open_nursery() as nursery:
           value = await nursery.start(my_server)
           try:
               yield value
           finally:
               # Kill the server when the scope exits
               nursery.cancel_scope.cancel()

This is the same, but back to 3.5:

.. code-block:: python3

   from async_generator import async_generator, yield_, asynccontextmanager

   @asynccontextmanager
   @async_generator
   async def background_server():
       async with trio.open_nursery() as nursery:
           value = await nursery.start(my_server)
           try:
               await yield_(value)
           finally:
               # Kill the server when the scope exits
               nursery.cancel_scope.cancel()

(And if you're on 3.6, you can use ``@asynccontextmanager`` with
native generators.)


Let's do this
=============

* Install: ``python3 -m pip install -U async_generator`` (or on Windows,
  maybe ``py -3 -m pip install -U async_generator``

* Manual: https://async-generator.readthedocs.io/

* Bug tracker and source code: https://github.com/python-trio/async_generator

* Real-time chat: https://gitter.im/python-trio/general

* License: MIT or Apache 2, your choice

* Contributor guide: https://trio.readthedocs.io/en/latest/contributing.html

* Code of conduct: Contributors are requested to follow our `code of
  conduct
  <https://trio.readthedocs.io/en/latest/code-of-conduct.html>`__ in
  all project spaces.


How come some of those links talk about "trio"?
===============================================

`Trio <https://trio.readthedocs.io>`__ is a new async concurrency
library for Python that's obsessed with usability and correctness – we
want to make it *easy* to get things *right*. The ``async_generator``
library is maintained by the Trio project as part of that mission, and
because Trio uses ``async_generator`` internally.

You can use ``async_generator`` with any async library. It works great
with ``asyncio``, or Twisted, or whatever you like. (But we think Trio
is pretty sweet.)



  * [async_timeout-3.0.1](https://github.com/aio-libs/async_timeout/) async-timeout
=============
.. image:: https://travis-ci.org/aio-libs/async-timeout.svg?branch=master
    :target: https://travis-ci.org/aio-libs/async-timeout
.. image:: https://codecov.io/gh/aio-libs/async-timeout/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/aio-libs/async-timeout
.. image:: https://img.shields.io/pypi/v/async-timeout.svg
    :target: https://pypi.python.org/pypi/async-timeout
.. image:: https://badges.gitter.im/Join%20Chat.svg
    :target: https://gitter.im/aio-libs/Lobby
    :alt: Chat on Gitter

asyncio-compatible timeout context manager.


Usage example
-------------


The context manager is useful in cases when you want to apply timeout
logic around block of code or in cases when ``asyncio.wait_for()`` is
not suitable. Also it's much faster than ``asyncio.wait_for()``
because ``timeout`` doesn't create a new task.

The ``timeout(timeout, *, loop=None)`` call returns a context manager
that cancels a block on *timeout* expiring::

   async with timeout(1.5):
       await inner()

1. If ``inner()`` is executed faster than in ``1.5`` seconds nothing
   happens.
2. Otherwise ``inner()`` is cancelled internally by sending
   ``asyncio.CancelledError`` into but ``asyncio.TimeoutError`` is
   raised outside of context manager scope.

*timeout* parameter could be ``None`` for skipping timeout functionality.


Context manager has ``.expired`` property for check if timeout happens
exactly in context manager::

   async with timeout(1.5) as cm:
       await inner()
   print(cm.expired)

The property is ``True`` if ``inner()`` execution is cancelled by
timeout context manager.

If ``inner()`` call explicitly raises ``TimeoutError`` ``cm.expired``
is ``False``.

Installation
------------

::

   $ pip install async-timeout

The library is Python 3 only!



Authors and License
-------------------

The module is written by Andrew Svetlov.

It's *Apache 2* licensed and freely available.


CHANGES
=======

3.0.1 (2018-10-09)
------------------

- More aggressive typing (#48)

3.0.0 (2018-05-05)
------------------

- Drop Python 3.4, the minimal supported version is Python 3.5.3

- Provide type annotations

2.0.1 (2018-03-13)
------------------

* Fix ``PendingDeprecationWarning`` on Python 3.7 (#33)


2.0.0 (2017-10-09)
------------------

* Changed `timeout <= 0` behaviour

  * Backward incompatibility change, prior this version `0` was
    shortcut for `None`
  * when timeout <= 0 `TimeoutError` raised faster

1.4.0 (2017-09-09)
------------------

* Implement `remaining` property (#20)

  * If timeout is not started yet or started unconstrained:
    `remaining` is `None`
  * If timeout is expired: `remaining` is `0.0`
  * All others: roughly amount of time before `TimeoutError` is triggered

1.3.0 (2017-08-23)
------------------

* Don't suppress nested exception on timeout. Exception context points
  on cancelled line with suspended `await` (#13)

* Introduce `.timeout` property (#16)

* Add methods for using as async context manager (#9)

1.2.1 (2017-05-02)
------------------

* Support unpublished event loop's "current_task" api.


1.2.0 (2017-03-11)
------------------

* Extra check on context manager exit

* 0 is no-op timeout


1.1.0 (2016-10-20)
------------------

* Rename to `async-timeout`

1.0.0 (2016-09-09)
------------------

* The first release.



  * [atomicwrites-1.3.0](https://github.com/untitaker/python-atomicwrites) ===================
python-atomicwrites
===================

.. image:: https://travis-ci.org/untitaker/python-atomicwrites.svg?branch=master
    :target: https://travis-ci.org/untitaker/python-atomicwrites

.. image:: https://ci.appveyor.com/api/projects/status/vadc4le3c27to59x/branch/master?svg=true
   :target: https://ci.appveyor.com/project/untitaker/python-atomicwrites/branch/master

Atomic file writes.

.. code-block:: python

    from atomicwrites import atomic_write

    with atomic_write('foo.txt', overwrite=True) as f:
        f.write('Hello world.')
        # "foo.txt" doesn't exist yet.

    # Now it does.


Features that distinguish it from other similar libraries (see `Alternatives and Credit`_):

- Race-free assertion that the target file doesn't yet exist. This can be
  controlled with the ``overwrite`` parameter.

- Windows support, although not well-tested. The MSDN resources are not very
  explicit about which operations are atomic. I'm basing my assumptions off `a
  comment
  <https://social.msdn.microsoft.com/Forums/windowsdesktop/en-US/449bb49d-8acc-48dc-a46f-0760ceddbfc3/movefileexmovefilereplaceexisting-ntfs-same-volume-atomic?forum=windowssdk#a239bc26-eaf0-4920-9f21-440bd2be9cc8>`_
  by `Doug Crook
  <https://social.msdn.microsoft.com/Profile/doug%20e.%20cook>`_, who appears
  to be a Microsoft employee:

      FAQ: Is MoveFileEx atomic
      Frequently asked question: Is MoveFileEx atomic if the existing and new
      files are both on the same drive?

      The simple answer is "usually, but in some cases it will silently fall-back
      to a non-atomic method, so don't count on it".

      The implementation of MoveFileEx looks something like this: [...]

      The problem is if the rename fails, you might end up with a CopyFile, which
      is definitely not atomic.

      If you really need atomic-or-nothing, you can try calling
      NtSetInformationFile, which is unsupported but is much more likely to be
      atomic. 

- Simple high-level API that wraps a very flexible class-based API.

- Consistent error handling across platforms.


How it works
============

It uses a temporary file in the same directory as the given path. This ensures
that the temporary file resides on the same filesystem.

The temporary file will then be atomically moved to the target location: On
POSIX, it will use ``rename`` if files should be overwritten, otherwise a
combination of ``link`` and ``unlink``. On Windows, it uses MoveFileEx_ through
stdlib's ``ctypes`` with the appropriate flags.

Note that with ``link`` and ``unlink``, there's a timewindow where the file
might be available under two entries in the filesystem: The name of the
temporary file, and the name of the target file.

Also note that the permissions of the target file may change this way. In some
situations a ``chmod`` can be issued without any concurrency problems, but
since that is not always the case, this library doesn't do it by itself.

.. _MoveFileEx: https://msdn.microsoft.com/en-us/library/windows/desktop/aa365240%28v=vs.85%29.aspx

fsync
-----

On POSIX, ``fsync`` is invoked on the temporary file after it is written (to
flush file content and metadata), and on the parent directory after the file is
moved (to flush filename).

``fsync`` does not take care of disks' internal buffers, but there don't seem
to be any standard POSIX APIs for that. On OS X, ``fcntl`` is used with
``F_FULLFSYNC`` instead of ``fsync`` for that reason.

On Windows, `_commit <https://msdn.microsoft.com/en-us/library/17618685.aspx>`_
is used, but there are no guarantees about disk internal buffers.

Alternatives and Credit
=======================

Atomicwrites is directly inspired by the following libraries (and shares a
minimal amount of code):

- The Trac project's `utility functions
  <http://www.edgewall.org/docs/tags-trac-0.11.7/epydoc/trac.util-pysrc.html>`_,
  also used in `Werkzeug <http://werkzeug.pocoo.org/>`_ and
  `mitsuhiko/python-atomicfile
  <https://github.com/mitsuhiko/python-atomicfile>`_. The idea to use
  ``ctypes`` instead of ``PyWin32`` originated there.

- `abarnert/fatomic <https://github.com/abarnert/fatomic>`_. Windows support
  (based on ``PyWin32``) was originally taken from there.

Other alternatives to atomicwrites include:

- `sashka/atomicfile <https://github.com/sashka/atomicfile>`_. Originally I
  considered using that, but at the time it was lacking a lot of features I
  needed (Windows support, overwrite-parameter, overriding behavior through
  subclassing).

- The `Boltons library collection <https://github.com/mahmoud/boltons>`_
  features a class for atomic file writes, which seems to have a very similar
  ``overwrite`` parameter. It is lacking Windows support though.

License
=======

Licensed under the MIT, see ``LICENSE``.

  * [attrs-19.1.0](https://www.attrs.org/) .. image:: https://www.attrs.org/en/latest/_static/attrs_logo.png
   :alt: attrs Logo

======================================
``attrs``: Classes Without Boilerplate
======================================

.. image:: https://readthedocs.org/projects/attrs/badge/?version=stable
   :target: https://www.attrs.org/en/stable/?badge=stable
   :alt: Documentation Status

.. image:: https://attrs.visualstudio.com/attrs/_apis/build/status/python-attrs.attrs?branchName=master
   :target: https://attrs.visualstudio.com/attrs/_build/latest?definitionId=1&branchName=master
   :alt: CI Status

.. image:: https://codecov.io/github/python-attrs/attrs/branch/master/graph/badge.svg
   :target: https://codecov.io/github/python-attrs/attrs
   :alt: Test Coverage

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/psf/black
   :alt: Code style: black

.. teaser-begin

``attrs`` is the Python package that will bring back the **joy** of **writing classes** by relieving you from the drudgery of implementing object protocols (aka `dunder <https://nedbatchelder.com/blog/200605/dunder.html>`_ methods).

Its main goal is to help you to write **concise** and **correct** software without slowing down your code.

.. -spiel-end-

For that, it gives you a class decorator and a way to declaratively define the attributes on that class:

.. -code-begin-

.. code-block:: pycon

   >>> import attr

   >>> @attr.s
   ... class SomeClass(object):
   ...     a_number = attr.ib(default=42)
   ...     list_of_numbers = attr.ib(factory=list)
   ...
   ...     def hard_math(self, another_number):
   ...         return self.a_number + sum(self.list_of_numbers) * another_number


   >>> sc = SomeClass(1, [1, 2, 3])
   >>> sc
   SomeClass(a_number=1, list_of_numbers=[1, 2, 3])

   >>> sc.hard_math(3)
   19
   >>> sc == SomeClass(1, [1, 2, 3])
   True
   >>> sc != SomeClass(2, [3, 2, 1])
   True

   >>> attr.asdict(sc)
   {'a_number': 1, 'list_of_numbers': [1, 2, 3]}

   >>> SomeClass()
   SomeClass(a_number=42, list_of_numbers=[])

   >>> C = attr.make_class("C", ["a", "b"])
   >>> C("foo", "bar")
   C(a='foo', b='bar')


After *declaring* your attributes ``attrs`` gives you:

- a concise and explicit overview of the class's attributes,
- a nice human-readable ``__repr__``,
- a complete set of comparison methods (equality and ordering),
- an initializer,
- and much more,

*without* writing dull boilerplate code again and again and *without* runtime performance penalties.

On Python 3.6 and later, you can often even drop the calls to ``attr.ib()`` by using `type annotations <https://www.attrs.org/en/latest/types.html>`_.

This gives you the power to use actual classes with actual types in your code instead of confusing ``tuple``\ s or `confusingly behaving <https://www.attrs.org/en/stable/why.html#namedtuples>`_ ``namedtuple``\ s.
Which in turn encourages you to write *small classes* that do `one thing well <https://www.destroyallsoftware.com/talks/boundaries>`_.
Never again violate the `single responsibility principle <https://en.wikipedia.org/wiki/Single_responsibility_principle>`_ just because implementing ``__init__`` et al is a painful drag.


.. -testimonials-

Testimonials
============

**Amber Hawkie Brown**, Twisted Release Manager and Computer Owl:

  Writing a fully-functional class using attrs takes me less time than writing this testimonial.


**Glyph Lefkowitz**, creator of `Twisted <https://twistedmatrix.com/>`_, `Automat <https://pypi.org/project/Automat/>`_, and other open source software, in `The One Python Library Everyone Needs <https://glyph.twistedmatrix.com/2016/08/attrs.html>`_:

  I’m looking forward to is being able to program in Python-with-attrs everywhere.
  It exerts a subtle, but positive, design influence in all the codebases I’ve see it used in.


**Kenneth Reitz**, creator of `Requests <https://github.com/psf/requests>`_ (`on paper no less <https://twitter.com/hynek/status/866817877650751488>`_!):

  attrs—classes for humans.  I like it.


**Łukasz Langa**, creator of `Black <https://github.com/psf/black>`_, prolific Python core developer, and release manager for Python 3.8 and 3.9:

  I'm increasingly digging your attr.ocity. Good job!


.. -end-

.. -project-information-

Getting Help
============

Please use the ``python-attrs`` tag on `StackOverflow <https://stackoverflow.com/questions/tagged/python-attrs>`_ to get help.

Answering questions of your fellow developers is also great way to help the project!


Project Information
===================

``attrs`` is released under the `MIT <https://choosealicense.com/licenses/mit/>`_ license,
its documentation lives at `Read the Docs <https://www.attrs.org/>`_,
the code on `GitHub <https://github.com/python-attrs/attrs>`_,
and the latest release on `PyPI <https://pypi.org/project/attrs/>`_.
It’s rigorously tested on Python 2.7, 3.4+, and PyPy.

We collect information on **third-party extensions** in our `wiki <https://github.com/python-attrs/attrs/wiki/Extensions-to-attrs>`_.
Feel free to browse and add your own!

If you'd like to contribute to ``attrs`` you're most welcome and we've written `a little guide <https://www.attrs.org/en/latest/contributing.html>`_ to get you started!


Release Information
===================

19.2.0 (2019-10-01)
-------------------

Backward-incompatible Changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Removed deprecated ``Attribute`` attribute ``convert`` per scheduled removal on 2019/1.
  This planned deprecation is tracked in issue `#307 <https://github.com/python-attrs/attrs/issues/307>`_.
  `#504 <https://github.com/python-attrs/attrs/issues/504>`_
- ``__lt__``, ``__le__``, ``__gt__``, and ``__ge__`` do not consider subclasses comparable anymore.

  This has been deprecated since 18.2.0 and was raising a ``DeprecationWarning`` for over a year.
  `#570 <https://github.com/python-attrs/attrs/issues/570>`_


Deprecations
^^^^^^^^^^^^

- The ``cmp`` argument to ``attr.s()`` and ``attr.ib()`` is now deprecated.

  Please use ``eq`` to add equality methods (``__eq__`` and ``__ne__``) and ``order`` to add ordering methods (``__lt__``, ``__le__``, ``__gt__``, and ``__ge__``) instead – just like with `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_.

  Both are effectively ``True`` by default but it's enough to set ``eq=False`` to disable both at once.
  Passing ``eq=False, order=True`` explicitly will raise a ``ValueError`` though.

  Since this is arguably a deeper backward-compatibility break, it will have an extended deprecation period until 2021-06-01.
  After that day, the ``cmp`` argument will be removed.

  ``attr.Attribute`` also isn't orderable anymore.
  `#574 <https://github.com/python-attrs/attrs/issues/574>`_


Changes
^^^^^^^

- Updated ``attr.validators.__all__`` to include new validators added in `#425`_.
  `#517 <https://github.com/python-attrs/attrs/issues/517>`_
- Slotted classes now use a pure Python mechanism to rewrite the ``__class__`` cell when rebuilding the class, so ``super()`` works even on environments where ``ctypes`` is not installed.
  `#522 <https://github.com/python-attrs/attrs/issues/522>`_
- When collecting attributes using ``@attr.s(auto_attribs=True)``, attributes with a default of ``None`` are now deleted too.
  `#523 <https://github.com/python-attrs/attrs/issues/523>`_,
  `#556 <https://github.com/python-attrs/attrs/issues/556>`_
- Fixed ``attr.validators.deep_iterable()`` and ``attr.validators.deep_mapping()`` type stubs.
  `#533 <https://github.com/python-attrs/attrs/issues/533>`_
- ``attr.validators.is_callable()`` validator now raises an exception ``attr.exceptions.NotCallableError``, a subclass of ``TypeError``, informing the received value.
  `#536 <https://github.com/python-attrs/attrs/issues/536>`_
- ``@attr.s(auto_exc=True)`` now generates classes that are hashable by ID, as the documentation always claimed it would.
  `#543 <https://github.com/python-attrs/attrs/issues/543>`_,
  `#563 <https://github.com/python-attrs/attrs/issues/563>`_
- Added ``attr.validators.matches_re()`` that checks string attributes whether they match a regular expression.
  `#552 <https://github.com/python-attrs/attrs/issues/552>`_
- Keyword-only attributes (``kw_only=True``) and attributes that are excluded from the ``attrs``'s ``__init__`` (``init=False``) now can appear before mandatory attributes.
  `#559 <https://github.com/python-attrs/attrs/issues/559>`_
- The fake filename for generated methods is now more stable.
  It won't change when you restart the process.
  `#560 <https://github.com/python-attrs/attrs/issues/560>`_
- The value passed to ``@attr.ib(repr=…)`` can now be either a boolean (as before) or a callable.
  That callable must return a string and is then used for formatting the attribute by the generated ``__repr__()`` method.
  `#568 <https://github.com/python-attrs/attrs/issues/568>`_
- Added ``attr.__version_info__`` that can be used to reliably check the version of ``attrs`` and write forward- and backward-compatible code.
  Please check out the `section on deprecated APIs <http://www.attrs.org/en/stable/api.html#deprecated-apis>`_ on how to use it.
  `#580 <https://github.com/python-attrs/attrs/issues/580>`_

 .. _`#425`: https://github.com/python-attrs/attrs/issues/425

`Full changelog <https://www.attrs.org/en/stable/changelog.html>`_.

Credits
=======

``attrs`` is written and maintained by `Hynek Schlawack <https://hynek.me/>`_.

The development is kindly supported by `Variomedia AG <https://www.variomedia.de/>`_.

A full list of contributors can be found in `GitHub's overview <https://github.com/python-attrs/attrs/graphs/contributors>`_.

It’s the spiritual successor of `characteristic <https://characteristic.readthedocs.io/>`_ and aspires to fix some of it clunkiness and unfortunate decisions.
Both were inspired by Twisted’s `FancyEqMixin <https://twistedmatrix.com/documents/current/api/twisted.python.util.FancyEqMixin.html>`_ but both are implemented using class decorators because `subclassing is bad for you <https://www.youtube.com/watch?v=3MNVP9-hglc>`_, m’kay?



  * [autopep8-1.4.4](https://github.com/hhatto/autopep8) ========
autopep8
========

.. image:: https://img.shields.io/pypi/v/autopep8.svg
    :target: https://pypi.org/project/autopep8/
    :alt: PyPI Version

.. image:: https://travis-ci.org/hhatto/autopep8.svg?branch=master
    :target: https://travis-ci.org/hhatto/autopep8
    :alt: Build status

autopep8 automatically formats Python code to conform to the `PEP 8`_ style
guide. It uses the pycodestyle_ utility to determine what parts of the code
needs to be formatted. autopep8 is capable of fixing most of the formatting
issues_ that can be reported by pycodestyle.

.. _PEP 8: https://www.python.org/dev/peps/pep-0008/
.. _issues: https://pycodestyle.readthedocs.org/en/latest/intro.html#error-codes

.. contents::


Installation
============

From pip::

    $ pip install --upgrade autopep8

Consider using the ``--user`` option_.

.. _option: https://pip.pypa.io/en/latest/user_guide/#user-installs


Requirements
============

autopep8 requires pycodestyle_.

.. _pycodestyle: https://github.com/PyCQA/pycodestyle


Usage
=====

To modify a file in place (with aggressive level 2)::

    $ autopep8 --in-place --aggressive --aggressive <filename>

Before running autopep8.

.. code-block:: python

    import math, sys;

    def example1():
        ####This is a long comment. This should be wrapped to fit within 72 characters.
        some_tuple=(   1,2, 3,'a'  );
        some_variable={'long':'Long code lines should be wrapped within 79 characters.',
        'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],
        'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,
        20,300,40000,500000000,60000000000000000]}}
        return (some_tuple, some_variable)
    def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));
    class Example3(   object ):
        def __init__    ( self, bar ):
         #Comments should have a space after the hash.
         if bar : bar+=1;  bar=bar* bar   ; return bar
         else:
                        some_string = """
    		           Indentation in multiline strings should not be touched.
    Only actual code should be reindented.
    """
                        return (sys.path, some_string)

After running autopep8.

.. code-block:: python

    import math
    import sys


    def example1():
        # This is a long comment. This should be wrapped to fit within 72
        # characters.
        some_tuple = (1, 2, 3, 'a')
        some_variable = {
            'long': 'Long code lines should be wrapped within 79 characters.',
            'other': [
                math.pi,
                100,
                200,
                300,
                9876543210,
                'This is a long string that goes on'],
            'more': {
                'inner': 'This whole logical line should be wrapped.',
                some_tuple: [
                    1,
                    20,
                    300,
                    40000,
                    500000000,
                    60000000000000000]}}
        return (some_tuple, some_variable)


    def example2(): return ('' in {'f': 2}) in {'has_key() is deprecated': True}


    class Example3(object):
        def __init__(self, bar):
            # Comments should have a space after the hash.
            if bar:
                bar += 1
                bar = bar * bar
                return bar
            else:
                some_string = """
    		           Indentation in multiline strings should not be touched.
    Only actual code should be reindented.
    """
                return (sys.path, some_string)

Options::

    usage: autopep8 [-h] [--version] [-v] [-d] [-i] [--global-config filename]
                    [--ignore-local-config] [-r] [-j n] [-p n] [-a]
                    [--experimental] [--exclude globs] [--list-fixes]
                    [--ignore errors] [--select errors] [--max-line-length n]
                    [--line-range line line] [--hang-closing] [--exit-code]
                    [files [files ...]]

    Automatically formats Python code to conform to the PEP 8 style guide.

    positional arguments:
      files                 files to format or '-' for standard in

    optional arguments:
      -h, --help            show this help message and exit
      --version             show program's version number and exit
      -v, --verbose         print verbose messages; multiple -v result in more
                            verbose messages
      -d, --diff            print the diff for the fixed source
      -i, --in-place        make changes to files in place
      --global-config filename
                            path to a global pep8 config file; if this file does
                            not exist then this is ignored (default:
                            ~/.config/pep8)
      --ignore-local-config
                            don't look for and apply local config files; if not
                            passed, defaults are updated with any config files in
                            the project's root directory
      -r, --recursive       run recursively over directories; must be used with
                            --in-place or --diff
      -j n, --jobs n        number of parallel jobs; match CPU count if value is
                            less than 1
      -p n, --pep8-passes n
                            maximum number of additional pep8 passes (default:
                            infinite)
      -a, --aggressive      enable non-whitespace changes; multiple -a result in
                            more aggressive changes
      --experimental        enable experimental fixes
      --exclude globs       exclude file/directory names that match these comma-
                            separated globs
      --list-fixes          list codes for fixes; used by --ignore and --select
      --ignore errors       do not fix these errors/warnings (default:
                            E226,E24,W50,W690)
      --select errors       fix only these errors/warnings (e.g. E4,W)
      --max-line-length n   set maximum allowed line length (default: 79)
      --line-range line line, --range line line
                            only fix errors found within this inclusive range of
                            line numbers (e.g. 1 99); line numbers are indexed at
                            1
      --hang-closing        hang-closing option passed to pycodestyle
      --exit-code           change to behavior of exit code. default behavior of
                            return value, 0 is no differences, 1 is error exit.
                            return 2 when add this option. 2 is exists
                            differences.


Features
========

autopep8 fixes the following issues_ reported by pycodestyle_::

    E101 - Reindent all lines.
    E11  - Fix indentation.
    E121 - Fix indentation to be a multiple of four.
    E122 - Add absent indentation for hanging indentation.
    E123 - Align closing bracket to match opening bracket.
    E124 - Align closing bracket to match visual indentation.
    E125 - Indent to distinguish line from next logical line.
    E126 - Fix over-indented hanging indentation.
    E127 - Fix visual indentation.
    E128 - Fix visual indentation.
    E129 - Fix visual indentation.
    E131 - Fix hanging indent for unaligned continuation line.
    E133 - Fix missing indentation for closing bracket.
    E20  - Remove extraneous whitespace.
    E211 - Remove extraneous whitespace.
    E22  - Fix extraneous whitespace around keywords.
    E224 - Remove extraneous whitespace around operator.
    E225 - Fix missing whitespace around operator.
    E226 - Fix missing whitespace around arithmetic operator.
    E227 - Fix missing whitespace around bitwise/shift operator.
    E228 - Fix missing whitespace around modulo operator.
    E231 - Add missing whitespace.
    E241 - Fix extraneous whitespace around keywords.
    E242 - Remove extraneous whitespace around operator.
    E251 - Remove whitespace around parameter '=' sign.
    E252 - Missing whitespace around parameter equals.
    E26  - Fix spacing after comment hash for inline comments.
    E265 - Fix spacing after comment hash for block comments.
    E266 - Fix too many leading '#' for block comments.
    E27  - Fix extraneous whitespace around keywords.
    E301 - Add missing blank line.
    E302 - Add missing 2 blank lines.
    E303 - Remove extra blank lines.
    E304 - Remove blank line following function decorator.
    E305 - Expected 2 blank lines after end of function or class.
    E306 - Expected 1 blank line before a nested definition.
    E401 - Put imports on separate lines.
    E402 - Fix module level import not at top of file
    E501 - Try to make lines fit within --max-line-length characters.
    E502 - Remove extraneous escape of newline.
    E701 - Put colon-separated compound statement on separate lines.
    E70  - Put semicolon-separated compound statement on separate lines.
    E711 - Fix comparison with None.
    E712 - Fix comparison with boolean.
    E713 - Use 'not in' for test for membership.
    E714 - Use 'is not' test for object identity.
    E721 - Use "isinstance()" instead of comparing types directly.
    E722 - Fix bare except.
    E731 - Use a def when use do not assign a lambda expression.
    W291 - Remove trailing whitespace.
    W292 - Add a single newline at the end of the file.
    W293 - Remove trailing whitespace on blank line.
    W391 - Remove trailing blank lines.
    W503 - Fix line break before binary operator.
    W504 - Fix line break after binary operator.
    W601 - Use "in" rather than "has_key()".
    W602 - Fix deprecated form of raising exception.
    W603 - Use "!=" instead of "<>"
    W604 - Use "repr()" instead of backticks.
    W605 - Fix invalid escape sequence 'x'.
    W690 - Fix various deprecated code (via lib2to3).

autopep8 also fixes some issues not found by pycodestyle_.

- Correct deprecated or non-idiomatic Python code (via ``lib2to3``). Use this
  for making Python 2.7 code more compatible with Python 3. (This is triggered
  if ``W690`` is enabled.)
- Normalize files with mixed line endings.
- Put a blank line between a class docstring and its first method
  declaration. (Enabled with ``E301``.)
- Remove blank lines between a function declaration and its docstring. (Enabled
  with ``E303``.)

autopep8 avoids fixing some issues found by pycodestyle_.

- ``E112``/``E113`` for non comments are reports of bad indentation that break
  syntax rules. These should not be modified at all.
- ``E265``, which refers to spacing after comment hash, is ignored if the
  comment looks like code. autopep8 avoids modifying these since they are not
  real comments. If you really want to get rid of the pycodestyle_ warning,
  consider just removing the commented-out code. (This can be automated via
  eradicate_.)

.. _eradicate: https://github.com/myint/eradicate


More advanced usage
===================

By default autopep8 only makes whitespace changes. Thus, by default, it does
not fix ``E711`` and ``E712``. (Changing ``x == None`` to ``x is None`` may
change the meaning of the program if ``x`` has its ``__eq__`` method
overridden.) Nor does it correct deprecated code ``W6``. To enable these
more aggressive fixes, use the ``--aggressive`` option::

    $ autopep8 --aggressive <filename>

Use multiple ``--aggressive`` to increase the aggressiveness level. For
example, ``E712`` requires aggressiveness level 2 (since ``x == True`` could be
changed to either ``x`` or ``x is True``, but autopep8 chooses the former).

``--aggressive`` will also shorten lines more aggressively. It will also remove
trailing whitespace more aggressively. (Usually, we don't touch trailing
whitespace in docstrings and other multiline strings. And to do even more
aggressive changes to docstrings, use docformatter_.)

.. _docformatter: https://github.com/myint/docformatter

To enable only a subset of the fixes, use the ``--select`` option. For example,
to fix various types of indentation issues::

    $ autopep8 --select=E1,W1 <filename>

Similarly, to just fix deprecated code::

    $ autopep8 --aggressive --select=W6 <filename>

The above is useful when trying to port a single code base to work with both
Python 2 and Python 3 at the same time.

If the file being fixed is large, you may want to enable verbose progress
messages::

    $ autopep8 -v <filename>


Use as a module
===============

The simplest way of using autopep8 as a module is via the ``fix_code()``
function:

    >>> import autopep8
    >>> autopep8.fix_code('x=       123\n')
    'x = 123\n'

Or with options:

    >>> import autopep8
    >>> autopep8.fix_code('x.has_key(y)\n',
    ...                   options={'aggressive': 1})
    'y in x\n'
    >>> autopep8.fix_code('print( 123 )\n',
    ...                   options={'ignore': ['E']})
    'print( 123 )\n'


Configuration
=============

By default, if ``$HOME/.config/pycodestyle`` (``~\.pycodestyle`` in Windows
environment) exists, it will be used as global configuration file.
Alternatively, you can specify the global configuration file with the
``--global-config`` option.

Also, if ``setup.cfg``, ``tox.ini``, ``.pep8`` and ``.flake8`` files exist
in the directory where the target file exists, it will be used as the
configuration file.

``pep8``, ``pycodestyle``, and ``flake8`` can be used as a section.

configuration file example::

    [pycodestyle]
    max_line_length = 120
    ignore = E501


Testing
=======

Test cases are in ``test/test_autopep8.py``. They can be run directly via
``python test/test_autopep8.py`` or via tox_. The latter is useful for
testing against multiple Python interpreters. (We currently test against
CPython versions 2.7, 3.4, 3.5, 3.6 and 3.7. We also test against PyPy.)

.. _`tox`: https://pypi.org/project/tox/

Broad spectrum testing is available via ``test/acid.py``. This script runs
autopep8 against Python code and checks for correctness and completeness of the
code fixes. It can check that the bytecode remains identical.
``test/acid_pypi.py`` makes use of ``acid.py`` to test against the latest
released packages on PyPI.


Troubleshooting
===============

``pkg_resources.DistributionNotFound``
--------------------------------------

If you are using an ancient version of ``setuptools``, you might encounter
``pkg_resources.DistributionNotFound`` when trying to run ``autopep8``. Try
upgrading ``setuptools`` to workaround this ``setuptools`` problem::

    $ pip install --upgrade setuptools

Use ``sudo`` if you are installing to the system.


Links
=====

* PyPI_
* GitHub_
* `Travis CI`_
* Coveralls_

.. _PyPI: https://pypi.org/project/autopep8/
.. _GitHub: https://github.com/hhatto/autopep8
.. _`Travis CI`: https://travis-ci.org/hhatto/autopep8
.. _`Coveralls`: https://coveralls.io/r/hhatto/autopep8

  * [awscli-1.16.211](http://aws.amazon.com/cli/) =======
aws-cli
=======

.. image:: https://travis-ci.org/aws/aws-cli.svg?branch=develop
   :target: https://travis-ci.org/aws/aws-cli
   :alt: Build Status

.. image:: https://badges.gitter.im/aws/aws-cli.svg
   :target: https://gitter.im/aws/aws-cli
   :alt: Gitter


This package provides a unified command line interface to Amazon Web Services.

The aws-cli package works on Python versions:

* 2.6.5 and greater
* 2.7.x and greater
* 3.3.x and greater
* 3.4.x and greater
* 3.5.x and greater
* 3.6.x and greater
* 3.7.x and greater

.. attention::
   We recommend that all customers regularly monitor the
   `Amazon Web Services Security Bulletins website`_ for any important security bulletins related to
   aws-cli.


------------
Installation
------------

The easiest way to install aws-cli is to use `pip`_ in a ``virtualenv``::

    $ pip install awscli

or, if you are not installing in a ``virtualenv``, to install globally::

    $ sudo pip install awscli

or for your user::

    $ pip install --user awscli

If you have the aws-cli installed and want to upgrade to the latest version
you can run::

    $ pip install --upgrade awscli

.. note::

    On OS X, if you see an error regarding the version of six that came with
    distutils in El Capitan, use the ``--ignore-installed`` option::

        $ sudo pip install awscli --ignore-installed six


This will install the aws-cli package as well as all dependencies.  You can
also just `download the tarball`_.  Once you have the
awscli directory structure on your workstation, you can just run::

    $ cd <path_to_awscli>
    $ python setup.py install

If you want to run the ``develop`` branch of the CLI, see the
"CLI Dev Version" section below.


------------
CLI Releases
------------

The release notes for the AWS CLI can be found `here <https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst>`__.


------------------
Command Completion
------------------

The aws-cli package includes a very useful command completion feature.
This feature is not automatically installed so you need to configure it manually.
To enable tab completion for bash either use the built-in command ``complete``::

    $ complete -C aws_completer aws

Or add ``bin/aws_bash_completer`` file under ``/etc/bash_completion.d``,
``/usr/local/etc/bash_completion.d`` or any other ``bash_completion.d`` location.

For tcsh::

    $ complete aws 'p/*/`aws_completer`/'

You should add this to your startup scripts to enable it for future sessions.

For zsh please refer to ``bin/aws_zsh_completer.sh``.  Source that file, e.g.
from your ``~/.zshrc``, and make sure you run ``compinit`` before::

    $ source bin/aws_zsh_completer.sh

For now the bash compatibility auto completion (``bashcompinit``) is used.
For further details please refer to the top of ``bin/aws_zsh_completer.sh``.

---------------
Getting Started
---------------

Before using aws-cli, you need to tell it about your AWS credentials.  You
can do this in several ways:

* Environment variables
* Shared credentials file
* Config file
* IAM Role

The quickest way to get started is to run the ``aws configure`` command::

    $ aws configure
    AWS Access Key ID: foo
    AWS Secret Access Key: bar
    Default region name [us-west-2]: us-west-2
    Default output format [None]: json

To use environment variables, do the following::

    $ export AWS_ACCESS_KEY_ID=<access_key>
    $ export AWS_SECRET_ACCESS_KEY=<secret_key>

To use the shared credentials file, create an INI formatted file like this::

    [default]
    aws_access_key_id=foo
    aws_secret_access_key=bar

    [testing]
    aws_access_key_id=foo
    aws_secret_access_key=bar

and place it in ``~/.aws/credentials`` (or in
``%UserProfile%\.aws/credentials`` on Windows). If you wish to place the
shared credentials file in a different location than the one specified above,
you need to tell aws-cli where to find it.  Do this by setting
the appropriate environment variable::

    $ export AWS_SHARED_CREDENTIALS_FILE=/path/to/shared_credentials_file

To use a config file, create a configuration file like this::

    [default]
    aws_access_key_id=<default access key>
    aws_secret_access_key=<default secret key>
    # Optional, to define default region for this profile.
    region=us-west-1

    [profile testing]
    aws_access_key_id=<testing access key>
    aws_secret_access_key=<testing secret key>
    region=us-west-2

and place it in ``~/.aws/config`` (or in ``%UserProfile%\.aws\config`` on Windows). If you wish to place the config file in a different location than the one
specified above, you need to tell aws-cli where to find it.  Do this by setting
the appropriate environment variable::

    $ export AWS_CONFIG_FILE=/path/to/config_file

As you can see, you can have multiple ``profiles`` defined in both the shared
credentials file and the  configuration file. You can then specify which
profile to use by using the ``--profile`` option. If no profile is specified
the ``default`` profile is used.

In the config file, except for the default profile, you
**must** prefix each config section of a profile group with ``profile``.
For example, if you have a profile named "testing" the section header would
be ``[profile testing]``.

The final option for credentials is highly recommended if you are
using aws-cli on an EC2 instance.  IAM Roles are
a great way to have credentials installed automatically on your
instance.  If you are using IAM Roles, aws-cli will find them and use
them automatically.

----------------------------
Other Configurable Variables
----------------------------

In addition to credentials, a number of other variables can be
configured either with environment variables, configuration file
entries or both.  The following table documents these.

============================= =========== ============================= ================================= ==================================
Variable                      Option      Config Entry                  Environment Variable              Description
============================= =========== ============================= ================================= ==================================
profile                       --profile   profile                       AWS_PROFILE                       Default profile name
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
region                        --region    region                        AWS_DEFAULT_REGION                Default AWS Region
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
config_file                                                             AWS_CONFIG_FILE                   Alternate location of config
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
credentials_file                                                        AWS_SHARED_CREDENTIALS_FILE       Alternate location of credentials
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
output                        --output    output                        AWS_DEFAULT_OUTPUT                Default output style
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
ca_bundle                     --ca-bundle ca_bundle                     AWS_CA_BUNDLE                     CA Certificate Bundle
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
access_key                                aws_access_key_id             AWS_ACCESS_KEY_ID                 AWS Access Key
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
secret_key                                aws_secret_access_key         AWS_SECRET_ACCESS_KEY             AWS Secret Key
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
token                                     aws_session_token             AWS_SESSION_TOKEN                 AWS Token (temp credentials)
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
cli_timestamp_format                      cli_timestamp_format                                            Output format of timestamps
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
metadata_service_timeout                  metadata_service_timeout      AWS_METADATA_SERVICE_TIMEOUT      EC2 metadata timeout
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
metadata_service_num_attempts             metadata_service_num_attempts AWS_METADATA_SERVICE_NUM_ATTEMPTS EC2 metadata retry count
----------------------------- ----------- ----------------------------- --------------------------------- ----------------------------------
parameter_validation                      parameter_validation                                            Toggles local parameter validation
============================= =========== ============================= ================================= ==================================

^^^^^^^^
Examples
^^^^^^^^

If you get tired of specifying a ``--region`` option on the command line
all of the time, you can specify a default region to use whenever no
explicit ``--region`` option is included using the ``region`` variable.
To specify this using an environment variable::

    $ export AWS_DEFAULT_REGION=us-west-2

To include it in your config file::

    [default]
    aws_access_key_id=<default access key>
    aws_secret_access_key=<default secret key>
    region=us-west-1

Similarly, the ``profile`` variable can be used to specify which profile to use
if one is not explicitly specified on the command line via the
``--profile`` option.  To set this via environment variable::

    $ export AWS_PROFILE=testing

The ``profile`` variable can not be specified in the configuration file
since it would have to be associated with a profile and would defeat the
purpose.

^^^^^^^^^^^^^^^^^^^
Further Information
^^^^^^^^^^^^^^^^^^^

For more information about configuration options, please refer the
`AWS CLI Configuration Variables topic <http://docs.aws.amazon.com/cli/latest/topic/config-vars.html#cli-aws-help-config-vars>`_. You can access this topic
from the CLI as well by running ``aws help config-vars``.


----------------------------------------
Accessing Services With Global Endpoints
----------------------------------------

Some services, such as *AWS Identity and Access Management* (IAM)
have a single, global endpoint rather than different endpoints for
each region.

To make access to these services simpler, aws-cli will automatically
use the global endpoint unless you explicitly supply a region (using
the ``--region`` option) or a profile (using the ``--profile`` option).
Therefore, the following::

    $ aws iam list-users

will automatically use the global endpoint for the IAM service
regardless of the value of the ``AWS_DEFAULT_REGION`` environment
variable or the ``region`` variable specified in your profile.

--------------------
JSON Parameter Input
--------------------

Many options that need to be provided are simple string or numeric
values.  However, some operations require JSON data structures
as input parameters either on the command line or in files.

For example, consider the command to authorize access to an EC2
security group.  In this case, we will add ingress access to port 22
for all IP addresses::

    $ aws ec2 authorize-security-group-ingress --group-name MySecurityGroup \
      --ip-permissions '{"FromPort":22,"ToPort":22,"IpProtocol":"tcp","IpRanges":[{"CidrIp": "0.0.0.0/0"}]}'

--------------------------
File-based Parameter Input
--------------------------

Some parameter values are so large or so complex that it would be easier
to place the parameter value in a file and refer to that file rather than
entering the value directly on the command line.

Let's use the ``authorize-security-group-ingress`` command shown above.
Rather than provide the value of the ``--ip-permissions`` parameter directly
in the command, you could first store the values in a file.  Let's call
the file ``ip_perms.json``::

    {"FromPort":22,
     "ToPort":22,
     "IpProtocol":"tcp",
     "IpRanges":[{"CidrIp":"0.0.0.0/0"}]}

Then, we could make the same call as above like this::

    $ aws ec2 authorize-security-group-ingress --group-name MySecurityGroup \
        --ip-permissions file://ip_perms.json

The ``file://`` prefix on the parameter value signals that the parameter value
is actually a reference to a file that contains the actual parameter value.
aws-cli will open the file, read the value and use that value as the
parameter value.

This is also useful when the parameter is really referring to file-based
data.  For example, the ``--user-data`` option of the ``aws ec2 run-instances``
command or the ``--public-key-material`` parameter of the
``aws ec2 import-key-pair`` command.

-------------------------
URI-based Parameter Input
-------------------------

Similar to the file-based input described above, aws-cli also includes a
way to use data from a URI as the value of a parameter.  The idea is exactly
the same except the prefix used is ``https://`` or ``http://``::

    $ aws ec2 authorize-security-group-ingress --group-name MySecurityGroup \
        --ip-permissions http://mybucket.s3.amazonaws.com/ip_perms.json

--------------
Command Output
--------------

The default output for commands is currently JSON.  You can use the
``--query`` option to extract the output elements from this JSON document.
For more information on the expression language used for the ``--query``
argument, you can read the
`JMESPath Tutorial <http://jmespath.org/tutorial.html>`__.

^^^^^^^^
Examples
^^^^^^^^

Get a list of IAM user names::

    $ aws iam list-users --query Users[].UserName

Get a list of key names and their sizes in an S3 bucket::

    $ aws s3api list-objects --bucket b --query Contents[].[Key,Size]

Get a list of all EC2 instances and include their Instance ID, State Name,
and their Name (if they've been tagged with a Name)::

    $ aws ec2 describe-instances --query \
      'Reservations[].Instances[].[InstanceId,State.Name,Tags[?Key==`Name`] | [0].Value]'


You may also find the `jq <http://stedolan.github.com/jq/>`_ tool useful in
processing the JSON output for other uses.

There is also an ASCII table format available.  You can select this style with
the ``--output table`` option or you can make this style your default output
style via environment variable or config file entry as described above.
Try adding ``--output table`` to the above commands.


---------------
CLI Dev Version
---------------

If you are just interested in using the latest released version of the AWS CLI,
please see the Installation_ section above.  This section is for anyone who
wants to install the development version of the CLI.  You normally would not
need to do this unless:

* You are developing a feature for the CLI and plan on submitting a Pull
  Request.
* You want to test the latest changes of the CLI before they make it into an
  official release.

The latest changes to the CLI are in the ``develop`` branch on github.  This is
the default branch when you clone the git repository.

Additionally, there are several other packages that are developed in lockstep
with the CLI.  This includes:

* `botocore <https://github.com/boto/botocore>`__
* `jmespath <https://github.com/boto/jmespath>`__

If you just want to install a snapshot of the latest development version of
the CLI, you can use the ``requirements.txt`` file included in this repo.
This file points to the development version of the above packages::

    cd <path_to_awscli>
    pip install -r requirements.txt
    pip install -e .

However, to keep up to date, you will continually have to run the
``pip install -r requirements.txt`` file to pull in the latest changes
from the develop branches of botocore, jmespath, etc.

You can optionally clone each of those repositories and run "pip install -e ."
for each repository::

    git clone <jmespath> && cd jmespath/
    pip install -e . && cd ..
    git clone <botocore> && cd botocore/
    pip install -e . && cd ..
    git clone <awscli> && cd aws-cli/
    pip install -e .


------------
Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `aws-cli <https://stackoverflow.com/questions/tagged/aws-cli>`__
* Come join the AWS CLI community chat on `gitter <https://gitter.im/aws/aws-cli>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/aws/aws-cli/issues/new>`__



.. _`Amazon Web Services Security Bulletins website`: https://aws.amazon.com/security/security-bulletins
.. _pip: http://www.pip-installer.org/en/latest/
.. _`download the tarball`: https://pypi.org/project/awscli/
  * [azure-4.0.0](https://github.com/Azure/azure-sdk-for-python) Microsoft Azure SDK for Python
==============================

This is the Microsoft Azure bundle.

This package does not contain any code in itself. It installs a set
of packages that provide Microsoft Azure functionality.

All packages in this bundle have been tested with Python 2.7, 3.4, 3.5, 3.6 and 3.7

This package uses PEP440 syntax, and thus requires pip >= 6.0 and/or setuptools >= 8.0
to be installed.


Documentation
=============

All documentation related to these packages can be found at http://docs.microsoft.com/python/azure/


Features
========

This version of the Azure package bundle consists of the following
packages. Follow the links for more information on each package.
Note that versions are fixed at the minor version number level
(i.e. no breaking changes can be introduced, but features are allowed)

-  `azure-mgmt v4.x <https://pypi.python.org/pypi/azure-mgmt>`__
-  `azure-applicationinsights v0.1.x <https://pypi.python.org/pypi/azure-applicationinsights>`__
-  `azure-batch v4.x <https://pypi.python.org/pypi/azure-batch>`__
-  `azure-cosmosb-table v1.x <https://pypi.python.org/pypi/azure-cosmosdb-table>`__
-  `azure-datalake-store v0.0.x <https://pypi.python.org/pypi/azure-datalake-store>`__
-  `azure-eventgrid v1.x <https://pypi.python.org/pypi/azure-eventgrid>`__
-  `azure-graphrbac v0.40.x <https://pypi.python.org/pypi/azure-graphrbac>`__
-  `azure-keyvault v1.x <https://pypi.python.org/pypi/azure-keyvault>`__
-  `azure-loganalytics v0.1.x <https://pypi.python.org/pypi/azure-loganalytics>`__
-  `azure-servicebus v0.21.x <https://pypi.python.org/pypi/azure-servicebus>`__
-  `azure-servicefabric v6.3.0.0 <https://pypi.python.org/pypi/azure-servicefabric>`__
-  `azure-servicemanagement-legacy v0.20.x <https://pypi.python.org/pypi/azure-servicemanagement-legacy>`__
-  `azure-storage-blob v1.x <https://pypi.python.org/pypi/azure-storage-blob>`__
-  `azure-storage-queue v1.x <https://pypi.python.org/pypi/azure-storage-queue>`__
-  `azure-storage-file v1.x <https://pypi.python.org/pypi/azure-storage-file>`__

Note that if you don't need all of these packages, you can install/uninstall them individually.


Installation
============

To install the Azure package bundle, type:

.. code:: shell

    pip install azure


Upgrade
=======

This package is not compatible with `azure-storage`.
If you installed `azure-storage`, or if you installed `azure` 1.x/2.x and didn't
uninstall `azure-storage`, you must uninstall `azure-storage` first:

.. code:: shell

    pip uninstall azure-storage


Upgrading from azure<1.0 is not supported. You must uninstall the old version first.

.. code:: shell

    pip uninstall azure
    pip install azure


Compatibility
=============

For details on the breaking changes, see the PyPI page of each individual package.



  * [azure-common-1.1.23](https://github.com/Azure/azure-sdk-for-python) Microsoft Azure SDK for Python
==============================

This is the Microsoft Azure common code.

This package provides shared code by the Azure packages.

If you are looking to install the Azure client libraries, see the
`azure <https://pypi.python.org/pypi/azure>`__ bundle package.


.. image::  https://azure-sdk-impressions.azurewebsites.net/api/impressions/azure-sdk-for-python%2Fazure-common%2FREADME.png


.. :changelog:

Release History
===============

1.1.23 (2019-06-24)
+++++++++++++++++++

- Add Monitor into Profile v2019_03_01_hybrid (requires azure-mgmt-monitor >= 0.7.0)

1.1.22 (2019-06-06)
+++++++++++++++++++

- Fix KeyVaultClient support for get_client_from_auth_file

1.1.21 (2019-05-21)
+++++++++++++++++++

- Fix compute support in profile v2019_03_01_hybrid

1.1.20 (2019-04-30)
+++++++++++++++++++

- Add support for profile v2019_03_01_hybrid
- Fix profile v2018_03_01_hybrid for DNS definition

1.1.19 (2019-04-18)
+++++++++++++++++++

- Azure Stack support for get_client_from_auth_file and get_client_from_json_dict

1.1.18 (2019-01-29)
+++++++++++++++++++

- Remove deprecated extra dependencies

1.1.17 (2019-01-15)
+++++++++++++++++++

- Fix KeyVaultClient creation with get_client_from_cli_profile

Thanks to patrikn for the contribution

1.1.16 (2018-09-26)
+++++++++++++++++++

- azure-nspkg is not installed anymore on Python 3 (PEP420-based namespace package)

1.1.15 (2018-09-13)
+++++++++++++++++++

**Features**

- Adding profile v2018-03-01-hybrid definition

1.1.14 (2018-07-23)
+++++++++++++++++++

**Features**

- Adding KeyVault to profile v2017_03_09_profile

1.1.13 (2018-07-03)
+++++++++++++++++++

**Features**

- get_azure_cli_credentials has a new parameter "with_tenant" to get default CLI tenant ID

**Bugfixes**

- get_client_from_cli_profile now supports the "azure-graphrbac" package #2867
- get_client_from_auth_file now supports the "azure-graphrbac" package #2867

1.1.12 (2018-05-29)
+++++++++++++++++++

**Features**

- Add Authorization profile definition

1.1.11 (2018-05-08)
+++++++++++++++++++

**Features**

- Add support for "resource" in "get_azure_cli_credentials"

1.1.10 (2018-04-30)
+++++++++++++++++++

**Bugfixes**

- Fix MultiApiClientMixin.__init__ to be a real mixin

1.1.9 (2018-04-03)
++++++++++++++++++

**Features**

- Add "azure.profiles" namespace #2247

**Bugfixes**

- get_client_from_cli_profile now supports Datalake #2318

1.1.8 (2017-07-28)
++++++++++++++++++

**Bugfix**

- Fix get_client_from_auth_file and get_client_from_json_dict on Python 2.7

Thank you to @jayden-at-arista for the contribution.

1.1.7 (2017-07-19)
++++++++++++++++++

- Adds azure.common.client_factory.get_client_from_auth_file
- Adds azure.common.client_factory.get_client_from_json_dict

1.1.6 (2017-05-16)
++++++++++++++++++

- Adds azure.common.client_factory.get_client_from_cli_profile

1.1.5 (2017-04-11)
++++++++++++++++++

- "extra_requires" autorest is deprecated and should not be used anymore
- This wheel package is now built with the azure wheel extension

1.1.4 (2016-05-25)
++++++++++++++++++

- Support for msrest/msrestazure 0.4.x series
- Drop support for msrest/msrestazure 0.3.x series

1.1.3 (2016-04-26)
++++++++++++++++++

- Support for msrest/msrestazure 0.3.x series
- Drop support for msrest/msrestazure 0.2.x series

1.1.2 (2016-03-28)
++++++++++++++++++

- Support for msrest/msrestazure 0.2.x series
- Drop support for msrest/msrestazure 0.1.x series

1.1.1 (2016-03-07)
++++++++++++++++++

- Move msrestazure depency as "extra_requires"

1.1.0 (2016-03-04)
++++++++++++++++++

- Support for msrest/msrestazure 0.1.x series
- Adds alias from msrestazure.azure_active_directory.* to azure.common.credentials

1.0.0 (2015-08-31)
++++++++++++++++++

Initial release, extracted from azure==0.11.1



  * [backcall-0.1.0](https://github.com/takluyver/backcall) ========
backcall
========

.. image:: https://travis-ci.org/takluyver/backcall.png?branch=master
        :target: https://travis-ci.org/takluyver/backcall

Specifications for callback functions passed in to an API

If your code lets other people supply callback functions, it's important to
specify the function signature you expect, and check that functions support that.
Adding extra parameters later would break other peoples code unless you're careful.

backcall provides a way of specifying the callback signature using a prototype
function::

    from backcall import callback_prototype
    
    @callback_prototype
    def handle_ping(sender, delay=None):
        # Specify positional parameters without a default, and keyword
        # parameters with a default.
        pass
    
    def register_ping_handler(callback):
        # This checks and adapts the function passed in:
        callback = handle_ping(callback)
        ping_callbacks.append(callback)

If the callback takes fewer parameters than your prototype, *backcall* will wrap
it in a function that discards the extra arguments. If the callback expects
more arguments, a TypeError is thrown when it is registered.
  * [backports.csv-1.0.7](https://github.com/ryanhiebert/backports.csv) ================================================
backports.csv: Backport of Python 3's csv module
================================================

.. image:: https://img.shields.io/pypi/v/backports.csv.svg
   :target: https://pypi.python.org/pypi/backports.csv
   :alt: Latest Version

.. image:: https://travis-ci.org/ryanhiebert/backports.csv.svg?branch=master
   :target: https://travis-ci.org/ryanhiebert/backports.csv

.. image:: https://badges.gitter.im/ryanhiebert/backports.csv.svg
   :alt: Join the chat at https://gitter.im/ryanhiebert/backports.csv
   :target: https://gitter.im/ryanhiebert/backports.csv?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. image:: https://requires.io/github/ryanhiebert/backports.csv/requirements.svg?branch=master
   :target: https://requires.io/github/ryanhiebert/backports.csv/requirements/?branch=master
   :alt: Requirements Status

The API of the csv module in Python 2 is drastically different from
the csv module in Python 3. This is due, for the most part, to the
difference between str in Python 2 and Python 3.

The semantics of Python 3's version are more useful because they support
unicode natively, while Python 2's csv does not.

Installation
============

.. code-block:: sh

    pip install backports.csv

Usage
=====

First make sure you're starting your file off right:

.. code-block:: python

    from backports import csv


Then be careful with your files to handle the encoding.
If you're working with a binary file-like object,
``io.TextIOWrapper`` can be very helpful.
If you're dealing with a file, you can just use ``io.open``
instead of Python 2's ``open`` builtin, and it works
just like Python 3's builtin ``open``.

.. code-block:: python

    from backports import csv
    import io

    def read_csv(filename):
        with io.open(filename, newline='', encoding='utf-8') as f:
            for row in csv.reader(f):
                yield row

    def write_csv(filename, rows):
        with io.open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            for row in rows:
                writer.writerow(row)

Note: It should always be safe to specify ``newline=''``,
since the csv module does its own (universal) newline handling.


1.0.7 (2019-03-10)
++++++++++++++++++

* Add tests to ``MANIFEST.in``.
  - thanks to @jayvdb for the pull request

1.0.6 (2018-05-22)
++++++++++++++++++

* Pass reader error messages along. (#28)
  This should help make errors more transparent.
  - thanks to @mpeteuil for the pull request

1.0.5 (2017-05-29)
++++++++++++++++++

* Fix bug in README example. (#22)
  - thanks to @tantale for the bug report
* Allow ``None`` as quotechar when using ``QUOTE_NONE``. (#23)
  - thanks to @thanatos for the bug report

1.0.4 (2017-02-17)
++++++++++++++++++

* Return write value from writerow. (#20)
  - thanks to @therg

1.0.3 (2017-01-23)
++++++++++++++++++

* Add LICENSE file (#18).

1.0.2 (2016-09-15)
++++++++++++++++++

* Avoid quoting any numeric types when using ``QUOTE_NONNUMERIC``.
  - thanks to @torfsen for the bug report

1.0.1 (2016-02-11)
++++++++++++++++++

* Better error messages for invalid dialects.
  - thanks to @kengruven for the bug report


1.0 (2016-02-11)
++++++++++++++++

* Initial Release



  * [backports.functools_lru_cache-1.5](https://github.com/jaraco/backports.functools_lru_cache) .. image:: https://img.shields.io/pypi/v/backports.functools_lru_cache.svg
   :target: https://pypi.org/project/backports.functools_lru_cache

.. image:: https://img.shields.io/pypi/pyversions/backports.functools_lru_cache.svg

.. image:: https://img.shields.io/travis/jaraco/backports.functools_lru_cache/master.svg
   :target: https://travis-ci.org/jaraco/backports.functools_lru_cache

.. .. image:: https://readthedocs.org/projects/backportsfunctools_lru_cache/badge/?version=latest
..    :target: https://backportsfunctools_lru_cache.readthedocs.io/en/latest/?badge=latest

Backport of functools.lru_cache from Python 3.3 as published at `ActiveState
<http://code.activestate.com/recipes/578078/>`_.

Usage
=====

Consider using this technique for importing the 'lru_cache' function::

    try:
        from functools import lru_cache
    except ImportError:
        from backports.functools_lru_cache import lru_cache



  * [backports.shutil_get_terminal_size-1.0.0](https://github.com/chrippa/backports.shutil_get_terminal_size) backports.shutil_get_terminal_size
==================================

A backport of the `get_terminal_size`_ function from Python 3.3's shutil.

Unlike the original version it is written in pure Python rather than C,
so it might be a tiny bit slower.

.. _get_terminal_size: https://docs.python.org/3/library/shutil.html#shutil.get_terminal_size


Example usage
-------------

::

    >>> from backports.shutil_get_terminal_size import get_terminal_size
    >>> get_terminal_size()
    terminal_size(columns=105, lines=33)


History
=======

1.0.0 (2014-08-19)
------------------

First release.
  * [backports.shutil_which-3.5.2](https://github.com/minrk/backports.shutil_which) 
Use Python 3 shutil.which on Python 2::

    try:
        from shutil import which
    except ImportError:
        from backports.shutil_which import which



  * [backports.weakref-1.0.post1](https://github.com/pjdelport/backports.weakref) =================
backports.weakref
=================

This package provides backports of new features in Python's weakref_ module
under the backports_ namespace.

.. _weakref: https://docs.python.org/3.5/library/weakref.html
.. _backports: https://pypi.python.org/pypi/backports

.. image:: https://img.shields.io/pypi/v/backports.weakref.svg
    :target: https://pypi.python.org/pypi/backports.weakref

.. image:: https://img.shields.io/badge/source-GitHub-lightgrey.svg
    :target: https://github.com/pjdelport/backports.weakref

.. image:: https://img.shields.io/github/issues/pjdelport/backports.weakref.svg
    :target: https://github.com/pjdelport/backports.weakref/issues?q=is:open

.. image:: https://travis-ci.org/pjdelport/backports.weakref.svg?branch=master
    :target: https://travis-ci.org/pjdelport/backports.weakref

.. image:: https://codecov.io/github/pjdelport/backports.weakref/coverage.svg?branch=master
    :target: https://codecov.io/github/pjdelport/backports.weakref?branch=master


Supported Python versions
=========================

* CPython: 2.7, 3.4, 3.5, 3.6
* PyPy


Backported functionality
========================

* `weakref.finalize`_ (new in Python 3.4)

.. _`weakref.finalize`: https://docs.python.org/3.5/library/weakref.html#weakref.finalize


Contributing
============

See `<HACKING.rst>`__.
  * [bagit-1.7.0](https://libraryofcongress.github.io/bagit-python/) bagit-python
============

|Build Status| |Coverage Status|

bagit is a Python library and command line utility for working with
`BagIt <http://purl.org/net/bagit>`__ style packages.

Installation
------------

bagit.py is a single-file python module that you can drop into your
project as needed or you can install globally with:

::

    pip install bagit

Python v2.7+ is required.

Command Line Usage
------------------

When you install bagit you should get a command-line program called
bagit.py which you can use to turn an existing directory into a bag:

::

    bagit.py --contact-name 'John Kunze' /directory/to/bag

Finding Bagit on your system
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``bagit.py`` program should be available in your normal command-line
window (Terminal on OS X, Command Prompt or Powershell on Windows,
etc.). If you are unsure where it was installed you can also request
that Python search for ``bagit`` as a Python module: simply replace
``bagit.py`` with ``python -m bagit``:

::

    python -m bagit --help

On some systems Python may have been installed as ``python3``, ``py``,
etc. – simply use the same name you use to start an interactive Python
shell:

::

    py -m bagit --help
    python3 -m bagit --help

Configuring BagIt
~~~~~~~~~~~~~~~~~

You can pass in key/value metadata for the bag using options like
``--contact-name`` above, which get persisted to the bag-info.txt. For a
complete list of bag-info.txt properties you can use as commmand line
arguments see ``--help``.

Since calculating checksums can take a while when creating a bag, you
may want to calculate them in parallel if you are on a multicore
machine. You can do that with the ``--processes`` option:

::

    bagit.py --processes 4 /directory/to/bag

To specify which checksum algorithm(s) to use when generating the
manifest, use the --md5, --sha1, --sha256 and/or --sha512 flags (MD5 is
generated by default).

::

    bagit.py --sha1 /path/to/bag
    bagit.py --sha256 /path/to/bag
    bagit.py --sha512 /path/to/bag

If you would like to validate a bag you can use the --validate flag.

::

    bagit.py --validate /path/to/bag

If you would like to take a quick look at the bag to see if it seems
valid by just examining the structure of the bag, and comparing its
payload-oxum (byte count and number of files) then use the ``--fast``
flag.

::

    bagit.py --validate --fast /path/to/bag

And finally, if you'd like to parallelize validation to take advantage
of multiple CPUs you can:

::

    bagit.py --validate --processes 4 /path/to/bag

Using BagIt in your programs
----------------------------

You can also use BagIt programatically in your own Python programs by
importing the ``bagit`` module.

Create
~~~~~~

To create a bag you would do this:

.. code:: python

    bag = bagit.make_bag('mydir', {'Contact-Name': 'John Kunze'})

``make_bag`` returns a Bag instance. If you have a bag already on disk
and would like to create a Bag instance for it, simply call the
constructor directly:

.. code:: python

    bag = bagit.Bag('/path/to/bag')

Update Bag Metadata
~~~~~~~~~~~~~~~~~~~

You can change the metadata persisted to the bag-info.txt by using the
``info`` property on a ``Bag``.

.. code:: python

    # load the bag
    bag = bagit.Bag('/path/to/bag')

    # update bag info metadata
    bag.info['Internal-Sender-Description'] = 'Updated on 2014-06-28.'
    bag.info['Authors'] = ['John Kunze', 'Andy Boyko']
    bag.save()

Update Bag Manifests
~~~~~~~~~~~~~~~~~~~~

By default ``save`` will not update manifests. This guards against a
situation where a call to ``save`` to persist bag metadata accidentally
regenerates manifests for an invalid bag. If you have modified the
payload of a bag by adding, modifying or deleting files in the data
directory, and wish to regenerate the manifests set the ``manifests``
parameter to True when calling ``save``.

.. code:: python


    import shutil, os

    # add a file
    shutil.copyfile('newfile', '/path/to/bag/data/newfile')

    # remove a file
    os.remove('/path/to/bag/data/file')

    # persist changes
    bag.save(manifests=True)

The save method takes an optional processes parameter which will
determine how many processes are used to regenerate the checksums. This
can be handy on multicore machines.

Validation
~~~~~~~~~~

If you would like to see if a bag is valid, use its ``is_valid`` method:

.. code:: python

    bag = bagit.Bag('/path/to/bag')
    if bag.is_valid():
        print("yay :)")
    else:
        print("boo :(")

If you'd like to get a detailed list of validation errors, execute the
``validate`` method and catch the ``BagValidationError`` exception. If
the bag's manifest was invalid (and it wasn't caught by the payload
oxum) the exception's ``details`` property will contain a list of
``ManifestError``\ s that you can introspect on. Each ManifestError,
will be of type ``ChecksumMismatch``, ``FileMissing``,
``UnexpectedFile``.

So for example if you want to print out checksums that failed to
validate you can do this:

.. code:: python


    bag = bagit.Bag("/path/to/bag")

    try:
      bag.validate()

    except bagit.BagValidationError as e:
        for d in e.details:
            if isinstance(d, bagit.ChecksumMismatch):
                print("expected %s to have %s checksum of %s but found %s" %
                      (d.path, d.algorithm, d.expected, d.found))

To iterate through a bag's manifest and retrieve checksums for the
payload files use the bag's entries dictionary:

.. code:: python

    bag = bagit.Bag("/path/to/bag")

    for path, fixity in bag.entries.items():
      print("path:%s md5:%s" % (path, fixity["md5"]))

Contributing to bagit-python development
----------------------------------------

::

    % git clone git://github.com/LibraryOfCongress/bagit-python.git
    % cd bagit-python
    # MAKE CHANGES
    % python test.py

Running the tests
~~~~~~~~~~~~~~~~~

You can quickly run the tests by having setuptools install dependencies:

::

    python setup.py test

Once your code is working, you can use
`Tox <https://tox.readthedocs.io/>`__ to run the tests with every
supported version of Python which you have installed on the local
system:

::

    tox

If you have Docker installed, you can run the tests under Linux inside a
container:

::

    % docker build -t bagit:latest . && docker run -it bagit:latest

Benchmarks
----------

If you'd like to see how increasing parallelization of bag creation on
your system effects the time to create a bag try using the included
bench utility:

::

    % ./bench.py

License
-------

|cc0|

Note: By contributing to this project, you agree to license your work
under the same terms as those that govern this project's distribution.

.. |Build Status| image:: https://travis-ci.org/LibraryOfCongress/bagit-python.svg?branch=master
   :target: http://travis-ci.org/LibraryOfCongress/bagit-python
.. |Coverage Status| image:: https://coveralls.io/repos/github/LibraryOfCongress/bagit-python/badge.svg?branch=master
   :target: https://coveralls.io/github/LibraryOfCongress/bagit-python?branch=master
.. |cc0| image:: http://i.creativecommons.org/p/zero/1.0/88x31.png
   :target: http://creativecommons.org/publicdomain/zero/1.0/

  * [bandit-1.6.2](https://bandit.readthedocs.io/en/latest/) .. image:: https://github.com/PyCQA/bandit/blob/master/logo/logotype-sm.png
    :alt: Bandit

======

.. image:: https://travis-ci.org/PyCQA/bandit.svg?branch=master
    :target: https://travis-ci.org/PyCQA/bandit/
    :alt: Build Status

.. image:: https://readthedocs.org/projects/bandit/badge/?version=latest
    :target: https://readthedocs.org/projects/bandit/
    :alt: Docs Status

.. image:: https://img.shields.io/pypi/v/bandit.svg
    :target: https://pypi.org/project/bandit/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/bandit.svg
    :target: https://pypi.org/project/bandit/
    :alt: Python Versions

.. image:: https://img.shields.io/pypi/format/bandit.svg
    :target: https://pypi.org/project/bandit/
    :alt: Format

.. image:: https://img.shields.io/badge/license-Apache%202-blue.svg
    :target: https://github.com/PyCQA/bandit/blob/master/LICENSE
    :alt: License

A security linter from PyCQA

* Free software: Apache license
* Documentation: https://bandit.readthedocs.io/en/latest/
* Source: https://github.com/PyCQA/bandit
* Bugs: https://github.com/PyCQA/bandit/issues

Overview
--------
Bandit is a tool designed to find common security issues in Python code. To do
this Bandit processes each file, builds an AST from it, and runs appropriate
plugins against the AST nodes. Once Bandit has finished scanning all the files
it generates a report.

Bandit was originally developed within the OpenStack Security Project and
later rehomed to PyCQA.

Installation
------------
Bandit is distributed on PyPI. The best way to install it is with pip:


Create a virtual environment (optional)::

    virtualenv bandit-env

Install Bandit::

    pip install bandit
    # Or if you're working with a Python 3 project
    pip3 install bandit

Run Bandit::

    bandit -r path/to/your/code


Bandit can also be installed from source. To do so, download the source tarball
from PyPI, then install it::

    python setup.py install


Usage
-----
Example usage across a code tree::

    bandit -r ~/your_repos/project

Example usage across the ``examples/`` directory, showing three lines of
context and only reporting on the high-severity issues::

    bandit examples/*.py -n 3 -lll

Bandit can be run with profiles. To run Bandit against the examples directory
using only the plugins listed in the ``ShellInjection`` profile::

    bandit examples/*.py -p ShellInjection

Bandit also supports passing lines of code to scan using standard input. To
run Bandit with standard input::

    cat examples/imports.py | bandit -

Usage::

    $ bandit -h
    usage: bandit [-h] [-r] [-a {file,vuln}] [-n CONTEXT_LINES] [-c CONFIG_FILE]
                  [-p PROFILE] [-t TESTS] [-s SKIPS] [-l] [-i]
                  [-f {csv,custom,html,json,screen,txt,xml,yaml}]
                  [--msg-template MSG_TEMPLATE] [-o [OUTPUT_FILE]] [-v] [-d] [-q]
                  [--ignore-nosec] [-x EXCLUDED_PATHS] [-b BASELINE]
                  [--ini INI_PATH] [--version]
                  [targets [targets ...]]

    Bandit - a Python source code security analyzer

    positional arguments:
      targets               source file(s) or directory(s) to be tested

    optional arguments:
      -h, --help            show this help message and exit
      -r, --recursive       find and process files in subdirectories
      -a {file,vuln}, --aggregate {file,vuln}
                            aggregate output by vulnerability (default) or by
                            filename
      -n CONTEXT_LINES, --number CONTEXT_LINES
                            maximum number of code lines to output for each issue
      -c CONFIG_FILE, --configfile CONFIG_FILE
                            optional config file to use for selecting plugins and
                            overriding defaults
      -p PROFILE, --profile PROFILE
                            profile to use (defaults to executing all tests)
      -t TESTS, --tests TESTS
                            comma-separated list of test IDs to run
      -s SKIPS, --skip SKIPS
                            comma-separated list of test IDs to skip
      -l, --level           report only issues of a given severity level or higher
                            (-l for LOW, -ll for MEDIUM, -lll for HIGH)
      -i, --confidence      report only issues of a given confidence level or
                            higher (-i for LOW, -ii for MEDIUM, -iii for HIGH)
      -f {csv,custom,html,json,screen,txt,xml,yaml}, --format {csv,custom,html,json,screen,txt,xml,yaml}
                            specify output format
      --msg-template MSG_TEMPLATE
                            specify output message template (only usable with
                            --format custom), see CUSTOM FORMAT section for list
                            of available values
      -o [OUTPUT_FILE], --output [OUTPUT_FILE]
                            write report to filename
      -v, --verbose         output extra information like excluded and included
                            files
      -d, --debug           turn on debug mode
      -q, --quiet, --silent
                            only show output in the case of an error
      --ignore-nosec        do not skip lines with # nosec comments
      -x EXCLUDED_PATHS, --exclude EXCLUDED_PATHS
                            comma-separated list of paths (glob patterns supported)
                            to exclude from scan (note that these are in addition
                            to the excluded paths provided in the config file)
      -b BASELINE, --baseline BASELINE
                            path of a baseline report to compare against (only
                            JSON-formatted files are accepted)
      --ini INI_PATH        path to a .bandit file that supplies command line
                            arguments
      --version             show program's version number and exit

    CUSTOM FORMATTING
    -----------------

    Available tags:

        {abspath}, {relpath}, {line},  {test_id},
        {severity}, {msg}, {confidence}, {range}

    Example usage:

        Default template:
        bandit -r examples/ --format custom --msg-template \
        "{abspath}:{line}: {test_id}[bandit]: {severity}: {msg}"

        Provides same output as:
        bandit -r examples/ --format custom

        Tags can also be formatted in python string.format() style:
        bandit -r examples/ --format custom --msg-template \
        "{relpath:20.20s}: {line:03}: {test_id:^8}: DEFECT: {msg:>20}"

        See python documentation for more information about formatting style:
        https://docs.python.org/3.4/library/string.html

    The following tests were discovered and loaded:
    -----------------------------------------------

      B101  assert_used
      B102  exec_used
      B103  set_bad_file_permissions
      B104  hardcoded_bind_all_interfaces
      B105  hardcoded_password_string
      B106  hardcoded_password_funcarg
      B107  hardcoded_password_default
      B108  hardcoded_tmp_directory
      B110  try_except_pass
      B112  try_except_continue
      B201  flask_debug_true
      B301  pickle
      B302  marshal
      B303  md5
      B304  ciphers
      B305  cipher_modes
      B306  mktemp_q
      B307  eval
      B308  mark_safe
      B309  httpsconnection
      B310  urllib_urlopen
      B311  random
      B312  telnetlib
      B313  xml_bad_cElementTree
      B314  xml_bad_ElementTree
      B315  xml_bad_expatreader
      B316  xml_bad_expatbuilder
      B317  xml_bad_sax
      B318  xml_bad_minidom
      B319  xml_bad_pulldom
      B320  xml_bad_etree
      B321  ftplib
      B322  input
      B323  unverified_context
      B324  hashlib_new_insecure_functions
      B325  tempnam
      B401  import_telnetlib
      B402  import_ftplib
      B403  import_pickle
      B404  import_subprocess
      B405  import_xml_etree
      B406  import_xml_sax
      B407  import_xml_expat
      B408  import_xml_minidom
      B409  import_xml_pulldom
      B410  import_lxml
      B411  import_xmlrpclib
      B412  import_httpoxy
      B413  import_pycrypto
      B501  request_with_no_cert_validation
      B502  ssl_with_bad_version
      B503  ssl_with_bad_defaults
      B504  ssl_with_no_version
      B505  weak_cryptographic_key
      B506  yaml_load
      B507  ssh_no_host_key_verification
      B601  paramiko_calls
      B602  subprocess_popen_with_shell_equals_true
      B603  subprocess_without_shell_equals_true
      B604  any_other_function_with_shell_equals_true
      B605  start_process_with_a_shell
      B606  start_process_with_no_shell
      B607  start_process_with_partial_path
      B608  hardcoded_sql_expressions
      B609  linux_commands_wildcard_injection
      B610  django_extra_used
      B611  django_rawsql_used
      B701  jinja2_autoescape_false
      B702  use_of_mako_templates
      B703  django_mark_safe

Baseline
--------
Bandit allows specifying the path of a baseline report to compare against using the base line argument (i.e. ``-b BASELINE`` or ``--baseline BASELINE``). 

::
  
   bandit -b BASELINE

This is useful for ignoring known vulnerabilities that you believe are non-issues (e.g. a cleartext password in a unit test). To generate a baseline report simply run Bandit with the output format set to ``json`` (only JSON-formatted files are accepted as a baseline) and output file path specified:

::

    bandit -f json -o PATH_TO_OUTPUT_FILE


Version control integration
---------------------------

Use `pre-commit <https://pre-commit.com/>`_. Once you `have it
installed <https://pre-commit.com/#install>`_, add this to the
`.pre-commit-config.yaml` in your repository
(be sure to update `rev` to point to a real git tag/revision!)::

    repos:
    -   repo: https://github.com/PyCQA/bandit
        rev: '' # Update me!
        hooks:
        - id: bandit


Then run `pre-commit install` and you're ready to go.

Configuration
-------------
An optional config file may be supplied and may include:
 - lists of tests which should or shouldn't be run
 - exclude_dirs - sections of the path, that if matched, will be excluded from
   scanning (glob patterns supported)
 - overridden plugin settings - may provide different settings for some
   plugins

Per Project Command Line Args
-----------------------------
Projects may include a `.bandit` file that specifies command line arguments
that should be supplied for that project. The currently supported arguments
are:

 - targets: comma separated list of target dirs/files to run bandit on
 - exclude: comma separated list of excluded paths
 - skips: comma separated list of tests to skip
 - tests: comma separated list of tests to run

To use this, put a .bandit file in your project's directory. For example:

::

   [bandit]
   exclude: /test

::

   [bandit]
   tests: B101,B102,B301


Exclusions
----------
In the event that a line of code triggers a Bandit issue, but that the line
has been reviewed and the issue is a false positive or acceptable for some
other reason, the line can be marked with a ``# nosec`` and any results
associated with it will not be reported.

For example, although this line may cause Bandit to report a potential
security issue, it will not be reported::

    self.process = subprocess.Popen('/bin/echo', shell=True)  # nosec


Vulnerability Tests
-------------------
Vulnerability tests or "plugins" are defined in files in the plugins directory.

Tests are written in Python and are autodiscovered from the plugins directory.
Each test can examine one or more type of Python statements. Tests are marked
with the types of Python statements they examine (for example: function call,
string, import, etc).

Tests are executed by the ``BanditNodeVisitor`` object as it visits each node
in the AST.

Test results are maintained in the ``BanditResultStore`` and aggregated for
output at the completion of a test run.


Writing Tests
-------------
To write a test:
 - Identify a vulnerability to build a test for, and create a new file in
   examples/ that contains one or more cases of that vulnerability.
 - Consider the vulnerability you're testing for, mark the function with one
   or more of the appropriate decorators:
   - @checks('Call')
   - @checks('Import', 'ImportFrom')
   - @checks('Str')
 - Create a new Python source file to contain your test, you can reference
   existing tests for examples.
 - The function that you create should take a parameter "context" which is
   an instance of the context class you can query for information about the
   current element being examined.  You can also get the raw AST node for
   more advanced use cases.  Please see the context.py file for more.
 - Extend your Bandit configuration file as needed to support your new test.
 - Execute Bandit against the test file you defined in examples/ and ensure
   that it detects the vulnerability.  Consider variations on how this
   vulnerability might present itself and extend the example file and the test
   function accordingly.


Extending Bandit
----------------

Bandit allows users to write and register extensions for checks and formatters.
Bandit will load plugins from two entry-points:

- `bandit.formatters`
- `bandit.plugins`

Formatters need to accept 4 things:

- `result_store`: An instance of `bandit.core.BanditResultStore`
- `file_list`: The list of files which were inspected in the scope
- `scores`: The scores awarded to each file in the scope
- `excluded_files`: The list of files that were excluded from the scope

Plugins tend to take advantage of the `bandit.checks` decorator which allows
the author to register a check for a particular type of AST node. For example

::

    @bandit.checks('Call')
    def prohibit_unsafe_deserialization(context):
        if 'unsafe_load' in context.call_function_name_qual:
            return bandit.Issue(
                severity=bandit.HIGH,
                confidence=bandit.HIGH,
                text="Unsafe deserialization detected."
            )

To register your plugin, you have two options:

1. If you're using setuptools directly, add something like the following to
   your ``setup`` call::

        # If you have an imaginary bson formatter in the bandit_bson module
        # and a function called `formatter`.
        entry_points={'bandit.formatters': ['bson = bandit_bson:formatter']}
        # Or a check for using mako templates in bandit_mako that
        entry_points={'bandit.plugins': ['mako = bandit_mako']}

2. If you're using pbr, add something like the following to your `setup.cfg`
   file::

        [entry_points]
        bandit.formatters =
            bson = bandit_bson:formatter
        bandit.plugins =
            mako = bandit_mako

Contributing
------------
Contributions to Bandit are always welcome!

The best way to get started with Bandit is to grab the source::

    git clone https://github.com/PyCQA/bandit.git

You can test any changes with tox::

    pip install tox
    tox -e pep8
    tox -e py27
    tox -e py35
    tox -e docs
    tox -e cover

Please make PR requests using your own branch, and not master::

    git checkout -b mychange
    git push origin mychange

Reporting Bugs
--------------
Bugs should be reported on github. To file a bug against Bandit, visit:
https://github.com/PyCQA/bandit/issues

Under Which Version of Python Should I Install Bandit?
------------------------------------------------------
The answer to this question depends on the project(s) you will be running
Bandit against. If your project is only compatible with Python 2.7, you
should install Bandit to run under Python 2.7. If your project is only
compatible with Python 3.5, then use 3.5 respectively. If your project supports
both, you *could* run Bandit with both versions but you don't have to.

Bandit uses the `ast` module from Python's standard library in order to
analyze your Python code. The `ast` module is only able to parse Python code
that is valid in the version of the interpreter from which it is imported. In
other words, if you try to use Python 2.7's `ast` module to parse code written
for 3.5 that uses, for example, `yield from` with asyncio, then you'll have
syntax errors that will prevent Bandit from working properly. Alternatively,
if you are relying on 2.7's octal notation of `0777` then you'll have a syntax
error if you run Bandit on 3.x.


References
==========

Bandit docs: https://bandit.readthedocs.io/en/latest/

Python AST module documentation: https://docs.python.org/2/library/ast.html

Green Tree Snakes - the missing Python AST docs:
https://greentreesnakes.readthedocs.org/en/latest/

Documentation of the various types of AST nodes that Bandit currently covers
or could be extended to cover:
https://greentreesnakes.readthedocs.org/en/latest/nodes.html


  * [bbknn-1.3.5](https://github.com/Teichlab/bbknn) 
  * [bcbio-gff-0.6.6](https://github.com/chapmanb/bcbb/tree/master/gff) 
  * [bcrypt-3.1.4](https://github.com/pyca/bcrypt/) bcrypt
======

.. image:: https://img.shields.io/pypi/v/bcrypt.svg
    :target: https://pypi.org/project/bcrypt/
    :alt: Latest Version

.. image:: https://travis-ci.org/pyca/bcrypt.svg?branch=master
    :target: https://travis-ci.org/pyca/bcrypt

.. image:: https://dev.azure.com/pyca/bcrypt/_apis/build/status/bcrypt-CI?branchName=master
    :target: https://dev.azure.com/pyca/bcrypt/_build/latest?definitionId=8&branchName=master

Good password hashing for your software and your servers


Installation
============

To install bcrypt, simply:

.. code:: bash

    $ pip install bcrypt

Note that bcrypt should build very easily on Linux provided you have a C compiler, headers for Python (if you're not using pypy), and headers for the libffi libraries available on your system.

For Debian and Ubuntu, the following command will ensure that the required dependencies are installed:

.. code:: bash

    $ sudo apt-get install build-essential libffi-dev python-dev

For Fedora and RHEL-derivatives, the following command will ensure that the required dependencies are installed:

.. code:: bash

    $ sudo yum install gcc libffi-devel python-devel

Alternatives
============

While bcrypt remains a good choice for password storage depending on your specific use case you may also want to consider using scrypt (either via `standard library`_ or `cryptography`_) or argon2id via `argon2_cffi`_.

Changelog
=========

3.1.7
-----

* Set a ``setuptools`` lower bound for PEP517 wheel building.
* We no longer distribute 32-bit ``manylinux1`` wheels. Continuing to produce
  them was a maintenance burden.

3.1.6
-----

* Added support for compilation on Haiku.

3.1.5
-----

* Added support for compilation on AIX.
* Dropped Python 2.6 and 3.3 support.
* Switched to using ``abi3`` wheels for Python 3. If you are not getting a
  wheel on a compatible platform please upgrade your ``pip`` version.

3.1.4
-----

* Fixed compilation with mingw and on illumos.

3.1.3
-----
* Fixed a compilation issue on Solaris.
* Added a warning when using too few rounds with ``kdf``.

3.1.2
-----
* Fixed a compile issue affecting big endian platforms.
* Fixed invalid escape sequence warnings on Python 3.6.
* Fixed building in non-UTF8 environments on Python 2.

3.1.1
-----
* Resolved a ``UserWarning`` when used with ``cffi`` 1.8.3.

3.1.0
-----
* Added support for ``checkpw``, a convenience method for verifying a password.
* Ensure that you get a ``$2y$`` hash when you input a ``$2y$`` salt.
* Fixed a regression where ``$2a`` hashes were vulnerable to a wraparound bug.
* Fixed compilation under Alpine Linux.

3.0.0
-----
* Switched the C backend to code obtained from the OpenBSD project rather than
  openwall.
* Added support for ``bcrypt_pbkdf`` via the ``kdf`` function.

2.0.0
-----
* Added support for an adjustible prefix when calling ``gensalt``.
* Switched to CFFI 1.0+

Usage
-----

Password Hashing
~~~~~~~~~~~~~~~~

Hashing and then later checking that a password matches the previous hashed
password is very simple:

.. code:: pycon

    >>> import bcrypt
    >>> password = b"super secret password"
    >>> # Hash a password for the first time, with a randomly-generated salt
    >>> hashed = bcrypt.hashpw(password, bcrypt.gensalt())
    >>> # Check that an unhashed password matches one that has previously been
    >>> # hashed
    >>> if bcrypt.checkpw(password, hashed):
    ...     print("It Matches!")
    ... else:
    ...     print("It Does not Match :(")

KDF
~~~

As of 3.0.0 ``bcrypt`` now offers a ``kdf`` function which does ``bcrypt_pbkdf``.
This KDF is used in OpenSSH's newer encrypted private key format.

.. code:: pycon

    >>> import bcrypt
    >>> key = bcrypt.kdf(
    ...     password=b'password',
    ...     salt=b'salt',
    ...     desired_key_bytes=32,
    ...     rounds=100)


Adjustable Work Factor
~~~~~~~~~~~~~~~~~~~~~~
One of bcrypt's features is an adjustable logarithmic work factor. To adjust
the work factor merely pass the desired number of rounds to
``bcrypt.gensalt(rounds=12)`` which defaults to 12):

.. code:: pycon

    >>> import bcrypt
    >>> password = b"super secret password"
    >>> # Hash a password for the first time, with a certain number of rounds
    >>> hashed = bcrypt.hashpw(password, bcrypt.gensalt(14))
    >>> # Check that a unhashed password matches one that has previously been
    >>> #   hashed
    >>> if bcrypt.checkpw(password, hashed):
    ...     print("It Matches!")
    ... else:
    ...     print("It Does not Match :(")


Adjustable Prefix
~~~~~~~~~~~~~~~~~

Another one of bcrypt's features is an adjustable prefix to let you define what
libraries you'll remain compatible with. To adjust this, pass either ``2a`` or
``2b`` (the default) to ``bcrypt.gensalt(prefix=b"2b")`` as a bytes object.

As of 3.0.0 the ``$2y$`` prefix is still supported in ``hashpw`` but deprecated.

Maximum Password Length
~~~~~~~~~~~~~~~~~~~~~~~

The bcrypt algorithm only handles passwords up to 72 characters, any characters
beyond that are ignored. To work around this, a common approach is to hash a
password with a cryptographic hash (such as ``sha256``) and then base64
encode it to prevent NULL byte problems before hashing the result with
``bcrypt``:

.. code:: pycon

    >>> password = b"an incredibly long password" * 10
    >>> hashed = bcrypt.hashpw(
    ...     base64.b64encode(hashlib.sha256(password).digest()),
    ...     bcrypt.gensalt()
    ... )

Compatibility
-------------

This library should be compatible with py-bcrypt and it will run on Python
2.7, 3.4+, and PyPy 2.6+.

C Code
------

This library uses code from OpenBSD.

Security
--------

``bcrypt`` follows the `same security policy as cryptography`_, if you
identify a vulnerability, we ask you to contact us privately.

.. _`same security policy as cryptography`: https://cryptography.io/en/latest/security/
.. _`standard library`: https://docs.python.org/3/library/hashlib.html#hashlib.scrypt
.. _`argon2_cffi`: https://argon2-cffi.readthedocs.io
.. _`cryptography`: https://cryptography.io/en/latest/hazmat/primitives/key-derivation-functions/#cryptography.hazmat.primitives.kdf.scrypt.Scrypt
  * [bd2k-python-lib-1.14a1.dev48](https://github.com/BD2KGenomics/bd2k-python-lib) 
  * [beautifulsoup4-4.8.0](http://www.crummy.com/software/BeautifulSoup/bs4/) Beautiful Soup is a library that makes it easy to scrape information
from web pages. It sits atop an HTML or XML parser, providing Pythonic
idioms for iterating, searching, and modifying the parse tree.

# Quick start

```
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup("<p>Some<b>bad<i>HTML")
>>> print soup.prettify()
<html>
<body>
<p>
Some
<b>
bad
<i>
HTML
</i>
</b>
</p>
</body>
</html>

>>> soup.find(text="bad")
u'bad'

>>> soup.i
<i>HTML</i>

>>> soup = BeautifulSoup("<tag1>Some<tag2/>bad<tag3>XML", "xml")
>>> print soup.prettify()
<?xml version="1.0" encoding="utf-8">
<tag1>
Some
<tag2 />
bad
<tag3>
XML
</tag3>
</tag1>
```

To go beyond the basics, [comprehensive documentation is available](http://www.crummy.com/software/BeautifulSoup/bs4/doc/).

# Links

* [Homepage](http://www.crummy.com/software/BeautifulSoup/bs4/)
* [Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)
* [Discussion group](http://groups.google.com/group/beautifulsoup/)
* [Development](https://code.launchpad.net/beautifulsoup/)
* [Bug tracker](https://bugs.launchpad.net/beautifulsoup/)
* [Complete changelog](https://bazaar.launchpad.net/~leonardr/beautifulsoup/bs4/view/head:/CHANGELOG)

# Note on Python 2 sunsetting

Since 2012, Beautiful Soup has been developed as a Python 2 library
which is automatically converted to Python 3 code as necessary. This
makes it impossible to take advantages of some features of Python
3.

For this reason, I plan to discontinue Beautiful Soup's Python 2
support at some point after January 1, 2021: one year after the sunset
date for Python 2 itself. Beyond that point, new Beautiful Soup
development will exclusively target Python 3. Of course, older
releases of Beautiful Soup, which support both versions, will continue
to be available.

# Supporting the project

If you use Beautiful Soup as part of your professional work, please consider a
[Tidelift subscription](https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=readme).
This will support many of the free software projects your organization
depends on, not just Beautiful Soup.

If you use Beautiful Soup for personal projects, the best way to say
thank you is to read
[Tool Safety](https://www.crummy.com/software/BeautifulSoup/zine/), a zine I
wrote about what Beautiful Soup has taught me about software
development.

# Building the documentation

The bs4/doc/ directory contains full documentation in Sphinx
format. Run `make html` in that directory to create HTML
documentation.

# Running the unit tests

Beautiful Soup supports unit test discovery from the project root directory:

```
$ nosetests
```

```
$ python -m unittest discover -s bs4
```

If you checked out the source tree, you should see a script in the
home directory called test-all-versions. This script will run the unit
tests under Python 2, then create a temporary Python 3 conversion of
the source and run the unit tests again under Python 3.



  * [betamax-0.8.1](https://github.com/sigmavirus24/betamax) betamax
=======

Betamax is a VCR_ imitation for requests. This will make mocking out requests
much easier. It is tested on `Travis CI`_.

Put in a more humorous way: "Betamax records your HTTP interactions so the NSA
does not have to."

Example Use
-----------

.. code-block:: python

    from betamax import Betamax
    from requests import Session
    from unittest import TestCase

    with Betamax.configure() as config:
        config.cassette_library_dir = 'tests/fixtures/cassettes'


    class TestGitHubAPI(TestCase):
        def setUp(self):
            self.session = Session()
            self.headers.update(...)

        # Set the cassette in a line other than the context declaration
        def test_user(self):
            with Betamax(self.session) as vcr:
                vcr.use_cassette('user')
                resp = self.session.get('https://api.github.com/user',
                                        auth=('user', 'pass'))
                assert resp.json()['login'] is not None

        # Set the cassette in line with the context declaration
        def test_repo(self):
            with Betamax(self.session).use_cassette('repo'):
                resp = self.session.get(
                    'https://api.github.com/repos/sigmavirus24/github3.py'
                    )
                assert resp.json()['owner'] != {}

What does it even do?
---------------------

If you are unfamiliar with VCR_, you might need a better explanation of what
Betamax does.

Betamax intercepts every request you make and attempts to find a matching
request that has already been intercepted and recorded. Two things can then
happen:

1. If there is a matching request, it will return the response that is
   associated with it.
2. If there is **not** a matching request and it is allowed to record new
   responses, it will make the request, record the response and return the
   response.

Recorded requests and corresponding responses - also known as interactions -
are stored in files called cassettes. (An example cassette can be seen in
the `examples section of the documentation`_.) The directory you store your
cassettes in is called your library, or your `cassette library`_.

VCR Cassette Compatibility
--------------------------

Betamax can use any VCR-recorded cassette as of this point in time. The only
caveat is that python-requests returns a URL on each response. VCR does not
store that in a cassette now but we will. Any VCR-recorded cassette used to
playback a response will unfortunately not have a URL attribute on responses
that are returned. This is a minor annoyance but not something that can be
fixed.

Contributing
------------

You can check out the project board on waffle.io_ to see what the status of
each issue is.

.. _VCR: https://github.com/vcr/vcr
.. _Travis CI: https://travis-ci.org/sigmavirus24/betamax
.. _waffle.io: https://waffle.io/sigmavirus24/betamax
.. _examples section of the documentation:
    http://betamax.readthedocs.org/en/latest/api.html#examples
.. _cassette library:
    http://betamax.readthedocs.org/en/latest/cassettes.html


History
=======

0.8.1 - 2018-03-13
------------------

- Previous attempts to sanitize cassette names were incomplete.
  Sanitization has become more thorough which could have some affects on
  existing cassette files. **This may cause new cassettes to be generated.**

- Fix bug where there may be an exception raised in a
  ``betamax.exceptions.BetamaxError`` repr.

0.8.0 - 2016-08-16
------------------

- Add ``betamax_parametrized_recorder`` and ``betamax_parametrized_session``
  to our list of pytest fixtures so that users will have parametrized cassette
  names when writing parametrized tests with our fixtures. (I wonder if I can
  mention parametrization a bunch more times so I can say parametrize a lot in
  this bullet note.)
- Add ``ValidationError`` and a set of subclasses for each possible validation
  error.
- Raise ``InvalidOption`` on unknown cassette options rather than silently
  ignoring extra options.
- Raise a subclass of ``ValidationError`` when a particular cassette option is
  invalid, rather than silently ignoring the validation failure.

0.7.2 - 2016-08-04
------------------

- Fix bug with query string matcher where query-strings without values (e.g.,
  ``?foo&bar`` as opposed to ``?foo=1&bar=2``) were treated as if there were
  no query string.

0.7.1 - 2016-06-14
------------------

- Fix issue #108 by effectively copying the items in the match_requests_on
  list into the match_options set on a Cassette instance

0.7.0 - 2016-04-29
------------------

- Add ``before_record`` and ``before_playback`` hooks

- Allow per-cassette placeholders to be merged and override global
  placeholders

- Fix bug where the ``QueryMatcher`` failed matching on high Unicode points

0.6.0 - 2016-04-12
------------------

- Add ``betamax_recorder`` pytest fixture

- Change default behaviour to allow duplicate interactions to be recorded in
  single cassette

- Add ``allow_playback_repeats`` to allow an interaction to be used more than
  once from a single cassette

- Always return a new ``Response`` object from an Interaction to allow for a
  streaming response to be usable multiple times

- Remove CI support for Pythons 2.6 and 3.2

0.5.1 - 2015-10-24
------------------

- Fix bugs with requests 2.8.x integration

- Fix bugs with older versions of requests that were missing an HTTPHeaderDict
  implementation

0.5.0 - 2015-07-15
------------------

- Add unittest integration in ``betamax.fixtures.unittest``

- Add pytest integration in ``betamax.fixtures.pytest``

- Add a decorator as a short cut for ``use_cassette``

- Fix bug where body bytes were not always encoded on Python 3.2+

  Fixed by @bboe

0.4.2 - 2015-04-18
------------------

- Fix issue #58 reported by @bboe

  Multiple cookies were not being properly stored or replayed after being
  recorded.

- @leighlondon converted ``__all__`` to a tuple

0.4.1 - 2014-09-24
------------------

- Fix issue #39 reported by @buttscicles

  This bug did not properly parse the Set-Cookie header with multiple cookies
  when replaying a recorded response.

0.4.0 - 2014-07-29
------------------

- Allow the user to pass placeholders to ``Betamax#use_cassette``.

- Include Betamax's version number in cassettes

0.3.2 - 2014-06-05
------------------

- Fix request and response bodies courtesy of @dgouldin

0.3.1 - 2014-05-28
------------------

- Fix GitHub Issue #35 - Placeholders were not being properly applied to
  request bodies. This release fixes that so placeholders are now behave as
  expected with recorded request bodies.

0.3.0 - 2014-05-23
------------------

- Add ``Betamax#start`` and ``Betamax#stop`` to allow users to start recording
  and stop without using a context-manager.

- Add ``digest-auth`` matcher to help users match the right request when using
  requests' ``HTTPDigestAuth``.

- Reorganize and refactor the cassettes, matchers, and serializers modules.

- Refactor some portions of code a bit.

- ``Cassette.cassette_name`` no longer is the relative path to the file in
  which the cassette is saved. To access that information use
  ``Cassette.cassette_path``. The ``cassette_name`` attribute is now the name
  that you pass to ``Betamax#use_cassette``.

0.2.0 - 2014-04-12
------------------

- Fix bug where new interactions recorded under ``new_episodes`` or ``all``
  were not actually saved to disk.

- Match URIs in a far more intelligent way.

- Use the Session's original adapters when making new requests

  In the event the Session has a custom adapter mounted, e.g., the SSLAdapter
  in requests-toolbelt, then we should probably use that.

- Add ``on_init`` hook to ``BaseMatcher`` so matcher authors can customize
  initialization

- Add support for custom Serialization formats. See the docs for more info.

- Add support for preserving exact body bytes.

- Deprecate ``serialize`` keyword to ``Betamax#use_cassette`` in preference
  for ``serialize_with`` (to be more similar to VCR).

0.1.6 - 2013-12-07
------------------

- Fix how global settings and per-invocation options are persisted and
  honored. (#10)

- Support ``match_requests_on`` as a parameter sent to
  ``Betamax#use_cassette``. (No issue)

0.1.5 - 2013-09-27
------------------

- Make sure what we pass to ``base64.b64decode`` is a bytes object

0.1.4 - 2013-09-27
------------------

- Do not try to sanitize something that may not exist.

0.1.3 - 2013-09-27
------------------

- Fix issue when response has a Content-Encoding of gzip and we need to
  preserve the original bytes of the message.

0.1.2 - 2013-09-21
------------------

- Fix issues with how requests parses cookies out of responses

- Fix unicode issues with ``Response#text`` (trying to use ``Response#json``
  raises exception because it cannot use string decoding on a unicode string)

0.1.1 - 2013-09-19
------------------

- Fix issue where there is a unicode character not in ``range(128)``

0.1.0 - 2013-09-17
------------------

- Initial Release

- Support for VCR generated cassettes (JSON only)

- Support for ``re_record_interval``

- Support for the ``once``, ``all``, ``new_episodes``, ``all`` cassette modes

- Support for filtering sensitive data

- Support for the following methods of request matching:

  - Method

  - URI

  - Host

  - Path

  - Query String

  - Body

  - Headers



  * [billiard-3.6.0.0](https://github.com/celery/billiard) ========
billiard
========
:version: 3.6.0.0

|build-status-lin| |build-status-win| |license| |wheel| |pyversion| |pyimp|

.. |build-status-lin| image:: https://secure.travis-ci.org/celery/billiard.png?branch=master
    :alt: Build status on Linux
    :target: https://travis-ci.org/celery/billiard

.. |build-status-win| image:: https://ci.appveyor.com/api/projects/status/github/celery/billiard?png=true&branch=master
    :alt: Build status on Windows
    :target: https://ci.appveyor.com/project/ask/billiard

.. |license| image:: https://img.shields.io/pypi/l/billiard.svg
    :alt: BSD License
    :target: https://opensource.org/licenses/BSD-3-Clause

.. |wheel| image:: https://img.shields.io/pypi/wheel/billiard.svg
    :alt: Billiard can be installed via wheel
    :target: https://pypi.org/project/billiard/

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/billiard.svg
    :alt: Supported Python versions.
    :target: https://pypi.org/project/billiard/

.. |pyimp| image:: https://img.shields.io/pypi/implementation/billiard.svg
    :alt: Support Python implementations.
    :target: https://pypi.org/project/billiard/

About
-----

``billiard`` is a fork of the Python 2.7 `multiprocessing <https://docs.python.org/library/multiprocessing.html>`_
package. The multiprocessing package itself is a renamed and updated version of
R Oudkerk's `pyprocessing <https://pypi.org/project/processing/>`_ package.
This standalone variant draws its fixes/improvements from python-trunk and provides
additional bug fixes and improvements.

- This package would not be possible if not for the contributions of not only
  the current maintainers but all of the contributors to the original pyprocessing
  package listed `here <http://pyprocessing.berlios.de/doc/THANKS.html>`_.

- Also, it is a fork of the multiprocessing backport package by Christian Heims.

- It includes the no-execv patch contributed by R. Oudkerk.

- And the Pool improvements previously located in `Celery`_.

- Billiard is used in and is a dependency for `Celery`_ and is maintained by the
  Celery team.

.. _`Celery`: http://celeryproject.org

Documentation
-------------

The documentation for ``billiard`` is available on `Read the Docs <https://billiard.readthedocs.io>`_.

Bug reporting
-------------

Please report bugs related to multiprocessing at the
`Python bug tracker <https://bugs.python.org/>`_. Issues related to billiard
should be reported at https://github.com/celery/billiard/issues.



  * [biolib-0.0.46](http://pypi.python.org/pypi/biolib/) 
  * [biopython-1.74](https://biopython.org/) .. image:: https://img.shields.io/pypi/v/biopython.svg
   :alt: Biopython on the Python Package Index (PyPI)
   :target: https://pypi.python.org/pypi/biopython
.. image:: https://img.shields.io/conda/vn/conda-forge/biopython.svg
   :alt: Biopython on the Conda package conda-forge channel
   :target: https://anaconda.org/conda-forge/biopython
.. image:: https://img.shields.io/travis/biopython/biopython/master.svg?logo=travis
   :alt: Linux testing with TravisCI
   :target: https://travis-ci.org/biopython/biopython/branches
.. image:: https://img.shields.io/appveyor/ci/biopython/biopython/master.svg?logo=appveyor
   :alt: Windows testing with AppVeyor
   :target: https://ci.appveyor.com/project/biopython/biopython/history
.. image:: https://img.shields.io/codecov/c/github/biopython/biopython/master.svg
   :alt: TravisCI test coverage
   :target: https://codecov.io/github/biopython/biopython/
.. image:: http://depsy.org/api/package/pypi/biopython/badge.svg
   :alt: Research software impact on Depsy
   :target: http://depsy.org/package/python/biopython

.. image:: https://github.com/biopython/biopython/raw/master/Doc/images/biopython_logo_m.png
   :alt: The Biopython Project
   :target: http://biopython.org

Biopython README file
=====================

The Biopython Project is an international association of developers of freely
available Python tools for computational molecular biology.

Our user-centric documentation is hosted on http://biopython.org including
the main Biopython Tutorial and Cookbook:

* HTML - http://biopython.org/DIST/docs/tutorial/Tutorial.html
* PDF - http://biopython.org/DIST/docs/tutorial/Tutorial.pdf

This README file is intended primarily for people interested in working
with the Biopython source code, either one of the releases from the
http://biopython.org website, or from our repository on GitHub
https://github.com/biopython/biopython

The `NEWS <https://github.com/biopython/biopython/blob/master/NEWS.rst>`_
file summarises the changes in each release of Biopython.

The Biopython package is open source software made available under generous
terms. Please see the `LICENSE
<https://github.com/biopython/biopython/blob/master/LICENSE.rst>`_ file for
further details.

If you use Biopython in work contributing to a scientific publication, we ask
that you cite our application note (below) or one of the module specific
publications (listed on our website):

Cock, P.J.A. et al. Biopython: freely available Python tools for computational
molecular biology and bioinformatics. Bioinformatics 2009 Jun 1; 25(11) 1422-3
http://dx.doi.org/10.1093/bioinformatics/btp163 pmid:19304878


For the impatient
=================

Python 2.7.9 onwards, and Python 3.4 onwards, include the package management
system "pip" which should allow you to install Biopython (and its dependency
NumPy if needed), upgrade or uninstall with just one terminal command::

    pip install biopython
    pip install --upgrade biopython
    pip uninstall biopython

Since Biopython 1.70 we have provided pre-compiled binary wheel packages on
PyPI for Linux, Mac OS X and Windows. This means pip install should be quick,
and not require a compiler.

As a developer or potential contributor, you may wish to download, build and
install Biopython yourself. This is described below.


Python Requirements
===================

We currently recommend using Python 3.7 from http://www.python.org

Biopython is currently supported and tested on the following Python
implementations:

- Python 2.7, 3.4, 3.5, 3.6, 3.7 -- see http://www.python.org

  Python 3 is the primary development platform for Biopython. We will drop
  support for Python 2.7 no later than 2020, in line with the end-of-life or
  sunset date for Python 2.7 itself.

- PyPy2.7 v6.0.0, or PyPy3.5 v6.0.0 -- see http://www.pypy.org

  Aside from ``Bio.trie`` (which does not compile as ``marshal.h`` is
  currently missing under PyPy), everything should work. Older versions
  of PyPy mostly work too.

- Jython 2.7 -- see http://www.jython.org

  We have decided to deprecate support for Jython, but aside from
  ``Bio.Restriction``, modules with C code, or dependent on SQLite3 or NumPy,
  everything should work. There are some known issues with test failures
  which have not yet been resolved.

Biopython 1.68 was our final release to support Python 2.6.


Optional Dependencies
=====================

Biopython requires NumPy (see http://www.numpy.org) which will be installed
automatically if you install Biopython with pip (see below for compiling
Biopython yourself).

Depending on which parts of Biopython you plan to use, there are a number of
other optional Python dependencies, which can be installed later if needed:

- ReportLab, see http://www.reportlab.com/opensource/ (optional)
  This package is only used in ``Bio.Graphics``, so if you do not need this
  functionality, you will not need to install this package.

- matplotlib, see http://matplotlib.org/ (optional)
  ``Bio.Phylo`` uses this package to plot phylogenetic trees.

- networkx, see http://networkx.lanl.gov/ (optional) and
  pygraphviz or pydot, see http://networkx.lanl.gov/pygraphviz/ and
  http://code.google.com/p/pydot/ (optional)
  These packages are used for certain niche functions in ``Bio.Phylo``.

- rdflib, see https://github.com/RDFLib/rdflib (optional)
  This package is used in the CDAO parser under ``Bio.Phylo``.

- psycopg2, see http://initd.org/psycopg/ (optional) or
  PyGreSQL (pgdb), see http://www.pygresql.org/ (optional)
  These packages are used by ``BioSQL`` to access a PostgreSQL database.

- MySQL Connector/Python, see http://dev.mysql.com/downloads/connector/python/
  This package is used by ``BioSQL`` to access a MySQL database, and is
  supported on Python 2 and 3 and PyPy too.

- mysqlclient, see https://github.com/PyMySQL/mysqlclient-python (optional)
  This is a fork of the older MySQLdb and is used by ``BioSQL`` to access a
  MySQL database. It is supported by Python 2.7, Python 3.5 and above, PyPy 2,
  and PyPy 3.

Note that some of these libraries are not available for PyPy or Jython,
and not all are available for Python 3 yet either.

In addition there are a number of useful third party tools you may wish to
install such as standalone NCBI BLAST, EMBOSS or ClustalW.


Installation From Source
========================

We recommend using the pre-compiled binary wheels available on PyPI using::

    pip install biopython

However, if you need to compile Biopython yourself, the following are
required at compile time - unless you are using Jython (support for which is
deprecated).

- Python including development header files like ``python.h``, which on Linux
  are often not installed by default (trying looking for and installing a
  package named ``python-dev`` or ``python-devel`` as well as the ``python``
  pacakge).

- Appropriate C compiler for your version of Python, for example GCC on Linux,
  MSVC on Windows. For Mac OS X, or as it is now branded, macOS, use Apple's
  command line tools, which can be installed with the terminal command::

      xcode-select --install

  This will offer to install Apple's XCode development suite - you can, but it
  is not needed and takes a lot of disk space.

Then either download and decompress our source code, or fetch it using git.
Now change directory to the Biopython source code folder and run::

    python setup.py build
    python setup.py test
    sudo python setup.py install

Substitute ``python`` with your specific version, for example ``python3``,
``pypy`` or ``jython``.

To exlude tests that require an internet connection (and which may take a long
time), use the ``--offline`` option::

    python setup.py test --offline

If you need to do additional configuration, e.g. changing
the install directory prefix, please type ``python setup.py``, or see the
documentation here:

* HTML - http://biopython.org/DIST/docs/install/Installation.html
* PDF - http://biopython.org/DIST/docs/install/Installation.pdf


Testing
=======

Biopython includes a suite of regression tests to check if everything is
running correctly. To run the tests, go to the biopython source code
directory and type::

    python setup.py build
    python setup.py test

If you want to skip the online tests (which is recommended when doing repeated
testing), use::

    python setup.py test --offline

Do not panic if you see messages warning of skipped tests::

    test_DocSQL ... skipping. Install MySQLdb if you want to use Bio.DocSQL.

This most likely means that a package is not installed.  You can
ignore this if it occurs in the tests for a module that you were not
planning on using.  If you did want to use that module, please install
the required dependency and re-run the tests.

Some of the tests may fail due to network issues, this is often down to
chance or a service outage. If the problem does not go away on
re-running the tests, you can use the ``--offline`` option.

There is more testing information in the Biopython Tutorial & Cookbook.


Experimental code
=================

Biopython 1.61 introduced a new warning, ``Bio.BiopythonExperimentalWarning``,
which is used to mark any experimental code included in the otherwise
stable Biopython releases. Such 'beta' level code is ready for wider
testing, but still likely to change, and should only be tried by early
adopters in order to give feedback via the biopython-dev mailing list.

We'd expect such experimental code to reach stable status within one or two
releases, at which point our normal policies about trying to preserve
backwards compatibility would apply.


Bugs
====

While we try to ship a robust package, bugs inevitably pop up.  If you are
having problems that might be caused by a bug in Biopython, it is possible
that it has already been identified. Update to the latest release if you are
not using it already, and retry. If the problem persists, please search our
bug database and our mailing lists to see if it has already been reported
(and hopefully fixed), and if not please do report the bug. We can't fix
problems we don't know about ;)

* Old issue tracker: https://redmine.open-bio.org/projects/biopython
* Current issue tracker: https://github.com/biopython/biopython/issues

If you suspect the problem lies within a parser, it is likely that the data
format has changed and broken the parsing code.  (The text BLAST and GenBank
formats seem to be particularly fragile.)  Thus, the parsing code in
Biopython is sometimes updated faster than we can build Biopython releases.
You can get the most recent parser by pulling the relevant files (e.g. the
ones in ``Bio.SeqIO`` or ``Bio.Blast``) from our git repository. However, be
careful when doing this, because the code in github is not as well-tested
as released code, and may contain new dependencies.

Finally, you can send a bug report to the bug database or the mailing list at
biopython@biopython.org (subscription required).  In the bug report, please
let us know:

1. Which operating system and hardware (32 bit or 64 bit) you are using
2. Python version
3. Biopython version (or git commit/date)
4. Traceback that occurs (the full error message)

And also ideally:

5. Example code that breaks
6. A data file that causes the problem


Contributing, Bug Reports
=========================

Biopython is run by volunteers from all over the world, with many types of
backgrounds. We are always looking for people interested in helping with code
development, web-site management, documentation writing, technical
administration, and whatever else comes up.

If you wish to contribute, please first read `CONTRIBUTING.rst
<https://github.com/biopython/biopython/blob/master/CONTRIBUTING.rst>`_ here,
visit our web site http://biopython.org and join our mailing list:
http://biopython.org/wiki/Mailing_lists


Distribution Structure
======================

- ``README.rst``  -- This file.
- ``NEWS.rst``    -- Release notes and news.
- ``LICENSE.rst`` -- What you can do with the code.
- ``CONTRIB.rst`` -- An (incomplete) list of people who helped Biopython in
  one way or another.
- ``CONTRIBUTING.rst`` -- An overview about how to contribute to Biopython.
- ``DEPRECATED.rst`` -- Contains information about modules in Biopython that
  were removed or no longer recommended for use, and how to update code that
  uses those modules.
- ``MANIFEST.in`` -- Configures which files to include in releases.
- ``setup.py``    -- Installation file.
- ``Bio/``        -- The main code base code.
- ``BioSQL/``     -- Code for using Biopython with BioSQL databases.
- ``Doc/``        -- Documentation.
- ``Scripts/``    -- Miscellaneous, possibly useful, standalone scripts.
- ``Tests/``      -- Regression testing code including sample data files.
  * [bitarray-1.0.1](https://github.com/ilanschnell/bitarray) ======================================
bitarray: efficient arrays of booleans
======================================

This module provides an object type which efficiently represents an array
of booleans.  Bitarrays are sequence types and behave very much like usual
lists.  Eight bits are represented by one byte in a contiguous block of
memory.  The user can select between two representations: little-endian
and big-endian.  All of the functionality is implemented in C.
Methods for accessing the machine representation are provided.
This can be useful when bit level access to binary files is required,
such as portable bitmap image files (.pbm).  Also, when dealing with
compressed data which uses variable bit length encoding, you may find
this module useful.


Key features
------------

 * All functionality implemented in C.

 * Bitarray objects behave very much like a list object, in particular
   slicing (including slice assignment and deletion) is supported.

 * The bit endianness can be specified for each bitarray object, see below.

 * Packing and unpacking to other binary data formats,
   e.g. `numpy.ndarray <http://www.scipy.org/Tentative_NumPy_Tutorial>`_,
   is possible.

 * Fast methods for encoding and decoding variable bit length prefix codes

 * Bitwise operations: ``&, |, ^, &=, |=, ^=, ~``

 * Sequential search

 * Pickling and unpickling of bitarray objects possible.

 * Bitarray objects support the buffer protocol (Python 2.7 and above)

 * On 32-bit systems, a bitarray object can contain up to 2^34 elements,
   that is 16 Gbits (on 64-bit machines up to 2^63 elements in theory).


Installation
------------

bitarray can be installed from source::

   $ tar xzf bitarray-1.0.1.tar.gz
   $ cd bitarray-1.0.1
   $ python setup.py install

On Unix systems, the latter command may have to be executed with root
privileges.  You can also pip install bitarray.
Once you have installed the package, you may want to test it::

   $ python -c 'import bitarray; bitarray.test()'
   bitarray is installed in: /usr/local/lib/python2.7/site-packages/bitarray
   bitarray version: 1.0.1
   2.7.2 (r271:86832, Nov 29 2010) [GCC 4.2.1 (SUSE Linux)]
   .........................................................................
   .................................................
   ----------------------------------------------------------------------
   Ran 146 tests in 2.164s
   
   OK

You can always import the function test,
and ``test().wasSuccessful()`` will return ``True`` when the test went well.



Using the module
----------------

As mentioned above, bitarray objects behave very much like lists, so
there is not too much to learn.  The biggest difference from list objects
is the ability to access the machine representation of the object.
When doing so, the bit endianness is of importance; this issue is
explained in detail in the section below.  Here, we demonstrate the
basic usage of bitarray objects:

   >>> from bitarray import bitarray
   >>> a = bitarray()            # create empty bitarray
   >>> a.append(True)
   >>> a.extend([False, True, True])
   >>> a
   bitarray('1011')

Bitarray objects can be instantiated in different ways:

   >>> a = bitarray(2**20)       # bitarray of length 1048576 (uninitialized)
   >>> bitarray('1001011')       # from a string
   bitarray('1001011')
   >>> lst = [True, False, False, True, False, True, True]
   >>> bitarray(lst)             # from list, tuple, iterable
   bitarray('1001011')

Bits can be assigned from any Python object, if the value can be interpreted
as a truth value.  You can think of this as Python's built-in function bool()
being applied, whenever casting an object:

   >>> a = bitarray([42, '', True, {}, 'foo', None])
   >>> a
   bitarray('101010')
   >>> a.append(a)      # note that bool(a) is True
   >>> a.count(42)      # counts occurrences of True (not 42)
   4L
   >>> a.remove('')     # removes first occurrence of False
   >>> a
   bitarray('110101')

Like lists, bitarray objects support slice assignment and deletion:

   >>> a = bitarray(50)
   >>> a.setall(False)
   >>> a[11:37:3] = 9 * bitarray([True])
   >>> a
   bitarray('00000000000100100100100100100100100100000000000000')
   >>> del a[12::3]
   >>> a
   bitarray('0000000000010101010101010101000000000')
   >>> a[-6:] = bitarray('10011')
   >>> a
   bitarray('000000000001010101010101010100010011')
   >>> a += bitarray('000111')
   >>> a[9:]
   bitarray('001010101010101010100010011000111')

In addition, slices can be assigned to booleans, which is easier (and
faster) than assigning to a bitarray in which all values are the same:

   >>> a = 20 * bitarray('0')
   >>> a[1:15:3] = True
   >>> a
   bitarray('01001001001001000000')

This is easier and faster than:

   >>> a = 20 * bitarray('0')
   >>> a[1:15:3] = 5 * bitarray('1')
   >>> a
   bitarray('01001001001001000000')

Note that in the latter we have to create a temporary bitarray whose length
must be known or calculated.


Bit endianness
--------------

Since a bitarray allows addressing of individual bits, where the machine
represents 8 bits in one byte, there are two obvious choices for this
mapping: little- and big-endian.
When creating a new bitarray object, the endianness can always be
specified explicitly:

   >>> a = bitarray(endian='little')
   >>> a.frombytes(b'A')
   >>> a
   bitarray('10000010')
   >>> b = bitarray('11000010', endian='little')
   >>> b.tobytes()
   'C'

Here, the low-bit comes first because little-endian means that increasing
numeric significance corresponds to an increasing address (index).
So a[0] is the lowest and least significant bit, and a[7] is the highest
and most significant bit.

   >>> a = bitarray(endian='big')
   >>> a.frombytes(b'A')
   >>> a
   bitarray('01000001')
   >>> a[6] = 1
   >>> a.tobytes()
   'C'

Here, the high-bit comes first because big-endian
means "most-significant first".
So a[0] is now the lowest and most significant bit, and a[7] is the highest
and least significant bit.

The bit endianness is a property attached to each bitarray object.
When comparing bitarray objects, the endianness (and hence the machine
representation) is irrelevant; what matters is the mapping from indices
to bits:

   >>> bitarray('11001', endian='big') == bitarray('11001', endian='little')
   True

Bitwise operations (``&, |, ^, &=, |=, ^=, ~``) are implemented efficiently
using the corresponding byte operations in C, i.e. the operators act on the
machine representation of the bitarray objects.  Therefore, one has to be
cautious when applying the operation to bitarrays with different endianness.

When converting to and from machine representation, using
the ``tobytes``, ``frombytes``, ``tofile`` and ``fromfile`` methods,
the endianness matters:

   >>> a = bitarray(endian='little')
   >>> a.frombytes(b'\x01')
   >>> a
   bitarray('10000000')
   >>> b = bitarray(endian='big')
   >>> b.frombytes(b'\x80')
   >>> b
   bitarray('10000000')
   >>> a == b
   True
   >>> a.tobytes() == b.tobytes()
   False

The endianness can not be changed once an object is created.
However, since creating a bitarray from another bitarray just copies the
memory representing the data, you can create a new bitarray with different
endianness:

   >>> a = bitarray('11100000', endian='little')
   >>> a
   bitarray('11100000')
   >>> b = bitarray(a, endian='big')
   >>> b
   bitarray('00000111')
   >>> a == b
   False
   >>> a.tobytes() == b.tobytes()
   True

The default bit endianness is currently big-endian; however, this may change
in the future, and when dealing with the machine representation of bitarray
objects, it is recommended to always explicitly specify the endianness.

Unless explicitly converting to machine representation, using
the ``tobytes``, ``frombytes``, ``tofile`` and ``fromfile`` methods,
the bit endianness will have no effect on any computation, and one
can safely ignore setting the endianness, and other details of this section.


Buffer protocol
---------------

Python 2.7 provides memoryview objects, which allow Python code to access
the internal data of an object that supports the buffer protocol without
copying.  Bitarray objects support this protocol, with the memory being
interpreted as simple bytes.

   >>> a = bitarray('01000001' '01000010' '01000011', endian='big')
   >>> v = memoryview(a)
   >>> len(v)
   3
   >>> v[-1]
   'C'
   >>> v[:2].tobytes()
   'AB'
   >>> v.readonly  # changing a bitarray's memory is also possible
   False
   >>> v[1] = 'o'
   >>> a
   bitarray('010000010110111101000011')


Variable bit length prefix codes
--------------------------------

The method ``encode`` takes a dictionary mapping symbols to bitarrays
and an iterable, and extends the bitarray object with the encoded symbols
found while iterating.  For example:

   >>> d = {'H':bitarray('111'), 'e':bitarray('0'),
   ...      'l':bitarray('110'), 'o':bitarray('10')}
   ...
   >>> a = bitarray()
   >>> a.encode(d, 'Hello')
   >>> a
   bitarray('111011011010')

Note that the string ``'Hello'`` is an iterable, but the symbols are not
limited to characters, in fact any immutable Python object can be a symbol.
Taking the same dictionary, we can apply the ``decode`` method which will
return a list of the symbols:

   >>> a.decode(d)
   ['H', 'e', 'l', 'l', 'o']
   >>> ''.join(a.decode(d))
   'Hello'

Since symbols are not limited to being characters, it is necessary to return
them as elements of a list, rather than simply returning the joined string.


Reference
---------

**The bitarray class:**

``bitarray([initial], [endian=string])``
   Return a new bitarray object whose items are bits initialized from
   the optional initial, and endianness.
   If no object is provided, the bitarray is initialized to have length zero.
   The initial object may be of the following types:
   
   int, long
       Create bitarray of length given by the integer.  The initial values
       in the array are random, because only the memory allocated.
   
   string
       Create bitarray from a string of '0's and '1's.
   
   list, tuple, iterable
       Create bitarray from a sequence, each element in the sequence is
       converted to a bit using truth value value.
   
   bitarray
       Create bitarray from another bitarray.  This is done by copying the
       memory holding the bitarray data, and is hence very fast.
   
   The optional keyword arguments 'endian' specifies the bit endianness of the
   created bitarray object.
   Allowed values are 'big' and 'little' (default is 'big').
   
   Note that setting the bit endianness only has an effect when accessing the
   machine representation of the bitarray, i.e. when using the methods: tofile,
   fromfile, tobytes, frombytes.


**A bitarray object supports the following methods:**

``all()`` -> bool
   Returns True when all bits in the array are True.


``any()`` -> bool
   Returns True when any bit in the array is True.


``append(item)``
   Append the value bool(item) to the end of the bitarray.


``buffer_info()`` -> tuple
   Return a tuple (address, size, endianness, unused, allocated) giving the
   current memory address, the size (in bytes) used to hold the bitarray's
   contents, the bit endianness as a string, the number of unused bits
   (e.g. a bitarray of length 11 will have a buffer size of 2 bytes and
   5 unused bits), and the size (in bytes) of the allocated memory.


``bytereverse()``
   For all bytes representing the bitarray, reverse the bit order (in-place).
   Note: This method changes the actual machine values representing the
   bitarray; it does not change the endianness of the bitarray object.


``copy()`` -> bitarray
   Return a copy of the bitarray.


``count([value])`` -> int
   Return number of occurrences of value (defaults to True) in the bitarray.


``decode(code)`` -> list
   Given a prefix code (a dict mapping symbols to bitarrays),
   decode the content of the bitarray and return it as a list of symbols.


``encode(code, iterable)``
   Given a prefix code (a dict mapping symbols to bitarrays),
   iterate over the iterable object with symbols, and extend the bitarray
   with the corresponding bitarray for each symbols.


``endian()`` -> str
   Return the bit endianness as a string (either 'little' or 'big').


``extend(object)``
   Append bits to the end of the bitarray.  The objects which can be passed
   to this method are the same iterable objects which can given to a bitarray
   object upon initialization.


``fill()`` -> int
   Adds zeros to the end of the bitarray, such that the length of the bitarray
   will be a multiple of 8.  Returns the number of bits added (0..7).


``frombytes(bytes)``
   Append from a byte string, interpreted as machine values.


``fromfile(f, [n])``
   Read n bytes from the file object f and append them to the bitarray
   interpreted as machine values.  When n is omitted, as many bytes are
   read until EOF is reached.


``fromstring(str)``
   Append from a string, interpreting the string as machine values.
   Deprecated since version 0.4.0, use ``frombytes()`` instead.


``index(value, [start, [stop]])`` -> int
   Return index of the first occurrence of bool(value) in the bitarray.
   Raises ValueError if the value is not present.


``insert(i, item)``
   Insert bool(item) into the bitarray before position i.


``invert()``
   Invert all bits in the array (in-place),
   i.e. convert each 1-bit into a 0-bit and vice versa.


``iterdecode(code)`` -> iterator
   Given a prefix code (a dict mapping symbols to bitarrays),
   decode the content of the bitarray and return an iterator over
   the symbols.


``itersearch(bitarray)`` -> iterator
   Searches for the given a bitarray in self, and return an iterator over
   the start positions where bitarray matches self.


``length()`` -> int
   Return the length, i.e. number of bits stored in the bitarray.
   This method is preferred over __len__ (used when typing ``len(a)``),
   since __len__ will fail for a bitarray object with 2^31 or more elements
   on a 32bit machine, whereas this method will return the correct value,
   on 32bit and 64bit machines.


``pack(bytes)``
   Extend the bitarray from a byte string, where each characters corresponds to
   a single bit.  The character b'\x00' maps to bit 0 and all other characters
   map to bit 1.
   This method, as well as the unpack method, are meant for efficient
   transfer of data between bitarray objects to other python objects
   (for example NumPy's ndarray object) which have a different view of memory.


``pop([i])`` -> item
   Return the i-th (default last) element and delete it from the bitarray.
   Raises IndexError if bitarray is empty or index is out of range.


``remove(item)``
   Remove the first occurrence of bool(item) in the bitarray.
   Raises ValueError if item is not present.


``reverse()``
   Reverse the order of bits in the array (in-place).


``search(bitarray, [limit])`` -> list
   Searches for the given bitarray in self, and return the list of start
   positions.
   The optional argument limits the number of search results to the integer
   specified.  By default, all search results are returned.


``setall(value)``
   Set all bits in the bitarray to bool(value).


``sort(reverse=False)``
   Sort the bits in the array (in-place).


``to01()`` -> str
   Return a string containing '0's and '1's, representing the bits in the
   bitarray object.
   Note: To extend a bitarray from a string containing '0's and '1's,
   use the extend method.


``tobytes()`` -> bytes
   Return the byte representation of the bitarray.
   When the length of the bitarray is not a multiple of 8, the few remaining
   bits (1..7) are set to 0.


``tofile(f)``
   Write all bits (as machine values) to the file object f.
   When the length of the bitarray is not a multiple of 8,
   the remaining bits (1..7) are set to 0.


``tolist()`` -> list
   Return an ordinary list with the items in the bitarray.
   Note that the list object being created will require 32 or 64 times more
   memory than the bitarray object, which may cause a memory error if the
   bitarray is very large.
   Also note that to extend a bitarray with elements from a list,
   use the extend method.


``tostring()`` -> str
   Return the string representing (machine values) of the bitarray.
   When the length of the bitarray is not a multiple of 8, the few remaining
   bits (1..7) are set to 0.
   Deprecated since version 0.4.0, use ``tobytes()`` instead.


``unpack(zero=b'\x00', one=b'\xff')`` -> bytes
   Return a byte string containing one character for each bit in the bitarray,
   using the specified mapping.
   See also the pack method.


**Functions defined in the module:**

``test(verbosity=1, repeat=1)`` -> TextTestResult
   Run self-test, and return unittest.runner.TextTestResult object.


``bitdiff(a, b)`` -> int
   Return the difference between two bitarrays a and b.
   This is function does the same as (a ^ b).count(), but is more memory
   efficient, as no intermediate bitarray object gets created


``bits2bytes(n)`` -> int
   Return the number of bytes necessary to store n bits.


Change log
----------

**1.0.1** (2019-07-19):

  * fix readme to pass ``twine check``


**1.0.0** (2019-07-15):

  * fix bitarrays beings created from unicode in Python 2
  * use ``PyBytes_*`` in C code, treating the Py3k function names as default,
    which also removes all redefinitions of ``PyString_*``
  * handle negative arguments of .index() method consistently with how
    they are treated for lists
  * add a few more comments to the C code
  * move imports outside tests: pickle, io, etc.
  * drop Python 2.5 support


**0.9.3** (2019-05-20):

  * refactor resize() - only shrink allocated memory if new size falls
    lower than half the allocated size
  * improve error message when trying to initialize from float or complex


Please find the complete change log
`here <https://github.com/ilanschnell/bitarray/blob/master/CHANGE_LOG>`_.

  * [bitstring-3.1.5](https://github.com/scott-griffiths/bitstring) 

.. image:: /doc/bitstring_logo_small.png

**bitstring** is a pure Python module designed to help make
the creation and analysis of binary data as simple and natural as possible.

Bitstrings can be constructed from integers (big and little endian), hex,
octal, binary, strings or files. They can be sliced, joined, reversed,
inserted into, overwritten, etc. with simple functions or slice notation.
They can also be read from, searched and replaced, and navigated in,
similar to a file or stream.

bitstring is open source software, and has been released under the MIT
licence.

This module works in both Python 2.7 and Python 3.

Installation
------------

Probably all you need to do is::

     pip install bitstring     

Alternatively if you have downloaded and unzipped the package then you need to run the
``setup.py`` script with the 'install' argument::

     python setup.py install

You may need to run this with root privileges on Unix-like systems.

Documentation
-------------
The manual for the bitstring module is available here
<http://packages.python.org/bitstring>. It contains a walk-through of all
the features and a complete reference section.

It is also available as a PDF as part of the source download.


Simple Examples
---------------
Creation::

     >>> a = BitArray(bin='00101')
     >>> b = Bits(a_file_object)
     >>> c = BitArray('0xff, 0b101, 0o65, uint:6=22')
     >>> d = pack('intle:16, hex=a, 0b1', 100, a='0x34f')
     >>> e = pack('<16h', *range(16))

Different interpretations, slicing and concatenation::

     >>> a = BitArray('0x1af')
     >>> a.hex, a.bin, a.uint
     ('1af', '000110101111', 431)
     >>> a[10:3:-1].bin
     '1110101'
     >>> 3*a + '0b100'
     BitArray('0o0657056705674')

Reading data sequentially::

     >>> b = BitStream('0x160120f')
     >>> b.read(12).hex
     '160'
     >>> b.pos = 0
     >>> b.read('uint:12')
     352
     >>> b.readlist('uint:12, bin:3')
     [288, '111']

Searching, inserting and deleting::

     >>> c = BitArray('0b00010010010010001111')   # c.hex == '0x1248f'
     >>> c.find('0x48')
     (8,)
     >>> c.replace('0b001', '0xabc')
     >>> c.insert('0b0000')
     >>> del c[12:16]

Unit Tests
----------

The 400+ unit tests should all pass for Python 2.7 and later. To run them, from the `test` 
directory run::

     python -m unittest discover

----

The bitstring module has been released as open source under the MIT License.
Copyright (c) 2019 Scott Griffiths

For more information see the project's homepage on GitHub:
<https://github.com/scott-griffiths/bitstring>




  * [black-19.3b0](https://github.com/ambv/black) ![Black Logo](https://raw.githubusercontent.com/ambv/black/master/docs/_static/logo2-readme.png)
<h2 align="center">The Uncompromising Code Formatter</h2>

<p align="center">
<a href="https://travis-ci.org/ambv/black"><img alt="Build Status" src="https://travis-ci.org/ambv/black.svg?branch=master"></a>
<a href="https://black.readthedocs.io/en/stable/?badge=stable"><img alt="Documentation Status" src="https://readthedocs.org/projects/black/badge/?version=stable"></a>
<a href="https://coveralls.io/github/ambv/black?branch=master"><img alt="Coverage Status" src="https://coveralls.io/repos/github/ambv/black/badge.svg?branch=master"></a>
<a href="https://github.com/ambv/black/blob/master/LICENSE"><img alt="License: MIT" src="https://black.readthedocs.io/en/stable/_static/license.svg"></a>
<a href="https://pypi.org/project/black/"><img alt="PyPI" src="https://black.readthedocs.io/en/stable/_static/pypi.svg"></a>
<a href="https://pepy.tech/project/black"><img alt="Downloads" src="https://pepy.tech/badge/black"></a>
<a href="https://github.com/ambv/black"><img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
</p>

> “Any color you like.”


*Black* is the uncompromising Python code formatter.  By using it, you
agree to cede control over minutiae of hand-formatting.  In return,
*Black* gives you speed, determinism, and freedom from `pycodestyle`
nagging about formatting.  You will save time and mental energy for
more important matters.

Blackened code looks the same regardless of the project you're reading.
Formatting becomes transparent after a while and you can focus on the
content instead.

*Black* makes code review faster by producing the smallest diffs
possible.

Try it out now using the [Black Playground](https://black.now.sh).

---

*Contents:* **[Installation and usage](#installation-and-usage)** |
**[Code style](#the-black-code-style)** |
**[pyproject.toml](#pyprojecttoml)** |
**[Editor integration](#editor-integration)** |
**[blackd](#blackd)** |
**[Version control integration](#version-control-integration)** |
**[Ignoring unmodified files](#ignoring-unmodified-files)** |
**[Testimonials](#testimonials)** |
**[Show your style](#show-your-style)** |
**[Contributing](#contributing-to-black)** |
**[Change Log](#change-log)** |
**[Authors](#authors)**

---

## Installation and usage

### Installation

*Black* can be installed by running `pip install black`.  It requires
Python 3.6.0+ to run but you can reformat Python 2 code with it, too.


### Usage

To get started right away with sensible defaults:

```
black {source_file_or_directory}
```

### Command line options

*Black* doesn't provide many options.  You can list them by running
`black --help`:

```text
black [OPTIONS] [SRC]...

Options:
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2>/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
```

*Black* is a well-behaved Unix-style command-line tool:
* it does nothing if no sources are passed to it;
* it will read from standard input and write to standard output if `-`
  is used as the filename;
* it only outputs messages to users on standard error;
* exits with code 0 unless an internal error occurred (or `--check` was
  used).


### NOTE: This is a beta product

*Black* is already successfully used by several projects, small and big.
It also sports a decent test suite.  However, it is still very new.
Things will probably be wonky for a while. This is made explicit by the
"Beta" trove classifier, as well as by the "b" in the version number.
What this means for you is that **until the formatter becomes stable,
you should expect some formatting to change in the future**.  That being
said, no drastic stylistic changes are planned, mostly responses to bug
reports.

Also, as a temporary safety measure, *Black* will check that the
reformatted code still produces a valid AST that is equivalent to the
original.  This slows it down.  If you're feeling confident, use
``--fast``.


## The *Black* code style

*Black* reformats entire files in place.  It is not configurable.  It
doesn't take previous formatting into account.  It doesn't reformat
blocks that start with `# fmt: off` and end with `# fmt: on`. `# fmt: on/off`
have to be on the same level of indentation. It also
recognizes [YAPF](https://github.com/google/yapf)'s block comments to
the same effect, as a courtesy for straddling code.


### How *Black* wraps lines

*Black* ignores previous formatting and applies uniform horizontal
and vertical whitespace to your code.  The rules for horizontal
whitespace can be summarized as: do whatever makes `pycodestyle` happy.
The coding style used by *Black* can be viewed as a strict subset of
PEP 8.

As for vertical whitespace, *Black* tries to render one full expression
or simple statement per line.  If this fits the allotted line length,
great.
```py3
# in:

l = [1,
     2,
     3,
]

# out:

l = [1, 2, 3]
```

If not, *Black* will look at the contents of the first outer matching
brackets and put that in a separate indented line.
```py3
# in:

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

# out:

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)
```

If that still doesn't fit the bill, it will decompose the internal
expression further using the same rule, indenting matching brackets
every time.  If the contents of the matching brackets pair are
comma-separated (like an argument list, or a dict literal, and so on)
then *Black* will first try to keep them on the same line with the
matching brackets.  If that doesn't work, it will put all of them in
separate lines.
```py3
# in:

def very_important_function(template: str, *variables, file: os.PathLike, engine: str, header: bool = True, debug: bool = False):
    """Applies `variables` to the `template` and writes to `file`."""
    with open(file, 'w') as f:
        ...

# out:

def very_important_function(
    template: str,
    *variables,
    file: os.PathLike,
    engine: str,
    header: bool = True,
    debug: bool = False,
):
    """Applies `variables` to the `template` and writes to `file`."""
    with open(file, "w") as f:
        ...
```

You might have noticed that closing brackets are always dedented and
that a trailing comma is always added.  Such formatting produces smaller
diffs; when you add or remove an element, it's always just one line.
Also, having the closing bracket dedented provides a clear delimiter
between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the
example above).

If a data structure literal (tuple, list, set, dict) or a line of "from"
imports cannot fit in the allotted length, it's always split into one
element per line.  This minimizes diffs as well as enables readers of
code to find which commit introduced a particular entry.  This also
makes *Black* compatible with [isort](https://pypi.org/p/isort/) with
the following configuration.

<details>
<summary>A compatible `.isort.cfg`</summary>

```
[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
```

The equivalent command line is:
```
$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
```
</details>

### Line length

You probably noticed the peculiar default line length.  *Black* defaults
to 88 characters per line, which happens to be 10% over 80.  This number
was found to produce significantly shorter files than sticking with 80
(the most popular), or even 79 (used by the standard library).  In
general, [90-ish seems like the wise choice](https://youtu.be/wf-BqAjZb8M?t=260).

If you're paid by the line of code you write, you can pass
`--line-length` with a lower number.  *Black* will try to respect that.
However, sometimes it won't be able to without breaking other rules.  In
those rare cases, auto-formatted code will exceed your allotted limit.

You can also increase it, but remember that people with sight disabilities
find it harder to work with line lengths exceeding 100 characters.
It also adversely affects side-by-side diff review  on typical screen
resolutions.  Long lines also make it harder to present code neatly
in documentation or talk slides.

If you're using Flake8, you can bump `max-line-length` to 88 and forget
about it.  Alternatively, use [Bugbear](https://github.com/PyCQA/flake8-bugbear)'s
B950 warning instead of E501 and keep the max line length at 80 which
you are probably already using.  You'd do it like this:
```ini
[flake8]
max-line-length = 80
...
select = C,E,F,W,B,B950
ignore = E501
```

You'll find *Black*'s own .flake8 config file is configured like this.
If you're curious about the reasoning behind B950, 
[Bugbear's documentation](https://github.com/PyCQA/flake8-bugbear#opinionated-warnings)
explains it.  The tl;dr is "it's like highway speed limits, we won't
bother you if you overdo it by a few km/h".


### Empty lines

*Black* avoids spurious vertical whitespace.  This is in the spirit of
PEP 8 which says that in-function vertical whitespace should only be
used sparingly.

*Black* will allow single empty lines inside functions, and single and
double empty lines on module level left by the original editors, except
when they're within parenthesized expressions.  Since such expressions
are always reformatted to fit minimal space, this whitespace is lost.

It will also insert proper spacing before and after function definitions.
It's one line before and after inner functions and two lines before and
after module-level functions and classes.  *Black* will not put empty
lines between function/class definitions and standalone comments that
immediately precede the given function/class.

*Black* will enforce single empty lines between a class-level docstring
and the first following field or method.  This conforms to
[PEP 257](https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings).

*Black* won't insert empty lines after function docstrings unless that
empty line is required due to an inner function starting immediately
after.


### Trailing commas

*Black* will add trailing commas to expressions that are split
by comma where each element is on its own line.  This includes function
signatures.

Unnecessary trailing commas are removed if an expression fits in one
line.  This makes it 1% more likely that your line won't exceed the
allotted line length limit.  Moreover, in this scenario, if you added
another argument to your call, you'd probably fit it in the same line
anyway.  That doesn't make diffs any larger.

One exception to removing trailing commas is tuple expressions with
just one element.  In this case *Black* won't touch the single trailing
comma as this would unexpectedly change the underlying data type.  Note
that this is also the case when commas are used while indexing.  This is
a tuple in disguise: ```numpy_array[3, ]```.

One exception to adding trailing commas is function signatures
containing `*`, `*args`, or `**kwargs`.  In this case a trailing comma
is only safe to use on Python 3.6.  *Black* will detect if your file is
already 3.6+ only and use trailing commas in this situation.  If you
wonder how it knows, it looks for f-strings and existing use of trailing
commas in function signatures that have stars in them.  In other words,
if you'd like a trailing comma in this situation and *Black* didn't
recognize it was safe to do so, put it there manually and *Black* will
keep it.


### Strings

*Black* prefers double quotes (`"` and `"""`) over single quotes (`'`
and `'''`).  It will replace the latter with the former as long as it
does not result in more backslash escapes than before.

*Black* also standardizes string prefixes, making them always lowercase.
On top of that, if your code is already Python 3.6+ only or it's using
the `unicode_literals` future import, *Black* will remove `u` from the
string prefix as it is meaningless in those scenarios.

The main reason to standardize on a single form of quotes is aesthetics.
Having one kind of quotes everywhere reduces reader distraction.
It will also enable a future version of *Black* to merge consecutive
string literals that ended up on the same line (see
[#26](https://github.com/ambv/black/issues/26) for details).

Why settle on double quotes?  They anticipate apostrophes in English
text.  They match the docstring standard described in [PEP 257](https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring).
An empty string in double quotes (`""`) is impossible to confuse with
a one double-quote regardless of fonts and syntax highlighting used.
On top of this, double quotes for strings are consistent with C which
Python interacts a lot with.

On certain keyboard layouts like US English, typing single quotes is
a bit easier than double quotes.  The latter requires use of the Shift
key.  My recommendation here is to keep using whatever is faster to type
and let *Black* handle the transformation.

If you are adopting *Black* in a large project with pre-existing string
conventions (like the popular ["single quotes for data, double quotes for
human-readable strings"](https://stackoverflow.com/a/56190)), you can
pass `--skip-string-normalization` on the command line.  This is meant as
an adoption helper, avoid using this for new projects.

### Numeric literals

*Black* standardizes most numeric literals to use lowercase letters for the
syntactic parts and uppercase letters for the digits themselves: `0xAB`
instead of `0XAB` and `1e10` instead of `1E10`. Python 2 long literals are
styled as `2L` instead of `2l` to avoid confusion between `l` and `1`.


### Line breaks & binary operators

*Black* will break a line before a binary operator when splitting a block
of code over multiple lines. This is so that *Black* is compliant with the
recent changes in the [PEP 8](https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator)
style guide, which emphasizes that this approach improves readability.

This behaviour may raise ``W503 line break before binary operator`` warnings in
style guide enforcement tools like Flake8. Since ``W503`` is not PEP 8 compliant,
you should tell Flake8 to ignore these warnings.


### Slices

PEP 8 [recommends](https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements)
to treat ``:`` in slices as a binary operator with the lowest priority, and to
leave an equal amount of space on either side, except if a parameter is omitted
(e.g. ``ham[1 + 1 :]``). It also states that for extended slices, both ``:``
operators have to have the same amount of spacing, except if a parameter is
omitted (``ham[1 + 1 ::]``). *Black* enforces these rules consistently.

This behaviour may raise ``E203 whitespace before ':'`` warnings in style guide
enforcement tools like Flake8. Since ``E203`` is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.


### Parentheses

Some parentheses are optional in the Python grammar.  Any expression can
be wrapped in a pair of parentheses to form an atom.  There are a few
interesting cases:

- `if (...):`
- `while (...):`
- `for (...) in (...):`
- `assert (...), (...)`
- `from X import (...)`
- assignments like:
  - `target = (...)`
  - `target: type = (...)`
  - `some, *un, packing = (...)`
  - `augmented += (...)`

In those cases, parentheses are removed when the entire statement fits
in one line, or if the inner expression doesn't have any delimiters to
further split on.  If there is only a single delimiter and the expression
starts or ends with a bracket, the parenthesis can also be successfully
omitted since the existing bracket pair will organize the expression
neatly anyway.  Otherwise, the parentheses are added.

Please note that *Black* does not add or remove any additional nested
parentheses that you might want to have for clarity or further
code organization.  For example those parentheses are not going to be
removed:
```py3
return not (this or that)
decision = (maybe.this() and values > 0) or (maybe.that() and values < 0)
```


### Call chains

Some popular APIs, like ORMs, use call chaining.  This API style is known
as a [fluent interface](https://en.wikipedia.org/wiki/Fluent_interface).
*Black* formats those by treating dots that follow a call or an indexing
operation like a very low priority delimiter.  It's easier to show the
behavior than to explain it.  Look at the example:
```py3
def example(session):
    result = (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id == account_id,
            models.Customer.email == email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )
```


### Typing stub files

PEP 484 describes the syntax for type hints in Python.  One of the
use cases for typing is providing type annotations for modules which
cannot contain them directly (they might be written in C, or they might
be third-party, or their implementation may be overly dynamic, and so on).

To solve this, [stub files with the `.pyi` file
extension](https://www.python.org/dev/peps/pep-0484/#stub-files) can be
used to describe typing information for an external module.  Those stub
files omit the implementation of classes and functions they
describe, instead they only contain the structure of the file (listing
globals, functions, and classes with their members).  The recommended
code style for those files is more terse than PEP 8:

* prefer `...` on the same line as the class/function signature;
* avoid vertical whitespace between consecutive module-level functions,
  names, or methods and fields within a single class;
* use a single blank line between top-level class definitions, or none
  if the classes are very small.

*Black* enforces the above rules.  There are additional guidelines for
formatting `.pyi` file that are not enforced yet but might be in
a future version of the formatter:

* all function bodies should be empty (contain `...` instead of the body);
* do not use docstrings;
* prefer `...` over `pass`;
* for arguments with a default, use `...` instead of the actual default;
* avoid using string literals in type annotations, stub files support
  forward references natively (like Python 3.7 code with `from __future__
  import annotations`);
* use variable annotations instead of type comments, even for stubs that
  target older versions of Python;
* for arguments that default to `None`, use `Optional[]` explicitly;
* use `float` instead of `Union[int, float]`.


## pyproject.toml

*Black* is able to read project-specific default values for its
command line options from a `pyproject.toml` file.  This is
especially useful for specifying custom `--include` and `--exclude`
patterns for your project.

**Pro-tip**: If you're asking yourself "Do I need to configure anything?"
the answer is "No".  *Black* is all about sensible defaults.


### What on Earth is a `pyproject.toml` file?

[PEP 518](https://www.python.org/dev/peps/pep-0518/) defines
`pyproject.toml` as a configuration file to store build system
requirements for Python projects.  With the help of tools
like [Poetry](https://poetry.eustace.io/) or
[Flit](https://flit.readthedocs.io/en/latest/) it can fully replace the
need for `setup.py` and `setup.cfg` files.


### Where *Black* looks for the file

By default *Black* looks for `pyproject.toml` starting from the common
base directory of all files and directories passed on the command line.
If it's not there, it looks in parent directories.  It stops looking
when it finds the file, or a `.git` directory, or a `.hg` directory,
or the root of the file system, whichever comes first.

If you're formatting standard input, *Black* will look for configuration
starting from the current working directory.

You can also explicitly specify the path to a particular file that you
want with `--config`.  In this situation *Black* will not look for any
other file.

If you're running with `--verbose`, you will see a blue message if
a file was found and used.

Please note `blackd` will not use `pyproject.toml` configuration.


### Configuration format

As the file extension suggests, `pyproject.toml` is a [TOML](https://github.com/toml-lang/toml) file.  It contains separate
sections for different tools.  *Black* is using the `[tool.black]`
section.  The option keys are the same as long names of options on
the command line.

Note that you have to use single-quoted strings in TOML for regular
expressions. It's the equivalent of r-strings in Python.  Multiline
strings are treated as verbose regular expressions by Black.  Use `[ ]`
to denote a significant space character.

<details>
<summary>Example `pyproject.toml`</summary>

```toml
[tool.black]
line-length = 88
target_version = ['py37']
include = '\.pyi?$'
exclude = '''

(
  /(
      \.eggs         # exclude a few common directories in the
    | \.git          # root of the project
    | \.hg
    | \.mypy_cache
    | \.tox
    | \.venv
    | _build
    | buck-out
    | build
    | dist
  )/
  | foo.py           # also separately exclude a file named foo.py in
                     # the root of the project
)
'''
```

</details>

### Lookup hierarchy

Command-line options have defaults that you can see in `--help`.
A `pyproject.toml` can override those defaults.  Finally, options
provided by the user on the command line override both.

*Black* will only ever use one `pyproject.toml` file during an entire
run. It doesn't look for multiple files, and doesn't compose
configuration from different levels of the file hierarchy.


## Editor integration

### Emacs

Use [proofit404/blacken](https://github.com/proofit404/blacken).


### PyCharm/IntelliJ IDEA

1. Install `black`.

```console
$ pip install black
```

2. Locate your `black` installation folder.

  On macOS / Linux / BSD:

```console
$ which black
/usr/local/bin/black  # possible location
```

  On Windows:

```console
$ where black
%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location
```

3. Open External tools in PyCharm/IntelliJ IDEA

  On macOS:

```PyCharm -> Preferences -> Tools -> External Tools```

  On Windows / Linux / BSD:

```File -> Settings -> Tools -> External Tools```

4. Click the + icon to add a new external tool with the following values:
    - Name: Black
    - Description: Black is the uncompromising Python code formatter.
    - Program: <install_location_from_step_2>
    - Arguments: `"$FilePath$"`

5. Format the currently opened file by selecting `Tools -> External Tools -> black`.
    - Alternatively, you can set a keyboard shortcut by navigating to `Preferences or Settings -> Keymap -> External Tools -> External Tools - Black`.

6. Optionally, run Black on every file save:

    1. Make sure you have the [File Watcher](https://plugins.jetbrains.com/plugin/7177-file-watchers) plugin installed.
    2. Go to `Preferences or Settings -> Tools -> File Watchers` and click `+` to add a new watcher:
        - Name: Black
        - File type: Python
        - Scope: Project Files
        - Program: <install_location_from_step_2>
        - Arguments: `$FilePath$`
        - Output paths to refresh: `$FilePath$`
        - Working directory: `$ProjectFileDir$`
	- Uncheck "Auto-save edited files to trigger the watcher"

### Vim

Commands and shortcuts:

* `:Black` to format the entire file (ranges not supported);
* `:BlackUpgrade` to upgrade *Black* inside the virtualenv;
* `:BlackVersion` to get the current version of *Black* inside the
  virtualenv.

Configuration:
* `g:black_fast` (defaults to `0`)
* `g:black_linelength` (defaults to `88`)
* `g:black_skip_string_normalization` (defaults to `0`)
* `g:black_virtualenv` (defaults to `~/.vim/black`)

To install with [vim-plug](https://github.com/junegunn/vim-plug):

```
Plug 'ambv/black'
```

or with [Vundle](https://github.com/VundleVim/Vundle.vim):

```
Plugin 'ambv/black'
```

or you can copy the plugin from [plugin/black.vim](https://github.com/ambv/black/tree/master/plugin/black.vim).
Let me know if this requires any changes to work with Vim 8's builtin
`packadd`, or Pathogen, and so on.

This plugin **requires Vim 7.0+ built with Python 3.6+ support**.  It
needs Python 3.6 to be able to run *Black* inside the Vim process which
is much faster than calling an external command.

On first run, the plugin creates its own virtualenv using the right
Python version and automatically installs *Black*. You can upgrade it later
by calling `:BlackUpgrade` and restarting Vim.

If you need to do anything special to make your virtualenv work and
install *Black* (for example you want to run a version from master),
create a virtualenv manually and point `g:black_virtualenv` to it.
The plugin will use it.

To run *Black* on save, add the following line to `.vimrc` or `init.vim`:

```
autocmd BufWritePre *.py execute ':Black'
```

**How to get Vim with Python 3.6?**
On Ubuntu 17.10 Vim comes with Python 3.6 by default.
On macOS with Homebrew run: `brew install vim --with-python3`.
When building Vim from source, use:
`./configure --enable-python3interp=yes`. There's many guides online how
to do this.


### Visual Studio Code

Use the [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
([instructions](https://code.visualstudio.com/docs/python/editing#_formatting)).


### SublimeText 3

Use [sublack plugin](https://github.com/jgirardet/sublack).


### Jupyter Notebook Magic

Use [blackcellmagic](https://github.com/csurfer/blackcellmagic).


### Python Language Server

If your editor supports the [Language Server Protocol](https://langserver.org/)
(Atom, Sublime Text, Visual Studio Code and many more), you can use
the [Python Language Server](https://github.com/palantir/python-language-server) with the
[pyls-black](https://github.com/rupert/pyls-black) plugin.


### Atom/Nuclide

Use [python-black](https://atom.io/packages/python-black).


### Other editors

Other editors will require external contributions.

Patches welcome! ✨ 🍰 ✨

Any tool that can pipe code through *Black* using its stdio mode (just
[use `-` as the file name](https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2)).
The formatted code will be returned on stdout (unless `--check` was
passed).  *Black* will still emit messages on stderr but that shouldn't
affect your use case.

This can be used for example with PyCharm's or IntelliJ's [File Watchers](https://www.jetbrains.com/help/pycharm/file-watchers.html).

## blackd

`blackd` is a small HTTP server that exposes *Black*'s functionality over
a simple protocol. The main benefit of using it is to avoid paying the
cost of starting up a new *Black* process every time you want to blacken
a file.

### Usage

`blackd` is not packaged alongside *Black* by default because it has additional
dependencies. You will need to do `pip install black[d]` to install it.

You can start the server on the default port, binding only to the local interface
by running `blackd`. You will see a single line mentioning the server's version,
and the host and port it's listening on. `blackd` will then print an access log
similar to most web servers on standard output, merged with any exception traces
caused by invalid formatting requests.

`blackd` provides even less options than *Black*. You can see them by running
`blackd --help`:

```text
Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
```

### Protocol

`blackd` only accepts `POST` requests at the `/` path. The body of the request
should contain the python source code to be formatted, encoded
according to the `charset` field in the `Content-Type` request header. If no
`charset` is specified, `blackd` assumes `UTF-8`.

There are a few HTTP headers that control how the source is formatted. These
correspond to command line flags for *Black*. There is one exception to this:
`X-Protocol-Version` which if present, should have the value `1`, otherwise the
request is rejected with `HTTP 501` (Not Implemented).

The headers controlling how code is formatted are:

 - `X-Line-Length`: corresponds to the `--line-length` command line flag.
 - `X-Skip-String-Normalization`: corresponds to the `--skip-string-normalization`
    command line flag. If present and its value is not the empty string, no string
    normalization will be performed.
 - `X-Fast-Or-Safe`: if set to `fast`, `blackd` will act as *Black* does when
    passed the `--fast` command line flag.
 - `X-Python-Variant`: if set to `pyi`, `blackd` will act as *Black* does when
    passed the `--pyi` command line flag. Otherwise, its value must correspond to
    a Python version or a set of comma-separated Python versions, optionally
    prefixed with `py`. For example, to request code that is compatible
    with Python 3.5 and 3.6, set the header to `py3.5,py3.6`.

If any of these headers are set to invalid values, `blackd` returns a `HTTP 400`
error response, mentioning the name of the problematic header in the message body.

Apart from the above, `blackd` can produce the following response codes:

 - `HTTP 204`: If the input is already well-formatted. The response body is
	empty.
 - `HTTP 200`: If formatting was needed on the input. The response body
	contains the blackened Python code, and the `Content-Type` header is set
	accordingly.
 - `HTTP 400`: If the input contains a syntax error. Details of the error are
	returned in the response body.
 - `HTTP 500`: If there was any kind of error while trying to format the input.
	The response body contains a textual representation of the error.

## Version control integration

Use [pre-commit](https://pre-commit.com/). Once you [have it
installed](https://pre-commit.com/#install), add this to the
`.pre-commit-config.yaml` in your repository:
```yaml
repos:
-   repo: https://github.com/ambv/black
    rev: stable
    hooks:
    - id: black
      language_version: python3.6
```
Then run `pre-commit install` and you're ready to go.

Avoid using `args` in the hook.  Instead, store necessary configuration
in `pyproject.toml` so that editors and command-line usage of Black all
behave consistently for your project.  See *Black*'s own `pyproject.toml`
for an example.

If you're already using Python 3.7, switch the `language_version`
accordingly. Finally, `stable` is a tag that is pinned to the latest
release on PyPI.  If you'd rather run on master, this is also an option.


## Ignoring unmodified files

*Black* remembers files it has already formatted, unless the `--diff` flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the *Black* version and the system on which *Black*
is run. The file is non-portable. The standard location on common operating systems
is:

* Windows: `C:\\Users\<username>\AppData\Local\black\black\Cache\<version>\cache.<line-length>.<file-mode>.pickle`
* macOS: `/Users/<username>/Library/Caches/black/<version>/cache.<line-length>.<file-mode>.pickle`
* Linux: `/home/<username>/.cache/black/<version>/cache.<line-length>.<file-mode>.pickle`

`file-mode` is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.


## Testimonials

**Dusty Phillips**, [writer](https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=dusty+phillips):

> *Black* is opinionated so you don't have to be.

**Hynek Schlawack**, [creator of `attrs`](https://www.attrs.org/), core
developer of Twisted and CPython:

> An auto-formatter that doesn't suck is all I want for Xmas!

**Carl Meyer**, [Django](https://www.djangoproject.com/) core developer:

> At least the name is good.

**Kenneth Reitz**, creator of [`requests`](http://python-requests.org/)
and [`pipenv`](https://docs.pipenv.org/):

> This vastly improves the formatting of our code. Thanks a ton!


## Show your style

Use the badge in your project's README.md:

```markdown
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)
```

Using the badge in README.rst:
```
.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black
```

Looks like this: [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)


## License

MIT


## Contributing to *Black*

In terms of inspiration, *Black* is about as configurable as *gofmt*.
This is deliberate.

Bug reports and fixes are always welcome!  However, before you suggest a
new feature or configuration knob, ask yourself why you want it.  If it
enables better integration with some workflow, fixes an inconsistency,
speeds things up, and so on - go for it!  On the other hand, if your
answer is "because I don't like a particular formatting" then you're not
ready to embrace *Black* yet. Such changes are unlikely to get accepted.
You can still try but prepare to be disappointed.

More details can be found in [CONTRIBUTING](CONTRIBUTING.md).


## Change Log

### 19.3b0

* new option `--target-version` to control which Python versions
  *Black*-formatted code should target (#618)

* deprecated `--py36` (use `--target-version=py36` instead) (#724)

* *Black* no longer normalizes numeric literals to include `_` separators (#696)

* long `del` statements are now split into multiple lines (#698)

* type comments are no longer mangled in function signatures

* improved performance of formatting deeply nested data structures (#509)

* *Black* now properly formats multiple files in parallel on
  Windows (#632)

* *Black* now creates cache files atomically which allows it to be used
  in parallel pipelines (like `xargs -P8`) (#673)

* *Black* now correctly indents comments in files that were previously
  formatted with tabs (#262)

* `blackd` now supports CORS (#622)

### 18.9b0

* numeric literals are now formatted by *Black* (#452, #461, #464, #469):

  * numeric literals are normalized to include `_` separators on Python 3.6+ code

  * added `--skip-numeric-underscore-normalization` to disable the above behavior and
    leave numeric underscores as they were in the input

  * code with `_` in numeric literals is recognized as Python 3.6+

  * most letters in numeric literals are lowercased (e.g., in `1e10`, `0x01`)

  * hexadecimal digits are always uppercased (e.g. `0xBADC0DE`)

* added `blackd`, see [its documentation](#blackd) for more info (#349)

* adjacent string literals are now correctly split into multiple lines (#463)

* trailing comma is now added to single imports that don't fit on a line (#250)

* cache is now populated when `--check` is successful for a file which speeds up
  consecutive checks of properly formatted unmodified files (#448)

* whitespace at the beginning of the file is now removed (#399)

* fixed mangling [pweave](http://mpastell.com/pweave/) and
  [Spyder IDE](https://pythonhosted.org/spyder/) special comments (#532)

* fixed unstable formatting when unpacking big tuples (#267)

* fixed parsing of `__future__` imports with renames (#389)

* fixed scope of `# fmt: off` when directly preceding `yield` and other nodes (#385)

* fixed formatting of lambda expressions with default arguments (#468)

* fixed ``async for`` statements: *Black* no longer breaks them into separate
  lines (#372)

* note: the Vim plugin stopped registering ``,=`` as a default chord as it turned out
  to be a bad idea (#415)


### 18.6b4

* hotfix: don't freeze when multiple comments directly precede `# fmt: off` (#371)


### 18.6b3

* typing stub files (`.pyi`) now have blank lines added after constants (#340)

* `# fmt: off` and `# fmt: on` are now much more dependable:

  * they now work also within bracket pairs (#329)

  * they now correctly work across function/class boundaries (#335)

  * they now work when an indentation block starts with empty lines or misaligned
    comments (#334)

* made Click not fail on invalid environments; note that Click is right but the
  likelihood we'll need to access non-ASCII file paths when dealing with Python source
  code is low (#277)

* fixed improper formatting of f-strings with quotes inside interpolated
  expressions (#322)

* fixed unnecessary slowdown when long list literals where found in a file

* fixed unnecessary slowdown on AST nodes with very many siblings

* fixed cannibalizing backslashes during string normalization

* fixed a crash due to symbolic links pointing outside of the project directory (#338)


### 18.6b2

* added `--config` (#65)

* added `-h` equivalent to `--help` (#316)

* fixed improper unmodified file caching when `-S` was used

* fixed extra space in string unpacking (#305)

* fixed formatting of empty triple quoted strings (#313)

* fixed unnecessary slowdown in comment placement calculation on lines without
  comments


### 18.6b1

* hotfix: don't output human-facing information on stdout (#299)

* hotfix: don't output cake emoji on non-zero return code (#300)


### 18.6b0

* added `--include` and `--exclude` (#270)

* added `--skip-string-normalization` (#118)

* added `--verbose` (#283)

* the header output in `--diff` now actually conforms to the unified diff spec

* fixed long trivial assignments being wrapped in unnecessary parentheses (#273)

* fixed unnecessary parentheses when a line contained multiline strings (#232)

* fixed stdin handling not working correctly if an old version of Click was
  used (#276)

* *Black* now preserves line endings when formatting a file in place (#258)


### 18.5b1

* added `--pyi` (#249)

* added `--py36` (#249)

* Python grammar pickle caches are stored with the formatting caches, making
  *Black* work in environments where site-packages is not user-writable (#192)

* *Black* now enforces a PEP 257 empty line after a class-level docstring
  (and/or fields) and the first method

* fixed invalid code produced when standalone comments were present in a trailer
  that was omitted from line splitting on a large expression (#237)

* fixed optional parentheses being removed within `# fmt: off` sections (#224)

* fixed invalid code produced when stars in very long imports were incorrectly
  wrapped in optional parentheses (#234)

* fixed unstable formatting when inline comments were moved around in
  a trailer that was omitted from line splitting on a large expression
  (#238)

* fixed extra empty line between a class declaration and the first
  method if no class docstring or fields are present (#219)

* fixed extra empty line between a function signature and an inner
  function or inner class (#196)


### 18.5b0

* call chains are now formatted according to the
  [fluent interfaces](https://en.wikipedia.org/wiki/Fluent_interface)
  style (#67)

* data structure literals (tuples, lists, dictionaries, and sets) are
  now also always exploded like imports when they don't fit in a single
  line (#152)

* slices are now formatted according to PEP 8 (#178)

* parentheses are now also managed automatically on the right-hand side
  of assignments and return statements (#140)

* math operators now use their respective priorities for delimiting multiline
  expressions (#148)

* optional parentheses are now omitted on expressions that start or end
  with a bracket and only contain a single operator (#177)

* empty parentheses in a class definition are now removed (#145, #180)

* string prefixes are now standardized to lowercase and `u` is removed
  on Python 3.6+ only code and Python 2.7+ code with the `unicode_literals`
  future import (#188, #198, #199)

* typing stub files (`.pyi`) are now formatted in a style that is consistent
  with PEP 484 (#207, #210)

* progress when reformatting many files is now reported incrementally

* fixed trailers (content with brackets) being unnecessarily exploded
  into their own lines after a dedented closing bracket (#119)

* fixed an invalid trailing comma sometimes left in imports (#185)

* fixed non-deterministic formatting when multiple pairs of removable parentheses
  were used (#183)

* fixed multiline strings being unnecessarily wrapped in optional
  parentheses in long assignments (#215)

* fixed not splitting long from-imports with only a single name

* fixed Python 3.6+ file discovery by also looking at function calls with
  unpacking. This fixed non-deterministic formatting if trailing commas
  where used both in function signatures with stars and function calls
  with stars but the former would be reformatted to a single line.

* fixed crash on dealing with optional parentheses (#193)

* fixed "is", "is not", "in", and "not in" not considered operators for
  splitting purposes

* fixed crash when dead symlinks where encountered


### 18.4a4

* don't populate the cache on `--check` (#175)


### 18.4a3

* added a "cache"; files already reformatted that haven't changed on disk
  won't be reformatted again (#109)

* `--check` and `--diff` are no longer mutually exclusive (#149)

* generalized star expression handling, including double stars; this
  fixes multiplication making expressions "unsafe" for trailing commas (#132)

* *Black* no longer enforces putting empty lines behind control flow statements
  (#90)

* *Black* now splits imports like "Mode 3 + trailing comma" of isort (#127)

* fixed comment indentation when a standalone comment closes a block (#16, #32)

* fixed standalone comments receiving extra empty lines if immediately preceding
  a class, def, or decorator (#56, #154)

* fixed `--diff` not showing entire path (#130)

* fixed parsing of complex expressions after star and double stars in
  function calls (#2)

* fixed invalid splitting on comma in lambda arguments (#133)

* fixed missing splits of ternary expressions (#141)


### 18.4a2

* fixed parsing of unaligned standalone comments (#99, #112)

* fixed placement of dictionary unpacking inside dictionary literals (#111)

* Vim plugin now works on Windows, too

* fixed unstable formatting when encountering unnecessarily escaped quotes
  in a string (#120)


### 18.4a1

* added `--quiet` (#78)

* added automatic parentheses management (#4)

* added [pre-commit](https://pre-commit.com) integration (#103, #104)

* fixed reporting on `--check` with multiple files (#101, #102)

* fixed removing backslash escapes from raw strings (#100, #105)


### 18.4a0

* added `--diff` (#87)

* add line breaks before all delimiters, except in cases like commas, to
  better comply with PEP 8 (#73)

* standardize string literals to use double quotes (almost) everywhere
  (#75)

* fixed handling of standalone comments within nested bracketed
  expressions; *Black* will no longer produce super long lines or put all
  standalone comments at the end of the expression (#22)

* fixed 18.3a4 regression: don't crash and burn on empty lines with
  trailing whitespace (#80)

* fixed 18.3a4 regression: `# yapf: disable` usage as trailing comment
  would cause *Black* to not emit the rest of the file (#95)

* when CTRL+C is pressed while formatting many files, *Black* no longer
  freaks out with a flurry of asyncio-related exceptions

* only allow up to two empty lines on module level and only single empty
  lines within functions (#74)


### 18.3a4

* `# fmt: off` and `# fmt: on` are implemented (#5)

* automatic detection of deprecated Python 2 forms of print statements
  and exec statements in the formatted file (#49)

* use proper spaces for complex expressions in default values of typed
  function arguments (#60)

* only return exit code 1 when --check is used (#50)

* don't remove single trailing commas from square bracket indexing
  (#59)

* don't omit whitespace if the previous factor leaf wasn't a math
  operator (#55)

* omit extra space in kwarg unpacking if it's the first argument (#46)

* omit extra space in [Sphinx auto-attribute comments](http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute)
  (#68)


### 18.3a3

* don't remove single empty lines outside of bracketed expressions
  (#19)

* added ability to pipe formatting from stdin to stdin (#25)

* restored ability to format code with legacy usage of `async` as
  a name (#20, #42)

* even better handling of numpy-style array indexing (#33, again)


### 18.3a2

* changed positioning of binary operators to occur at beginning of lines
  instead of at the end, following [a recent change to PEP 8](https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b)
  (#21)

* ignore empty bracket pairs while splitting. This avoids very weirdly
  looking formattings (#34, #35)

* remove a trailing comma if there is a single argument to a call

* if top level functions were separated by a comment, don't put four
  empty lines after the upper function

* fixed unstable formatting of newlines with imports

* fixed unintentional folding of post scriptum standalone comments
  into last statement if it was a simple statement (#18, #28)

* fixed missing space in numpy-style array indexing (#33)

* fixed spurious space after star-based unary expressions (#31)


### 18.3a1

* added `--check`

* only put trailing commas in function signatures and calls if it's
  safe to do so. If the file is Python 3.6+ it's always safe, otherwise
  only safe if there are no `*args` or `**kwargs` used in the signature
  or call. (#8)

* fixed invalid spacing of dots in relative imports (#6, #13)

* fixed invalid splitting after comma on unpacked variables in for-loops
  (#23)

* fixed spurious space in parenthesized set expressions (#7)

* fixed spurious space after opening parentheses and in default
  arguments (#14, #17)

* fixed spurious space after unary operators when the operand was
  a complex expression (#15)


### 18.3a0

* first published version, Happy 🍰 Day 2018!

* alpha quality

* date-versioned (see: https://calver.org/)


## Authors

Glued together by [Łukasz Langa](mailto:lukasz@langa.pl).

Maintained with [Carol Willing](mailto:carolcode@willingconsulting.com),
[Carl Meyer](mailto:carl@oddbird.net),
[Jelle Zijlstra](mailto:jelle.zijlstra@gmail.com),
[Mika Naylor](mailto:mail@autophagy.io), and
[Zsolt Dollenstein](mailto:zsol.zsol@gmail.com).

Multiple contributions by:
* [Anthony Sottile](mailto:asottile@umich.edu)
* [Artem Malyshev](mailto:proofit404@gmail.com)
* [Christian Heimes](mailto:christian@python.org)
* [Daniel M. Capella](mailto:polycitizen@gmail.com)
* [Eli Treuherz](mailto:eli@treuherz.com)
* hauntsaninja
* Hugo van Kemenade
* [Ivan Katanić](mailto:ivan.katanic@gmail.com)
* [Jonas Obrist](mailto:ojiidotch@gmail.com)
* [Luka Sterbic](mailto:luka.sterbic@gmail.com)
* [Miguel Gaiowski](mailto:miggaiowski@gmail.com)
* [Miroslav Shubernetskiy](mailto:miroslav@miki725.com)
* [Neraste](mailto:neraste.herr10@gmail.com)
* [Osaetin Daniel](mailto:osaetindaniel@gmail.com)
* [Peter Bengtsson](mailto:mail@peterbe.com)
* [Stavros Korokithakis](mailto:hi@stavros.io)
* [Sunil Kapil](mailto:snlkapil@gmail.com)
* [Utsav Shah](mailto:ukshah2@illinois.edu)
* [Vishwas B Sharma](mailto:sharma.vishwas88@gmail.com)
* [Chuck Wooters](mailto:chuck.wooters@microsoft.com)



  * [blaze-0.10.1](UNKNOWN) .. image:: https://raw.github.com/blaze/blaze/master/docs/source/svg/blaze_med.png
   :align: center

|Build Status| |Coverage Status| |Join the chat at
https://gitter.im/blaze/blaze|

**Blaze** translates a subset of modified NumPy and Pandas-like syntax
to databases and other computing systems. Blaze allows Python users a
familiar interface to query data living in other data storage systems.

Example
=======

We point blaze to a simple dataset in a foreign database (PostgreSQL).
Instantly we see results as we would see them in a Pandas DataFrame.

.. code:: python

    >>> import blaze as bz
    >>> iris = bz.Data('postgresql://localhost::iris')
    >>> iris
        sepal_length  sepal_width  petal_length  petal_width      species
    0            5.1          3.5           1.4          0.2  Iris-setosa
    1            4.9          3.0           1.4          0.2  Iris-setosa
    2            4.7          3.2           1.3          0.2  Iris-setosa
    3            4.6          3.1           1.5          0.2  Iris-setosa

These results occur immediately. Blaze does not pull data out of
Postgres, instead it translates your Python commands into SQL (or
others.)

.. code:: python

    >>> iris.species.distinct()
               species
    0      Iris-setosa
    1  Iris-versicolor
    2   Iris-virginica

    >>> bz.by(iris.species, smallest=iris.petal_length.min(),
    ...                      largest=iris.petal_length.max())
               species  largest  smallest
    0      Iris-setosa      1.9       1.0
    1  Iris-versicolor      5.1       3.0
    2   Iris-virginica      6.9       4.5

This same example would have worked with a wide range of databases,
on-disk text or binary files, or remote data.

What Blaze is not
=================

Blaze does not perform computation. It relies on other systems like SQL,
Spark, or Pandas to do the actual number crunching. It is not a
replacement for any of these systems.

Blaze does not implement the entire NumPy/Pandas API, nor does it
interact with libraries intended to work with NumPy/Pandas. This is the
cost of using more and larger data systems.

Blaze is a good way to inspect data living in a large database, perform
a small but powerful set of operations to query that data, and then
transform your results into a format suitable for your favorite Python
tools.

In the Abstract
===============

Blaze separates the computations that we want to perform:

.. code:: python

    >>> accounts = Symbol('accounts', 'var * {id: int, name: string, amount: int}')

    >>> deadbeats = accounts[accounts.amount < 0].name

From the representation of data

.. code:: python

    >>> L = [[1, 'Alice',   100],
    ...      [2, 'Bob',    -200],
    ...      [3, 'Charlie', 300],
    ...      [4, 'Denis',   400],
    ...      [5, 'Edith',  -500]]

Blaze enables users to solve data-oriented problems

.. code:: python

    >>> list(compute(deadbeats, L))
    ['Bob', 'Edith']

But the separation of expression from data allows us to switch between
different backends.

Here we solve the same problem using Pandas instead of Pure Python.

.. code:: python

    >>> df = DataFrame(L, columns=['id', 'name', 'amount'])

    >>> compute(deadbeats, df)
    1      Bob
    4    Edith
    Name: name, dtype: object

Blaze doesn't compute these results, Blaze intelligently drives other
projects to compute them instead. These projects range from simple Pure
Python iterators to powerful distributed Spark clusters. Blaze is built
to be extended to new systems as they evolve.

Getting Started
===============

Blaze is available on conda or on PyPI

::

    conda install blaze
    pip install blaze

Development builds are accessible

::

    conda install blaze -c blaze
    pip install http://github.com/blaze/blaze --upgrade

You may want to view `the docs <http://blaze.pydata.org>`__, `the
tutorial <http://github.com/blaze/blaze-tutorial>`__, `some
blogposts <http://continuum.io/blog/tags/blaze>`__, or the `mailing list
archives <https://groups.google.com/a/continuum.io/forum/#!forum/blaze-dev>`__.


Development setup
=================

The quickest way to install all Blaze dependencies with ``conda`` is as
follows

::

    conda install blaze spark -c blaze -c anaconda-cluster -y
    conda remove odo blaze blaze-core datashape -y

After running these commands, clone ``odo``, ``blaze``, and ``datashape`` from
GitHub directly.  These three projects release together.  Run ``python setup.py
develop`` to make development installations of each.


License
=======

Released under BSD license. See `LICENSE.txt <LICENSE.txt>`__ for
details.

Blaze development is sponsored by Continuum Analytics.

.. |Build Status| image:: https://travis-ci.org/blaze/blaze.png
   :target: https://travis-ci.org/blaze/blaze
.. |Coverage Status| image:: https://coveralls.io/repos/blaze/blaze/badge.png
   :target: https://coveralls.io/r/blaze/blaze
.. |Join the chat at https://gitter.im/blaze/blaze| image:: https://badges.gitter.im/Join%20Chat.svg
   :target: https://gitter.im/blaze/blaze?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
  * [bleach-3.1.0](https://github.com/mozilla/bleach) ======
Bleach
======

.. image:: https://travis-ci.org/mozilla/bleach.svg?branch=master
   :target: https://travis-ci.org/mozilla/bleach

.. image:: https://badge.fury.io/py/bleach.svg
   :target: http://badge.fury.io/py/bleach

Bleach is an allowed-list-based HTML sanitizing library that escapes or strips
markup and attributes.

Bleach can also linkify text safely, applying filters that Django's ``urlize``
filter cannot, and optionally setting ``rel`` attributes, even on links already
in the text.

Bleach is intended for sanitizing text from *untrusted* sources. If you find
yourself jumping through hoops to allow your site administrators to do lots of
things, you're probably outside the use cases. Either trust those users, or
don't.

Because it relies on html5lib_, Bleach is as good as modern browsers at dealing
with weird, quirky HTML fragments. And *any* of Bleach's methods will fix
unbalanced or mis-nested tags.

The version on GitHub_ is the most up-to-date and contains the latest bug
fixes. You can find full documentation on `ReadTheDocs`_.

:Code:           https://github.com/mozilla/bleach
:Documentation:  https://bleach.readthedocs.io/
:Issue tracker:  https://github.com/mozilla/bleach/issues
:IRC:            ``#bleach`` on irc.mozilla.org
:License:        Apache License v2; see LICENSE file


Reporting Bugs
==============

For regular bugs, please report them `in our issue tracker
<https://github.com/mozilla/bleach/issues>`_.

If you believe that you've found a security vulnerability, please `file a secure
bug report in our bug tracker
<https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&product=Webtools&component=Bleach-security&groups=webtools-security>`_
or send an email to *security AT mozilla DOT org*.

For more information on security-related bug disclosure and the PGP key to use
for sending encrypted mail or to verify responses received from that address,
please read our wiki page at
`<https://www.mozilla.org/en-US/security/#For_Developers>`_.


Security
========

Bleach is a security-focused library.

We have a responsible security vulnerability reporting process. Please use
that if you're reporting a security issue.

Security issues are fixed in private. After we land such a fix, we'll do a
release.

For every release, we mark security issues we've fixed in the ``CHANGES`` in
the **Security issues** section. We include any relevant CVE links.


Installing Bleach
=================

Bleach is available on PyPI_, so you can install it with ``pip``::

    $ pip install bleach


Upgrading Bleach
================

.. warning::

   Before doing any upgrades, read through `Bleach Changes
   <https://bleach.readthedocs.io/en/latest/changes.html>`_ for backwards
   incompatible changes, newer versions, etc.


Basic use
=========

The simplest way to use Bleach is:

.. code-block:: python

    >>> import bleach

    >>> bleach.clean('an <script>evil()</script> example')
    u'an &lt;script&gt;evil()&lt;/script&gt; example'

    >>> bleach.linkify('an http://example.com url')
    u'an <a href="http://example.com" rel="nofollow">http://example.com</a> url


Code of conduct
===============

This project and repository is governed by Mozilla's code of conduct and
etiquette guidelines. For more details please see the `Mozilla Community
Participation Guidelines
<https://www.mozilla.org/about/governance/policies/participation/>`_ and
`Developer Etiquette Guidelines
<https://bugzilla.mozilla.org/page.cgi?id=etiquette.html>`_.


.. _html5lib: https://github.com/html5lib/html5lib-python
.. _GitHub: https://github.com/mozilla/bleach
.. _ReadTheDocs: https://bleach.readthedocs.io/
.. _PyPI: http://pypi.python.org/pypi/bleach


Bleach changes
==============

Version 3.1.0 (January 9th, 2019)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

* Add ``recognized_tags`` argument to the linkify ``Linker`` class. This
  fixes issues when linkifying on its own and having some tags get escaped.
  It defaults to a list of HTML5 tags. Thank you, Chad Birch! (#409)

**Bug fixes**

* Add ``six>=1.9`` to requirements. Thank you, Dave Shawley (#416)

* Fix cases where attribute names could have invalid characters in them.
  (#419)

* Fix problems with ``LinkifyFilter`` not being able to match links
  across ``&amp;``. (#422)

* Fix ``InputStreamWithMemory`` when the ``BleachHTMLParser`` is
  parsing ``meta`` tags. (#431)

* Fix doctests. (#357)


Version 3.0.2 (October 11th, 2018)
----------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Merge ``Characters`` tokens after sanitizing them. This fixes issues in the
  ``LinkifyFilter`` where it was only linkifying parts of urls. (#374)


Version 3.0.1 (October 9th, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

* Support Python 3.7. It supported Python 3.7 just fine, but we added 3.7 to
  the list of Python environments we test so this is now officially supported.
  (#377)

**Bug fixes**

* Fix ``list`` object has no attribute ``lower`` in ``clean``. (#398)
* Fix ``abbr`` getting escaped in ``linkify``. (#400)


Version 3.0.0 (October 3rd, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

* A bunch of functions were moved from one module to another.

  These were moved from ``bleach.sanitizer`` to ``bleach.html5lib_shim``:

  * ``convert_entity``
  * ``convert_entities``
  * ``match_entity``
  * ``next_possible_entity``
  * ``BleachHTMLSerializer``
  * ``BleachHTMLTokenizer``
  * ``BleachHTMLParser``

  These functions and classes weren't documented and aren't part of the
  public API, but people read code and might be using them so we're
  considering it an incompatible API change.

  If you're using them, you'll need to update your code.

**Features**

* Bleach no longer depends on html5lib. html5lib==1.0.1 is now vendored into
  Bleach. You can remove it from your requirements file if none of your other
  requirements require html5lib.

  This means Bleach will now work fine with other libraries that depend on
  html5lib regardless of what version of html5lib they require. (#386)

**Bug fixes**

* Fixed tags getting added when using clean or linkify. This was a
  long-standing regression from the Bleach 2.0 rewrite. (#280, #392)

* Fixed ``<isindex>`` getting replaced with a string. Now it gets escaped or
  stripped depending on whether it's in the allowed tags or not. (#279)


Version 2.1.4 (August 16th, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

* Dropped support for Python 3.3. (#328)

**Features**

None

**Bug fixes**

* Handle ambiguous ampersands in correctly. (#359)


Version 2.1.3 (March 5th, 2018)
-------------------------------

**Security fixes**

* Attributes that have URI values weren't properly sanitized if the
  values contained character entities. Using character entities, it
  was possible to construct a URI value with a scheme that was not
  allowed that would slide through unsanitized.

  This security issue was introduced in Bleach 2.1. Anyone using
  Bleach 2.1 is highly encouraged to upgrade.

  https://bugzilla.mozilla.org/show_bug.cgi?id=1442745

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Fixed some other edge cases for attribute URI value sanitizing and
  improved testing of this code.


Version 2.1.2 (December 7th, 2017)
----------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Support html5lib-python 1.0.1. (#337)

* Add deprecation warning for supporting html5lib-python < 1.0.

* Switch to semver.


Version 2.1.1 (October 2nd, 2017)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Fix ``setup.py`` opening files when ``LANG=``. (#324)


Version 2.1 (September 28th, 2017)
----------------------------------

**Security fixes**

* Convert control characters (backspace particularly) to "?" preventing
  malicious copy-and-paste situations. (#298)

  See `<https://github.com/mozilla/bleach/issues/298>`_ for more details.

  This affects all previous versions of Bleach. Check the comments on that
  issue for ways to alleviate the issue if you can't upgrade to Bleach 2.1.


**Backwards incompatible changes**

* Redid versioning. ``bleach.VERSION`` is no longer available. Use the string
  version at ``bleach.__version__`` and parse it with
  ``pkg_resources.parse_version``. (#307)

* clean, linkify: linkify and clean should only accept text types; thank you,
  Janusz! (#292)

* clean, linkify: accept only unicode or utf-8-encoded str (#176)


**Features**


**Bug fixes**

* ``bleach.clean()`` no longer unescapes entities including ones that are missing
  a ``;`` at the end which can happen in urls and other places. (#143)

* linkify: fix http links inside of mailto links; thank you, sedrubal! (#300)

* clarify security policy in docs (#303)

* fix dependency specification for html5lib 1.0b8, 1.0b9, and 1.0b10; thank you,
  Zoltán! (#268)

* add Bleach vs. html5lib comparison to README; thank you, Stu Cox! (#278)

* fix KeyError exceptions on tags without href attr; thank you, Alex Defsen!
  (#273)

* add test website and scripts to test ``bleach.clean()`` output in browser;
  thank you, Greg Guthe!


Version 2.0 (March 8th, 2017)
-----------------------------

**Security fixes**

* None


**Backwards incompatible changes**

* Removed support for Python 2.6. #206

* Removed support for Python 3.2. #224

* Bleach no longer supports html5lib < 0.99999999 (8 9s).

  This version is a rewrite to use the new sanitizing API since the old
  one was dropped in html5lib 0.99999999 (8 9s).

  If you're using 0.9999999 (7 9s) upgrade to 0.99999999 (8 9s) or higher.

  If you're using 1.0b8 (equivalent to 0.9999999 (7 9s)), upgrade to 1.0b9
  (equivalent to 0.99999999 (8 9s)) or higher.

* ``bleach.clean`` and friends were rewritten

  ``clean`` was reimplemented as an html5lib filter and happens at a different
  step in the HTML parsing -> traversing -> serializing process. Because of
  that, there are some differences in clean's output as compared with previous
  versions.

  Amongst other things, this version will add end tags even if the tag in
  question is to be escaped.

* ``bleach.clean`` and friends attribute callables now take three arguments:
  tag, attribute name and attribute value. Previously they only took attribute
  name and attribute value.

  All attribute callables will need to be updated.

* ``bleach.linkify`` was rewritten

  ``linkify`` was reimplemented as an html5lib Filter. As such, it no longer
  accepts a ``tokenizer`` argument.

  The callback functions for adjusting link attributes now takes a namespaced
  attribute.

  Previously you'd do something like this::

      def check_protocol(attrs, is_new):
          if not attrs.get('href', '').startswith('http:', 'https:')):
              return None
          return attrs

  Now it's more like this::

      def check_protocol(attrs, is_new):
          if not attrs.get((None, u'href'), u'').startswith(('http:', 'https:')):
              #            ^^^^^^^^^^^^^^^
              return None
          return attrs

  Further, you need to make sure you're always using unicode values. If you
  don't then html5lib will raise an assertion error that the value is not
  unicode.

  All linkify filters will need to be updated.

* ``bleach.linkify`` and friends had a ``skip_pre`` argument--that's been
  replaced with a more general ``skip_tags`` argument.

  Before, you might do::

      bleach.linkify(some_text, skip_pre=True)

  The equivalent with Bleach 2.0 is::

      bleach.linkify(some_text, skip_tags=['pre'])

  You can skip other tags, too, like ``style`` or ``script`` or other places
  where you don't want linkification happening.

  All uses of linkify that use ``skip_pre`` will need to be updated.


**Changes**

* Supports Python 3.6.

* Supports html5lib >= 0.99999999 (8 9s).

* There's a ``bleach.sanitizer.Cleaner`` class that you can instantiate with your
  favorite clean settings for easy reuse.

* There's a ``bleach.linkifier.Linker`` class that you can instantiate with your
  favorite linkify settings for easy reuse.

* There's a ``bleach.linkifier.LinkifyFilter`` which is an htm5lib filter that
  you can pass as a filter to ``bleach.sanitizer.Cleaner`` allowing you to clean
  and linkify in one pass.

* ``bleach.clean`` and friends can now take a callable as an attributes arg value.

* Tons of bug fixes.

* Cleaned up tests.

* Documentation fixes.


Version 1.5 (November 4th, 2016)
--------------------------------

**Security fixes**

* None

**Backwards incompatible changes**

* clean: The list of ``ALLOWED_PROTOCOLS`` now defaults to http, https and
  mailto.

  Previously it was a long list of protocols something like ed2k, ftp, http,
  https, irc, mailto, news, gopher, nntp, telnet, webcal, xmpp, callto, feed,
  urn, aim, rsync, tag, ssh, sftp, rtsp, afs, data. #149

**Changes**

* clean: Added ``protocols`` to arguments list to let you override the list of
  allowed protocols. Thank you, Andreas Malecki! #149

* linkify: Fix a bug involving periods at the end of an email address. Thank you,
  Lorenz Schori! #219

* linkify: Fix linkification of non-ascii ports. Thank you Alexandre, Macabies!
  #207

* linkify: Fix linkify inappropriately removing node tails when dropping nodes.
  #132

* Fixed a test that failed periodically. #161

* Switched from nose to py.test. #204

* Add test matrix for all supported Python and html5lib versions. #230

* Limit to html5lib ``>=0.999,!=0.9999,!=0.99999,<0.99999999`` because 0.9999
  and 0.99999 are busted.

* Add support for ``python setup.py test``. #97


Version 1.4.3 (May 23rd, 2016)
------------------------------

**Security fixes**

* None

**Changes**

* Limit to html5lib ``>=0.999,<0.99999999`` because of impending change to
  sanitizer api. #195


Version 1.4.2 (September 11, 2015)
----------------------------------

**Changes**

* linkify: Fix hang in linkify with ``parse_email=True``. #124

* linkify: Fix crash in linkify when removing a link that is a first-child. #136

* Updated TLDs.

* linkify: Don't remove exterior brackets when linkifying. #146


Version 1.4.1 (December 15, 2014)
---------------------------------

**Changes**

* Consistent order of attributes in output.

* Python 3.4 support.


Version 1.4 (January 12, 2014)
------------------------------

**Changes**

* linkify: Update linkify to use etree type Treewalker instead of simpletree.

* Updated html5lib to version ``>=0.999``.

* Update all code to be compatible with Python 3 and 2 using six.

* Switch to Apache License.


Version 1.3
-----------

* Used by Python 3-only fork.


Version 1.2.2 (May 18, 2013)
----------------------------

* Pin html5lib to version 0.95 for now due to major API break.


Version 1.2.1 (February 19, 2013)
---------------------------------

* ``clean()`` no longer considers ``feed:`` an acceptable protocol due to
  inconsistencies in browser behavior.


Version 1.2 (January 28, 2013)
------------------------------

* ``linkify()`` has changed considerably. Many keyword arguments have been
  replaced with a single callbacks list. Please see the documentation for more
  information.

* Bleach will no longer consider unacceptable protocols when linkifying.

* ``linkify()`` now takes a tokenizer argument that allows it to skip
  sanitization.

* ``delinkify()`` is gone.

* Removed exception handling from ``_render``. ``clean()`` and ``linkify()`` may
  now throw.

* ``linkify()`` correctly ignores case for protocols and domain names.

* ``linkify()`` correctly handles markup within an <a> tag.


Version 1.1.5
-------------


Version 1.1.4
-------------


Version 1.1.3 (July 10, 2012)
-----------------------------

* Fix parsing bare URLs when parse_email=True.


Version 1.1.2 (June 1, 2012)
----------------------------

* Fix hang in style attribute sanitizer. (#61)

* Allow ``/`` in style attribute values.


Version 1.1.1 (February 17, 2012)
---------------------------------

* Fix tokenizer for html5lib 0.9.5.


Version 1.1.0 (October 24, 2011)
--------------------------------

* ``linkify()`` now understands port numbers. (#38)

* Documented character encoding behavior. (#41)

* Add an optional target argument to ``linkify()``.

* Add ``delinkify()`` method. (#45)

* Support subdomain whitelist for ``delinkify()``. (#47, #48)


Version 1.0.4 (September 2, 2011)
---------------------------------

* Switch to SemVer git tags.

* Make ``linkify()`` smarter about trailing punctuation. (#30)

* Pass ``exc_info`` to logger during rendering issues.

* Add wildcard key for attributes. (#19)

* Make ``linkify()`` use the ``HTMLSanitizer`` tokenizer. (#36)

* Fix URLs wrapped in parentheses. (#23)

* Make ``linkify()`` UTF-8 safe. (#33)


Version 1.0.3 (June 14, 2011)
-----------------------------

* ``linkify()`` works with 3rd level domains. (#24)

* ``clean()`` supports vendor prefixes in style values. (#31, #32)

* Fix ``linkify()`` email escaping.


Version 1.0.2 (June 6, 2011)
----------------------------

* ``linkify()`` supports email addresses.

* ``clean()`` supports callables in attributes filter.


Version 1.0.1 (April 12, 2011)
------------------------------

* ``linkify()`` doesn't drop trailing slashes. (#21)
* ``linkify()`` won't linkify 'libgl.so.1'. (#22)



  * [blessings-1.7](https://github.com/erikrose/blessings) =========
Blessings
=========

Coding with Blessings looks like this... ::

    from blessings import Terminal

    t = Terminal()

    print t.bold('Hi there!')
    print t.bold_red_on_bright_green('It hurts my eyes!')

    with t.location(0, t.height - 1):
        print 'This is at the bottom.'

Or, for byte-level control, you can drop down and play with raw terminal
capabilities::

    print '{t.bold}All your {t.red}bold and red base{t.normal}'.format(t=t)
    print t.wingo(2)

`Full API Reference <https://blessings.readthedocs.io/>`_

The Pitch
=========

Blessings lifts several of curses_' limiting assumptions, and it makes your
code pretty, too:

* Use styles, color, and maybe a little positioning without necessarily
  clearing the whole
  screen first.
* Leave more than one screenful of scrollback in the buffer after your program
  exits, like a well-behaved command-line app should.
* Get rid of all those noisy, C-like calls to ``tigetstr`` and ``tparm``, so
  your code doesn't get crowded out by terminal bookkeeping.
* Act intelligently when somebody redirects your output to a file, omitting the
  terminal control codes the user doesn't want to see (optional).

.. _curses: http://docs.python.org/library/curses.html

Before And After
----------------

Without Blessings, this is how you'd print some underlined text at the bottom
of the screen::

    from curses import tigetstr, setupterm, tparm
    from fcntl import ioctl
    from os import isatty
    import struct
    import sys
    from termios import TIOCGWINSZ

    # If we want to tolerate having our output piped to other commands or
    # files without crashing, we need to do all this branching:
    if hasattr(sys.stdout, 'fileno') and isatty(sys.stdout.fileno()):
        setupterm()
        sc = tigetstr('sc')
        cup = tigetstr('cup')
        rc = tigetstr('rc')
        underline = tigetstr('smul')
        normal = tigetstr('sgr0')
    else:
        sc = cup = rc = underline = normal = ''
    print sc  # Save cursor position.
    if cup:
        # tigetnum('lines') doesn't always update promptly, hence this:
        height = struct.unpack('hhhh', ioctl(0, TIOCGWINSZ, '\000' * 8))[0]
        print tparm(cup, height - 1, 0)  # Move cursor to bottom.
    print 'This is {under}underlined{normal}!'.format(under=underline,
                                                      normal=normal)
    print rc  # Restore cursor position.

That was long and full of incomprehensible trash! Let's try it again, this time
with Blessings::

    from blessings import Terminal

    term = Terminal()
    with term.location(0, term.height - 1):
        print 'This is', term.underline('pretty!')

Much better.

What It Provides
================

Blessings provides just one top-level object: ``Terminal``. Instantiating a
``Terminal`` figures out whether you're on a terminal at all and, if so, does
any necessary terminal setup. After that, you can proceed to ask it all sorts
of things about the terminal. Terminal terminal terminal.

Simple Formatting
-----------------

Lots of handy formatting codes ("capabilities" in low-level parlance) are
available as attributes on a ``Terminal``. For example::

    from blessings import Terminal

    term = Terminal()
    print 'I am ' + term.bold + 'bold' + term.normal + '!'

Though they are strings at heart, you can also use them as callable wrappers so
you don't have to say ``normal`` afterward::

    print 'I am', term.bold('bold') + '!'

Or, if you want fine-grained control while maintaining some semblance of
brevity, you can combine it with Python's string formatting, which makes
attributes easy to access::

    print 'All your {t.red}base {t.underline}are belong to us{t.normal}'.format(t=term)

Simple capabilities of interest include...

* ``bold``
* ``reverse``
* ``underline``
* ``no_underline`` (which turns off underlining)
* ``blink``
* ``normal`` (which turns off everything, even colors)

Here are a few more which are less likely to work on all terminals:

* ``dim``
* ``italic`` and ``no_italic``
* ``shadow`` and ``no_shadow``
* ``standout`` and ``no_standout``
* ``subscript`` and ``no_subscript``
* ``superscript`` and ``no_superscript``
* ``flash`` (which flashes the screen once)

Note that, while the inverse of ``underline`` is ``no_underline``, the only way
to turn off ``bold`` or ``reverse`` is ``normal``, which also cancels any
custom colors. This is because there's no portable way to tell the terminal to
undo certain pieces of formatting, even at the lowest level.

You might also notice that the above aren't the typical incomprehensible
terminfo capability names; we alias a few of the harder-to-remember ones for
readability. However, you aren't limited to these: you can reference any
string-returning capability listed on the `terminfo man page`_ by the name
under the "Cap-name" column: for example, ``term.rum``.

.. _`terminfo man page`: http://www.manpagez.com/man/5/terminfo/

Color
-----

16 colors, both foreground and background, are available as easy-to-remember
attributes::

    from blessings import Terminal

    term = Terminal()
    print term.red + term.on_green + 'Red on green? Ick!' + term.normal
    print term.bright_red + term.on_bright_blue + 'This is even worse!' + term.normal

You can also call them as wrappers, which sets everything back to normal at the
end::

    print term.red_on_green('Red on green? Ick!')
    print term.yellow('I can barely see it.')

The available colors are...

* ``black``
* ``red``
* ``green``
* ``yellow``
* ``blue``
* ``magenta``
* ``cyan``
* ``white``

You can set the background color instead of the foreground by prepending
``on_``, as in ``on_blue``. There is also a ``bright`` version of each color:
for example, ``on_bright_blue``.

There is also a numerical interface to colors, which takes an integer from
0-15::

    term.color(5) + 'Hello' + term.normal
    term.on_color(3) + 'Hello' + term.normal

    term.color(5)('Hello')
    term.on_color(3)('Hello')

If some color is unsupported (for instance, if only the normal colors are
available, not the bright ones), trying to use it will, on most terminals, have
no effect: the foreground and background colors will stay as they were. You can
get fancy and do different things depending on the supported colors by checking
`number_of_colors`_.

.. _`number_of_colors`: http://packages.python.org/blessings/#blessings.Terminal.number_of_colors

Compound Formatting
-------------------

If you want to do lots of crazy formatting all at once, you can just mash it
all together::

    from blessings import Terminal

    term = Terminal()
    print term.bold_underline_green_on_yellow + 'Woo' + term.normal

Or you can use your newly coined attribute as a wrapper, which implicitly sets
everything back to normal afterward::

    print term.bold_underline_green_on_yellow('Woo')

This compound notation comes in handy if you want to allow users to customize
the formatting of your app: just have them pass in a format specifier like
"bold_green" on the command line, and do a quick ``getattr(term,
that_option)('Your text')`` when you do your formatting.

I'd be remiss if I didn't credit couleur_, where I probably got the idea for
all this mashing.

.. _couleur: http://pypi.python.org/pypi/couleur

Moving The Cursor
-----------------

When you want to move the cursor to output text at a specific spot, you have
a few choices.

Moving Temporarily
~~~~~~~~~~~~~~~~~~

Most often, you'll need to flit to a certain location, print something, and
then return: for example, when updating a progress bar at the bottom of the
screen. ``Terminal`` provides a context manager for doing this concisely::

    from blessings import Terminal

    term = Terminal()
    with term.location(0, term.height - 1):
        print 'Here is the bottom.'
    print 'This is back where I came from.'

Parameters to ``location()`` are ``x`` and then ``y``, but you can also pass
just one of them, leaving the other alone. For example... ::

    with term.location(y=10):
        print 'We changed just the row.'

If you're doing a series of ``move`` calls (see below) and want to return the
cursor to its original position afterward, call ``location()`` with no
arguments, and it will do only the position restoring::

    with term.location():
        print term.move(1, 1) + 'Hi'
        print term.move(9, 9) + 'Mom'

Note that, since ``location()`` uses the terminal's built-in
position-remembering machinery, you can't usefully nest multiple calls. Use
``location()`` at the outermost spot, and use simpler things like ``move``
inside.

Moving Permanently
~~~~~~~~~~~~~~~~~~

If you just want to move and aren't worried about returning, do something like
this::

    from blessings import Terminal

    term = Terminal()
    print term.move(10, 1) + 'Hi, mom!'

``move``
  Position the cursor elsewhere. Parameters are y coordinate, then x
  coordinate.
``move_x``
  Move the cursor to the given column.
``move_y``
  Move the cursor to the given row.

How does all this work? These are simply more terminal capabilities, wrapped to
give them nicer names. The added wrinkle--that they take parameters--is also
given a pleasant treatment: rather than making you dig up ``tparm()`` all the
time, we simply make these capabilities into callable strings. You'd get the
raw capability strings if you were to just print them, but they're fully
parametrized if you pass params to them as if they were functions.

Consequently, you can also reference any other string-returning capability
listed on the `terminfo man page`_ by its name under the "Cap-name" column.

.. _`terminfo man page`: http://www.manpagez.com/man/5/terminfo/

One-Notch Movement
~~~~~~~~~~~~~~~~~~

Finally, there are some parameterless movement capabilities that move the
cursor one character in various directions:

* ``move_left``
* ``move_right``
* ``move_up``
* ``move_down``

For example... ::

    print term.move_up + 'Howdy!'

Height And Width
----------------

It's simple to get the height and width of the terminal, in characters::

    from blessings import Terminal

    term = Terminal()
    height = term.height
    width = term.width

These are newly updated each time you ask for them, so they're safe to use from
SIGWINCH handlers.

Clearing The Screen
-------------------

Blessings provides syntactic sugar over some screen-clearing capabilities:

``clear``
  Clear the whole screen.
``clear_eol``
  Clear to the end of the line.
``clear_bol``
  Clear backward to the beginning of the line.
``clear_eos``
  Clear to the end of screen.

Full-Screen Mode
----------------

Perhaps you have seen a full-screen program, such as an editor, restore the
exact previous state of the terminal upon exiting, including, for example, the
command-line prompt from which it was launched. Curses pretty much forces you
into this behavior, but Blessings makes it optional. If you want to do the
state-restoration thing, use these capabilities:

``enter_fullscreen``
    Switch to the terminal mode where full-screen output is sanctioned. Print
    this before you do any output.
``exit_fullscreen``
    Switch back to normal mode, restoring the exact state from before
    ``enter_fullscreen`` was used.

Using ``exit_fullscreen`` will wipe away any trace of your program's output, so
reserve it for when you don't want to leave anything behind in the scrollback.

There's also a context manager you can use as a shortcut::

    from blessings import Terminal

    term = Terminal()
    with term.fullscreen():
        # Print some stuff.

Besides brevity, another advantage is that it switches back to normal mode even
if an exception is raised in the ``with`` block.

Pipe Savvy
----------

If your program isn't attached to a terminal, like if it's being piped to
another command or redirected to a file, all the capability attributes on
``Terminal`` will return empty strings. You'll get a nice-looking file without
any formatting codes gumming up the works.

If you want to override this--like if you anticipate your program being piped
through ``less -r``, which handles terminal escapes just fine--pass
``force_styling=True`` to the ``Terminal`` constructor.

In any case, there is a ``does_styling`` attribute on ``Terminal`` that lets
you see whether your capabilities will return actual, working formatting codes.
If it's false, you should refrain from drawing progress bars and other frippery
and just stick to content, since you're apparently headed into a pipe::

    from blessings import Terminal

    term = Terminal()
    if term.does_styling:
        with term.location(0, term.height - 1):
            print 'Progress: [=======>   ]'
    print term.bold('Important stuff')

Shopping List
=============

There are decades of legacy tied up in terminal interaction, so attention to
detail and behavior in edge cases make a difference. Here are some ways
Blessings has your back:

* Uses the terminfo database so it works with any terminal type
* Provides up-to-the-moment terminal height and width, so you can respond to
  terminal size changes (SIGWINCH signals). (Most other libraries query the
  ``COLUMNS`` and ``LINES`` environment variables or the ``cols`` or ``lines``
  terminal capabilities, which don't update promptly, if at all.)
* Avoids making a mess if the output gets piped to a non-terminal
* Works great with standard Python string templating
* Provides convenient access to all terminal capabilities, not just a sugared
  few
* Outputs to any file-like object, not just stdout
* Keeps a minimum of internal state, so you can feel free to mix and match with
  calls to curses or whatever other terminal libraries you like

Blessings does not provide...

* Native color support on the Windows command prompt. However, it should work
  when used in concert with colorama_.

.. _colorama: http://pypi.python.org/pypi/colorama/0.2.4

Bugs
====

Bugs or suggestions? Visit the `issue tracker`_.

.. _`issue tracker`: https://github.com/erikrose/blessings/issues/

Blessings tests are run automatically by `Travis CI`_.

.. _`Travis CI`: https://travis-ci.org/erikrose/blessings/

.. image:: https://travis-ci.org/erikrose/blessings.svg?branch=master
    :target: https://travis-ci.org/erikrose/blessings


License
=======

Blessings is under the MIT License. See the LICENSE file.

Version History
===============

1.7
  * Drop support for Python 2.6 and 3.3, which are end-of-lifed.
  * Switch from 2to3 to the ``six`` library.

1.6.1
  * Don't crash if ``number_of_colors()`` is called when run in a non-terminal
    or when ``does_styling`` is otherwise false.

1.6
  * Add ``does_styling`` property. This takes ``force_styling`` into account
    and should replace most uses of ``is_a_tty``.
  * Make ``is_a_tty`` a read-only property, like ``does_styling``. Writing to
    it never would have done anything constructive.
  * Add ``fullscreen()`` and ``hidden_cursor()`` to the auto-generated docs.
  * Fall back to ``LINES`` and ``COLUMNS`` environment vars to find height and
    width. (jquast)
  * Support terminal types, such as kermit and avatar, that use bytes 127-255
    in their escape sequences. (jquast)

1.5.1
  * Clean up fabfile, removing the redundant ``test`` command.
  * Add Travis support.
  * Make ``python setup.py test`` work without spurious errors on 2.6.
  * Work around a tox parsing bug in its config file.
  * Make context managers clean up after themselves even if there's an
    exception. (Vitja Makarov)
  * Parametrizing a capability no longer crashes when there is no tty. (Vitja
    Makarov)

1.5
  * Add syntactic sugar and documentation for ``enter_fullscreen`` and
    ``exit_fullscreen``.
  * Add context managers ``fullscreen()`` and ``hidden_cursor()``.
  * Now you can force a ``Terminal`` never to emit styles by passing
    ``force_styling=None``.

1.4
  * Add syntactic sugar for cursor visibility control and single-space-movement
    capabilities.
  * Endorse the ``location()`` idiom for restoring cursor position after a
    series of manual movements.
  * Fix a bug in which ``location()`` wouldn't do anything when passed zeroes.
  * Allow tests to be run with ``python setup.py test``.

1.3
  * Added ``number_of_colors``, which tells you how many colors the terminal
    supports.
  * Made ``color(n)`` and ``on_color(n)`` callable to wrap a string, like the
    named colors can. Also, make them both fall back to the ``setf`` and
    ``setb`` capabilities (like the named colors do) if the ANSI ``setaf`` and
    ``setab`` aren't available.
  * Allowed ``color`` attr to act as an unparametrized string, not just a
    callable.
  * Made ``height`` and ``width`` examine any passed-in stream before falling
    back to stdout. (This rarely if ever affects actual behavior; it's mostly
    philosophical.)
  * Made caching simpler and slightly more efficient.
  * Got rid of a reference cycle between Terminals and FormattingStrings.
  * Updated docs to reflect that terminal addressing (as in ``location()``) is
    0-based.

1.2
  * Added support for Python 3! We need 3.2.3 or greater, because the curses
    library couldn't decide whether to accept strs or bytes before that
    (http://bugs.python.org/issue10570).
  * Everything that comes out of the library is now unicode. This lets us
    support Python 3 without making a mess of the code, and Python 2 should
    continue to work unless you were testing types (and badly). Please file a
    bug if this causes trouble for you.
  * Changed to the MIT License for better world domination.
  * Added Sphinx docs.

1.1
  * Added nicely named attributes for colors.
  * Introduced compound formatting.
  * Added wrapper behavior for styling and colors.
  * Let you force capabilities to be non-empty, even if the output stream is
    not a terminal.
  * Added the ``is_a_tty`` attribute for telling whether the output stream is a
    terminal.
  * Sugared the remaining interesting string capabilities.
  * Let ``location()`` operate on just an x *or* y coordinate.

1.0
  * Extracted Blessings from nose-progressive, my `progress-bar-having,
    traceback-shortcutting, rootin', tootin' testrunner`_. It provided the
    tootin' functionality.

.. _`progress-bar-having, traceback-shortcutting, rootin', tootin' testrunner`: http://pypi.python.org/pypi/nose-progressive/



  * [blinker-1.4](http://pythonhosted.org/blinker/) =======
Blinker
=======

Blinker provides a fast dispatching system that allows any number of
interested parties to subscribe to events, or "signals".

Signal receivers can subscribe to specific senders or receive signals
sent by any sender.

    >>> from blinker import signal
    >>> started = signal('round-started')
    >>> def each(round):
    ...     print "Round %s!" % round
    ...
    >>> started.connect(each)
    
    >>> def round_two(round):
    ...     print "This is round two."
    ...
    >>> started.connect(round_two, sender=2)
  
    >>> for round in range(1, 4):
    ...     started.send(round)
    ...
    Round 1!
    Round 2!
    This is round two.
    Round 3!

See the `Blinker documentation <https://pythonhosted.org/blinker/>`_ for more information.

Requirements
------------

Blinker requires Python 2.4 or higher, Python 3.0 or higher, or Jython 2.5 or higher.

Changelog Summary
-----------------

1.4 (July 23, 2015)

 - Verified Python 3.4 support (no changes needed)
 - Additional bookkeeping cleanup for non-ANY connections at disconnect
   time.
 - Added Signal._cleanup_bookeeping() to prune stale bookkeeping on
   demand

1.3 (July 3, 2013)

 - The global signal stash behind blinker.signal() is now backed by a
   regular name-to-Signal dictionary. Previously, weak references were
   held in the mapping and ephemeral usage in code like
   ``signal('foo').connect(...)`` could have surprising program behavior
   depending on import order of modules.
 - blinker.Namespace is now built on a regular dict. Use
   blinker.WeakNamespace for the older, weak-referencing behavior.
 - Signal.connect('text-sender') uses an alternate hashing strategy to
   avoid sharp edges in text identity.

1.2 (October 26, 2011)

 - Added Signal.receiver_connected and Signal.receiver_disconnected
   per-Signal signals.
 - Deprecated the global 'receiver_connected' signal.
 - Verified Python 3.2 support (no changes needed!)

1.1 (July 21, 2010)

 - Added ``@signal.connect_via(sender)`` decorator
 - Added ``signal.connected_to`` shorthand name for the
   ``temporarily_connected_to`` context manager.

1.0 (March 28, 2010)

 - Python 3.x compatibility

0.9 (February 26, 2010)

 - Sphinx docs, project website
 - Added ``with a_signal.temporarily_connected_to(receiver): ...`` support
  * [blist-1.3.6](http://stutzbachenterprises.com/blist/) blist: a list-like type with better performance
===============================================

The ``blist`` is a drop-in replacement for the Python list that provides
better performance when modifying large lists.  The blist package also
provides ``sortedlist``, ``sortedset``, ``weaksortedlist``,
``weaksortedset``, ``sorteddict``, and ``btuple`` types.

Full documentation is at the link below:

http://stutzbachenterprises.com/blist-doc/

Python's built-in list is a dynamically-sized array; to insert or
remove an item from the beginning or middle of the list, it has to
move most of the list in memory, i.e., O(n) operations.  The blist
uses a flexible, hybrid array/tree structure and only needs to move a
small portion of items in memory, specifically using O(log n)
operations.

For small lists, the blist and the built-in list have virtually
identical performance.

To use the blist, you simply change code like this:

>>> items = [5, 6, 2]
>>> more_items = function_that_returns_a_list()

to:

>>> from blist import blist
>>> items = blist([5, 6, 2])
>>> more_items = blist(function_that_returns_a_list())

Here are some of the use cases where the blist asymptotically
outperforms the built-in list:

========================================== ================  =========
Use Case                                   blist             list
========================================== ================  =========
Insertion into or removal from a list      O(log n)          O(n)
Taking slices of lists                     O(log n)          O(n)
Making shallow copies of lists             O(1)              O(n)
Changing slices of lists                   O(log n + log k)  O(n+k)
Multiplying a list to make a sparse list   O(log k)          O(kn)
Maintain a sorted lists with bisect.insort O(log**2 n)       O(n)
========================================== ================  =========

So you can see the performance of the blist in more detail, several
performance graphs available at the following link:
http://stutzbachenterprises.com/blist/

Example usage:

>>> from blist import *
>>> x = blist([0])             # x is a blist with one element
>>> x *= 2**29                 # x is a blist with > 500 million elements
>>> x.append(5)                # append to x
>>> y = x[4:-234234]           # Take a 500 million element slice from x
>>> del x[3:1024]              # Delete a few thousand elements from x

Other data structures
---------------------

The blist package provides other data structures based on the blist:

- sortedlist
- sortedset
- weaksortedlist
- weaksortedset
- sorteddict
- btuple

These additional data structures are only available in Python 2.6 or
higher, as they make use of Abstract Base Classes.

The sortedlist is a list that's always sorted.  It's iterable and
indexable like a Python list, but to modify a sortedlist the same
methods you would use on a Python set (add, discard, or remove).

>>> from blist import sortedlist
>>> my_list = sortedlist([3,7,2,1])
>>> my_list
sortedlist([1, 2, 3, 7])
>>> my_list.add(5)
>>> my_list[3]
5
>>>

The sortedlist constructor takes an optional "key" argument, which may
be used to change the sort order just like the sorted() function.

>>> from blist import sortedlist
>>> my_list = sortedlist([3,7,2,1], key=lambda i: -i)
sortedlist([7, 3, 2, 1]
>>>

The sortedset is a set that's always sorted.  It's iterable and
indexable like a Python list, but modified like a set.  Essentially,
it's just like a sortedlist except that duplicates are ignored.

>>> from blist import sortedset
>>> my_set = sortedset([3,7,2,2])
sortedset([2, 3, 7]
>>>

The weaksortedlist and weaksortedset are weakref variations of the
sortedlist and sortedset.

The sorteddict works just like a regular dict, except the keys are
always sorted.  The sorteddict should not be confused with Python
2.7's OrderedDict type, which remembers the insertion order of the
keys.

>>> from blist import sorteddict
>>> my_dict = sorteddict({1: 5, 6: 8, -5: 9})
>>> my_dict.keys()
[-5, 1, 6]
>>>

The btuple is a drop-in replacement for the built-in tuple.  Compared
to the built-in tuple, the btuple offers the following advantages:

- Constructing a btuple from a blist takes O(1) time.
- Taking a slice of a btuple takes O(n) time, where n is the size of
  the original tuple.  The size of the slice does not matter.

>>> from blist import blist, btuple
>>> x = blist([0])             # x is a blist with one element
>>> x *= 2**29                 # x is a blist with > 500 million elements
>>> y = btuple(x)              # y is a btuple with > 500 million elements

Installation instructions
-------------------------

Python 2.5 or higher is required.  If building from the source
distribution, the Python header files are also required.  In either
case, just run:

       python setup.py install

If you're running Linux and see a bunch of compilation errors from
GCC, you probably do not have the Python header files installed.
They're usually located in a package called something like
"python2.6-dev".

The blist package will be installed in the 'site-packages' directory of
your Python installation.  (Unless directed elsewhere; see the
"Installing Python Modules" section of the Python manuals for details
on customizing installation locations, etc.).

If you downloaded the source distribution and wish to run the
associated test suite, you can also run:

        python setup.py test

which will verify the correct installation and functioning of the
package.  The tests require Python 2.6 or higher.

Feedback
--------

We're eager to hear about your experiences with the blist.  You can
email me at daniel@stutzbachenterprises.com.  Alternately, bug reports
and feature requests may be reported on our bug tracker at:
http://github.com/DanielStutzbach/blist/issues

How we test
-----------

In addition to the tests include in the source distribution, we
perform the following to add extra rigor to our testing process:

    1. We use a "fuzzer": a program that randomly generates list
       operations, performs them using both the blist and the built-in
       list, and compares the results.

    2. We use a modified Python interpreter where we have replaced the
       array-based built-in list with the blist.  Then, we run all of
       the regular Python unit tests.
  * [blosc-1.8.1](http://github.com/blosc/python-blosc) python-blosc: a Python wrapper for the extremely fast Blosc compression library
===============================================================================

:Author: Francesc Alted
:Author: Valentin Haenel
:Contact: faltet@gmail.com
:Contact: valentin@haenel.co
:Github: https://github.com/Blosc/python-blosc
:URL: http://python-blosc.blosc.org
:Travis CI: |travis|
:Appveyor: |appveyor|
:PyPi: |version|
:Anaconda: |anaconda|
:Gitter: |gitter|

.. |travis| image:: https://travis-ci.org/Blosc/python-blosc.png?branch=master
        :target: https://travis-ci.org/Blosc/python-blosc
.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/dexdkko8omge6o3s/branch/master?svg=true
        :target: https://ci.appveyor.com/project/FrancescAlted/python-blosc/branch/master
.. |version| image:: https://img.shields.io/pypi/v/blosc.png
        :target: https://pypi.python.org/pypi/blosc
.. |anaconda| image:: https://anaconda.org/conda-forge/python-blosc/badges/version.svg
        :target: https://anaconda.org/conda-forge/python-blosc
.. |gitter| image:: https://badges.gitter.im/Blosc/c-blosc.svg
        :target: https://gitter.im/Blosc/c-blosc



What it is
==========

Blosc (http://blosc.org) is a high performance compressor optimized for
binary data.  It has been designed to transmit data to the processor
cache faster than the traditional, non-compressed, direct memory fetch
approach via a memcpy() OS call.

Blosc works well for compressing numerical arrays that contains data
with relatively low entropy, like sparse data, time series, grids with
regular-spaced values, etc.

python-blosc a Python package that wraps Blosc.  python-blosc supports
Python 2.7 and 3.4 or higher versions.


Code of Conduct
===============

The Blosc community has adopted a Code of Conduct that we expect project
participants to adhere to.  Please read the `full text <CODE_OF_CONDUCT.md>`_
so that you can understand what actions will and will not be tolerated.

Installing
==========


You can install binary packages with ``conda``:

.. code-block:: console

    $ conda install -c conda-forge python-blosc

Or, install it as a typical Python source package (requires c-compiler and
Python headers) from PyPi using ``pip``:

.. code-block:: console

    $ pip install blosc


Documentation
=============

The Sphinx based documentation is here:

http://python-blosc.blosc.org

Also, some examples are available on python-blosc wiki page:

http://github.com/blosc/python-blosc/wiki

Lastly, here is the `recording
<https://www.youtube.com/watch?v=rilU44j_wUU&list=PLNkWzv63CorW83NY3U93gUar645jTXpJF&index=15>`_
and the `slides
<http://www.blosc.org/docs/haenel-ep14-compress-me-stupid.pdf>`_ from the talk
"Compress me stupid" at the EuroPython 2014.

Building
========

If you need more control, there are different ways to compile python-blosc,
depending if you want to link with an already installed Blosc library or not.


Installing via setuptools
-------------------------

`python-blosc` comes with the Blosc sources with it and can be built with:

.. code-block:: console

    $ python setup.py build_clib
    $ python setup.py build_ext --inplace

Any codec can be enabled (`=1`) or disabled (`=0`) on this build-path with the appropriate
OS environment variables `INCLUDE_LZ4`, `INCLUDE_SNAPPY`, `INCLUDE_ZLIB`, and
`INCLUDE_ZLIB`. By default all the codecs in Blosc are enabled except Snappy
(due to some issues with C++ with the `gcc` toolchain).

Compiler specific optimisations are automatically enabled by inspecting
the CPU flags building Blosc. They can be manually disabled by setting
the following environmental variables: `DISABLE_BLOSC_SSE2` and
`DISABLE_BLOSC_AVX2`.

`setuptools` is limited to using the compiler specified in the environment
variable `CC` which on posix systems is usually `gcc`. This often causes
trouble with the Snappy codec, which is written in C++, and as a result Snappy
is no longer compiled by default. This problem is not known to affect MSVC or
clang. Snappy is considered optional in Blosc as its compression performance
is below that of the other codecs.

That's all. You can proceed with testing section now.


Compiling with an installed Blosc library
-----------------------------------------

This approach uses pre-built, fully optimized versions of Blosc built via
CMake.

Go to https://github.com/Blosc/c-blosc/releases and download and install
the C-Blosc library.  Then, you can tell python-blosc where is the
C-Blosc library in a couple of ways:

Using an environment variable:

.. code-block:: console

    $ BLOSC_DIR=/usr/local     (or "set BLOSC_DIR=\blosc" on Win)
    $ export BLOSC_DIR         (not needed on Win)
    $ python setup.py build_clib
    $ python setup.py build_ext --inplace

Using a flag:

.. code-block:: console

    $ python setup.py build_clib
    $ python setup.py build_ext --inplace --blosc=/usr/local


Testing
=======

After compiling, you can quickly check that the package is sane by
running the doctests in ``blosc/test.py``:

.. code-block:: console

    $ PYTHONPATH=.   (or "set PYTHONPATH=." on Win)
    $ export PYTHONPATH=.  (not needed on Win)
    $ python blosc/test.py  (add -v for verbose mode)

Or alternatively, you can use the third-party ``nosetests`` script:

.. code-block:: console

    $ nosetests --with-doctest (add -v for verbose mode)

Once installed, you can re-run the tests at any time with:

.. code-block:: console

    $ python -c "import blosc; blosc.test()"

Benchmarking
============

If curious, you may want to run a small benchmark that compares a plain
NumPy array copy against compression through different compressors in
your Blosc build:

.. code-block:: console

  $ PYTHONPATH=. python bench/compress_ptr.py

Just to whet your appetite, here are the results for an Intel Xeon
E5-2695 v3 @ 2.30GHz, running Python 3.5, CentOS 7, but YMMV (and
will vary!)::

  -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
  python-blosc version: 1.5.1.dev0
  Blosc version: 1.11.2 ($Date:: 2017-01-27 #$)
  Compressors available: ['blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib', 'zstd']
  Compressor library versions:
    BloscLZ: 1.0.5
    LZ4: 1.7.5
    Snappy: 1.1.1
    Zlib: 1.2.7
    Zstd: 1.1.2
  Python version: 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06)
  [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
  Platform: Linux-3.10.0-327.18.2.el7.x86_64-x86_64 (#1 SMP Thu May 12 11:03:55 UTC 2016)
  Linux dist: CentOS Linux 7.2.1511
  Processor: x86_64
  Byte-ordering: little
  Detected cores: 56
  Number of threads to use by default: 4
    -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
  Creating NumPy arrays with 10**8 int64/float64 elements:
    *** ctypes.memmove() *** Time for memcpy():	0.276 s	(2.70 GB/s)

  Times for compressing/decompressing with clevel=5 and 24 threads

  *** the arange linear distribution ***
    *** blosclz , noshuffle  ***  0.382 s (1.95 GB/s) / 0.300 s (2.48 GB/s)	Compr. ratio:   1.0x
    *** blosclz , shuffle    ***  0.042 s (17.77 GB/s) / 0.027 s (27.18 GB/s)	Compr. ratio:  57.1x
    *** blosclz , bitshuffle ***  0.094 s (7.94 GB/s) / 0.041 s (18.28 GB/s)	Compr. ratio:  74.0x
    *** lz4     , noshuffle  ***  0.156 s (4.79 GB/s) / 0.052 s (14.30 GB/s)	Compr. ratio:   2.0x
    *** lz4     , shuffle    ***  0.033 s (22.58 GB/s) / 0.034 s (22.03 GB/s)	Compr. ratio:  68.6x
    *** lz4     , bitshuffle ***  0.059 s (12.63 GB/s) / 0.053 s (14.18 GB/s)	Compr. ratio:  33.1x
    *** lz4hc   , noshuffle  ***  0.443 s (1.68 GB/s) / 0.070 s (10.62 GB/s)	Compr. ratio:   2.0x
    *** lz4hc   , shuffle    ***  0.102 s (7.31 GB/s) / 0.029 s (25.42 GB/s)	Compr. ratio:  97.5x
    *** lz4hc   , bitshuffle ***  0.206 s (3.62 GB/s) / 0.038 s (19.85 GB/s)	Compr. ratio: 180.5x
    *** snappy  , noshuffle  ***  0.154 s (4.84 GB/s) / 0.056 s (13.28 GB/s)	Compr. ratio:   2.0x
    *** snappy  , shuffle    ***  0.044 s (16.89 GB/s) / 0.047 s (15.95 GB/s)	Compr. ratio:  17.4x
    *** snappy  , bitshuffle ***  0.064 s (11.58 GB/s) / 0.061 s (12.26 GB/s)	Compr. ratio:  18.2x
    *** zlib    , noshuffle  ***  1.172 s (0.64 GB/s) / 0.135 s (5.50 GB/s)	Compr. ratio:   5.3x
    *** zlib    , shuffle    ***  0.260 s (2.86 GB/s) / 0.086 s (8.67 GB/s)	Compr. ratio: 120.8x
    *** zlib    , bitshuffle ***  0.262 s (2.84 GB/s) / 0.094 s (7.96 GB/s)	Compr. ratio: 260.1x
    *** zstd    , noshuffle  ***  0.973 s (0.77 GB/s) / 0.093 s (8.00 GB/s)	Compr. ratio:   7.8x
    *** zstd    , shuffle    ***  0.093 s (7.97 GB/s) / 0.023 s (32.71 GB/s)	Compr. ratio: 156.7x
    *** zstd    , bitshuffle ***  0.115 s (6.46 GB/s) / 0.029 s (25.60 GB/s)	Compr. ratio: 320.6x

  *** the linspace linear distribution ***
    *** blosclz , noshuffle  ***  0.341 s (2.19 GB/s) / 0.291 s (2.56 GB/s)	Compr. ratio:   1.0x
    *** blosclz , shuffle    ***  0.132 s (5.65 GB/s) / 0.023 s (33.10 GB/s)	Compr. ratio:   2.0x
    *** blosclz , bitshuffle ***  0.166 s (4.50 GB/s) / 0.036 s (20.89 GB/s)	Compr. ratio:   2.8x
    *** lz4     , noshuffle  ***  0.142 s (5.26 GB/s) / 0.028 s (27.07 GB/s)	Compr. ratio:   1.0x
    *** lz4     , shuffle    ***  0.093 s (8.01 GB/s) / 0.030 s (24.87 GB/s)	Compr. ratio:   3.4x
    *** lz4     , bitshuffle ***  0.102 s (7.31 GB/s) / 0.039 s (19.13 GB/s)	Compr. ratio:   5.3x
    *** lz4hc   , noshuffle  ***  0.700 s (1.06 GB/s) / 0.044 s (16.77 GB/s)	Compr. ratio:   1.1x
    *** lz4hc   , shuffle    ***  0.203 s (3.67 GB/s) / 0.021 s (36.22 GB/s)	Compr. ratio:   8.6x
    *** lz4hc   , bitshuffle ***  0.342 s (2.18 GB/s) / 0.028 s (26.50 GB/s)	Compr. ratio:  14.2x
    *** snappy  , noshuffle  ***  0.271 s (2.75 GB/s) / 0.274 s (2.72 GB/s)	Compr. ratio:   1.0x
    *** snappy  , shuffle    ***  0.099 s (7.54 GB/s) / 0.042 s (17.55 GB/s)	Compr. ratio:   4.2x
    *** snappy  , bitshuffle ***  0.127 s (5.86 GB/s) / 0.043 s (17.20 GB/s)	Compr. ratio:   6.1x
    *** zlib    , noshuffle  ***  1.525 s (0.49 GB/s) / 0.158 s (4.70 GB/s)	Compr. ratio:   1.6x
    *** zlib    , shuffle    ***  0.346 s (2.15 GB/s) / 0.098 s (7.59 GB/s)	Compr. ratio:  10.7x
    *** zlib    , bitshuffle ***  0.420 s (1.78 GB/s) / 0.104 s (7.20 GB/s)	Compr. ratio:  18.0x
    *** zstd    , noshuffle  ***  1.061 s (0.70 GB/s) / 0.096 s (7.79 GB/s)	Compr. ratio:   1.9x
    *** zstd    , shuffle    ***  0.203 s (3.68 GB/s) / 0.052 s (14.21 GB/s)	Compr. ratio:  14.2x
    *** zstd    , bitshuffle ***  0.251 s (2.97 GB/s) / 0.047 s (15.84 GB/s)	Compr. ratio:  22.2x

  *** the random distribution ***
    *** blosclz , noshuffle  ***  0.340 s (2.19 GB/s) / 0.285 s (2.61 GB/s)	Compr. ratio:   1.0x
    *** blosclz , shuffle    ***  0.091 s (8.21 GB/s) / 0.017 s (44.29 GB/s)	Compr. ratio:   3.9x
    *** blosclz , bitshuffle ***  0.080 s (9.27 GB/s) / 0.029 s (26.12 GB/s)	Compr. ratio:   6.1x
    *** lz4     , noshuffle  ***  0.150 s (4.95 GB/s) / 0.027 s (28.05 GB/s)	Compr. ratio:   2.4x
    *** lz4     , shuffle    ***  0.068 s (11.02 GB/s) / 0.029 s (26.03 GB/s)	Compr. ratio:   4.5x
    *** lz4     , bitshuffle ***  0.063 s (11.87 GB/s) / 0.054 s (13.70 GB/s)	Compr. ratio:   6.2x
    *** lz4hc   , noshuffle  ***  0.645 s (1.15 GB/s) / 0.019 s (39.22 GB/s)	Compr. ratio:   3.5x
    *** lz4hc   , shuffle    ***  0.257 s (2.90 GB/s) / 0.022 s (34.62 GB/s)	Compr. ratio:   5.1x
    *** lz4hc   , bitshuffle ***  0.128 s (5.80 GB/s) / 0.029 s (25.52 GB/s)	Compr. ratio:   6.2x
    *** snappy  , noshuffle  ***  0.164 s (4.54 GB/s) / 0.048 s (15.46 GB/s)	Compr. ratio:   2.2x
    *** snappy  , shuffle    ***  0.082 s (9.09 GB/s) / 0.043 s (17.39 GB/s)	Compr. ratio:   4.3x
    *** snappy  , bitshuffle ***  0.071 s (10.48 GB/s) / 0.046 s (16.08 GB/s)	Compr. ratio:   5.0x
    *** zlib    , noshuffle  ***  1.223 s (0.61 GB/s) / 0.093 s (7.97 GB/s)	Compr. ratio:   4.0x
    *** zlib    , shuffle    ***  0.636 s (1.17 GB/s) / 0.126 s (5.89 GB/s)	Compr. ratio:   5.5x
    *** zlib    , bitshuffle ***  0.327 s (2.28 GB/s) / 0.109 s (6.81 GB/s)	Compr. ratio:   6.2x
    *** zstd    , noshuffle  ***  1.432 s (0.52 GB/s) / 0.103 s (7.27 GB/s)	Compr. ratio:   4.2x
    *** zstd    , shuffle    ***  0.388 s (1.92 GB/s) / 0.031 s (23.71 GB/s)	Compr. ratio:   5.9x
    *** zstd    , bitshuffle ***  0.127 s (5.86 GB/s) / 0.033 s (22.77 GB/s)	Compr. ratio:   6.4x


Also, Blosc works quite well on ARM processors (even without NEON support yet)::

    -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
    python-blosc version: 1.4.4
    Blosc version: 1.11.2 ($Date:: 2017-01-27 #$)
    Compressors available: ['blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib', 'zstd']
    Compressor library versions:
      BloscLZ: 1.0.5
      LZ4: 1.7.5
      Snappy: 1.1.1
      Zlib: 1.2.8
      Zstd: 1.1.2
    Python version: 3.6.0 (default, Dec 31 2016, 21:20:16)
    [GCC 4.9.2]
    Platform: Linux-3.4.113-sun8i-armv7l (#50 SMP PREEMPT Mon Nov 14 08:41:55 CET 2016)
    Linux dist: debian 9.0
    Processor: not recognized
    Byte-ordering: little
    Detected cores: 4
    Number of threads to use by default: 4
    -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
      *** ctypes.memmove() *** Time for memcpy():   0.015 s (93.57 MB/s)

    Times for compressing/decompressing with clevel=5 and 4 threads

    *** user input ***
      *** blosclz , noshuffle  ***  0.015 s (89.93 MB/s) / 0.010 s (138.32 MB/s)    Compr. ratio:   2.7x
      *** blosclz , shuffle    ***  0.023 s (60.25 MB/s) / 0.012 s (112.71 MB/s)    Compr. ratio:   2.3x
      *** blosclz , bitshuffle ***  0.018 s (77.63 MB/s) / 0.021 s (66.76 MB/s)     Compr. ratio:   7.3x
      *** lz4     , noshuffle  ***  0.008 s (177.14 MB/s) / 0.009 s (159.00 MB/s)   Compr. ratio:   3.6x
      *** lz4     , shuffle    ***  0.010 s (131.29 MB/s) / 0.012 s (117.69 MB/s)   Compr. ratio:   3.5x
      *** lz4     , bitshuffle ***  0.015 s (89.97 MB/s) / 0.022 s (63.62 MB/s)     Compr. ratio:   8.4x
      *** lz4hc   , noshuffle  ***  0.071 s (19.30 MB/s) / 0.007 s (186.64 MB/s)    Compr. ratio:   8.6x
      *** lz4hc   , shuffle    ***  0.079 s (17.30 MB/s) / 0.014 s (95.99 MB/s)     Compr. ratio:   6.2x
      *** lz4hc   , bitshuffle ***  0.062 s (22.23 MB/s) / 0.027 s (51.53 MB/s)     Compr. ratio:   9.7x
      *** snappy  , noshuffle  ***  0.008 s (173.87 MB/s) / 0.009 s (148.77 MB/s)   Compr. ratio:   4.4x
      *** snappy  , shuffle    ***  0.011 s (123.22 MB/s) / 0.016 s (85.16 MB/s)    Compr. ratio:   4.4x
      *** snappy  , bitshuffle ***  0.015 s (89.02 MB/s) / 0.021 s (64.87 MB/s)     Compr. ratio:   6.2x
      *** zlib    , noshuffle  ***  0.047 s (29.26 MB/s) / 0.011 s (121.83 MB/s)    Compr. ratio:  14.7x
      *** zlib    , shuffle    ***  0.080 s (17.20 MB/s) / 0.022 s (63.61 MB/s)     Compr. ratio:   9.4x
      *** zlib    , bitshuffle ***  0.059 s (23.50 MB/s) / 0.033 s (41.10 MB/s)     Compr. ratio:  10.5x
      *** zstd    , noshuffle  ***  0.113 s (12.21 MB/s) / 0.011 s (124.64 MB/s)    Compr. ratio:  15.6x
      *** zstd    , shuffle    ***  0.154 s (8.92 MB/s) / 0.026 s (52.56 MB/s)      Compr. ratio:   9.9x
      *** zstd    , bitshuffle ***  0.116 s (11.86 MB/s) / 0.036 s (38.40 MB/s)     Compr. ratio:  11.4x

For details on the ARM benchmark see: https://github.com/Blosc/python-blosc/issues/105

In case you find your own results interesting, please report them back
to the authors!

License
=======

The software is licenses under a 3-Clause BSD licsense. A copy of the
python-blosc license can be found in `LICENSE.txt <LICENSE.txt>`_. A copy of all licenses can be
found in `LICENSES/ <LICENSES/>`_.

Mailing list
============

Discussion about this module is welcome in the Blosc list:

blosc@googlegroups.com

http://groups.google.es/group/blosc

----

  **Enjoy data!**


.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 72
.. End:

  * [bokeh-1.3.2](http://github.com/bokeh/bokeh) <a href="https://bokeh.org">
  <img src="https://static.bokeh.org/logos/logotype.svg" height="60" width="150" alt="Bokeh logotype" />
</a>

*Bokeh is a fiscally sponsored project of [NumFOCUS](https://numfocus.org), a nonprofit dedicated to supporting the open-source scientific computing community. If you like Bokeh and would like to support our mission, please consider [making a donation](https://numfocus.salsalabs.org/donate-to-bokeh/index.html).*

<table>

<tr>
  <td>Latest Release</td>
  <td>
    <img src="https://badge.fury.io/gh/bokeh%2Fbokeh.svg" alt="Latest release version" />
    <a href="https://badge.fury.io/js/bokehjs">
      <img src="https://badge.fury.io/js/bokehjs.svg" alt="npm version">
    </a>
  </td>

  <td>Conda</td>
  <td>
    <a href="https://bokeh.pydata.org/en/latest/docs/installation.html">
    <img src="https://pyviz.org/_static/cache/bokeh_conda_downloads_badge.svg"
         alt="Conda downloads per month" />
    </a>
  </td>
</tr>

<tr>
  <td>License</td>
  <td>
    <a href="https://github.com/bokeh/bokeh/blob/master/LICENSE.txt">
    <img src="https://img.shields.io/github/license/bokeh/bokeh.svg"
         alt="Bokeh license (BSD 3-clause)" />
    </a>
  </td>

  <td>PyPI</td>
  <td>
    <a href="https://bokeh.pydata.org/en/latest/docs/installation.html">
    <img src="https://img.shields.io/pypi/dm/bokeh.svg"
         alt="PyPI downloads per month" />
    </a>
  </td>
</tr>

<tr>
  <td>Sponsorship</td>
  <td>
    <a href="http://numfocus.org">
    <img src="https://img.shields.io/badge/powered%20by-NumFOCUS-black.svg?style=flat&colorA=5B5B5B&colorB=007D8A"
         alt="Powered by NumFOCUS" />
    </a>
  </td>

  <td>Live Tutorial</td>
  <td>
    <a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb">
    <img src="https://mybinder.org/badge.svg"
         alt="Live Bokeh tutorial notebooks on MyBinder" />
    </a>
  </td>
</tr>

<tr>
  <td>Build Status</td>
  <td>
    <a href="https://travis-ci.org/bokeh/bokeh">
    <img src="https://travis-ci.org/bokeh/bokeh.svg?branch=master"
         alt="Current TravisCI build status" />
    </a>
    <a href="https://ci.appveyor.com/project/bokeh-integrations/bokeh">
    <img src="https://ci.appveyor.com/api/projects/status/u4idf25dhp219mho?svg=true"
         alt="Current Appveyor build status" />
    </a>
  </td>

  <td>Support</td>
  <td>
    <a href="https://discourse.bokeh.org">
    <img src="https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg"
         alt="Community Support on discourse.bokeh.org" />
    </a>
  </td>
</tr>

<tr>
  <td>Static Analysis</td>
  <td>
    <a href="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master">
    <img src="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master"
         alt="BetterCodeHub static analysis" >
    </a>
  </td>

  <td>Twitter</td>
  <td>
    <a href="https://twitter.com/BokehPlots">
    <img src="https://img.shields.io/twitter/follow/bokehplots.svg?style=social&label=Follow"
         alt="Follow BokehPlots on Twitter" />
    </a>
  </td>
</tr>

</table>

Bokeh is an interactive visualization library for Python that enables beautiful
and meaningful visual presentation of data in modern web browsers. With Bokeh,
you can quickly and easily create interactive plots, dashboards, and data
applications.

Bokeh provides an elegant and concise way to construct versatile graphics while
delivering **high-performance** interactivity for large or streamed datasets.

[Interactive gallery](https://bokeh.pydata.org/en/latest/docs/gallery.html)
---------------------------------------------------------------------------

<p>
<table cellspacing="20">
<tr>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/image.html">
  <img alt="colormapped image plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/image_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/anscombe.html">
  <img alt="anscombe plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/anscombe_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/stocks.html">
  <img alt="stocks plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/stocks_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/lorenz.html">
  <img alt="lorenz attractor plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/lorenz_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/candlestick.html">
  <img alt="candlestick plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/candlestick_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/color_scatter.html">
  <img alt="scatter plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/scatter_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/iris_splom.html">
  <img alt="SPLOM plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/splom_t.png" />
  </a>
  </td>

</tr>
<tr>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/iris.html">
  <img alt="iris dataset plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/iris_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/histogram.html">
  <img alt="histogram plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/histogram_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/periodic.html">
  <img alt="periodic table plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/periodic_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/texas.html">
  <img alt="choropleth plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/choropleth_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/burtin.html">
  <img alt="burtin antibiotic data plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/burtin_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/streamline.html">
  <img alt="streamline plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/streamline_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/image_rgba.html">
  <img alt="RGBA image plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/image_rgba_t.png" />
  </a>
  </td>

</tr>
<tr>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/brewer.html">
  <img alt="stacked bars plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/stacked_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/quiver.html">
  <img alt="quiver plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/quiver_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/elements.html">
  <img alt="elements data plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/elements_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/boxplot.html">
  <img alt="boxplot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/boxplot_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/categorical.html">
  <img alt="categorical plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/categorical_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/unemployment.html">
  <img alt="unemployment data plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/unemployment_t.png" />
  </a>
  </td>

  <td>
  <a href="https://bokeh.pydata.org/en/latest/docs/gallery/les_mis.html">
  <img alt="Les Mis co-occurrence plot thumbnail" src="https://bokeh.pydata.org/en/latest/_images/les_mis_t.png" />
  </a>
  </td>

</tr>
</table>
</p>

Installation
------------
The easiest way to install Bokeh is using the [Anaconda Python distribution](https://www.anaconda.com/what-is-anaconda/) and its included *Conda* package management system. To install Bokeh and its required dependencies, enter the following command at a Bash or Windows command prompt:

```
conda install bokeh
```

To install using pip, enter the following command at a Bash or Windows command prompt:
```
pip install bokeh
```
For more information, refer to the [installation documentation](https://bokeh.pydata.org/en/latest/docs/user_guide/quickstart.html#quick-installation).

Once Bokeh is installed, check out the [Getting Started](https://bokeh.pydata.org/en/latest/docs/user_guide/quickstart.html#getting-started) section of the [Quickstart guide](https://bokeh.pydata.org/en/latest/docs/user_guide/quickstart.html).

Documentation
-------------
Visit the [Bokeh Front Page](https://bokeh.org) for information and [full documentation](https://bokeh.pydata.org), or [launch the Bokeh tutorial](https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb) to learn about Bokeh in live Jupyter Notebooks.

Contribute to Bokeh
-------------------
If you would like to contribute to Bokeh, please review the [Developer Guide](https://bokeh.pydata.org/en/latest/docs/dev_guide.html) and say hello on the [`bokeh-dev` chat channel](https://gitter.im/bokeh/bokeh-dev).

Follow us
---------
Follow us on Twitter [@bokehplots](https://twitter.com/BokehPlots)

<p align="center">
  <a href="https://www.numfocus.org/">
  <img src="https://github.com/bokeh/bokeh/blob/master/sphinx/source/_images/NumFocus_2C_CMYK.svg"
       alt="NumFocus Logo" width="400"/>
  </a>
</p>
  * [boltons-19.1.0](https://github.com/mahmoud/boltons) Functionality that should be in the standard library. Like
builtins, but Boltons.

Otherwise known as, "everyone's util.py," but cleaned up and
tested.

Contains over 230 BSD-licensed utility types and functions that can be
used as a package or independently. `Extensively documented on Read
the Docs <http://boltons.readthedocs.org>`_.

  * [boto3-1.9.201](https://github.com/boto/boto3) ===============================
Boto 3 - The AWS SDK for Python
===============================

|Build Status| |Version| |Gitter|

Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for
Python, which allows Python developers to write software that makes use
of services like Amazon S3 and Amazon EC2. You can find the latest, most
up to date, documentation at our `doc site`_, including a list of
services that are supported.


.. _boto: https://docs.pythonboto.org/
.. _`doc site`: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html
.. |Build Status| image:: http://img.shields.io/travis/boto/boto3/develop.svg?style=flat
    :target: https://travis-ci.org/boto/boto3
    :alt: Build Status
.. |Gitter| image:: https://badges.gitter.im/boto/boto3.svg
   :target: https://gitter.im/boto/boto3
   :alt: Gitter
.. |Downloads| image:: http://img.shields.io/pypi/dm/boto3.svg?style=flat
    :target: https://pypi.python.org/pypi/boto3/
    :alt: Downloads
.. |Version| image:: http://img.shields.io/pypi/v/boto3.svg?style=flat
    :target: https://pypi.python.org/pypi/boto3/
    :alt: Version
.. |License| image:: http://img.shields.io/pypi/l/boto3.svg?style=flat
    :target: https://github.com/boto/boto3/blob/develop/LICENSE
    :alt: License

Quick Start
-----------
First, install the library and set a default region:

.. code-block:: sh

    $ pip install boto3

Next, set up credentials (in e.g. ``~/.aws/credentials``):

.. code-block:: ini

    [default]
    aws_access_key_id = YOUR_KEY
    aws_secret_access_key = YOUR_SECRET

Then, set up a default region (in e.g. ``~/.aws/config``):

.. code-block:: ini

    [default]
    region=us-east-1

Then, from a Python interpreter:

.. code-block:: python

    >>> import boto3
    >>> s3 = boto3.resource('s3')
    >>> for bucket in s3.buckets.all():
            print(bucket.name)

Development
-----------

Getting Started
~~~~~~~~~~~~~~~
Assuming that you have Python and ``virtualenv`` installed, set up your
environment and install the required dependencies like this instead of
the ``pip install boto3`` defined above:

.. code-block:: sh

    $ git clone https://github.com/boto/boto3.git
    $ cd boto3
    $ virtualenv venv
    ...
    $ . venv/bin/activate
    $ pip install -r requirements.txt
    $ pip install -e .

Running Tests
~~~~~~~~~~~~~
You can run tests in all supported Python versions using ``tox``. By default,
it will run all of the unit and functional tests, but you can also specify your own
``nosetests`` options. Note that this requires that you have all supported
versions of Python installed, otherwise you must pass ``-e`` or run the
``nosetests`` command directly:

.. code-block:: sh

    $ tox
    $ tox -- unit/test_session.py
    $ tox -e py26,py33 -- integration/

You can also run individual tests with your default Python version:

.. code-block:: sh

    $ nosetests tests/unit

Generating Documentation
~~~~~~~~~~~~~~~~~~~~~~~~
Sphinx is used for documentation. You can generate HTML locally with the
following:

.. code-block:: sh

    $ pip install -r requirements-docs.txt
    $ cd docs
    $ make html


Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__
* Come join the AWS Python community chat on `gitter <https://gitter.im/boto/boto3>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/boto3/issues/new>`__
  * [botocore-1.12.201](https://github.com/boto/botocore) botocore
========

.. image:: https://secure.travis-ci.org/boto/botocore.png?branch=develop
   :target: http://travis-ci.org/boto/botocore

.. image:: https://codecov.io/github/boto/botocore/coverage.svg?branch=develop
    :target: https://codecov.io/github/boto/botocore?branch=develop


A low-level interface to a growing number of Amazon Web Services. The
botocore package is the foundation for the
`AWS CLI <https://github.com/aws/aws-cli>`__ as well as
`boto3 <https://github.com/boto/boto3>`__.


Documentation
-------------
Documentation for ``botocore`` can be found  `here <https://botocore.amazonaws.com/v1/documentation/api/latest/index.html>`__.


Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help. Please note many of the same resources available for ``boto3`` are
applicable for ``botocore``:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__
* Come join the AWS Python community chat on `gitter <https://gitter.im/boto/boto3>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/botocore/issues/new>`__
  * [bpython-0.18](http://www.bpython-interpreter.org/) bpython is a fancy interface to the Python
interpreter for Unix-like operating systems.


  * [brotlipy-0.7.0](https://github.com/python-hyper/brotlipy/) brotlipy
========

This library contains Python bindings for the reference Brotli encoder/decoder,
`available here`_. This allows Python software to use the Brotli compression
algorithm directly from Python code.

To use it simply, try this:

.. code-block:: python

    import brotli
    data = brotli.decompress(compressed_data)

More information can be found `in the documentation`_.

.. _available here: https://github.com/google/brotli
.. _in the documentation: https://brotlipy.readthedocs.org

License
-------

The source code of brotlipy is available under the MIT license. Brotli itself
is made available under the Version 2.0 of the Apache Software License. See the
LICENSE and libbrotli/LICENSE files for more information.

Authors
-------

brotlipy is maintained by Cory Benfield.


Changelog
=========

0.7.0 (2017-05-30)
------------------

- Update to v0.6.0 of the Brotli library.

0.6.0 (2016-09-08)
------------------

- Resolved a bug where ``decompress()`` would return an empty bytestring
  instead of erroring if the provided bytestring was small enough.
- Added the ``finish()`` method to the streaming decompressor.

0.5.1 (2016-08-17)
------------------

- Update to v0.5.2 of the Brotli library.
- Add new exception type (``Error``).
- Add compatiblity with C++ brotli library by aliasing ``Error`` to ``error``.
- Extra error checking of input parameters to the compressor.

0.5.0 (2016-08-16)
------------------

- Update to v0.5.0 of the Brotli library.
- Extend one-shot compression API to include all control parameters.
- Added streaming/incremental compression API.
- Added flags to control compression mode.

0.4.0 (2016-08-01)
------------------

Update to v0.4.0 of the Brotli library.

0.3.0 (2016-05-11)
------------------

Update to v0.3.0 of the Brotli library.

0.2.0 (2015-10-05)
------------------

Fix broken ``brotli.compress`` support on Windows.

0.1.3 (2015-10-05)
------------------

- Added basic for ``brotli.compress`` through a C wrapper included in this
  library.
  * [burrito-0.9.1](https://github.com/biocore/burrito) burrito
=======

|Build Status| |Coverage Status|

burrito, canonically pronounced *boar-eee-toe*, is a Python framework for
wrapping and controlling command-line applications.

What's with the name?
---------------------

This tool allows developers to wrap command-line applications, just as burritos
wrap delicious foods. Both hide the potentially unsightly details.

Installation
------------

To install burrito::

    pip install burrito

Running the tests
-----------------

To run burrito's unit tests::

    nosetests

The pre-history of burrito
--------------------------

burrito is derived from the `application controller framework <http://pycogent.org/examples/application_controller_framework.html>`__
code, which was originally added to `PyCogent <http://www.pycogent.org>`__ and
later moved to `scikit-bio <http://scikit-bio.org>`__. The contributors and/or
copyright holders have agreed to make the code they wrote for PyCogent
available under the BSD license. The original authors of the application
controller framework code in PyCogent are Greg Caporaso
(`@gregcaporaso <https://github.com/gregcaporaso>`__), Sandra Smit,
Micah Hamady, and Rob Knight (`@rob-knight <https://github.com/rob-knight>`__).

.. |Build Status| image:: https://travis-ci.org/biocore/burrito.svg?branch=master
   :target: https://travis-ci.org/biocore/burrito
.. |Coverage Status| image:: https://coveralls.io/repos/biocore/burrito/badge.png
   :target: https://coveralls.io/r/biocore/burrito
  * [burrito-fillings-0.1.1](https://github.com/biocore/burrito-fillings) The burrito-fillings project
  * [bz2file-0.98](https://github.com/nvawda/bz2file) Bz2file is a Python library for reading and writing bzip2-compressed files.

It contains a drop-in replacement for the file interface in the standard
library's ``bz2`` module, including features from the latest development
version of CPython that are not available in older releases.

Bz2file is compatible with CPython 2.6, 2.7, and 3.0 through 3.4, as well as
PyPy 2.0.


Features
--------

- Supports multi-stream files.

- Can read from or write to any file-like object.

- Can open files in either text or binary mode.

- Added methods: ``peek()``, ``read1()``, ``readinto()``, ``fileno()``,
  ``readable()``, ``writable()``, ``seekable()``.


Installation
------------

To install bz2file, run: ::

   $ pip install bz2file


Documentation
-------------

The ``open()`` function and ``BZ2File`` class in this module provide the same
features and interface as the ones in the standard library's ``bz2`` module in
the current development version of CPython, `documented here
<http://docs.python.org/dev/library/bz2.html>`_.


Version History
---------------

0.98: 19 January 2014

- Added support for the 'x' family of modes.
- Ignore non-bz2 data at the end of a file, rather than raising an exception.
- Tests now pass on PyPy.

0.95: 08 October 2012

- Added the ``open()`` function.
- Improved performance when reading in small chunks.
- Removed the ``fileobj`` argument to ``BZ2File()``. To wrap an existing file
  object, pass it as the first argument (``filename``).

0.9: 04 February 2012

- Initial release.
  * [cached-property-1.5.1](https://github.com/pydanny/cached-property) ===============================
cached-property
===============================

.. image:: https://img.shields.io/pypi/v/cached-property.svg
    :target: https://pypi.python.org/pypi/cached-property

.. image:: https://img.shields.io/travis/pydanny/cached-property/master.svg
    :target: https://travis-ci.org/pydanny/cached-property

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black
    :alt: Code style: black        


A decorator for caching properties in classes.

Why?
-----

* Makes caching of time or computational expensive properties quick and easy.
* Because I got tired of copy/pasting this code from non-web project to non-web project.
* I needed something really simple that worked in Python 2 and 3.

How to use it
--------------

Let's define a class with an expensive property. Every time you stay there the
price goes up by $50!

.. code-block:: python

    class Monopoly(object):

        def __init__(self):
            self.boardwalk_price = 500

        @property
        def boardwalk(self):
            # In reality, this might represent a database call or time
            # intensive task like calling a third-party API.
            self.boardwalk_price += 50
            return self.boardwalk_price

Now run it:

.. code-block:: python

    >>> monopoly = Monopoly()
    >>> monopoly.boardwalk
    550
    >>> monopoly.boardwalk
    600

Let's convert the boardwalk property into a ``cached_property``.

.. code-block:: python

    from cached_property import cached_property

    class Monopoly(object):

        def __init__(self):
            self.boardwalk_price = 500

        @cached_property
        def boardwalk(self):
            # Again, this is a silly example. Don't worry about it, this is
            #   just an example for clarity.
            self.boardwalk_price += 50
            return self.boardwalk_price

Now when we run it the price stays at $550.

.. code-block:: python

    >>> monopoly = Monopoly()
    >>> monopoly.boardwalk
    550
    >>> monopoly.boardwalk
    550
    >>> monopoly.boardwalk
    550

Why doesn't the value of ``monopoly.boardwalk`` change? Because it's a **cached property**!

Invalidating the Cache
----------------------

Results of cached functions can be invalidated by outside forces. Let's demonstrate how to force the cache to invalidate:

.. code-block:: python

    >>> monopoly = Monopoly()
    >>> monopoly.boardwalk
    550
    >>> monopoly.boardwalk
    550
    >>> # invalidate the cache
    >>> del monopoly.__dict__['boardwalk']
    >>> # request the boardwalk property again
    >>> monopoly.boardwalk
    600
    >>> monopoly.boardwalk
    600

Working with Threads
---------------------

What if a whole bunch of people want to stay at Boardwalk all at once? This means using threads, which
unfortunately causes problems with the standard ``cached_property``. In this case, switch to using the
``threaded_cached_property``:

.. code-block:: python

    from cached_property import threaded_cached_property

    class Monopoly(object):

        def __init__(self):
            self.boardwalk_price = 500

        @threaded_cached_property
        def boardwalk(self):
            """threaded_cached_property is really nice for when no one waits
                for other people to finish their turn and rudely start rolling
                dice and moving their pieces."""

            sleep(1)
            self.boardwalk_price += 50
            return self.boardwalk_price

Now use it:

.. code-block:: python

    >>> from threading import Thread
    >>> from monopoly import Monopoly
    >>> monopoly = Monopoly()
    >>> threads = []
    >>> for x in range(10):
    >>>     thread = Thread(target=lambda: monopoly.boardwalk)
    >>>     thread.start()
    >>>     threads.append(thread)

    >>> for thread in threads:
    >>>     thread.join()

    >>> self.assertEqual(m.boardwalk, 550)


Working with async/await (Python 3.5+)
--------------------------------------

The cached property can be async, in which case you have to use await
as usual to get the value. Because of the caching, the value is only
computed once and then cached:

.. code-block:: python

    from cached_property import cached_property

    class Monopoly(object):

        def __init__(self):
            self.boardwalk_price = 500

        @cached_property
        async def boardwalk(self):
            self.boardwalk_price += 50
            return self.boardwalk_price

Now use it:

.. code-block:: python

    >>> async def print_boardwalk():
    ...     monopoly = Monopoly()
    ...     print(await monopoly.boardwalk)
    ...     print(await monopoly.boardwalk)
    ...     print(await monopoly.boardwalk)
    >>> import asyncio
    >>> asyncio.get_event_loop().run_until_complete(print_boardwalk())
    550
    550
    550

Note that this does not work with threading either, most asyncio
objects are not thread-safe. And if you run separate event loops in
each thread, the cached version will most likely have the wrong event
loop. To summarize, either use cooperative multitasking (event loop)
or threading, but not both at the same time.


Timing out the cache
--------------------

Sometimes you want the price of things to reset after a time. Use the ``ttl``
versions of ``cached_property`` and ``threaded_cached_property``.

.. code-block:: python

    import random
    from cached_property import cached_property_with_ttl

    class Monopoly(object):

        @cached_property_with_ttl(ttl=5) # cache invalidates after 5 seconds
        def dice(self):
            # I dare the reader to implement a game using this method of 'rolling dice'.
            return random.randint(2,12)

Now use it:

.. code-block:: python

    >>> monopoly = Monopoly()
    >>> monopoly.dice
    10
    >>> monopoly.dice
    10
    >>> from time import sleep
    >>> sleep(6) # Sleeps long enough to expire the cache
    >>> monopoly.dice
    3
    >>> monopoly.dice
    3

**Note:** The ``ttl`` tools do not reliably allow the clearing of the cache. This
is why they are broken out into seperate tools. See https://github.com/pydanny/cached-property/issues/16.

Credits
--------

* Pip, Django, Werkzueg, Bottle, Pyramid, and Zope for having their own implementations. This package originally used an implementation that matched the Bottle version.
* Reinout Van Rees for pointing out the `cached_property` decorator to me.
* My awesome wife `@audreyr`_ who created `cookiecutter`_, which meant rolling this out took me just 15 minutes.
* @tinche for pointing out the threading issue and providing a solution.
* @bcho for providing the time-to-expire feature

.. _`@audreyr`: https://github.com/audreyr
.. _`cookiecutter`: https://github.com/audreyr/cookiecutter

Support This Project
---------------------------

This project is maintained by volunteers. Support their efforts by spreading the word about:

.. image:: https://cdn.shopify.com/s/files/1/0304/6901/t/2/assets/logo.png?8399580890922549623
   :name: Two Scoops Press
   :align: center
   :alt: Two Scoops Press
   :target: https://www.twoscoopspress.com




History
-------

1.5.1 (2018-08-05)
++++++++++++++++++

* Added formal support for Python 3.7
* Removed formal support for Python 3.3

1.4.3  (2018-06-14)
+++++++++++++++++++

* Catch SyntaxError from asyncio import on older versions of Python, thanks to @asottile

1.4.2 (2018-04-08)
++++++++++++++++++

* Really fixed tests, thanks to @pydanny

1.4.1 (2018-04-08)
++++++++++++++++++

* Added conftest.py to manifest so tests work properly off the tarball, thanks to @dotlambda
* Ensured new asyncio tests didn't break Python 2.7 builds on Debian, thanks to @pydanny
* Code formatting via black, thanks to @pydanny and @ambv


1.4.0 (2018-02-25)
++++++++++++++++++

* Added asyncio support, thanks to @vbraun
* Remove Python 2.6 support, whose end of life was 5 years ago, thanks to @pydanny


1.3.1 (2017-09-21)
++++++++++++++++++

* Validate for Python 3.6


1.3.0 (2015-11-24)
++++++++++++++++++

* Drop some non-ASCII characters from HISTORY.rst, thanks to @AdamWill
* Added official support for Python 3.5, thanks to @pydanny and @audreyr
* Removed confusingly placed lock from example, thanks to @ionelmc
* Corrected invalidation cache documentation, thanks to @proofit404
* Updated to latest Travis-CI environment, thanks to @audreyr

1.2.0 (2015-04-28)
++++++++++++++++++

* Overall code and test refactoring, thanks to @gsakkis
* Allow the del statement for resetting cached properties with ttl instead of del obj._cache[attr], thanks to @gsakkis.
* Uncovered a bug in PyPy, https://bitbucket.org/pypy/pypy/issue/2033/attributeerror-object-attribute-is-read, thanks to @gsakkis
* Fixed threaded_cached_property_with_ttl to actually be thread-safe, thanks to @gsakkis

1.1.0 (2015-04-04)
++++++++++++++++++

* Regression: As the cache was not always clearing, we've broken out the time to expire feature to its own set of specific tools, thanks to @pydanny
* Fixed typo in README, thanks to @zoidbergwill

1.0.0 (2015-02-13)
++++++++++++++++++

* Added timed to expire feature to ``cached_property`` decorator.
* **Backwards incompatiblity**: Changed ``del monopoly.boardwalk`` to ``del monopoly['boardwalk']`` in order to support the new TTL feature.

0.1.5 (2014-05-20)
++++++++++++++++++

* Added threading support with new ``threaded_cached_property`` decorator
* Documented cache invalidation
* Updated credits
* Sourced the bottle implementation

0.1.4 (2014-05-17)
++++++++++++++++++

* Fix the dang-blarged py_modules argument.

0.1.3 (2014-05-17)
++++++++++++++++++

* Removed import of package into ``setup.py``

0.1.2 (2014-05-17)
++++++++++++++++++

* Documentation fixes. Not opening up a RTFD instance for this because it's so simple to use.

0.1.1 (2014-05-17)
++++++++++++++++++

* setup.py fix. Whoops!

0.1.0 (2014-05-17)
++++++++++++++++++

* First release on PyPI.



  * [capturer-2.4](https://capturer.readthedocs.io) capturer: Easily capture stdout/stderr of the current process and subprocesses
==============================================================================

.. image:: https://travis-ci.org/xolox/python-capturer.svg?branch=master
   :target: https://travis-ci.org/xolox/python-capturer

.. image:: https://coveralls.io/repos/xolox/python-capturer/badge.svg?branch=master
  :target: https://coveralls.io/r/xolox/python-capturer?branch=master

The `capturer` package makes it easy to capture the stdout_ and stderr_ streams
of the current process *and subprocesses*. Output can be relayed to the
terminal in real time but is also available to the Python program for
additional processing. It's currently tested on cPython 2.6, 2.7, 3.4, 3.5, 3.6
and PyPy (2.7). It's tested on Linux and Mac OS X and may work on other unixes
but definitely won't work on Windows (due to the use of the platform dependent
pty_ module). For usage instructions please refer to the documentation_.

.. contents::
   :local:

Status
------

The `capturer` package was developed as a proof of concept over the course of a
weekend, because I was curious to see if it could be done (reliably). After a
weekend of extensive testing it seems to work fairly well so I'm publishing the
initial release as version 1.0, however I still consider this a proof of
concept because I don't have extensive "production" experience using it yet.
Here's hoping it works as well in practice as it did during my testing :-).

Installation
------------

The `capturer` package is available on PyPI_ which means installation should be
as simple as:

.. code-block:: sh

   $ pip install capturer

There's actually a multitude of ways to install Python packages (e.g. the `per
user site-packages directory`_, `virtual environments`_ or just installing
system wide) and I have no intention of getting into that discussion here, so
if this intimidates you then read up on your options before returning to these
instructions ;-).

Getting started
---------------

The easiest way to capture output is to use a context manager:

.. code-block:: python

   import subprocess
   from capturer import CaptureOutput

   with CaptureOutput() as capturer:
       # Generate some output from Python.
       print "Output from Python"
       # Generate output from a subprocess.
       subprocess.call(["echo", "Output from a subprocess"])
       # Get the output in each of the supported formats.
       assert capturer.get_bytes() == b'Output from Python\r\nOutput from a subprocess\r\n'
       assert capturer.get_lines() == [u'Output from Python', u'Output from a subprocess']
       assert capturer.get_text() == u'Output from Python\nOutput from a subprocess'

The use of a context manager (`the with statement`_) ensures that output
capturing is enabled and disabled at the appropriate time, regardless of
whether exceptions interrupt the normal flow of processing.

Note that the first call to `get_bytes()`_, `get_lines()`_ or `get_text()`_
will stop the capturing of output by default. This is intended as a sane
default to prevent partial reads (which can be confusing as hell when you don't
have experience with them). So we could have simply used ``print`` to show
the results without causing a recursive "captured output is printed and then
captured again" loop. There's an optional ``partial=True`` keyword argument
that can be used to disable this behavior (please refer to the documentation_
for details).

Design choices
--------------

There are existing solutions out there to capture the stdout_ and stderr_
streams of (Python) processes. The `capturer` package was created for a very
specific use case that wasn't catered for by existing solutions (that I could
find). This section documents the design choices that guided the development of
the `capturer` package:

.. contents::
  :local:

Intercepts writes to low level file descriptors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Libraries like capture_ and iocapture_ change Python's sys.stdout_ and
sys.stderr_ file objects to fake file objects (using StringIO_). This enables
capturing of (most) output written to the stdout_ and stderr_ streams from the
same Python process, however any output from subprocesses is unaffected by the
redirection and not captured.

The `capturer` package instead intercepts writes to low level file descriptors
(similar to and inspired by `how pytest does it`_). This enables capturing of
output written to the standard output and error streams from the same Python
process as well as any subprocesses.

Uses a pseudo terminal to emulate a real terminal
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The `capturer` package uses a pseudo terminal created using `pty.openpty()`_ to
capture output. This means subprocesses will use ANSI escape sequences because
they think they're connected to a terminal. In the current implementation you
can't opt out of this, but feel free to submit a feature request to change this
:-). This does have some drawbacks:

- The use of `pty.openpty()`_ means you need to be running in a UNIX like
  environment for `capturer` to work (Windows definitely isn't supported).

- All output captured is relayed on the stderr_ stream by default, so capturing
  changes the semantics of your programs. How much this matters obviously
  depends on your use case. For the use cases that triggered me to create
  `capturer` it doesn't matter, which explains why this is the default mode.

  There is experimental support for capturing stdout_ and stderr_ separately
  and relaying captured output to the appropriate original stream. Basically
  you call ``CaptureOutput(merged=False)`` and then you use the ``stdout`` and
  ``stderr`` attributes of the ``CaptureOutput`` object to get at the output
  captured on each stream.

  I say experimental because this method of capturing can unintentionally
  change the order in which captured output is emitted, in order to avoid
  interleaving output emitted on the stdout_ and stderr_ streams (which would
  most likely result in incomprehensible output). Basically output is relayed
  on each stream separately after each line break. This means interactive
  prompts that block on reading from standard input without emitting a line
  break won't show up (until it's too late ;-).

Relays output to the terminal in real time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The main use case of `capturer` is to capture all output of a snippet of Python
code (including any output by subprocesses) but also relay the output to the
terminal in real time. This has a couple of useful properties:

- Long running operations can provide the operator with real time feedback by
  emitting output on the terminal. This sounds obvious (and it is!) but it is
  non-trivial to implement (an understatement :-) when you *also* want to
  capture the output.

- Programs like gpg_ and ssh_ that use interactive password prompts will render
  their password prompt on the terminal in real time. This avoids the awkward
  interaction where a password prompt is silenced but the program still hangs,
  waiting for input on stdin_.

Contact
-------

The latest version of `capturer` is available on PyPI_ and GitHub_. The
documentation is hosted on `Read the Docs`_. For bug reports please create an
issue on GitHub_. If you have questions, suggestions, etc. feel free to send me
an e-mail at `peter@peterodding.com`_.

License
-------

This software is licensed under the `MIT license`_.

© 2017 Peter Odding.

A big thanks goes out to the pytest_ developers because pytest's mechanism for
capturing the output of subprocesses provided inspiration for the `capturer`
package. No code was copied, but both projects are MIT licensed anyway, so it's
not like it's very relevant :-).

.. External references:
.. _capture: https://pypi.python.org/pypi/capture
.. _documentation: https://capturer.readthedocs.io
.. _get_bytes(): https://capturer.readthedocs.io/en/latest/#capturer.CaptureOutput.get_bytes
.. _get_lines(): https://capturer.readthedocs.io/en/latest/#capturer.CaptureOutput.get_lines
.. _get_text(): https://capturer.readthedocs.io/en/latest/#capturer.CaptureOutput.get_text
.. _GitHub: https://github.com/xolox/python-capturer
.. _gpg: https://en.wikipedia.org/wiki/GNU_Privacy_Guard
.. _how pytest does it: https://pytest.org/latest/capture.html
.. _iocapture: https://pypi.python.org/pypi/iocapture
.. _MIT license: http://en.wikipedia.org/wiki/MIT_License
.. _per user site-packages directory: https://www.python.org/dev/peps/pep-0370/
.. _peter@peterodding.com: peter@peterodding.com
.. _pty.openpty(): https://docs.python.org/2/library/pty.html#pty.openpty
.. _pty: https://docs.python.org/2/library/pty.html
.. _PyPI: https://pypi.python.org/pypi/capturer
.. _pytest: https://pypi.python.org/pypi/pytest
.. _Read the Docs: https://capturer.readthedocs.io
.. _ssh: https://en.wikipedia.org/wiki/Secure_Shell
.. _stderr: https://en.wikipedia.org/wiki/Standard_streams#Standard_error_.28stderr.29
.. _stdin: https://en.wikipedia.org/wiki/Standard_streams#Standard_input_.28stdin.29
.. _stdout: https://en.wikipedia.org/wiki/Standard_streams#Standard_output_.28stdout.29
.. _StringIO: https://docs.python.org/2/library/stringio.html
.. _sys.stderr: https://docs.python.org/2/library/sys.html#sys.stderr
.. _sys.stdout: https://docs.python.org/2/library/sys.html#sys.stdout
.. _the with statement: https://docs.python.org/2/reference/compound_stmts.html#the-with-statement
.. _virtual environments: http://docs.python-guide.org/en/latest/dev/virtualenvs/



  * [cassandra-driver-3.18.0](http://github.com/datastax/python-driver) DataStax Python Driver for Apache Cassandra
===========================================

.. image:: https://travis-ci.org/datastax/python-driver.png?branch=master
   :target: https://travis-ci.org/datastax/python-driver

A modern, `feature-rich <https://github.com/datastax/python-driver#features>`_ and highly-tunable Python client library for Apache Cassandra (2.1+) using exclusively Cassandra's binary protocol and Cassandra Query Language v3.

The driver supports Python 2.7, 3.4, 3.5, 3.6 and 3.7.

If you require compatibility with DataStax Enterprise, use the `DataStax Enterprise Python Driver <http://docs.datastax.com/en/developer/python-dse-driver/>`_.

**Note:** DataStax products do not support big-endian systems.

Feedback Requested
------------------
**Help us focus our efforts!** Provide your input on the `Platform and Runtime Survey <https://docs.google.com/a/datastax.com/forms/d/10wkbKLqmqs91gvhFW5u43y60pg_geZDolVNrxfO5_48/viewform>`_ (we kept it short).

Features
--------
* `Synchronous <http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Session.execute>`_ and `Asynchronous <http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Session.execute_async>`_ APIs
* `Simple, Prepared, and Batch statements <http://datastax.github.io/python-driver/api/cassandra/query.html#cassandra.query.Statement>`_
* Asynchronous IO, parallel execution, request pipelining
* `Connection pooling <http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.get_core_connections_per_host>`_
* Automatic node discovery
* `Automatic reconnection <http://datastax.github.io/python-driver/api/cassandra/policies.html#reconnecting-to-dead-hosts>`_
* Configurable `load balancing <http://datastax.github.io/python-driver/api/cassandra/policies.html#load-balancing>`_ and `retry policies <http://datastax.github.io/python-driver/api/cassandra/policies.html#retrying-failed-operations>`_
* `Concurrent execution utilities <http://datastax.github.io/python-driver/api/cassandra/concurrent.html>`_
* `Object mapper <http://datastax.github.io/python-driver/object_mapper.html>`_

Installation
------------
Installation through pip is recommended::

    $ pip install cassandra-driver

For more complete installation instructions, see the
`installation guide <http://datastax.github.io/python-driver/installation.html>`_.

Documentation
-------------
The documentation can be found online `here <http://datastax.github.io/python-driver/index.html>`_.

A couple of links for getting up to speed:

* `Installation <http://datastax.github.io/python-driver/installation.html>`_
* `Getting started guide <http://datastax.github.io/python-driver/getting_started.html>`_
* `API docs <http://datastax.github.io/python-driver/api/index.html>`_
* `Performance tips <http://datastax.github.io/python-driver/performance.html>`_

Object Mapper
-------------
cqlengine (originally developed by Blake Eggleston and Jon Haddad, with contributions from the
community) is now maintained as an integral part of this package. Refer to
`documentation here <http://datastax.github.io/python-driver/object_mapper.html>`_.

Contributing
------------
See `CONTRIBUTING.md <https://github.com/datastax/python-driver/blob/master/CONTRIBUTING.rst>`_.

Reporting Problems
------------------
Please report any bugs and make any feature requests on the
`JIRA <https://datastax-oss.atlassian.net/browse/PYTHON>`_ issue tracker.

If you would like to contribute, please feel free to open a pull request.

Getting Help
------------
Your best options for getting help with the driver are the
`mailing list <https://groups.google.com/a/lists.datastax.com/forum/#!forum/python-driver-user>`_
and the ``#datastax-drivers`` channel in the `DataStax Academy Slack <https://academy.datastax.com/slack>`_.

License
-------
Copyright DataStax, Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

  * [cchardet-2.1.4](https://github.com/PyYoshi/cChardet) cChardet
========

cChardet is high speed universal character encoding detector. - binding to `uchardet`_.

.. image:: https://badge.fury.io/py/cchardet.svg
   :target: https://badge.fury.io/py/cchardet
   :alt: PyPI version
.. image:: https://travis-ci.org/PyYoshi/cChardet.svg?branch=master
   :target: https://travis-ci.org/PyYoshi/cChardet
   :alt: Travis Ci build status
.. image:: https://ci.appveyor.com/api/projects/status/lwkc4rgf3gncb1ne/branch/master?svg=true
   :target: https://ci.appveyor.com/project/PyYoshi/cchardet/branch/master
   :alt: AppVeyor build status

Supported Languages/Encodings
-----------------------------

-  International (Unicode)

   -  UTF-8
   -  UTF-16BE / UTF-16LE
   -  UTF-32BE / UTF-32LE / X-ISO-10646-UCS-4-34121 /
      X-ISO-10646-UCS-4-21431

-  Arabic

   -  ISO-8859-6
   -  WINDOWS-1256

-  Bulgarian

   -  ISO-8859-5
   -  WINDOWS-1251

-  Chinese

   -  ISO-2022-CN
   -  BIG5
   -  EUC-TW
   -  GB18030
   -  HZ-GB-2312

-  Croatian:

   -  ISO-8859-2
   -  ISO-8859-13
   -  ISO-8859-16
   -  Windows-1250
   -  IBM852
   -  MAC-CENTRALEUROPE

-  Czech

   -  Windows-1250
   -  ISO-8859-2
   -  IBM852
   -  MAC-CENTRALEUROPE

-  Danish

   -  ISO-8859-1
   -  ISO-8859-15
   -  WINDOWS-1252

-  English

   -  ASCII

-  Esperanto

   -  ISO-8859-3

-  Estonian

   -  ISO-8859-4
   -  ISO-8859-13
   -  ISO-8859-13
   -  Windows-1252
   -  Windows-1257

-  Finnish

   -  ISO-8859-1
   -  ISO-8859-4
   -  ISO-8859-9
   -  ISO-8859-13
   -  ISO-8859-15
   -  WINDOWS-1252

-  French

   -  ISO-8859-1
   -  ISO-8859-15
   -  WINDOWS-1252

-  German

   -  ISO-8859-1
   -  WINDOWS-1252

-  Greek

   -  ISO-8859-7
   -  WINDOWS-1253

-  Hebrew

   -  ISO-8859-8
   -  WINDOWS-1255

-  Hungarian:

   -  ISO-8859-2
   -  WINDOWS-1250

-  Irish Gaelic

   -  ISO-8859-1
   -  ISO-8859-9
   -  ISO-8859-15
   -  WINDOWS-1252

-  Italian

   -  ISO-8859-1
   -  ISO-8859-3
   -  ISO-8859-9
   -  ISO-8859-15
   -  WINDOWS-1252

-  Japanese

   -  ISO-2022-JP
   -  SHIFT\_JIS
   -  EUC-JP

-  Korean

   -  ISO-2022-KR
   -  EUC-KR / UHC

-  Lithuanian

   -  ISO-8859-4
   -  ISO-8859-10
   -  ISO-8859-13

-  Latvian

   -  ISO-8859-4
   -  ISO-8859-10
   -  ISO-8859-13

-  Maltese

   -  ISO-8859-3

-  Polish:

   -  ISO-8859-2
   -  ISO-8859-13
   -  ISO-8859-16
   -  Windows-1250
   -  IBM852
   -  MAC-CENTRALEUROPE

-  Portuguese

   -  ISO-8859-1
   -  ISO-8859-9
   -  ISO-8859-15
   -  WINDOWS-1252

-  Romanian:

   -  ISO-8859-2
   -  ISO-8859-16
   -  Windows-1250
   -  IBM852

-  Russian

   -  ISO-8859-5
   -  KOI8-R
   -  WINDOWS-1251
   -  MAC-CYRILLIC
   -  IBM866
   -  IBM855

-  Slovak

   -  Windows-1250
   -  ISO-8859-2
   -  IBM852
   -  MAC-CENTRALEUROPE

-  Slovene

   -  ISO-8859-2
   -  ISO-8859-16
   -  Windows-1250
   -  IBM852
   -  M

Example
-------

.. code-block:: python

    # -*- coding: utf-8 -*-
    import cchardet as chardet
    with open(r"src/tests/samples/wikipediaJa_One_Thousand_and_One_Nights_SJIS.txt", "rb") as f:
        msg = f.read()
        result = chardet.detect(msg)
        print(result)

Benchmark
---------

.. code-block:: bash

    $ cd src/
    $ pip install chardet
    $ python tests/bench.py


Results
~~~~~~~

CPU: Intel(R) Core(TM) i5-4690 CPU @ 3.50GHz

RAM: DDR3 1600Mhz 16GB

Platform: Ubuntu 16.04 amd64

Python 2.7.13
^^^^^^^^^^^^^

+-----------------+------------------+
|                 | Request (call/s) |
+=================+==================+
| chardet v3.0.2  |       0.36       |
+-----------------+------------------+
| cchardet v2.0.1 |     1396.42      |
+-----------------+------------------+

Python 3.6.1
^^^^^^^^^^^^

+-----------------+------------------+
|                 | Request (call/s) |
+=================+==================+
| chardet v3.0.2  |       0.35       |
+-----------------+------------------+
| cchardet v2.0.1 |     1467.77      |
+-----------------+------------------+


LICENSE
-------

See **COPYING** file.

Contact
-------

- `Issues`_


.. _uchardet: https://github.com/PyYoshi/uchardet
.. _Issues: https://github.com/PyYoshi/cChardet/issues?page=1&state=open

CHANGES
=======

2.1.4 (2018-09-26)
------------------

- disable LTO because become poor performance

2.1.3 (2018-09-26)
------------------

- support Python 3.7

2.1.2 (2018-09-26)
------------------

- enable `LTO`_ for wheel builds
- update Cython

.. _LTO: https://gcc.gnu.org/wiki/LinkTimeOptimization

2.1.1 (2017-07-01)
------------------

- fix that different results with different chuck sizes
- fix that assignments to nsSMState in nsCodingStateMachine result in unspecified behavior
- include COPYING in package

2.1.0 (2017-05-15)
------------------

- add cchardetect CLI script (`#30`_) `@craigds`_

.. _#30: https://github.com/PyYoshi/cChardet/pull/30
.. _@craigds: https://github.com/craigds

2.0.1 (2017-04-25)
------------------

- fix an issue where UTF-8 with a BOM would not be detected as UTF-8-SIG (fix `#28`_)
- pass NULL Byte to feed() / detect() (fix `#27`_)

.. _#28: https://github.com/PyYoshi/cChardet/issues/28
.. _#27: https://github.com/PyYoshi/cChardet/issues/27

2.0.0 (2017-04-06)
------------------

- Improve tests

2.0a4 (2017-04-05)
------------------

- Update uchardet repo (Fix buffer overflow)

2.0a3 (2017-03-29)
------------------

- Implement UniversalDetector (like chardet)

2.0a2 (2017-03-28)
------------------

- Update uchardet repo (Fix memory leak)

2.0a1 (2017-03-28)
------------------

- Replace `uchardet-enhanced`_ to `uchardet`_
- Remove Detector class

.. _uchardet-enhanced: https://bitbucket.org/medoc/uchardet-enhanced/overview
.. _uchardet: https://github.com/PyYoshi/uchardet

1.1.3 (2017-02-26)
------------------

- Support AArch64

1.1.2 (2017-01-08)
------------------

- Support Python 3.6

1.1.1 (2016-11-05)
------------------

- Use len() function (9e61cb9e96b138b0d18e5f9e013e144202ae4067)

- Remove detect function in _cchardet.pyx (25b581294fc0ae8f686ac9972c8549666766f695)

- Support manylinux1 wheel

1.1.0 (2016-10-17)
------------------

- Add Detector class

- Improve unit tests


  * [celery-4.3.0](http://celeryproject.org) .. image:: http://docs.celeryproject.org/en/latest/_images/celery-banner-small.png

|build-status| |coverage| |license| |wheel| |pyversion| |pyimp| |ocbackerbadge| |ocsponsorbadge|

:Version: 4.3.0 (rhubarb)
:Web: http://celeryproject.org/
:Download: https://pypi.org/project/celery/
:Source: https://github.com/celery/celery/
:Keywords: task, queue, job, async, rabbitmq, amqp, redis,
  python, distributed, actors

Donations
=========

This project relies on your generous donations.

If you are using Celery to create a commercial product, please consider becoming our `backer`_ or our `sponsor`_ to ensure Celery's future.

.. _`backer`: https://opencollective.com/celery#backer
.. _`sponsor`: https://opencollective.com/celery#sponsor


Sponsors
--------

`Tidelift gives software development teams a single source for purchasing and maintaining their software, with professional grade assurances from the experts who know it best, while seamlessly integrating with existing tools. <https://tidelift.com/subscription/pkg/pypi-celery?utm_source=pypi-celery&utm_medium=referral&utm_campaign=readme>`_


What's a Task Queue?
====================

Task queues are used as a mechanism to distribute work across threads or
machines.

A task queue's input is a unit of work, called a task, dedicated worker
processes then constantly monitor the queue for new work to perform.

Celery communicates via messages, usually using a broker
to mediate between clients and workers. To initiate a task a client puts a
message on the queue, the broker then delivers the message to a worker.

A Celery system can consist of multiple workers and brokers, giving way
to high availability and horizontal scaling.

Celery is written in Python, but the protocol can be implemented in any
language. In addition to Python there's node-celery_ for Node.js,
and a `PHP client`_.

Language interoperability can also be achieved by using webhooks
in such a way that the client enqueues an URL to be requested by a worker.

.. _node-celery: https://github.com/mher/node-celery
.. _`PHP client`: https://github.com/gjedeer/celery-php

What do I need?
===============

Celery version 4.3 runs on,

- Python (2.7, 3.4, 3.5, 3.6, 3.7)
- PyPy2.7 (6.0)
- PyPy3.5 (6.0)


This is the last version to support Python 2.7,
and from the next version (Celery 5.x) Python 3.5 or newer is required.

If you're running an older version of Python, you need to be running
an older version of Celery:

- Python 2.6: Celery series 3.1 or earlier.
- Python 2.5: Celery series 3.0 or earlier.
- Python 2.4 was Celery series 2.2 or earlier.

Celery is a project with minimal funding,
so we don't support Microsoft Windows.
Please don't open any issues related to that platform.

*Celery* is usually used with a message broker to send and receive messages.
The RabbitMQ, Redis transports are feature complete,
but there's also experimental support for a myriad of other solutions, including
using SQLite for local development.

*Celery* can run on a single machine, on multiple machines, or even
across datacenters.

Get Started
===========

If this is the first time you're trying to use Celery, or you're
new to Celery 4.2 coming from previous versions then you should read our
getting started tutorials:

- `First steps with Celery`_

    Tutorial teaching you the bare minimum needed to get started with Celery.

- `Next steps`_

    A more complete overview, showing more features.

.. _`First steps with Celery`:
    http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html

.. _`Next steps`:
    http://docs.celeryproject.org/en/latest/getting-started/next-steps.html

Celery is...
=============

- **Simple**

    Celery is easy to use and maintain, and does *not need configuration files*.

    It has an active, friendly community you can talk to for support,
    like at our `mailing-list`_, or the IRC channel.

    Here's one of the simplest applications you can make::

        from celery import Celery

        app = Celery('hello', broker='amqp://guest@localhost//')

        @app.task
        def hello():
            return 'hello world'

- **Highly Available**

    Workers and clients will automatically retry in the event
    of connection loss or failure, and some brokers support
    HA in way of *Primary/Primary* or *Primary/Replica* replication.

- **Fast**

    A single Celery process can process millions of tasks a minute,
    with sub-millisecond round-trip latency (using RabbitMQ,
    py-librabbitmq, and optimized settings).

- **Flexible**

    Almost every part of *Celery* can be extended or used on its own,
    Custom pool implementations, serializers, compression schemes, logging,
    schedulers, consumers, producers, broker transports, and much more.

It supports...
================

    - **Message Transports**

        - RabbitMQ_, Redis_, Amazon SQS

    - **Concurrency**

        - Prefork, Eventlet_, gevent_, single threaded (``solo``)

    - **Result Stores**

        - AMQP, Redis
        - memcached
        - SQLAlchemy, Django ORM
        - Apache Cassandra, IronCache, Elasticsearch

    - **Serialization**

        - *pickle*, *json*, *yaml*, *msgpack*.
        - *zlib*, *bzip2* compression.
        - Cryptographic message signing.

.. _`Eventlet`: http://eventlet.net/
.. _`gevent`: http://gevent.org/

.. _RabbitMQ: https://rabbitmq.com
.. _Redis: https://redis.io
.. _SQLAlchemy: http://sqlalchemy.org

Framework Integration
=====================

Celery is easy to integrate with web frameworks, some of which even have
integration packages:

    +--------------------+------------------------+
    | `Django`_          | not needed             |
    +--------------------+------------------------+
    | `Pyramid`_         | `pyramid_celery`_      |
    +--------------------+------------------------+
    | `Pylons`_          | `celery-pylons`_       |
    +--------------------+------------------------+
    | `Flask`_           | not needed             |
    +--------------------+------------------------+
    | `web2py`_          | `web2py-celery`_       |
    +--------------------+------------------------+
    | `Tornado`_         | `tornado-celery`_      |
    +--------------------+------------------------+

The integration packages aren't strictly necessary, but they can make
development easier, and sometimes they add important hooks like closing
database connections at ``fork``.

.. _`Django`: https://djangoproject.com/
.. _`Pylons`: http://pylonsproject.org/
.. _`Flask`: http://flask.pocoo.org/
.. _`web2py`: http://web2py.com/
.. _`Bottle`: https://bottlepy.org/
.. _`Pyramid`: http://docs.pylonsproject.org/en/latest/docs/pyramid.html
.. _`pyramid_celery`: https://pypi.org/project/pyramid_celery/
.. _`celery-pylons`: https://pypi.org/project/celery-pylons/
.. _`web2py-celery`: https://code.google.com/p/web2py-celery/
.. _`Tornado`: http://www.tornadoweb.org/
.. _`tornado-celery`: https://github.com/mher/tornado-celery/

.. _celery-documentation:

Documentation
=============

The `latest documentation`_ is hosted at Read The Docs, containing user guides,
tutorials, and an API reference.

.. _`latest documentation`: http://docs.celeryproject.org/en/latest/

.. _celery-installation:

Installation
============

You can install Celery either via the Python Package Index (PyPI)
or from source.

To install using ``pip``:

::


    $ pip install -U Celery

.. _bundles:

Bundles
-------

Celery also defines a group of bundles that can be used
to install Celery and the dependencies for a given feature.

You can specify these in your requirements or on the ``pip``
command-line by using brackets. Multiple bundles can be specified by
separating them by commas.

::


    $ pip install "celery[librabbitmq]"

    $ pip install "celery[librabbitmq,redis,auth,msgpack]"

The following bundles are available:

Serializers
~~~~~~~~~~~

:``celery[auth]``:
    for using the ``auth`` security serializer.

:``celery[msgpack]``:
    for using the msgpack serializer.

:``celery[yaml]``:
    for using the yaml serializer.

Concurrency
~~~~~~~~~~~

:``celery[eventlet]``:
    for using the ``eventlet`` pool.

:``celery[gevent]``:
    for using the ``gevent`` pool.

Transports and Backends
~~~~~~~~~~~~~~~~~~~~~~~

:``celery[librabbitmq]``:
    for using the librabbitmq C library.

:``celery[redis]``:
    for using Redis as a message transport or as a result backend.

:``celery[sqs]``:
    for using Amazon SQS as a message transport.

:``celery[tblib``]:
    for using the ``task_remote_tracebacks`` feature.

:``celery[memcache]``:
    for using Memcached as a result backend (using ``pylibmc``)

:``celery[pymemcache]``:
    for using Memcached as a result backend (pure-Python implementation).

:``celery[cassandra]``:
    for using Apache Cassandra as a result backend with DataStax driver.

:``celery[azureblockblob]``:
    for using Azure Storage as a result backend (using ``azure-storage``)

:``celery[s3]``:
    for using S3 Storage as a result backend.

:``celery[couchbase]``:
    for using Couchbase as a result backend.

:``celery[arangodb]``:
    for using ArangoDB as a result backend.

:``celery[elasticsearch]``:
    for using Elasticsearch as a result backend.

:``celery[riak]``:
    for using Riak as a result backend.

:``celery[cosmosdbsql]``:
    for using Azure Cosmos DB as a result backend (using ``pydocumentdb``)

:``celery[zookeeper]``:
    for using Zookeeper as a message transport.

:``celery[sqlalchemy]``:
    for using SQLAlchemy as a result backend (*supported*).

:``celery[pyro]``:
    for using the Pyro4 message transport (*experimental*).

:``celery[slmq]``:
    for using the SoftLayer Message Queue transport (*experimental*).

:``celery[consul]``:
    for using the Consul.io Key/Value store as a message transport or result backend (*experimental*).

:``celery[django]``:
    specifies the lowest version possible for Django support.

    You should probably not use this in your requirements, it's here
    for informational purposes only.


.. _celery-installing-from-source:

Downloading and installing from source
--------------------------------------

Download the latest version of Celery from PyPI:

https://pypi.org/project/celery/

You can install it by doing the following,:

::


    $ tar xvfz celery-0.0.0.tar.gz
    $ cd celery-0.0.0
    $ python setup.py build
    # python setup.py install

The last command must be executed as a privileged user if
you aren't currently using a virtualenv.

.. _celery-installing-from-git:

Using the development version
-----------------------------

With pip
~~~~~~~~

The Celery development version also requires the development
versions of ``kombu``, ``amqp``, ``billiard``, and ``vine``.

You can install the latest snapshot of these using the following
pip commands:

::


    $ pip install https://github.com/celery/celery/zipball/master#egg=celery
    $ pip install https://github.com/celery/billiard/zipball/master#egg=billiard
    $ pip install https://github.com/celery/py-amqp/zipball/master#egg=amqp
    $ pip install https://github.com/celery/kombu/zipball/master#egg=kombu
    $ pip install https://github.com/celery/vine/zipball/master#egg=vine

With git
~~~~~~~~

Please see the Contributing section.

.. _getting-help:

Getting Help
============

.. _mailing-list:

Mailing list
------------

For discussions about the usage, development, and future of Celery,
please join the `celery-users`_ mailing list.

.. _`celery-users`: https://groups.google.com/group/celery-users/

.. _irc-channel:

IRC
---

Come chat with us on IRC. The **#celery** channel is located at the `Freenode`_
network.

.. _`Freenode`: https://freenode.net

.. _bug-tracker:

Bug tracker
===========

If you have any suggestions, bug reports, or annoyances please report them
to our issue tracker at https://github.com/celery/celery/issues/

.. _wiki:

Wiki
====

https://wiki.github.com/celery/celery/

Credits
=======

.. _contributing-short:

Contributors
------------

This project exists thanks to all the people who contribute. Development of
`celery` happens at GitHub: https://github.com/celery/celery

You're highly encouraged to participate in the development
of `celery`. If you don't like GitHub (for some reason) you're welcome
to send regular patches.

Be sure to also read the `Contributing to Celery`_ section in the
documentation.

.. _`Contributing to Celery`:
    http://docs.celeryproject.org/en/master/contributing.html

|oc-contributors|

.. |oc-contributors| image:: https://opencollective.com/celery/contributors.svg?width=890&button=false
    :target: https://github.com/celery/celery/graphs/contributors

Backers
-------

Thank you to all our backers! 🙏 [`Become a backer`_]

.. _`Become a backer`: https://opencollective.com/celery#backer

|oc-backers|

.. |oc-backers| image:: https://opencollective.com/celery/backers.svg?width=890
    :target: https://opencollective.com/celery#backers

Sponsors
--------

Support this project by becoming a sponsor. Your logo will show up here with a
link to your website. [`Become a sponsor`_]

.. _`Become a sponsor`: https://opencollective.com/celery#sponsor

|oc-sponsors|

.. |oc-sponsors| image:: https://opencollective.com/celery/sponsor/0/avatar.svg
    :target: https://opencollective.com/celery/sponsor/0/website

.. _license:

License
=======

This software is licensed under the `New BSD License`. See the ``LICENSE``
file in the top distribution directory for the full license text.

.. # vim: syntax=rst expandtab tabstop=4 shiftwidth=4 shiftround

.. |build-status| image:: https://secure.travis-ci.org/celery/celery.png?branch=master
    :alt: Build status
    :target: https://travis-ci.org/celery/celery

.. |coverage| image:: https://codecov.io/github/celery/celery/coverage.svg?branch=master
    :target: https://codecov.io/github/celery/celery?branch=master

.. |license| image:: https://img.shields.io/pypi/l/celery.svg
    :alt: BSD License
    :target: https://opensource.org/licenses/BSD-3-Clause

.. |wheel| image:: https://img.shields.io/pypi/wheel/celery.svg
    :alt: Celery can be installed via wheel
    :target: https://pypi.org/project/celery/

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/celery.svg
    :alt: Supported Python versions.
    :target: https://pypi.org/project/celery/

.. |pyimp| image:: https://img.shields.io/pypi/implementation/celery.svg
    :alt: Support Python implementations.
    :target: https://pypi.org/project/celery/

.. |ocbackerbadge| image:: https://opencollective.com/celery/backers/badge.svg
    :alt: Backers on Open Collective
    :target: #backers

.. |ocsponsorbadge| image:: https://opencollective.com/celery/sponsors/badge.svg
    :alt: Sponsors on Open Collective
    :target: #sponsors



  * [certifi-2018.8.13](https://certifi.io/) Certifi: Python SSL Certificates
================================

`Certifi`_ is a carefully curated collection of Root Certificates for
validating the trustworthiness of SSL certificates while verifying the identity
of TLS hosts. It has been extracted from the `Requests`_ project.

Installation
------------

``certifi`` is available on PyPI. Simply install it with ``pip``::

    $ pip install certifi

Usage
-----

To reference the installed certificate authority (CA) bundle, you can use the
built-in function::

    >>> import certifi

    >>> certifi.where()
    '/usr/local/lib/python2.7/site-packages/certifi/cacert.pem'

Or from the command line::

    $ python -m certifi
    /usr/local/lib/python2.7/site-packages/certifi/cacert.pem

Enjoy!

1024-bit Root Certificates
~~~~~~~~~~~~~~~~~~~~~~~~~~

Browsers and certificate authorities have concluded that 1024-bit keys are
unacceptably weak for certificates, particularly root certificates. For this
reason, Mozilla has removed any weak (i.e. 1024-bit key) certificate from its
bundle, replacing it with an equivalent strong (i.e. 2048-bit or greater key)
certificate from the same CA. Because Mozilla removed these certificates from
its bundle, ``certifi`` removed them as well.

In previous versions, ``certifi`` provided the ``certifi.old_where()`` function
to intentionally re-add the 1024-bit roots back into your bundle. This was not
recommended in production and therefore was removed at the end of 2018.

.. _`Certifi`: https://certifi.io/en/latest/
.. _`Requests`: http://docs.python-requests.org/en/latest/



  * [cffi-1.11.5](http://cffi.readthedocs.org) 
CFFI
====

Foreign Function Interface for Python calling C code.
Please see the `Documentation <http://cffi.readthedocs.org/>`_.

Contact
-------

`Mailing list <https://groups.google.com/forum/#!forum/python-cffi>`_

  * [cftime-1.0.3.4]() # cftime
Time-handling functionality from netcdf4-python

[![Linux Build Status](https://travis-ci.org/Unidata/cftime.svg?branch=master)](https://travis-ci.org/Unidata/cftime)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/fl9taa9je4e6wi7n/branch/master?svg=true)](https://ci.appveyor.com/project/jswhit/cftime/branch/master)
[![PyPI package](https://badge.fury.io/py/cftime.svg)](http://python.org/pypi/cftime)
[![Coverage Status](https://coveralls.io/repos/github/Unidata/cftime/badge.svg?branch=master)](https://coveralls.io/github/Unidata/cftime?branch=master)
[![Tag Status](https://img.shields.io/github/tag/UniData/cftime.svg)](https://github.com/Unidata/cftime/tags)
[![Release Status](https://img.shields.io/github/release/UniData/cftime.svg)](https://github.com/Unidata/cftime/releases)
[![Commits Status](https://img.shields.io/github/commits-since/UniData/cftime/latest.svg)](https://github.com/UniData/cftime/commits/master)

## News
12/05/2018:  version 1.0.3.4 released (just to fix a problem with the source 
tarball on pypi).

12/05/2018:  version 1.0.3.1 released.  Bugfix release (fixed issue with installation
when cython not installed, regression on 32-bit platforms, workaround for pandas 
compatibility).

12/01/2018:  version 1.0.3 released. Test coverage with coveralls.io, improved round-tripping accuracy for non-real world calendars (like `360_day`).

10/27/2018:  version 1.0.2 released. Improved accuracy (from approximately 1000 microseconds to 10 microseconds on x86
platforms). Refactored calendar calculations now allow for negative reference years. num2date function now more than an
order of magnitude faster. `months since` units now allowed, but only for `360_day` calendar.

08/15/2018:  version 1.0.1 released.

11/8/2016: `cftime` was split out of the [netcdf4-python](https://github.com/Unidata/netcdf4-python) package.

## Quick Start
* Clone GitHub repository (`git clone https://github.com/Unidata/cftime.git`), or get source tarball from [PyPI](https://pypi.python.org/pypi/cftime). Links to Windows and OS X precompiled binary packages are also available on [PyPI](https://pypi.python.org/pypi/cftime).

* Make sure [numpy](http://www.numpy.org/) and [Cython](http://cython.org/) are
  installed and you have [Python](https://www.python.org) 2.7 or newer.

* Run `python setup.py build`, then `python setup.py install` (with `sudo` if necessary).

* To run all the tests, execute `py.test`.

## Documentation
See the online [docs](http://unidata.github.io/cftime) for more details.



  * [changeo-0.4.6](http://changeo.readthedocs.io) Change-O - Repertoire clonal assignment toolkit
================================================================================

Change-O is a collection of tools for processing the output of V(D)J alignment
tools, assigning clonal clusters to immunoglobulin (Ig) sequences, and
reconstructing germline sequences.
 
Dramatic improvements in high-throughput sequencing technologies now enable 
large-scale characterization of Ig repertoires, defined as the collection of
trans-membrane antigen-receptor proteins located on the surface of B cells and
T cells. Change-O is a suite of utilities to facilitate advanced analysis of
Ig and TCR sequences following germline segment assignment. Change-O
handles output from IMGT/HighV-QUEST and IgBLAST, and provides a wide variety of
clustering methods for assigning clonal groups to Ig sequences. Record sorting, 
grouping, and various database manipulation operations are also included.
  * [chardet-3.0.4](https://github.com/chardet/chardet) Chardet: The Universal Character Encoding Detector
--------------------------------------------------

.. image:: https://img.shields.io/travis/chardet/chardet/stable.svg
   :alt: Build status
   :target: https://travis-ci.org/chardet/chardet

.. image:: https://img.shields.io/coveralls/chardet/chardet/stable.svg
   :target: https://coveralls.io/r/chardet/chardet

.. image:: https://img.shields.io/pypi/v/chardet.svg
   :target: https://warehouse.python.org/project/chardet/
   :alt: Latest version on PyPI

.. image:: https://img.shields.io/pypi/l/chardet.svg
   :alt: License


Detects
 - ASCII, UTF-8, UTF-16 (2 variants), UTF-32 (4 variants)
 - Big5, GB2312, EUC-TW, HZ-GB-2312, ISO-2022-CN (Traditional and Simplified Chinese)
 - EUC-JP, SHIFT_JIS, CP932, ISO-2022-JP (Japanese)
 - EUC-KR, ISO-2022-KR (Korean)
 - KOI8-R, MacCyrillic, IBM855, IBM866, ISO-8859-5, windows-1251 (Cyrillic)
 - ISO-8859-5, windows-1251 (Bulgarian)
 - ISO-8859-1, windows-1252 (Western European languages)
 - ISO-8859-7, windows-1253 (Greek)
 - ISO-8859-8, windows-1255 (Visual and Logical Hebrew)
 - TIS-620 (Thai)

.. note::
   Our ISO-8859-2 and windows-1250 (Hungarian) probers have been temporarily
   disabled until we can retrain the models.

Requires Python 2.6, 2.7, or 3.3+.

Installation
------------

Install from `PyPI <https://pypi.python.org/pypi/chardet>`_::

    pip install chardet

Documentation
-------------

For users, docs are now available at https://chardet.readthedocs.io/.

Command-line Tool
-----------------

chardet comes with a command-line script which reports on the encodings of one
or more files::

    % chardetect somefile someotherfile
    somefile: windows-1252 with confidence 0.5
    someotherfile: ascii with confidence 1.0

About
-----

This is a continuation of Mark Pilgrim's excellent chardet. Previously, two
versions needed to be maintained: one that supported python 2.x and one that
supported python 3.x.  We've recently merged with `Ian Cordasco <https://github.com/sigmavirus24>`_'s
`charade <https://github.com/sigmavirus24/charade>`_ fork, so now we have one
coherent version that works for Python 2.6+.

:maintainer: Dan Blanchard



  * [click-7.0](https://palletsprojects.com/p/click/) \$ click\_
==========

Click is a Python package for creating beautiful command line interfaces
in a composable way with as little code as necessary. It's the "Command
Line Interface Creation Kit". It's highly configurable but comes with
sensible defaults out of the box.

It aims to make the process of writing command line tools quick and fun
while also preventing any frustration caused by the inability to
implement an intended CLI API.

Click in three points:

-   Arbitrary nesting of commands
-   Automatic help page generation
-   Supports lazy loading of subcommands at runtime


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    $ pip install click

Click supports Python 3.4 and newer, Python 2.7, and PyPy.

.. _pip: https://pip.pypa.io/en/stable/quickstart/


A Simple Example
----------------

What does it look like? Here is an example of a simple Click program:

.. code-block:: python

    import click

    @click.command()
    @click.option("--count", default=1, help="Number of greetings.")
    @click.option("--name", prompt="Your name",
                  help="The person to greet.")
    def hello(count, name):
        """Simple program that greets NAME for a total of COUNT times."""
        for _ in range(count):
            click.echo("Hello, %s!" % name)

    if __name__ == '__main__':
        hello()

And what it looks like when run:

.. code-block:: text

    $ python hello.py --count=3
    Your name: Click
    Hello, Click!
    Hello, Click!
    Hello, Click!


Donate
------

The Pallets organization develops and supports Click and other popular
packages. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, `please
donate today`_.

.. _please donate today: https://palletsprojects.com/donate


Links
-----

*   Website: https://palletsprojects.com/p/click/
*   Documentation: https://click.palletsprojects.com/
*   License: `BSD <https://github.com/pallets/click/blob/master/LICENSE.rst>`_
*   Releases: https://pypi.org/project/click/
*   Code: https://github.com/pallets/click
*   Issue tracker: https://github.com/pallets/click/issues
*   Test status:

    *   Linux, Mac: https://travis-ci.org/pallets/click
    *   Windows: https://ci.appveyor.com/project/pallets/click

*   Test coverage: https://codecov.io/gh/pallets/click



  * [cliff-2.15.0](https://docs.openstack.org/cliff/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/cliff.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

=======================================================
 cliff -- Command Line Interface Formulation Framework
=======================================================

cliff is a framework for building command line programs. It uses
`setuptools entry points`_ to provide subcommands, output formatters, and
other extensions.

.. _setuptools entry points: http://setuptools.readthedocs.io/en/latest/pkg_resources.html#convenience-api

* Free software: Apache license
* Documentation: https://docs.openstack.org/cliff/latest/
* Source: https://opendev.org/openstack/cliff
* Bugs: https://bugs.launchpad.net/python-cliff




  * [cloudpickle-1.2.1](https://github.com/cloudpipe/cloudpickle) # cloudpickle

[![Build Status](https://travis-ci.org/cloudpipe/cloudpickle.svg?branch=master
    )](https://travis-ci.org/cloudpipe/cloudpickle)
[![codecov.io](https://codecov.io/github/cloudpipe/cloudpickle/coverage.svg?branch=master)](https://codecov.io/github/cloudpipe/cloudpickle?branch=master)

`cloudpickle` makes it possible to serialize Python constructs not supported
by the default `pickle` module from the Python standard library.

`cloudpickle` is especially useful for **cluster computing** where Python
code is shipped over the network to execute on remote hosts, possibly close
to the data.

Among other things, `cloudpickle` supports pickling for **lambda functions**
along with **functions and classes defined interactively** in the
`__main__` module (for instance in a script, a shell or a Jupyter notebook).

Cloudpickle can only be used to send objects between the **exact same version
of Python**.

Using `cloudpickle` for **long-term object storage is not supported and
strongly discouraged.**

**Security notice**: one should **only load pickle data from trusted sources** as
otherwise `pickle.load` can lead to arbitrary code execution resulting in a critical
security vulnerability.


Installation
------------

The latest release of `cloudpickle` is available from
[pypi](https://pypi.python.org/pypi/cloudpickle):

    pip install cloudpickle


Examples
--------

Pickling a lambda expression:

```python
>>> import cloudpickle
>>> squared = lambda x: x ** 2
>>> pickled_lambda = cloudpickle.dumps(squared)

>>> import pickle
>>> new_squared = pickle.loads(pickled_lambda)
>>> new_squared(2)
4
```

Pickling a function interactively defined in a Python shell session
(in the `__main__` module):

```python
>>> CONSTANT = 42
>>> def my_function(data):
...    return data + CONSTANT
...
>>> pickled_function = cloudpickle.dumps(my_function)
>>> pickle.loads(pickled_function)(43)
85
```

Running the tests
-----------------

- With `tox`, to test run the tests for all the supported versions of
  Python and PyPy:

      pip install tox
      tox

  or alternatively for a specific environment:

      tox -e py37


- With `py.test` to only run the tests for your current version of
  Python:

      pip install -r dev-requirements.txt
      PYTHONPATH='.:tests' py.test


Note about function Annotations
-------------------------------

Note that because of design issues `Python`'s `typing` module, `cloudpickle`
supports pickling type annotations of dynamic functions for `Python` 3.7 and
later.  On `Python` 3.4, 3.5 and 3.6, those type annotations will be dropped
silently during pickling (example below):

```python
>>> import typing
>>> import cloudpickle
>>> def f(x: typing.Union[list, int]):
...     return x
>>> f
<function __main__.f(x:Union[list, int])>
>>> cloudpickle.loads(cloudpickle.dumps(f))  # drops f's annotations
<function __main__.f(x)>
```

History
-------

`cloudpickle` was initially developed by [picloud.com](http://web.archive.org/web/20140721022102/http://blog.picloud.com/2013/11/17/picloud-has-joined-dropbox/) and shipped as part of
the client SDK.

A copy of `cloudpickle.py` was included as part of PySpark, the Python
interface to [Apache Spark](https://spark.apache.org/). Davies Liu, Josh
Rosen, Thom Neale and other Apache Spark developers improved it significantly,
most notably to add support for PyPy and Python 3.

The aim of the `cloudpickle` project is to make that work available to a wider
audience outside of the Spark ecosystem and to make it easier to improve it
further notably with the help of a dedicated non-regression test suite.



  * [clyent-1.2.1](http://github.com/binstar/clyent) UNKNOWN
  * [cmarkgfm-0.4.2](https://github.com/jonparrott/cmarkgfm) cmarkgfm - Bindings to GitHub's cmark
=====================================

Minimalist bindings to GitHub's fork of cmark.

Installation
------------

This package is published as `cmarkgfm <https://pypi.org/project/cmarkgfm/>`__
and can be installed with `pip` or `pipenv`::

    pip install --user cmarkgfm
    pipenv install cmarkgfm

Wheels are provided for macOS, Linux, and Windows for Python 2.7, 3.4, 3.5, and 3.6.

Usage
-----

High-level usage is really straightforward. To render normal CommonMark
markdown:

.. code-block:: python

    import cmarkgfm

    html = cmarkgfm.markdown_to_html(markdown_text)


To render GitHub-flavored markdown:

.. code-block:: python

    import cmarkgfm

    html = cmarkgfm.github_flavored_markdown_to_html(markdown_text)


Contributing
------------

Pull requests are welcome. :)


License
-------

This project is under the MIT License. It includes components under differing
copyright under the ``third_party`` directory in this source tree.
  * [cmd2-0.9.15](https://github.com/python-cmd2/cmd2) cmd2: a tool for building interactive command line apps
=======================================================
[![Latest Version](https://img.shields.io/pypi/v/cmd2.svg?style=flat-square&label=latest%20stable%20version)](https://pypi.python.org/pypi/cmd2/)
[![Build status](https://img.shields.io/travis/python-cmd2/cmd2.svg?style=flat-square&label=unix%20build)](https://travis-ci.org/python-cmd2/cmd2)
[![Appveyor build status](https://img.shields.io/appveyor/ci/FedericoCeratto/cmd2.svg?style=flat-square&label=windows%20build)](https://ci.appveyor.com/project/FedericoCeratto/cmd2)
[![Azure Build status](https://python-cmd2.visualstudio.com/cmd2/_apis/build/status/python-cmd2.cmd2?branch=master)](https://python-cmd2.visualstudio.com/cmd2/_build/latest?definitionId=1&branch=master)
[![codecov](https://codecov.io/gh/python-cmd2/cmd2/branch/master/graph/badge.svg)](https://codecov.io/gh/python-cmd2/cmd2)
[![Documentation Status](https://readthedocs.org/projects/cmd2/badge/?version=latest)](http://cmd2.readthedocs.io/en/latest/?badge=latest)
<a href="https://discord.gg/RpVG6tk"><img src="https://img.shields.io/badge/chat-on%20discord-7289da.svg" alt="Chat"></a>

cmd2 is a tool for building interactive command line applications in Python. Its goal is to make it
quick and easy for developers to build feature-rich and user-friendly interactive command line
applications.  It provides a simple API which is an extension of Python's built-in
[cmd](https://docs.python.org/3/library/cmd.html) module.  cmd2 provides a wealth of features on top
of cmd to make your life easier and eliminates much of the boilerplate code which would be necessary
when using cmd.

Click on image below to watch a short video demonstrating the capabilities of cmd2:
[![Screenshot](cmd2.png)](https://youtu.be/DDU_JH6cFsA)

Main Features
-------------
- Searchable command history (`history` command and `<Ctrl>+r`) - optionally persistent
- Text file scripting of your application with `run_script` (`@`) and `_relative_run_script` (`@@`)
- Python scripting of your application with ``run_pyscript``
- Run shell commands with ``!``
- Pipe command output to shell commands with `|`
- Redirect command output to file with `>`, `>>`
- Bare `>`, `>>` with no filename send output to paste buffer (clipboard)
- `py` enters interactive Python console (opt-in `ipy` for IPython console)
- Option to display long output using a pager with ``cmd2.Cmd.ppaged()``
- Multi-line commands
- Special-character command shortcuts (beyond cmd's `?` and `!`)
- Command aliasing similar to bash `alias` command
- Macros, which are similar to aliases, but they can contain argument placeholders
- Ability to run commands at startup from an initialization script
- Settable environment parameters
- Parsing commands with arguments using `argparse`, including support for subcommands
- Unicode character support
- Good tab-completion of commands, subcommands, file system paths, and shell commands
- Automatic tab-completion of `argparse` flags when using one of the `cmd2` `argparse` decorators
- Support for Python 3.5+ on Windows, macOS, and Linux
- Trivial to provide built-in help for all commands
- Built-in regression testing framework for your applications (transcript-based testing)
- Transcripts for use with built-in regression can be automatically generated from `history -t` or `run_script -t`
- Alerts that seamlessly print while user enters text at prompt
- Colored and stylized output using `ansi.style()`

Python 2.7 support is EOL
-------------------------
The last version of cmd2 to support Python 2.7 is [0.8.9](https://pypi.org/project/cmd2/0.8.9/), released on August 21, 2018.

Supporting Python 2 was an increasing burden on our limited resources.  Switching to support only Python 3 is allowing
us to clean up the codebase, remove some cruft, and focus on developing new features.

Installation
------------
On all operating systems, the latest stable version of `cmd2` can be installed using pip:

```bash
pip install -U cmd2
```

cmd2 works with Python 3.5+ on Windows, macOS, and Linux. It is pure Python code with few 3rd-party dependencies.

For information on other installation options, see
[Installation Instructions](https://cmd2.readthedocs.io/en/latest/overview/installation.html) in the cmd2
documentation.


Documentation
-------------
The latest documentation for cmd2 can be read online here: https://cmd2.readthedocs.io/en/latest/

It is available in HTML, PDF, and ePub formats.


Feature Overview
----------------
Instructions for implementing each feature follow.

- Extension of the `cmd` module.  So capabilities provided by `cmd` still exist
    - Your applicaiton inherits from `cmd2.Cmd`, let's say you call this class `MyApp`
    ```Python
    import cmd2
    class MyApp(cmd2.Cmd):
      pass
    ```
    - Define a command named **foo** by creating a method named **do_foo**
    ```Python
    class MyApp(cmd2.Cmd):
        def do_foo(self, args):
            """This docstring is the built-in help for the foo command."""
            self.poutput(cmd2.style('foo bar baz', fg='red'))
    ```
    - By default the docstring for your **do_foo** method is the help for the **foo** command
        - NOTE: This doesn't apply if you use one of the `argparse` decorators mentioned below
    - Can provide more custom help by creating a **help_foo** method (except when using `argparse` decorators)
    - Can provide custom tab-completion for the **foo** command by creating a **complete_foo** method
    - Easy to upgrade an existing `cmd` app to `cmd2`
    - Run your `cmd2` app using the built-in REPL by executing the **cmdloop** method

- Searchable command history
    - Readline history using `<Ctrl>+r`, arrow keys, and other [Readline Shortcut keys](http://readline.kablamo.org/emacs.html)
        - Readline history can be persistent between application runs via optional argument to `cmd2.Cmd` initializer
    - `cmd2` `history` command provides flexible and powerful search
        - By design, this history does NOT persist between application runs
        - If you wish to exclude some of your custom commands from the history, append their names to the list at `Cmd.exclude_from_history`.
        - Do `help history` in any `cmd2` application for more information

- Simple scripting using text files with one command + arguments per line
    - See the [Command Scripts](https://cmd2.readthedocs.io/en/latest/features/scripting.html#command-scripts) section of the `cmd2` docs for more info
    - See [script.txt](https://github.com/python-cmd2/cmd2/blob/master/examples/scripts/script.txt) for a trivial example script that can be
    used in any `cmd2` application with the `run_script` command (or `@` shortcut)

- Powerful and flexible built-in Python scripting of your application using the `run_pyscript` command
    - Run arbitrary Python scripts within your `cmd2` application with the ability to also call custom `cmd2` commands
    - No separate API for your end users to learn
        - Syntax for calling `cmd2` commands in a `run_pyscript` is essentially identical to what they would enter on the command line
    - See the [Python Scripts](https://cmd2.readthedocs.io/en/latest/features/scripting.html#python-scripts) section of the `cmd2` docs for more info
    - Also see the [python_scripting.py](https://github.com/python-cmd2/cmd2/blob/master/examples/python_scripting.py) 
    example in conjunction with the [conditional.py](https://github.com/python-cmd2/cmd2/blob/master/examples/scripts/conditional.py) script

- Parsing commands with `argparse`
    - Two decorators provide built-in capability for using `argparse.ArgumentParser` to parse command arguments
        - `cmd2.with_argparser` - all arguments are parsed by the `ArgumentParser`
        - `cmd2.with_argparser_and_unknown_args` - any arguments not parsed by the `ArgumentParser` get passed as a list

    ```Python
    import argparse
    from cmd2 import with_argparser

    argparser = argparse.ArgumentParser()
    argparser.add_argument('-p', '--piglatin', action='store_true', help='atinLay')
    argparser.add_argument('-s', '--shout', action='store_true', help='N00B EMULATION MODE')
    argparser.add_argument('words', nargs='+', help='words to say')

    @with_argparser(argparser)
    def do_speak(self, args):
        """Repeats what you tell me to."""
        words = []
        for word in args.words:
            if args.piglatin:
                word = '%s%say' % (word[1:], word[0])
            if args.shout:
                word = word.upper()
            words.append(word)
        self.stdout.write('{}\n'.format(' '.join(words)))
    ```

    See [Argument Processing](https://cmd2.readthedocs.io/en/latest/features/argument_processing.html) in the docs for more details

    NOTE: `cmd2` also provides the `Cmd2ArgumentParser` customization of `argparse.ArgumentParser` for prettier formatting
    of help and error messages.

- `cmd2` applications function like a full-featured shell in many ways (and are cross-platform)
    - Run arbitrary shell commands by preceding them with `!` or `shell`
    - Redirect the output of any command to a file with `>` for overwrite or `>>` for append
        - If no file name provided after the `>`/`>>`, then output goes to the clipboard/pastebuffer
    - Pipe the output of any command to an arbitrary shell command with `|`
    - Create your own custom command aliases using the `alias` command
    - Create your own custom macros using the `macro` command (similar to aliases, but allow arguments)
    - Settable environment parameters that users can change during execution supported via `set` command
    - Option to display long output using a pager with ``cmd2.Cmd.ppaged()``
    - Optionally specify a startup script that end users can use to customize their environment

- Top-notch tab-completion capabilities which are easy to use but very powerful
    - For a command **foo** implement a **complete_foo** method to provide custom tab completion for that command
        - But the helper methods within `cmd2` discussed below mean you would rarely have to implement this from scratch
    - Commands which use one of the `argparse` decorators have automatic tab-completion of `argparse` flags
        - And also provide help hints for values associated with these flags
        - Experiment with the [argprint.py](https://github.com/python-cmd2/cmd2/blob/master/examples/arg_print.py) example
        using the **oprint** and **pprint** commands to get a feel for how this works
    - `path_complete` helper method provides flexible tab-completion of file system paths
        - See the [paged_output.py](https://github.com/python-cmd2/cmd2/blob/master/examples/paged_output.py) example for a simple use case
        - See the [python_scripting.py](https://github.com/python-cmd2/cmd2/blob/master/examples/python_scripting.py) example for a more full-featured use case
    - `flag_based_complete` helper method for tab completion based on a particular flag preceding the token being completed
        - See the [tab_completion.py](https://github.com/python-cmd2/cmd2/blob/master/examples/tab_completion.py) example for a demonstration of how to use this feature
    - `index_based_complete` helper method for tab completion based on a fixed position in the input string
        - See the [tab_completion.py](https://github.com/python-cmd2/cmd2/blob/master/examples/tab_completion.py) example for a demonstration of how to use this feature
    - `basic_complete` helper method for tab completion against a list
    - `delimiter_complete` helper method for tab completion against a list but each match is split on a delimiter 
        - See the [tab_autocompletion.py](https://github.com/python-cmd2/cmd2/blob/master/examples/tab_autocompletion.py) example for a demonstration of how to use this feature
    - `cmd2` in combination with `argparse` also provide several advanced capabilities for automatic tab-completion
            - See the [tab_autocompletion.py](https://github.com/python-cmd2/cmd2/blob/master/examples/tab_autocompletion.py) example for more info

- Multi-line commands

    Any command accepts multi-line input when its name is listed the `multiline_commands` optional argument to 
    `cmd2.Cmd.__init`. The program will keep expecting input until a line ends with any of the characters listed in the 
    `terminators` optional argument to `cmd2.Cmd.__init__()`  .  The default terminators are `;` and `\n` (empty newline).

- Special-character shortcut commands (beyond cmd's "@" and "!")

    To create a single-character shortcut for a command, update `Cmd.shortcuts`.

- Asynchronous alerts based on events happening in background threads
    - `cmd2` provides the following helper methods for providing information to users asynchronously even though the `cmd2`
    REPL is a line-oriented command interpreter:
        - `async_alert` - display an important message to the user while they are at the prompt in between commands
            - To the user it appears as if an alert message is printed above the prompt
        - `async_update_prompt` - update the prompt while the user is still typing at it
            - This is good for alerting the user to system changes dynamically in between commands
        - `set_window_title` - set the terminal window title
            - This changes the window title of the terminal that the user is running the `cmd2` app within


Tutorials
---------

* PyOhio 2019 presentation: 
    * [video](https://www.youtube.com/watch?v=pebeWrTqIIw)
    * [slides](https://github.com/python-cmd2/talks/blob/master/PyOhio_2019/cmd2-PyOhio_2019.pdf)
    * [example code](https://github.com/python-cmd2/talks/tree/master/PyOhio_2019/examples)


Example Application
-------------------

Example cmd2 application (**examples/example.py**):

```python
#!/usr/bin/env python
# coding=utf-8
"""
A sample application for cmd2.
"""
import argparse
import random
import sys
import cmd2

class CmdLineApp(cmd2.Cmd):
    """ Example cmd2 application. """

    # Setting this true makes it run a shell command if a cmd2/cmd command doesn't exist
    # default_to_shell = True
    MUMBLES = ['like', '...', 'um', 'er', 'hmmm', 'ahh']
    MUMBLE_FIRST = ['so', 'like', 'well']
    MUMBLE_LAST = ['right?']

    def __init__(self):
        self.maxrepeats = 3
        shortcuts = dict(cmd2.DEFAULT_SHORTCUTS)
        shortcuts.update({'&': 'speak'})

        # Set use_ipython to True to enable the "ipy" command which embeds and interactive IPython shell
        super().__init__(use_ipython=False, multiline_commands=['orate'], shortcuts=shortcuts)

        # Make maxrepeats settable at runtime
        self.settable['maxrepeats'] = 'max repetitions for speak command'

    speak_parser = argparse.ArgumentParser()
    speak_parser.add_argument('-p', '--piglatin', action='store_true', help='atinLay')
    speak_parser.add_argument('-s', '--shout', action='store_true', help='N00B EMULATION MODE')
    speak_parser.add_argument('-r', '--repeat', type=int, help='output [n] times')
    speak_parser.add_argument('words', nargs='+', help='words to say')

    @cmd2.with_argparser(speak_parser)
    def do_speak(self, args):
        """Repeats what you tell me to."""
        words = []
        for word in args.words:
            if args.piglatin:
                word = '%s%say' % (word[1:], word[0])
            if args.shout:
                word = word.upper()
            words.append(word)
        repetitions = args.repeat or 1
        for i in range(min(repetitions, self.maxrepeats)):
            # .poutput handles newlines, and accommodates output redirection too
            self.poutput(' '.join(words))

    do_say = do_speak  # now "say" is a synonym for "speak"
    do_orate = do_speak  # another synonym, but this one takes multi-line input

    mumble_parser = argparse.ArgumentParser()
    mumble_parser.add_argument('-r', '--repeat', type=int, help='how many times to repeat')
    mumble_parser.add_argument('words', nargs='+', help='words to say')

    @cmd2.with_argparser(mumble_parser)
    def do_mumble(self, args):
        """Mumbles what you tell me to."""
        repetitions = args.repeat or 1
        for i in range(min(repetitions, self.maxrepeats)):
            output = []
            if (random.random() < .33):
                output.append(random.choice(self.MUMBLE_FIRST))
            for word in args.words:
                if (random.random() < .40):
                    output.append(random.choice(self.MUMBLES))
                output.append(word)
            if (random.random() < .25):
                output.append(random.choice(self.MUMBLE_LAST))
            self.poutput(' '.join(output))

if __name__ == '__main__':
    app = CmdLineApp()
    sys.exit(app.cmdloop())
```

The following is a sample session running example.py.
Thanks to Cmd2's built-in transcript testing capability, it also serves as a test
suite for example.py when saved as *transcript_regex.txt*.
Running

```bash
python example.py -t transcript_regex.txt
```
will run all the commands in the transcript against `example.py`, verifying that the output produced
matches the transcript.

example/transcript_regex.txt:

```text
# Run this transcript with "python example.py -t transcript_regex.txt"
# The regex for editor will match whatever program you use.
# regexes on prompts just make the trailing space obvious
(Cmd) set
allow_ansi: Terminal
continuation_prompt: >/ /
debug: False
echo: False
editor: /.*?/
feedback_to_output: False
locals_in_py: True
maxrepeats: 3
prompt: (Cmd)/ /
quiet: False
timing: False
```

Regular expressions can be used anywhere within a transcript file simply by enclosing them within forward slashes, `/`.


Found a bug?
------------

If you think you've found a bug, please first read through the open [Issues](https://github.com/python-cmd2/cmd2/issues). If you're confident it's a new bug, go ahead and create a new GitHub issue. Be sure to include as much information as possible so we can reproduce the bug.  At a minimum, please state the following:

* ``cmd2`` version
* Python version
* OS name and version
* What you did to cause the bug to occur
* Include any traceback or error message associated with the bug


Open source projects using cmd2
-------------------------------

Here are a few examples of open-source projects which use `cmd2`:

* [CephFS Shell](http://docs.ceph.com/docs/master/cephfs/cephfs-shell/)
    * [Ceph](https://ceph.com/) is a distributed object, block, and file storage platform
* [JSShell](https://github.com/Den1al/JSShell)
    * An interactive multi-user web JavaScript shell
* [psiTurk](https://psiturk.org)
    * An open platform for science on Amazon Mechanical Turk
* [Jok3r](http://www.jok3r-framework.com)
    * Network & Web Pentest Automation Framework
* [Poseidon](https://github.com/CyberReboot/poseidon)
    * Leverages software-defined networks (SDNs) to acquire and then feed network traffic to a number of machine learning techniques
* [Unipacker](https://github.com/unipacker/unipacker)
    * Automatic and platform-independent unpacker for Windows binaries based on emulation
* [FLASHMINGO](https://github.com/fireeye/flashmingo)
    * Automatic analysis of SWF files based on some heuristics. Extensible via plugins.
* [tomcatmanager](https://github.com/tomcatmanager/tomcatmanager)
    * A command line tool and python library for managing a tomcat server
* [Expliot](https://gitlab.com/expliot_framework/expliot)
    * Internet of Things (IoT) exploitation framework
* [mptcpanalyzer](https://github.com/teto/mptcpanalyzer)
    * Tool to help analyze mptcp pcaps
* [clanvas](https://github.com/marklalor/clanvas)
    * Command-line client for Canvas by Instructure



  * [codecov-2.0.15](http://github.com/codecov/codecov-python) 
  * [collective.checkdocs-0.2](https://github.com/collective/collective.checkdocs) collective.checkdocs adds new distutils commands ``checkdocs`` and ``showdocs`` to validate restructured text in long_description field of Python eggs. 
This package aims to make Python egg help page publishing and editing easier.

Eggs' long description field, which is usually also the README.txt file of the package, is reST formatted text. This text is converted
to HTML to show on the package page when package is published in distribution repositories like PyPI or plone.org.
Unfortunately, since repositories do poor job to validate incoming reST text, errors in the text will result to broken published
package pages. 

Unpublishing is usually very cumbersome. 
We save our time by validating reST input using ``checkdocs`` and ``showdocs`` commands
before submitting eggs to PyPi.

Installation
============

Install this to your development Python run-time::

        easy_install collective.checkdocs


*Note*: On OSX at least Python version 2.5 or newer is required.
        
New commands
============

The following commands will be added to all setup.py installers.

checkdocs
---------

Run long_description through reST to HTML converter and print errors and warnings to the standard output.

Any errors or warnings will cause distutils to abort.

Example::

  python setup.py checkdocs
  <string>:4: (WARNING/2) Inline literal start-string without end-string.
  error: long_description had reST syntax errors

showdocs
---------

Run long_description through reST to HTML converter and display the result in a local web browser. Runs a web server in port 6969 until CTRL+C is pressed.

Example::

  python setup.py showdocs
  running showdocs
  reST to HTML conversion available at at http://localhost:6969/ - press CTRL+C to interrupt

Source code
------------

* https://svn.plone.org/svn/collective/collective.checkdocs

Authors
-------

`Mikko Ohtamaa <https://opensourcehacker.com>`_ 

Resources
---------

* long_description text is `restructured formatted <http://docutils.sourceforge.net/rst.html>`_

* `docutils <http://docutils.sourceforge.net/>`_

* `rstctl (inspiration and sources) <http://pypi.python.org/pypi/rstctl>`_

* `How it all begun <http://www.nabble.com/Checking-long_description-format-(reST)-before-it-is-sent-to-PyPI-td15613184.html>`_
  * [colorama-0.4.1](https://github.com/tartley/colorama) .. image:: https://img.shields.io/pypi/v/colorama.svg
    :target: https://pypi.org/project/colorama/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/colorama.svg
    :target: https://pypi.org/project/colorama/
    :alt: Supported Python versions

.. image:: https://travis-ci.org/tartley/colorama.svg?branch=master
    :target: https://travis-ci.org/tartley/colorama
    :alt: Build Status

Download and docs:
    https://pypi.org/project/colorama/
Source code & Development:
    https://github.com/tartley/colorama

Description
===========

Makes ANSI escape character sequences (for producing colored terminal text and
cursor positioning) work under MS Windows.

ANSI escape character sequences have long been used to produce colored terminal
text and cursor positioning on Unix and Macs. Colorama makes this work on
Windows, too, by wrapping ``stdout``, stripping ANSI sequences it finds (which
would appear as gobbledygook in the output), and converting them into the
appropriate win32 calls to modify the state of the terminal. On other platforms,
Colorama does nothing.

Colorama also provides some shortcuts to help generate ANSI sequences
but works fine in conjunction with any other ANSI sequence generation library,
such as the venerable Termcolor (https://pypi.org/project/termcolor/)
or the fabulous Blessings (https://pypi.org/project/blessings/).

This has the upshot of providing a simple cross-platform API for printing
colored terminal text from Python, and has the happy side-effect that existing
applications or libraries which use ANSI sequences to produce colored output on
Linux or Macs can now also work on Windows, simply by calling
``colorama.init()``.

An alternative approach is to install ``ansi.sys`` on Windows machines, which
provides the same behaviour for all applications running in terminals. Colorama
is intended for situations where that isn't easy (e.g., maybe your app doesn't
have an installer.)

Demo scripts in the source code repository print some colored text using
ANSI sequences. Compare their output under Gnome-terminal's built in ANSI
handling, versus on Windows Command-Prompt using Colorama:

.. image:: https://github.com/tartley/colorama/raw/master/screenshots/ubuntu-demo.png
    :width: 661
    :height: 357
    :alt: ANSI sequences on Ubuntu under gnome-terminal.

.. image:: https://github.com/tartley/colorama/raw/master/screenshots/windows-demo.png
    :width: 668
    :height: 325
    :alt: Same ANSI sequences on Windows, using Colorama.

These screengrabs show that, on Windows, Colorama does not support ANSI 'dim
text'; it looks the same as 'normal text'.


License
=======

Copyright Jonathan Hartley 2013. BSD 3-Clause license; see LICENSE file.


Dependencies
============

None, other than Python. Tested on Python 2.7, 3.4, 3.5 and 3.6.

Usage
=====

Initialisation
--------------

Applications should initialise Colorama using:

.. code-block:: python

    from colorama import init
    init()

On Windows, calling ``init()`` will filter ANSI escape sequences out of any
text sent to ``stdout`` or ``stderr``, and replace them with equivalent Win32
calls.

On other platforms, calling ``init()`` has no effect (unless you request other
optional functionality; see "Init Keyword Args", below). By design, this permits
applications to call ``init()`` unconditionally on all platforms, after which
ANSI output should just work.

To stop using colorama before your program exits, simply call ``deinit()``.
This will restore ``stdout`` and ``stderr`` to their original values, so that
Colorama is disabled. To resume using Colorama again, call ``reinit()``; it is
cheaper to calling ``init()`` again (but does the same thing).


Colored Output
--------------

Cross-platform printing of colored text can then be done using Colorama's
constant shorthand for ANSI escape sequences:

.. code-block:: python

    from colorama import Fore, Back, Style
    print(Fore.RED + 'some red text')
    print(Back.GREEN + 'and with a green background')
    print(Style.DIM + 'and in dim text')
    print(Style.RESET_ALL)
    print('back to normal now')

...or simply by manually printing ANSI sequences from your own code:

.. code-block:: python

    print('\033[31m' + 'some red text')
    print('\033[30m') # and reset to default color

...or, Colorama can be used happily in conjunction with existing ANSI libraries
such as Termcolor:

.. code-block:: python

    from colorama import init
    from termcolor import colored

    # use Colorama to make Termcolor work on Windows too
    init()

    # then use Termcolor for all colored text output
    print(colored('Hello, World!', 'green', 'on_red'))

Available formatting constants are::

    Fore: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
    Back: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
    Style: DIM, NORMAL, BRIGHT, RESET_ALL

``Style.RESET_ALL`` resets foreground, background, and brightness. Colorama will
perform this reset automatically on program exit.


Cursor Positioning
------------------

ANSI codes to reposition the cursor are supported. See ``demos/demo06.py`` for
an example of how to generate them.


Init Keyword Args
-----------------

``init()`` accepts some ``**kwargs`` to override default behaviour.

init(autoreset=False):
    If you find yourself repeatedly sending reset sequences to turn off color
    changes at the end of every print, then ``init(autoreset=True)`` will
    automate that:

    .. code-block:: python

        from colorama import init
        init(autoreset=True)
        print(Fore.RED + 'some red text')
        print('automatically back to default color again')

init(strip=None):
    Pass ``True`` or ``False`` to override whether ansi codes should be
    stripped from the output. The default behaviour is to strip if on Windows
    or if output is redirected (not a tty).

init(convert=None):
    Pass ``True`` or ``False`` to override whether to convert ANSI codes in the
    output into win32 calls. The default behaviour is to convert if on Windows
    and output is to a tty (terminal).

init(wrap=True):
    On Windows, colorama works by replacing ``sys.stdout`` and ``sys.stderr``
    with proxy objects, which override the ``.write()`` method to do their work.
    If this wrapping causes you problems, then this can be disabled by passing
    ``init(wrap=False)``. The default behaviour is to wrap if ``autoreset`` or
    ``strip`` or ``convert`` are True.

    When wrapping is disabled, colored printing on non-Windows platforms will
    continue to work as normal. To do cross-platform colored output, you can
    use Colorama's ``AnsiToWin32`` proxy directly:

    .. code-block:: python

        import sys
        from colorama import init, AnsiToWin32
        init(wrap=False)
        stream = AnsiToWin32(sys.stderr).stream

        # Python 2
        print >>stream, Fore.BLUE + 'blue text on stderr'

        # Python 3
        print(Fore.BLUE + 'blue text on stderr', file=stream)


Status & Known Problems
=======================

I've personally only tested it on Windows XP (CMD, Console2), Ubuntu
(gnome-terminal, xterm), and OS X.

Some presumably valid ANSI sequences aren't recognised (see details below),
but to my knowledge nobody has yet complained about this. Puzzling.

See outstanding issues and wishlist:
https://github.com/tartley/colorama/issues

If anything doesn't work for you, or doesn't do what you expected or hoped for,
I'd love to hear about it on that issues list, would be delighted by patches,
and would be happy to grant commit access to anyone who submits a working patch
or two.


Recognised ANSI Sequences
=========================

ANSI sequences generally take the form:

    ESC [ <param> ; <param> ... <command>

Where ``<param>`` is an integer, and ``<command>`` is a single letter. Zero or
more params are passed to a ``<command>``. If no params are passed, it is
generally synonymous with passing a single zero. No spaces exist in the
sequence; they have been inserted here simply to read more easily.

The only ANSI sequences that colorama converts into win32 calls are::

    ESC [ 0 m       # reset all (colors and brightness)
    ESC [ 1 m       # bright
    ESC [ 2 m       # dim (looks same as normal brightness)
    ESC [ 22 m      # normal brightness

    # FOREGROUND:
    ESC [ 30 m      # black
    ESC [ 31 m      # red
    ESC [ 32 m      # green
    ESC [ 33 m      # yellow
    ESC [ 34 m      # blue
    ESC [ 35 m      # magenta
    ESC [ 36 m      # cyan
    ESC [ 37 m      # white
    ESC [ 39 m      # reset

    # BACKGROUND
    ESC [ 40 m      # black
    ESC [ 41 m      # red
    ESC [ 42 m      # green
    ESC [ 43 m      # yellow
    ESC [ 44 m      # blue
    ESC [ 45 m      # magenta
    ESC [ 46 m      # cyan
    ESC [ 47 m      # white
    ESC [ 49 m      # reset

    # cursor positioning
    ESC [ y;x H     # position cursor at x across, y down
    ESC [ y;x f     # position cursor at x across, y down
    ESC [ n A       # move cursor n lines up
    ESC [ n B       # move cursor n lines down
    ESC [ n C       # move cursor n characters forward
    ESC [ n D       # move cursor n characters backward

    # clear the screen
    ESC [ mode J    # clear the screen

    # clear the line
    ESC [ mode K    # clear the line

Multiple numeric params to the ``'m'`` command can be combined into a single
sequence::

    ESC [ 36 ; 45 ; 1 m     # bright cyan text on magenta background

All other ANSI sequences of the form ``ESC [ <param> ; <param> ... <command>``
are silently stripped from the output on Windows.

Any other form of ANSI sequence, such as single-character codes or alternative
initial characters, are not recognised or stripped. It would be cool to add
them though. Let me know if it would be useful for you, via the Issues on
GitHub.


Development
===========

Help and fixes welcome!

Running tests requires:

- Michael Foord's ``mock`` module to be installed.
- Tests are written using 2010-era updates to ``unittest``

To run tests::

   python -m unittest discover -p *_test.py

This, like a few other handy commands, is captured in a ``Makefile``.

If you use nose to run the tests, you must pass the ``-s`` flag; otherwise,
``nosetests`` applies its own proxy to ``stdout``, which confuses the unit
tests.


Thanks
======
* Marc Schlaich (schlamar) for a ``setup.py`` fix for Python2.5.
* Marc Abramowitz, reported & fixed a crash on exit with closed ``stdout``,
  providing a solution to issue #7's setuptools/distutils debate,
  and other fixes.
* User 'eryksun', for guidance on correctly instantiating ``ctypes.windll``.
* Matthew McCormick for politely pointing out a longstanding crash on non-Win.
* Ben Hoyt, for a magnificent fix under 64-bit Windows.
* Jesse at Empty Square for submitting a fix for examples in the README.
* User 'jamessp', an observant documentation fix for cursor positioning.
* User 'vaal1239', Dave Mckee & Lackner Kristof for a tiny but much-needed Win7
  fix.
* Julien Stuyck, for wisely suggesting Python3 compatible updates to README.
* Daniel Griffith for multiple fabulous patches.
* Oscar Lesta for a valuable fix to stop ANSI chars being sent to non-tty
  output.
* Roger Binns, for many suggestions, valuable feedback, & bug reports.
* Tim Golden for thought and much appreciated feedback on the initial idea.
* User 'Zearin' for updates to the README file.
* John Szakmeister for adding support for light colors
* Charles Merriam for adding documentation to demos
* Jurko for a fix on 64-bit Windows CPython2.5 w/o ctypes
* Florian Bruhin for a fix when stdout or stderr are None
* Thomas Weininger for fixing ValueError on Windows
* Remi Rampin for better Github integration and fixes to the README file
* Simeon Visser for closing a file handle using 'with' and updating classifiers
  to include Python 3.3 and 3.4
* Andy Neff for fixing RESET of LIGHT_EX colors.
* Jonathan Hartley for the initial idea and implementation.

  * [coloredlogs-10.0](https://coloredlogs.readthedocs.io) coloredlogs: Colored terminal output for Python's logging module
================================================================

.. image:: https://travis-ci.org/xolox/python-coloredlogs.svg?branch=master
   :target: https://travis-ci.org/xolox/python-coloredlogs

.. image:: https://coveralls.io/repos/xolox/python-coloredlogs/badge.png?branch=master
   :target: https://coveralls.io/r/xolox/python-coloredlogs?branch=master

The `coloredlogs` package enables colored terminal output for Python's logging_
module. The ColoredFormatter_ class inherits from `logging.Formatter`_ and uses
`ANSI escape sequences`_ to render your logging messages in color. It uses only
standard colors so it should work on any UNIX terminal. It's currently tested
on Python 2.6, 2.7, 3.4, 3.5, 3.6 and PyPy. On Windows `coloredlogs`
automatically pulls in Colorama_ as a dependency and enables ANSI escape
sequence translation using Colorama. Here is a screen shot of the demo that is
printed when the command ``coloredlogs --demo`` is executed:

.. image:: https://coloredlogs.readthedocs.io/en/latest/_images/defaults.png

Note that the screenshot above includes custom logging levels defined by my
verboselogs_ package: if you install both `coloredlogs` and `verboselogs` it
will Just Work (`verboselogs` is of course not required to use
`coloredlogs`).

.. contents::
   :local:

Installation
------------

The `coloredlogs` package is available on PyPI_ which means installation should
be as simple as:

.. code-block:: sh

   $ pip install coloredlogs

There's actually a multitude of ways to install Python packages (e.g. the `per
user site-packages directory`_, `virtual environments`_ or just installing
system wide) and I have no intention of getting into that discussion here, so
if this intimidates you then read up on your options before returning to these
instructions ;-).

Usage
-----

Here's an example of how easy it is to get started:

.. code-block:: python

   import coloredlogs, logging

   # Create a logger object.
   logger = logging.getLogger(__name__)

   # By default the install() function installs a handler on the root logger,
   # this means that log messages from your code and log messages from the
   # libraries that you use will all show up on the terminal.
   coloredlogs.install(level='DEBUG')

   # If you don't want to see log messages from libraries, you can pass a
   # specific logger object to the install() function. In this case only log
   # messages originating from that logger will show up on the terminal.
   coloredlogs.install(level='DEBUG', logger=logger)

   # Some examples.
   logger.debug("this is a debugging message")
   logger.info("this is an informational message")
   logger.warning("this is a warning message")
   logger.error("this is an error message")
   logger.critical("this is a critical message")

Format of log messages
----------------------

The ColoredFormatter_ class supports user defined log formats so you can use
any log format you like. The default log format is as follows::

 %(asctime)s %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s

This log format results in the following output::

 2015-10-23 03:32:22 peter-macbook coloredlogs.demo[30462] DEBUG message with level 'debug'
 2015-10-23 03:32:23 peter-macbook coloredlogs.demo[30462] VERBOSE message with level 'verbose'
 2015-10-23 03:32:24 peter-macbook coloredlogs.demo[30462] INFO message with level 'info'
 ...

You can customize the log format and styling using environment variables as
well as programmatically, please refer to the `online documentation`_ for
details.

Enabling millisecond precision
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you're switching from `logging.basicConfig()`_ to `coloredlogs.install()`_
you may notice that timestamps no longer include milliseconds. This is because
coloredlogs doesn't output milliseconds in timestamps unless you explicitly
tell it to. There are three ways to do that:

1. The easy way is to pass the `milliseconds` argument to `coloredlogs.install()`_::

    coloredlogs.install(milliseconds=True)

   This became supported in `release 7.1`_ (due to `#16`_).

2. Alternatively you can change the log format `to include 'msecs'`_::

    %(asctime)s,%(msecs)03d %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s

   Here's what the call to `coloredlogs.install()`_ would then look like::

    coloredlogs.install(fmt='%(asctime)s,%(msecs)03d %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s')

   Customizing the log format also enables you to change the delimiter that
   separates seconds from milliseconds (the comma above). This became possible
   in `release 3.0`_ which added support for user defined log formats.

3. If the use of ``%(msecs)d`` isn't flexible enough you can instead add ``%f``
   to the date/time format, it will be replaced by the value of ``%(msecs)03d``.
   Support for the ``%f`` directive was added to `release 9.3`_ (due to `#45`_).

Changing text styles and colors
-------------------------------

The online documentation contains `an example of customizing the text styles and
colors <https://coloredlogs.readthedocs.io/en/latest/#changing-the-colors-styles>`_.

Colored output from cron
------------------------

When `coloredlogs` is used in a cron_ job, the output that's e-mailed to you by
cron won't contain any ANSI escape sequences because `coloredlogs` realizes
that it's not attached to an interactive terminal. If you'd like to have colors
e-mailed to you by cron there are two ways to make it happen:

.. contents::
   :local:

Modifying your crontab
~~~~~~~~~~~~~~~~~~~~~~

Here's an example of a minimal crontab::

    MAILTO="your-email-address@here"
    CONTENT_TYPE="text/html"
    * * * * * root coloredlogs --to-html your-command

The ``coloredlogs`` program is installed when you install the `coloredlogs`
Python package. When you execute ``coloredlogs --to-html your-command`` it runs
``your-command`` under the external program ``script`` (you need to have this
installed). This makes ``your-command`` think that it's attached to an
interactive terminal which means it will output ANSI escape sequences which
will then be converted to HTML by the ``coloredlogs`` program. Yes, this is a
bit convoluted, but it works great :-)

Modifying your Python code
~~~~~~~~~~~~~~~~~~~~~~~~~~

The ColoredCronMailer_ class provides a context manager that automatically
enables HTML output when the ``$CONTENT_TYPE`` variable has been correctly set
in the crontab.

This requires my capturer_ package which you can install using ``pip install
'coloredlogs[cron]'``. The ``[cron]`` extra will pull in capturer_ 2.4 or newer
which is required to capture the output while silencing it - otherwise you'd
get duplicate output in the emails sent by ``cron``.

The context manager can also be used to retroactively silence output that has
already been produced, this can be useful to avoid spammy cron jobs that have
nothing useful to do but still email their output to the system administrator
every few minutes :-).

Contact
-------

The latest version of `coloredlogs` is available on PyPI_ and GitHub_. The
`online documentation`_ is available on Read The Docs and includes a
changelog_. For bug reports please create an issue on GitHub_. If you have
questions, suggestions, etc. feel free to send me an e-mail at
`peter@peterodding.com`_.

License
-------

This software is licensed under the `MIT license`_.

© 2018 Peter Odding.


.. External references:
.. _#16: https://github.com/xolox/python-coloredlogs/issues/16
.. _#45: https://github.com/xolox/python-coloredlogs/issues/45
.. _ANSI escape sequences: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors
.. _capturer: https://pypi.python.org/pypi/capturer
.. _changelog: https://coloredlogs.readthedocs.org/en/latest/changelog.html
.. _Colorama: https://pypi.python.org/pypi/colorama
.. _ColoredCronMailer: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.converter.ColoredCronMailer
.. _ColoredFormatter: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.ColoredFormatter
.. _coloredlogs.install(): https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.install
.. _cron: https://en.wikipedia.org/wiki/Cron
.. _GitHub: https://github.com/xolox/python-coloredlogs
.. _logging.basicConfig(): https://docs.python.org/2/library/logging.html#logging.basicConfig
.. _logging.Formatter: https://docs.python.org/2/library/logging.html#logging.Formatter
.. _logging: https://docs.python.org/2/library/logging.html
.. _MIT license: https://en.wikipedia.org/wiki/MIT_License
.. _online documentation: https://coloredlogs.readthedocs.io/
.. _per user site-packages directory: https://www.python.org/dev/peps/pep-0370/
.. _peter@peterodding.com: peter@peterodding.com
.. _PyPI: https://pypi.python.org/pypi/coloredlogs
.. _release 3.0: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-3-0-2015-10-23
.. _release 7.1: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-7-1-2017-07-15
.. _release 9.3: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-9-3-2018-04-29
.. _to include 'msecs': https://stackoverflow.com/questions/6290739/python-logging-use-milliseconds-in-time-format
.. _verboselogs: https://pypi.python.org/pypi/verboselogs
.. _virtual environments: http://docs.python-guide.org/en/latest/dev/virtualenvs/



  * [configobj-5.0.6](https://github.com/DiffSK/configobj) **ConfigObj** is a simple but powerful config file reader and writer: an *ini
file round tripper*. Its main feature is that it is very easy to use, with a
straightforward programmer's interface and a simple syntax for config files.
It has lots of other features though :

* Nested sections (subsections), to any level
* List values
* Multiple line values
* Full Unicode support
* String interpolation (substitution)
* Integrated with a powerful validation system

    - including automatic type checking/conversion
    - and allowing default values
    - repeated sections

* All comments in the file are preserved
* The order of keys/sections is preserved
* Powerful ``unrepr`` mode for storing/retrieving Python data-types

| Release 5.0.6 improves error messages in certain edge cases
| Release 5.0.5 corrects a unicode-bug that still existed in writing files
| Release 5.0.4 corrects a unicode-bug that still existed in reading files after
| fixing lists of string in 5.0.3
| Release 5.0.3 corrects errors related to the incorrectly handling unicode
| encoding and writing out files
| Release 5.0.2 adds a specific error message when trying to install on
| Python versions older than 2.5
| Release 5.0.1 fixes a regression with unicode conversion not happening
| in certain cases PY2
| Release 5.0.0 updates the supported Python versions to 2.6, 2.7, 3.2, 3.3
| and is otherwise unchanged
| Release 4.7.2 fixes several bugs in 4.7.1
| Release 4.7.1 fixes a bug with the deprecated options keyword in
| 4.7.0.
| Release 4.7.0 improves performance adds features for validation and
| fixes some bugs.
  * [configparser-3.7.4](https://github.com/jaraco/configparser/) .. image:: https://img.shields.io/pypi/v/configparser.svg
   :target: https://pypi.org/project/configparser

.. image:: https://img.shields.io/pypi/pyversions/configparser.svg

.. image:: https://img.shields.io/travis/jaraco/configparser/master.svg
   :target: https://travis-ci.org/jaraco/configparser

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
   :alt: Code style: Black

.. .. image:: https://img.shields.io/appveyor/ci/jaraco/configparser/master.svg
..    :target: https://ci.appveyor.com/project/jaraco/configparser/branch/master

.. image:: https://readthedocs.org/projects/configparser/badge/?version=latest
   :target: https://configparser.readthedocs.io/en/latest/?badge=latest

.. image:: https://tidelift.com/badges/package/pypi/configparser
   :target: https://tidelift.com/subscription/pkg/pypi-configparser?utm_source=pypi-configparser&utm_medium=readme


The ancient ``ConfigParser`` module available in the standard library 2.x has
seen a major update in Python 3.2. This is a backport of those changes so that
they can be used directly in Python 2.6 - 3.5.

To use the ``configparser`` backport instead of the built-in version on both
Python 2 and Python 3, simply import it explicitly as a backport::

  from backports import configparser

If you'd like to use the backport on Python 2 and the built-in version on
Python 3, use that invocation instead::

  import configparser

For detailed documentation consult the vanilla version at
http://docs.python.org/3/library/configparser.html.

Why you'll love ``configparser``
--------------------------------

Whereas almost completely compatible with its older brother, ``configparser``
sports a bunch of interesting new features:

* full mapping protocol access (`more info
  <http://docs.python.org/3/library/configparser.html#mapping-protocol-access>`_)::

    >>> parser = ConfigParser()
    >>> parser.read_string("""
    [DEFAULT]
    location = upper left
    visible = yes
    editable = no
    color = blue

    [main]
    title = Main Menu
    color = green

    [options]
    title = Options
    """)
    >>> parser['main']['color']
    'green'
    >>> parser['main']['editable']
    'no'
    >>> section = parser['options']
    >>> section['title']
    'Options'
    >>> section['title'] = 'Options (editable: %(editable)s)'
    >>> section['title']
    'Options (editable: no)'

* there's now one default ``ConfigParser`` class, which basically is the old
  ``SafeConfigParser`` with a bunch of tweaks which make it more predictable for
  users. Don't need interpolation? Simply use
  ``ConfigParser(interpolation=None)``, no need to use a distinct
  ``RawConfigParser`` anymore.

* the parser is highly `customizable upon instantiation
  <http://docs.python.org/3/library/configparser.html#customizing-parser-behaviour>`__
  supporting things like changing option delimiters, comment characters, the
  name of the DEFAULT section, the interpolation syntax, etc.

* you can easily create your own interpolation syntax but there are two powerful
  implementations built-in (`more info
  <http://docs.python.org/3/library/configparser.html#interpolation-of-values>`__):

  * the classic ``%(string-like)s`` syntax (called ``BasicInterpolation``)

  * a new ``${buildout:like}`` syntax (called ``ExtendedInterpolation``)

* fallback values may be specified in getters (`more info
  <http://docs.python.org/3/library/configparser.html#fallback-values>`__)::

    >>> config.get('closet', 'monster',
    ...            fallback='No such things as monsters')
    'No such things as monsters'

* ``ConfigParser`` objects can now read data directly `from strings
  <http://docs.python.org/3/library/configparser.html#configparser.ConfigParser.read_string>`__
  and `from dictionaries
  <http://docs.python.org/3/library/configparser.html#configparser.ConfigParser.read_dict>`__.
  That means importing configuration from JSON or specifying default values for
  the whole configuration (multiple sections) is now a single line of code. Same
  goes for copying data from another ``ConfigParser`` instance, thanks to its
  mapping protocol support.

* many smaller tweaks, updates and fixes

A few words about Unicode
-------------------------

``configparser`` comes from Python 3 and as such it works well with Unicode.
The library is generally cleaned up in terms of internal data storage and
reading/writing files.  There are a couple of incompatibilities with the old
``ConfigParser`` due to that. However, the work required to migrate is well
worth it as it shows the issues that would likely come up during migration of
your project to Python 3.

The design assumes that Unicode strings are used whenever possible [1]_.  That
gives you the certainty that what's stored in a configuration object is text.
Once your configuration is read, the rest of your application doesn't have to
deal with encoding issues. All you have is text [2]_. The only two phases when
you should explicitly state encoding is when you either read from an external
source (e.g. a file) or write back.

Versioning
----------

This project uses `semver <https://semver.org/spec/v2.0.0.html>`_ to
communicate the impact of various releases while periodically syncing
with the upstream implementation in CPython.
`The changelog <https://github.com/jaraco/configparser/blob/master/CHANGES.rst>`_
serves as a reference indicating which versions incorporate
which upstream functionality.

Prior to the ``4.0.0`` release, `another scheme
<https://github.com/jaraco/configparser/blob/3.8.1/README.rst#versioning>`_
was used to associate the CPython and backports releases.

Maintenance
-----------

This backport was originally authored by Łukasz Langa, the current vanilla
``configparser`` maintainer for CPython and is currently maintained by
Jason R. Coombs:

* `configparser repository <https://github.com/jaraco/configparser>`_

* `configparser issue tracker <https://github.com/jaraco/configparser/issues>`_

Security Contact
----------------

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.

Conversion Process
------------------

This section is technical and should bother you only if you are wondering how
this backport is produced. If the implementation details of this backport are
not important for you, feel free to ignore the following content.

``configparser`` is converted using `python-future
<http://python-future.org>`_. The project takes the following
branching approach:

* the ``3.x`` branch holds unchanged files synchronized from the upstream
  CPython repository. The synchronization is currently done by manually copying
  the required files and stating from which CPython changeset they come from.

* the ``master`` branch holds a version of the ``3.x`` code with some tweaks
  that make it independent from libraries and constructions unavailable on 2.x.
  Code on this branch still *must* work on the corresponding Python 3.x but
  will also work on Python 2.6 and 2.7 (including PyPy).  You can check this
  running the supplied unit tests with ``tox``.

The process works like this:

1. In the ``3.x`` branch, run ``pip-run -- sync-upstream.py``, which
   downloads the latest stable release of Python and copies the relevant
   files from there into their new locations here and then commits those
   changes with a nice reference to the relevant upstream commit hash.

2. I check for new names in ``__all__`` and update imports in
   ``configparser.py`` accordingly. I run the tests on Python 3. Commit.

3. I merge the new commit to ``master``. I run ``tox``. Commit.

4. If there are necessary changes, I do them now (on ``master``). Note that
   the changes should be written in the syntax subset supported by Python
   2.6.

5. I run ``tox``. If it works, I update the docs and release the new version.
   Otherwise, I go back to point 3. I might use ``pasteurize`` to suggest me
   required changes but usually I do them manually to keep resulting code in
   a nicer form.


Footnotes
---------

.. [1] To somewhat ease migration, passing bytestrings is still supported but
       they are converted to Unicode for internal storage anyway. This means
       that for the vast majority of strings used in configuration files, it
       won't matter if you pass them as bytestrings or Unicode. However, if you
       pass a bytestring that cannot be converted to Unicode using the naive
       ASCII codec, a ``UnicodeDecodeError`` will be raised. This is purposeful
       and helps you manage proper encoding for all content you store in
       memory, read from various sources and write back.

.. [2] Life gets much easier when you understand that you basically manage
       **text** in your application.  You don't care about bytes but about
       letters.  In that regard the concept of content encoding is meaningless.
       The only time when you deal with raw bytes is when you write the data to
       a file.  Then you have to specify how your text should be encoded.  On
       the other end, to get meaningful text from a file, the application
       reading it has to know which encoding was used during its creation.  But
       once the bytes are read and properly decoded, all you have is text.  This
       is especially powerful when you start interacting with multiple data
       sources.  Even if each of them uses a different encoding, inside your
       application data is held in abstract text form.  You can program your
       business logic without worrying about which data came from which source.
       You can freely exchange the data you store between sources.  Only
       reading/writing files requires encoding your text to bytes.



  * [constantly-15.1.0](https://github.com/twisted/constantly) Constantly
==========

A library that provides symbolic constant support.
It includes collections and constants with text, numeric, and bit flag values.
Originally ``twisted.python.constants`` from the `Twisted <https://twistedmatrix.com/>`_ project.


Tests
-----

To run tests::

    $ tox

This will run tests on Python 2.7, 3.3, 3.4, and PyPy, as well as doing coverage and pyflakes checks.
  * [coverage-4.5.4](https://github.com/nedbat/coveragepy) .. Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0
.. For details: https://bitbucket.org/ned/coveragepy/src/default/NOTICE.txt

===========
Coverage.py
===========

Code coverage testing for Python.

|  |license| |versions| |status| |docs|
|  |ci-status| |win-ci-status| |codecov|
|  |kit| |format| |repos|
|  |tidelift| |saythanks|

.. downloads badge seems to be broken... |downloads|

Coverage.py measures code coverage, typically during test execution. It uses
the code analysis tools and tracing hooks provided in the Python standard
library to determine which lines are executable, and which have been executed.

.. |tideliftlogo| image:: https://nedbatchelder.com/pix/Tidelift_Logo_small.png
   :width: 75
   :alt: Tidelift
   :target: https://tidelift.com/subscription/pkg/pypi-coverage?utm_source=pypi-coverage&utm_medium=referral&utm_campaign=readme

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for coverage.py is available as part of the `Tidelift
       Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-coverage?utm_source=pypi-coverage&utm_medium=referral&utm_campaign=readme

Coverage.py runs on many versions of Python:

* CPython 2.6, 2.7 and 3.3 through beta 3.8.
* PyPy2 6.0 and PyPy3 6.0.
* Jython 2.7.1, though not for reporting.
* IronPython 2.7.7, though not for reporting.

Documentation is on `Read the Docs`_.  Code repository and issue tracker are on
`GitHub`_.

.. _Read the Docs: https://coverage.readthedocs.io/
.. _GitHub: https://github.com/nedbat/coveragepy


**New in 4.5:** Configurator plug-ins.

New in 4.4: Suppressable warnings, continuous coverage measurement.

New in 4.3: HTML ``--skip-covered``, sys.excepthook support, tox.ini
support.

New in 4.2: better support for multiprocessing and combining data.

New in 4.1: much-improved branch coverage.

New in 4.0: ``--concurrency``, plugins for non-Python files, setup.cfg
support, --skip-covered, HTML filtering, and more than 50 issues closed.


Getting Started
---------------

See the `Quick Start section`_ of the docs.

.. _Quick Start section: https://coverage.readthedocs.io/#quick-start


Contributing
------------

See the `Contributing section`_ of the docs.

.. _Contributing section: https://coverage.readthedocs.io/en/latest/contributing.html


Security
--------

To report a security vulnerability, please use the `Tidelift security
contact`_.  Tidelift will coordinate the fix and disclosure.

.. _Tidelift security contact: https://tidelift.com/security


License
-------

Licensed under the `Apache 2.0 License`_.  For details, see `NOTICE.txt`_.

.. _Apache 2.0 License: http://www.apache.org/licenses/LICENSE-2.0
.. _NOTICE.txt: https://bitbucket.org/ned/coveragepy/src/default/NOTICE.txt


.. |ci-status| image:: https://travis-ci.org/nedbat/coveragepy.svg?branch=master
    :target: https://travis-ci.org/nedbat/coveragepy
    :alt: Build status
.. |win-ci-status| image:: https://ci.appveyor.com/api/projects/status/kmeqpdje7h9r6vsf/branch/master?svg=true
    :target: https://ci.appveyor.com/project/nedbat/coveragepy
    :alt: Windows build status
.. |docs| image:: https://readthedocs.org/projects/coverage/badge/?version=latest&style=flat
    :target: https://coverage.readthedocs.io/
    :alt: Documentation
.. |reqs| image:: https://requires.io/github/nedbat/coveragepy/requirements.svg?branch=master
    :target: https://requires.io/github/nedbat/coveragepy/requirements/?branch=master
    :alt: Requirements status
.. |kit| image:: https://badge.fury.io/py/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: PyPI status
.. |format| image:: https://img.shields.io/pypi/format/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: Kit format
.. |downloads| image:: https://img.shields.io/pypi/dw/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: Weekly PyPI downloads
.. |versions| image:: https://img.shields.io/pypi/pyversions/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: Python versions supported
.. |status| image:: https://img.shields.io/pypi/status/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: Package stability
.. |license| image:: https://img.shields.io/pypi/l/coverage.svg
    :target: https://pypi.python.org/pypi/coverage
    :alt: License
.. |codecov| image:: http://codecov.io/github/nedbat/coveragepy/coverage.svg?branch=master&precision=2
    :target: http://codecov.io/github/nedbat/coveragepy?branch=master
    :alt: Coverage!
.. |repos| image:: https://repology.org/badge/tiny-repos/python:coverage.svg
    :target: https://repology.org/metapackage/python:coverage/versions
    :alt: Packaging status
.. |saythanks| image:: https://img.shields.io/badge/saythanks.io-%E2%98%BC-1EAEDB.svg
    :target: https://saythanks.io/to/nedbat
    :alt: Say thanks :)
.. |tidelift| image:: https://tidelift.com/badges/github/nedbat/coveragepy
    :target: https://tidelift.com/subscription/pkg/pypi-coverage?utm_source=pypi-coverage&utm_medium=referral&utm_campaign=readme
    :alt: Tidelift



  * [coveralls-1.8.2](http://github.com/coveralls-clients/coveralls-python) Coveralls for python
====================

.. image:: https://img.shields.io/circleci/project/github/coveralls-clients/coveralls-python/master.svg?style=flat-square
    :target: https://circleci.com/gh/coveralls-clients/coveralls-python

.. image:: https://img.shields.io/travis/coveralls-clients/coveralls-python/master.svg?style=flat-square
    :target: https://travis-ci.org/coveralls-clients/coveralls-python

.. image:: https://img.shields.io/coveralls/coveralls-clients/coveralls-python/master.svg?style=flat-square
    :target: https://coveralls.io/r/coveralls-clients/coveralls-python

.. image:: https://img.shields.io/pypi/dm/coveralls.svg?style=flat-square
    :target: https://pypi.org/project/coveralls/

.. image:: https://anaconda.org/conda-forge/coveralls/badges/version.svg
    :target: https://anaconda.org/conda-forge/coveralls

.. image:: https://img.shields.io/pypi/v/coveralls.svg?style=flat-square
    :target: https://pypi.org/project/coveralls/

.. image:: https://img.shields.io/pypi/pyversions/coveralls.svg?style=flat-square
    :target: https://pypi.org/project/coveralls/

.. image:: https://img.shields.io/pypi/implementation/coveralls.svg?style=flat-square
    :target: https://pypi.org/project/coveralls/

`coveralls.io`_ is a service for publishing your coverage stats online. This package provides seamless integration with `coverage.py`_ (and thus ``pytest``, ``nosetests``, etc...) in your Python projects::

    pip install coveralls
    coverage run --source=mypkg setup.py test
    coveralls

For more information and usage instructions, see our `documentation`_.

.. _coveralls.io: https://coveralls.io/
.. _coverage.py: https://coverage.readthedocs.io/en/latest/
.. _documentation: http://coveralls-python.readthedocs.io/en/latest/



  * [cryptography-2.3.1](https://github.com/pyca/cryptography) pyca/cryptography
=================

.. image:: https://img.shields.io/pypi/v/cryptography.svg
    :target: https://pypi.org/project/cryptography/
    :alt: Latest Version

.. image:: https://readthedocs.org/projects/cryptography/badge/?version=latest
    :target: https://cryptography.io
    :alt: Latest Docs

.. image:: https://travis-ci.org/pyca/cryptography.svg?branch=master
    :target: https://travis-ci.org/pyca/cryptography

.. image:: https://dev.azure.com/pyca/cryptography/_apis/build/status/Azure%20CI?branchName=master
    :target: https://dev.azure.com/pyca/cryptography/_build/latest?definitionId=3&branchName=master

.. image:: https://codecov.io/github/pyca/cryptography/coverage.svg?branch=master
    :target: https://codecov.io/github/pyca/cryptography?branch=master


``cryptography`` is a package which provides cryptographic recipes and
primitives to Python developers.  Our goal is for it to be your "cryptographic
standard library". It supports Python 2.7, Python 3.4+, and PyPy 5.4+.

``cryptography`` includes both high level recipes and low level interfaces to
common cryptographic algorithms such as symmetric ciphers, message digests, and
key derivation functions. For example, to encrypt something with
``cryptography``'s high level symmetric encryption recipe:

.. code-block:: pycon

    >>> from cryptography.fernet import Fernet
    >>> # Put this somewhere safe!
    >>> key = Fernet.generate_key()
    >>> f = Fernet(key)
    >>> token = f.encrypt(b"A really secret message. Not for prying eyes.")
    >>> token
    '...'
    >>> f.decrypt(token)
    'A really secret message. Not for prying eyes.'

You can find more information in the `documentation`_.

You can install ``cryptography`` with:

.. code-block:: console

    $ pip install cryptography

For full details see `the installation documentation`_.

Discussion
~~~~~~~~~~

If you run into bugs, you can file them in our `issue tracker`_.

We maintain a `cryptography-dev`_ mailing list for development discussion.

You can also join ``#cryptography-dev`` on Freenode to ask questions or get
involved.

Security
~~~~~~~~

Need to report a security issue? Please consult our `security reporting`_
documentation.


.. _`documentation`: https://cryptography.io/
.. _`the installation documentation`: https://cryptography.io/en/latest/installation/
.. _`issue tracker`: https://github.com/pyca/cryptography/issues
.. _`cryptography-dev`: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _`security reporting`: https://cryptography.io/en/latest/security/
  * [cssselect-1.0.3](https://github.com/scrapy/cssselect) ===================================
cssselect: CSS Selectors for Python
===================================

.. image:: https://img.shields.io/pypi/v/cssselect.svg
   :target: https://pypi.python.org/pypi/cssselect
   :alt: PyPI Version

.. image:: https://img.shields.io/pypi/pyversions/cssselect.svg
   :target: https://pypi.python.org/pypi/cssselect
   :alt: Supported Python Versions

.. image:: https://img.shields.io/travis/scrapy/cssselect/master.svg
   :target: https://travis-ci.org/scrapy/cssselect
   :alt: Build Status

.. image:: https://img.shields.io/codecov/c/github/scrapy/cssselect/master.svg
   :target: https://codecov.io/github/scrapy/cssselect?branch=master
   :alt: Coverage report

*cssselect* parses `CSS3 Selectors`_ and translate them to `XPath 1.0`_
expressions. Such expressions can be used in lxml_ or another XPath engine
to find the matching elements in an XML or HTML document.

This module used to live inside of lxml as ``lxml.cssselect`` before it was
extracted as a stand-alone project.

.. _CSS3 Selectors: https://www.w3.org/TR/css3-selectors/
.. _XPath 1.0: https://www.w3.org/TR/xpath/
.. _lxml: http://lxml.de/


Quick facts:

* Free software: BSD licensed
* Compatible with Python 2.7 and 3.4+
* Latest documentation `on Read the Docs <https://cssselect.readthedocs.io/>`_
* Source, issues and pull requests `on GitHub
  <https://github.com/scrapy/cssselect>`_
* Releases `on PyPI <http://pypi.python.org/pypi/cssselect>`_
* Install with ``pip install cssselect``



  * [curtsies-0.3.0](https://github.com/bpython/curtsies) [![Build Status](https://travis-ci.org/bpython/curtsies.svg?branch=master)](https://travis-ci.org/bpython/curtsies)
[![Documentation Status](https://readthedocs.org/projects/curtsies/badge/?version=latest)](https://readthedocs.org/projects/curtsies/?badge=latest)
![Curtsies Logo](http://ballingt.com/assets/curtsiestitle.png)

Curtsies is a Python 2.7 & 3.4+ compatible library for interacting with the terminal.
This is what using (nearly every feature of) curtsies looks like:

```python
from __future__ import unicode_literals  # convenient for Python 2
import random

from curtsies import FullscreenWindow, Input, FSArray
from curtsies.fmtfuncs import red, bold, green, on_blue, yellow

print(yellow('this prints normally, not to the alternate screen'))
with FullscreenWindow() as window:
    with Input() as input_generator:
        msg = red(on_blue(bold('Press escape to exit')))
        a = FSArray(window.height, window.width)
        a[0:1, 0:msg.width] = [msg]
        for c in input_generator:
            if c == '<ESC>':
                break
            elif c == '<SPACE>':
                a = FSArray(window.height, window.width)
            else:
                s = repr(c).decode()
                row = random.choice(range(window.height))
                column = random.choice(range(window.width-len(s)))
                color = random.choice([red, green, on_blue, yellow])
                a[row, column:column+len(s)] = [color(s)]
            window.render_to_terminal(a)
```

Paste it in a `something.py` file and try it out!

Installation: `pip install curtsies`

[Documentation](http://curtsies.readthedocs.org/en/latest/)

Primer
------

[FmtStr](http://curtsies.readthedocs.org/en/latest/FmtStr.html) objects are strings formatted with
colors and styles displayable in a terminal with [ANSI escape sequences](http://en.wikipedia.org/wiki/ANSI_escape_code>`_).

(the import statement shown below is outdated)

![](http://i.imgur.com/7lFaxsz.png)

[FSArray](http://curtsies.readthedocs.org/en/latest/FSArray.html) objects contain multiple such strings
with each formatted string on its own row, and FSArray
objects can be superimposed on each other
to build complex grids of colored and styled characters through composition.

(the import statement shown below is outdated)

![](http://i.imgur.com/rvTRPv1.png)

Such grids of characters can be rendered to the terminal in alternate screen mode
(no history, like `Vim`, `top` etc.) by [FullscreenWindow](http://curtsies.readthedocs.org/en/latest/window.html#curtsies.window.FullscreenWindow) objects
or normal history-preserving screen by [CursorAwareWindow](http://curtsies.readthedocs.org/en/latest/window.html#curtsies.window.CursorAwareWindow) objects.
User keyboard input events like pressing the up arrow key are detected by an
[Input](http://curtsies.readthedocs.org/en/latest/input.html) object.

Examples
--------

* [Tic-Tac-Toe](/examples/tictactoeexample.py)

![](http://i.imgur.com/AucB55B.png)

* [Avoid the X's game](/examples/gameexample.py)

![](http://i.imgur.com/nv1RQd3.png)

* [Bpython-curtsies uses curtsies](http://ballingt.com/2013/12/21/bpython-curtsies.html)

[![](http://i.imgur.com/r7rZiBS.png)](http://www.youtube.com/watch?v=lwbpC4IJlyA)

* [More examples](/examples)

About
-----

* [Curtsies Documentation](http://curtsies.readthedocs.org/en/latest/)
* Curtsies was written to for [bpython-curtsies](http://ballingt.com/2013/12/21/bpython-curtsies.html)
* `#bpython` on irc is a good place to talk about Curtsies, but feel free
  to open an issue if you're having a problem!
* Thanks to the many contributors!
* If all you need are colored strings, consider one of these [other
  libraries](http://curtsies.readthedocs.io/en/latest/FmtStr.html#fmtstr-rationale)!

  * [cutadapt-2.4](https://cutadapt.readthedocs.io/) .. image:: https://travis-ci.org/marcelm/cutadapt.svg?branch=master
    :target: https://travis-ci.org/marcelm/cutadapt
    :alt:

.. image:: https://img.shields.io/pypi/v/cutadapt.svg?branch=master
    :target: https://pypi.python.org/pypi/cutadapt
    :alt:

.. image:: https://codecov.io/gh/marcelm/cutadapt/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/marcelm/cutadapt
    :alt:

========
Cutadapt
========

Cutadapt finds and removes adapter sequences, primers, poly-A tails and other
types of unwanted sequence from your high-throughput sequencing reads.

Cleaning your data in this way is often required: Reads from small-RNA
sequencing contain the 3’ sequencing adapter because the read is longer than
the molecule that is sequenced. Amplicon reads start with a primer sequence.
Poly-A tails are useful for pulling out RNA from your sample, but often you
don’t want them to be in your reads.

Cutadapt helps with these trimming tasks by finding the adapter or primer
sequences in an error-tolerant way. It can also modify and filter reads in
various ways. Adapter sequences can contain IUPAC wildcard characters. Also,
paired-end reads and even colorspace data is supported. If you want, you can
also just demultiplex your input data, without removing adapter sequences at all.

Cutadapt comes with an extensive suite of automated tests and is available under
the terms of the MIT license.

If you use Cutadapt, please cite
`DOI:10.14806/ej.17.1.200 <http://dx.doi.org/10.14806/ej.17.1.200>`_ .


Links
-----

* `Documentation <https://cutadapt.readthedocs.io/>`_
* `Source code <https://github.com/marcelm/cutadapt/>`_
* `Report an issue <https://github.com/marcelm/cutadapt/issues>`_
* `Project page on PyPI (Python package index) <https://pypi.python.org/pypi/cutadapt/>`_
* `Follow @marcelm_ on Twitter <https://twitter.com/marcelm_>`_
* `Wrapper for the Galaxy platform <https://bitbucket.org/lance_parsons/cutadapt_galaxy_wrapper>`_



  * [cwl-0.0.1](https://github.com/kislyuk/jaml) 
  * [cwlref-runner-1.0](http://www.commonwl.org) This an optional companion package to "cwltool" which provides provides an
additional entry point under the alias "cwl-runner", which is the
implementation-agnostic name for the default CWL interpreter installed on a
host.
  * [cwltool-1.0.20190621234233](https://github.com/common-workflow-language/cwltool) ==================================================================
Common Workflow Language tool description reference implementation
==================================================================

CWL conformance tests: |Conformance Status| |Linux Status| |Windows Status| |Coverage Status| |Downloads|

|CommandLineTool Support| |DockerRequirement Support| |EnvVarRequirement Support| |ExpressionTool Support| 
|InitialWorkDirRequirement Support| |InlineJavascriptRequirement Support| |MultipleInputRequirement Support| |Core Support|
|ResourceRequirement Support| |ScatterRequirement Support| |SchemaDefRequirement Support| |ShellCommandequirement Support|
|StepInputRequirement Support| |SubWorkflowRequirement Support| |Workflow Support|

.. |Conformance Status| image:: https://ci.commonwl.org/buildStatus/icon?job=cwltool-conformance
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |Linux Status| image:: https://img.shields.io/travis/common-workflow-language/cwltool/master.svg?label=Linux%20builds
   :target: https://travis-ci.org/common-workflow-language/cwltool

.. |Windows Status| image:: https://img.shields.io/appveyor/ci/mr-c/cwltool/master.svg?label=Windows%20builds
   :target: https://ci.appveyor.com/project/mr-c/cwltool

.. |Coverage Status| image:: https://img.shields.io/codecov/c/github/common-workflow-language/cwltool.svg
   :target: https://codecov.io/gh/common-workflow-language/cwltool

.. |Downloads| image:: https://pepy.tech/badge/cwltool/month
   :target: https://pepy.tech/project/cwltool

.. |CommandLineTool Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/command_line_tool.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |DockerRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/docker.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |EnvVarRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/env_var.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |ExpressionTool Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/expression_tool.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |InitialWorkDirRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/initial_work_dir.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |InlineJavascriptRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/inline_javascript.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |MultipleInputRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/multiple_input.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |Core Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/required.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |ResourceRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/resource.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |ScatterRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/scatter.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |SchemaDefRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/schema_def.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |ShellCommandequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/shell_command.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |StepInputRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/step_input.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |SubWorkflowRequirement Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/subworkflow.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/

.. |Workflow Support| image:: https://flat.badgen.net/https/raw.githubusercontent.com/common-workflow-language/conformance/master/cwltool/cwl_v1.0/cwltool_latest/workflow.json
   :target: https://ci.commonwl.org/job/cwltool-conformance/


This is the reference implementation of the Common Workflow Language.  It is
intended to be feature complete and provide comprehensive validation of CWL
files as well as provide other tools related to working with CWL.

This is written and tested for
`Python <https://www.python.org/>`_ ``2.7 and 3.x {x = 5, 6, 7}``

The reference implementation consists of two packages.  The ``cwltool`` package
is the primary Python module containing the reference implementation in the
``cwltool`` module and console executable by the same name.

The ``cwlref-runner`` package is optional and provides an additional entry point
under the alias ``cwl-runner``, which is the implementation-agnostic name for the
default CWL interpreter installed on a host.

Install
-------

Your operating system may offer cwltool directly. For [Debian](https://tracker.debian.org/pkg/cwltool "Debian cwltool package tracker") or [Ubuntu](https://launchpad.net/ubuntu/+source/cwltool "Ubuntu Launchpad overview for cwltool") try

.. code:: bash

  apt-get install cwltool

Otherwise, to
avoid conflicting versions of the same library,
it is recommended to do the following:

.. code:: bash

  virtualenv -p python2 venv   # Create a virtual environment, can use `python3` as well
  source venv/bin/activate     # Activate environment before installing `cwltool`

Installing the official package from PyPi (will install "cwltool" package as
well)

.. code:: bash

  pip install cwlref-runner

If installing alongside another CWL implementation then

.. code:: bash

  pip install cwltool

Or you can install from source:

.. code:: bash

  git clone https://github.com/common-workflow-language/cwltool.git # clone cwltool repo
  cd cwltool         # Switch to source directory
  pip install .      # Install `cwltool` from source
  cwltool --version  # Check if the installation works correctly

Remember, if co-installing multiple CWL implementations then you need to
maintain which implementation ``cwl-runner`` points to via a symbolic file
system link or `another facility <https://wiki.debian.org/DebianAlternatives>`_.

=====
Usage
=====

Run on the command line
-----------------------

Simple command::

  cwl-runner [tool-or-workflow-description] [input-job-settings]

Or if you have multiple CWL implementations installed and you want to override
the default cwl-runner use::

  cwltool [tool-or-workflow-description] [input-job-settings]

Use with boot2docker
--------------------
boot2docker runs Docker inside a virtual machine and it only mounts ``Users``
on it. The default behavior of CWL is to create temporary directories under e.g.
``/Var`` which is not accessible to Docker containers.

To run CWL successfully with boot2docker you need to set the ``--tmpdir-prefix``
and ``--tmp-outdir-prefix`` to somewhere under ``/Users``::

    $ cwl-runner --tmp-outdir-prefix=/Users/username/project --tmpdir-prefix=/Users/username/project wc-tool.cwl wc-job.json

Using user-space replacements for Docker
----------------------------------------

Some shared computing environments don't support Docker software containers for technical or policy reasons.
As a work around, the CWL reference runner supports using alternative ``docker`` implementations on Linux
with the ``--user-space-docker-cmd`` option.

One such "user space" friendly docker replacement is ``udocker`` https://github.com/indigo-dc/udocker and another
is ``dx-docker`` https://wiki.dnanexus.com/Developer-Tutorials/Using-Docker-Images

udocker installation: https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#22-install-from-indigo-datacloud-repositories

dx-docker installation: start with the DNAnexus toolkit (see https://wiki.dnanexus.com/Downloads for instructions).

Run `cwltool` just as you normally would, but with the new option, e.g. from the conformance tests:

.. code:: bash

  cwltool --user-space-docker-cmd=udocker https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/v1.0/test-cwl-out2.cwl https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/empty.json

or

.. code:: bash

  cwltool --user-space-docker-cmd=dx-docker https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/v1.0/test-cwl-out2.cwl https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/empty.json

``cwltool`` can use `Singularity <http://singularity.lbl.gov/>`_ version 2.6.1
or later as a Docker container runtime.
``cwltool`` with Singularity will run software containers specified in
``DockerRequirement`` and therefore works with Docker images only, native
Singularity images are not supported. To use Singularity as the Docker container
runtime, provide ``--singularity`` command line option to ``cwltool``.
With Singularity, ``cwltool`` can pass all CWL v1.0 conformance tests, except
those involving Docker container ENTRYPOINTs.


.. code:: bash

  cwltool --singularity https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/v1.0/v1.0/cat3-tool-mediumcut.cwl https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/cat-job.json

Running a tool or workflow from remote or local locations
---------------------------------------------------------

``cwltool`` can run tool and workflow descriptions on both local and remote
systems via its support for HTTP[S] URLs.

Input job files and Workflow steps (via the `run` directive) can reference CWL
documents using absolute or relative local filesytem paths. If a relative path
is referenced and that document isn't found in the current directory then the
following locations will be searched:
http://www.commonwl.org/v1.0/CommandLineTool.html#Discovering_CWL_documents_on_a_local_filesystem

You can also use `cwldep <https://github.com/common-workflow-language/cwldep>`
to manage dependencies on external tools and workflows.

Overriding workflow requirements at load time
---------------------------------------------

Sometimes a workflow needs additional requirements to run in a particular
environment or with a particular dataset.  To avoid the need to modify the
underlying workflow, cwltool supports requirement "overrides".

The format of the "overrides" object is a mapping of item identifier (workflow,
workflow step, or command line tool) to the process requirements that should be applied.

.. code:: yaml

  cwltool:overrides:
    echo.cwl:
      requirements:
        EnvVarRequirement:
          envDef:
            MESSAGE: override_value

Overrides can be specified either on the command line, or as part of the job
input document.  Workflow steps are identified using the name of the workflow
file followed by the step name as a document fragment identifier "#id".
Override identifiers are relative to the toplevel workflow document.

.. code:: bash

  cwltool --overrides overrides.yml my-tool.cwl my-job.yml

.. code:: yaml

  input_parameter1: value1
  input_parameter2: value2
  cwltool:overrides:
    workflow.cwl#step1:
      requirements:
        EnvVarRequirement:
          envDef:
            MESSAGE: override_value

.. code:: bash

  cwltool my-tool.cwl my-job-with-overrides.yml


Combining parts of a workflow into a single document
----------------------------------------------------

Use ``--pack`` to combine a workflow made up of multiple files into a
single compound document.  This operation takes all the CWL files
referenced by a workflow and builds a new CWL document with all
Process objects (CommandLineTool and Workflow) in a list in the
``$graph`` field.  Cross references (such as ``run:`` and ``source:``
fields) are updated to internal references within the new packed
document.  The top level workflow is named ``#main``.

.. code:: bash

  cwltool --pack my-wf.cwl > my-packed-wf.cwl


Running only part of a workflow
-------------------------------

You can run a partial workflow with the ``--target`` (``-t``) option.  This
takes the name of an output parameter, workflow step, or input
parameter in the top level workflow.  You may provide multiple
targets.

.. code:: bash

  cwltool --target step3 my-wf.cwl

If a target is an output parameter, it will only run only the steps
that contribute to that output.  If a target is a workflow step, it
will run the workflow starting from that step.  If a target is an
input parameter, it will only run only the steps that are connected to
that input.

Use ``--print-targets`` to get a listing of the targets of a workflow.
To see exactly which steps will run, use ``--print-subgraph`` with
``--target`` to get a printout of the workflow subgraph for the
selected targets.

.. code:: bash

  cwltool --print-targets my-wf.cwl

  cwltool --target step3 --print-subgraph my-wf.cwl > my-wf-starting-from-step3.cwl


Visualizing a CWL document
--------------------------

The ``--print-dot`` option will print a file suitable for Graphviz ``dot`` program.  Here is a bash onliner to generate a Scalable Vector Graphic (SVG) file:

.. code:: bash

  cwltool --print-dot my-wf.cwl | dot -Tsvg > my-wf.svg

Modeling a CWL document as RDF
------------------------------

CWL documents can be expressed as RDF triple graphs.

.. code:: bash

  cwltool --print-rdf --rdf-serializer=turtle mywf.cwl


Leveraging SoftwareRequirements (Beta)
--------------------------------------

CWL tools may be decorated with ``SoftwareRequirement`` hints that cwltool
may in turn use to resolve to packages in various package managers or
dependency management systems such as `Environment Modules
<http://modules.sourceforge.net/>`__.

Utilizing ``SoftwareRequirement`` hints using cwltool requires an optional
dependency, for this reason be sure to use specify the ``deps`` modifier when
installing cwltool. For instance::

  $ pip install 'cwltool[deps]'

Installing cwltool in this fashion enables several new command line options.
The most general of these options is ``--beta-dependency-resolvers-configuration``.
This option allows one to specify a dependency resolver's configuration file.
This file may be specified as either XML or YAML and very simply describes various
plugins to enable to "resolve" ``SoftwareRequirement`` dependencies.

To discuss some of these plugins and how to configure them, first consider the
following ``hint`` definition for an example CWL tool.

.. code:: yaml

  SoftwareRequirement:
    packages:
    - package: seqtk
      version:
      - r93

Now imagine deploying cwltool on a cluster with Software Modules installed
and that a ``seqtk`` module is available at version ``r93``. This means cluster
users likely won't have the binary ``seqtk`` on their ``PATH`` by default, but after
sourcing this module with the command ``modulecmd sh load seqtk/r93`` ``seqtk`` is
available on the ``PATH``. A simple dependency resolvers configuration file, called
``dependency-resolvers-conf.yml`` for instance, that would enable cwltool to source
the correct module environment before executing the above tool would simply be:

.. code:: yaml

  - type: modules

The outer list indicates that one plugin is being enabled, the plugin parameters are
defined as a dictionary for this one list item. There is only one required parameter
for the plugin above, this is ``type`` and defines the plugin type. This parameter
is required for all plugins. The available plugins and the parameters
available for each are documented (incompletely) `here
<https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html>`__.
Unfortunately, this documentation is in the context of Galaxy tool
``requirement`` s instead of CWL ``SoftwareRequirement`` s, but the concepts map fairly directly.

cwltool is distributed with an example of such seqtk tool and sample corresponding
job. It could executed from the cwltool root using a dependency resolvers
configuration file such as the above one using the command::

  cwltool --beta-dependency-resolvers-configuration /path/to/dependency-resolvers-conf.yml \
      tests/seqtk_seq.cwl \
      tests/seqtk_seq_job.json

This example demonstrates both that cwltool can leverage
existing software installations and also handle workflows with dependencies
on different versions of the same software and libraries. However the above
example does require an existing module setup so it is impossible to test this example
"out of the box" with cwltool. For a more isolated test that demonstrates all
the same concepts - the resolver plugin type ``galaxy_packages`` can be used.

"Galaxy packages" are a lighter weight alternative to Environment Modules that are
really just defined by a way to lay out directories into packages and versions
to find little scripts that are sourced to modify the environment. They have
been used for years in Galaxy community to adapt Galaxy tools to cluster
environments but require neither knowledge of Galaxy nor any special tools to
setup. These should work just fine for CWL tools.

The cwltool source code repository's test directory is setup with a very simple
directory that defines a set of "Galaxy  packages" (but really just defines one
package named ``random-lines``). The directory layout is simply::

  tests/test_deps_env/
    random-lines/
      1.0/
        env.sh

If the ``galaxy_packages`` plugin is enabled and pointed at the
``tests/test_deps_env`` directory in cwltool's root and a ``SoftwareRequirement``
such as the following is encountered.

.. code:: yaml

  hints:
    SoftwareRequirement:
      packages:
      - package: 'random-lines'
        version:
        - '1.0'

Then cwltool will simply find that ``env.sh`` file and source it before executing
the corresponding tool. That ``env.sh`` script is only responsible for modifying
the job's ``PATH`` to add the required binaries.

This is a full example that works since resolving "Galaxy packages" has no
external requirements. Try it out by executing the following command from cwltool's
root directory::

  cwltool --beta-dependency-resolvers-configuration tests/test_deps_env_resolvers_conf.yml \
      tests/random_lines.cwl \
      tests/random_lines_job.json

The resolvers configuration file in the above example was simply:

.. code:: yaml

  - type: galaxy_packages
    base_path: ./tests/test_deps_env

It is possible that the ``SoftwareRequirement`` s in a given CWL tool will not
match the module names for a given cluster. Such requirements can be re-mapped
to specific deployed packages and/or versions using another file specified using
the resolver plugin parameter `mapping_files`. We will
demonstrate this using `galaxy_packages` but the concepts apply equally well
to Environment Modules or Conda packages (described below) for instance.

So consider the resolvers configuration file
(`tests/test_deps_env_resolvers_conf_rewrite.yml`):

.. code:: yaml

  - type: galaxy_packages
    base_path: ./tests/test_deps_env
    mapping_files: ./tests/test_deps_mapping.yml

And the corresponding mapping configuraiton file (`tests/test_deps_mapping.yml`):

.. code:: yaml

  - from:
      name: randomLines
      version: 1.0.0-rc1
    to:
      name: random-lines
      version: '1.0'

This is saying if cwltool encounters a requirement of ``randomLines`` at version
``1.0.0-rc1`` in a tool, to rewrite to our specific plugin as ``random-lines`` at
version ``1.0``. cwltool has such a test tool called ``random_lines_mapping.cwl``
that contains such a source ``SoftwareRequirement``. To try out this example with
mapping, execute the following command from the cwltool root directory::

  cwltool --beta-dependency-resolvers-configuration tests/test_deps_env_resolvers_conf_rewrite.yml \
      tests/random_lines_mapping.cwl \
      tests/random_lines_job.json

The previous examples demonstrated leveraging existing infrastructure to
provide requirements for CWL tools. If instead a real package manager is used
cwltool has the oppertunity to install requirements as needed. While initial
support for Homebrew/Linuxbrew plugins is available, the most developed such
plugin is for the `Conda <https://conda.io/docs/#>`__ package manager. Conda has the nice properties
of allowing multiple versions of a package to be installed simultaneously,
not requiring evalated permissions to install Conda itself or packages using
Conda, and being cross platform. For these reasons, cwltool may run as a normal
user, install its own Conda environment and manage multiple versions of Conda packages
on both Linux and Mac OS X.

The Conda plugin can be endlessly configured, but a sensible set of defaults
that has proven a powerful stack for dependency management within the Galaxy tool
development ecosystem can be enabled by simply passing cwltool the
``--beta-conda-dependencies`` flag.

With this we can use the seqtk example above without Docker and without
any externally managed services - cwltool should install everything it needs
and create an environment for the tool. Try it out with the follwing command::

  cwltool --beta-conda-dependencies tests/seqtk_seq.cwl tests/seqtk_seq_job.json

The CWL specification allows URIs to be attached to ``SoftwareRequirement`` s
that allow disambiguation of package names. If the mapping files described above
allow deployers to adapt tools to their infrastructure, this mechanism allows
tools to adapt their requirements to multiple package managers. To demonstrate
this within the context of the seqtk, we can simply break the package name we
use and then specify a specific Conda package as follows:

.. code:: yaml

  hints:
    SoftwareRequirement:
      packages:
      - package: seqtk_seq
        version:
        - '1.2'
        specs:
        - https://anaconda.org/bioconda/seqtk
        - https://packages.debian.org/sid/seqtk

The example can be executed using the command::

  cwltool --beta-conda-dependencies tests/seqtk_seq_wrong_name.cwl tests/seqtk_seq_job.json

The plugin framework for managing resolution of these software requirements
as maintained as part of `galaxy-lib <https://github.com/galaxyproject/galaxy-lib>`__ - a small, portable subset of the Galaxy
project. More information on configuration and implementation can be found
at the following links:

- `Dependency Resolvers in Galaxy <https://docs.galaxyproject.org/en/latest/admin/dependency_resolvers.html>`__
- `Conda for [Galaxy] Tool Dependencies <https://docs.galaxyproject.org/en/latest/admin/conda_faq.html>`__
- `Mapping Files - Implementation <https://github.com/galaxyproject/galaxy/commit/495802d229967771df5b64a2f79b88a0eaf00edb>`__
- `Specifications - Implementation <https://github.com/galaxyproject/galaxy/commit/81d71d2e740ee07754785306e4448f8425f890bc>`__
- `Initial cwltool Integration Pull Request <https://github.com/common-workflow-language/cwltool/pull/214>`__

Use with GA4GH Tool Registry API
--------------------------------

Cwltool can launch tools directly from `GA4GH Tool Registry API`_ endpoints.

By default, cwltool searches https://dockstore.org/ .  Use ``--add-tool-registry`` to add other registries to the search path.

For example ::

  cwltool quay.io/collaboratory/dockstore-tool-bamstats:develop test.json

and (defaults to latest when a version is not specified) ::

  cwltool quay.io/collaboratory/dockstore-tool-bamstats test.json

For this example, grab the test.json (and input file) from https://github.com/CancerCollaboratory/dockstore-tool-bamstats ::

  wget https://dockstore.org/api/api/ga4gh/v2/tools/quay.io%2Fbriandoconnor%2Fdockstore-tool-bamstats/versions/develop/PLAIN-CWL/descriptor/test.json
  wget https://github.com/CancerCollaboratory/dockstore-tool-bamstats/raw/develop/rna.SRR948778.bam


.. _`GA4GH Tool Registry API`: https://github.com/ga4gh/tool-registry-schemas

===========
Development
===========

Running tests locally
---------------------

-  Running basic tests ``(/tests)``:

To run the basic tests after installing `cwltool` execute the following:

.. code:: bash

  pip install -rtest-requirements.txt
  py.test --ignore cwltool/schemas/ --pyarg cwltool

To run various tests in all supported Python environments we use `tox <https://github.com/common-workflow-language/cwltool/tree/master/tox.ini>`_. To run the test suite in all supported Python environments
first downloading the complete code repository (see the ``git clone`` instructions above) and then run
the following in the terminal:
``pip install tox; tox``

List of all environment can be seen using:
``tox --listenvs``
and running a specfic test env using:
``tox -e <env name>``
and additionally run a specific test using this format:
``tox -e py36-unit -- tests/test_examples.py::TestParamMatching``

-  Running the entire suite of CWL conformance tests:

The GitHub repository for the CWL specifications contains a script that tests a CWL
implementation against a wide array of valid CWL files using the `cwltest <https://github.com/common-workflow-language/cwltest>`_
program

Instructions for running these tests can be found in the Common Workflow Language Specification repository at https://github.com/common-workflow-language/common-workflow-language/blob/master/CONFORMANCE_TESTS.md

Import as a module
------------------

Add

.. code:: python

  import cwltool

to your script.

The easiest way to use cwltool to run a tool or workflow from Python is to use a Factory

.. code:: python

  import cwltool.factory
  fac = cwltool.factory.Factory()

  echo = fac.make("echo.cwl")
  result = echo(inp="foo")

  # result["out"] == "foo"


CWL Tool Control Flow
---------------------

Technical outline of how cwltool works internally, for maintainers.

#. Use CWL ``load_tool()`` to load document.

   #. Fetches the document from file or URL
   #. Applies preprocessing (syntax/identifier expansion and normalization)
   #. Validates the document based on cwlVersion
   #. If necessary, updates the document to latest spec
   #. Constructs a Process object using ``make_tool()``` callback.  This yields a
      CommandLineTool, Workflow, or ExpressionTool.  For workflows, this
      recursively constructs each workflow step.
   #. To construct custom types for CommandLineTool, Workflow, or
      ExpressionTool, provide a custom ``make_tool()``

#. Iterate on the ``job()`` method of the Process object to get back runnable jobs.

   #. ``job()`` is a generator method (uses the Python iterator protocol)
   #. Each time the ``job()`` method is invoked in an iteration, it returns one
      of: a runnable item (an object with a ``run()`` method), ``None`` (indicating
      there is currently no work ready to run) or end of iteration (indicating
      the process is complete.)
   #. Invoke the runnable item by calling ``run()``.  This runs the tool and gets output.
   #. Output of a process is reported by an output callback.
   #. ``job()`` may be iterated over multiple times.  It will yield all the work
      that is currently ready to run and then yield None.

#. ``Workflow`` objects create a corresponding ``WorkflowJob`` and ``WorkflowJobStep`` objects to hold the workflow state for the duration of the job invocation.

   #. The WorkflowJob iterates over each WorkflowJobStep and determines if the
      inputs the step are ready.
   #. When a step is ready, it constructs an input object for that step and
      iterates on the ``job()`` method of the workflow job step.
   #. Each runnable item is yielded back up to top level run loop
   #. When a step job completes and receives an output callback, the
      job outputs are assigned to the output of the workflow step.
   #. When all steps are complete, the intermediate files are moved to a final
      workflow output, intermediate directories are deleted, and the output
      callback for the workflow is called.

#. ``CommandLineTool`` job() objects yield a single runnable object.

   #. The CommandLineTool ``job()`` method calls ``make_job_runner()`` to create a
      ``CommandLineJob`` object
   #. The job method configures the CommandLineJob object by setting public
      attributes
   #. The job method iterates over file and directories inputs to the
      CommandLineTool and creates a "path map".
   #. Files are mapped from their "resolved" location to a "target" path where
      they will appear at tool invocation (for example, a location inside a
      Docker container.)  The target paths are used on the command line.
   #. Files are staged to targets paths using either Docker volume binds (when
      using containers) or symlinks (if not).  This staging step enables files
      to be logically rearranged or renamed independent of their source layout.
   #. The ``run()`` method of CommandLineJob executes the command line tool or
      Docker container, waits for it to complete, collects output, and makes
      the output callback.


Extension points
----------------

The following functions can be passed to main() to override or augment
the listed behaviors.

executor
  ::

    executor(tool, job_order_object, runtimeContext, logger)
      (Process, Dict[Text, Any], RuntimeContext) -> Tuple[Dict[Text, Any], Text]

  An implementation of the toplevel workflow execution loop, should
  synchronously run a process object to completion and return the
  output object.

versionfunc
  ::

    ()
      () -> Text

  Return version string.

logger_handler
  ::

    logger_handler
      logging.Handler

  Handler object for logging.

The following functions can be set in LoadingContext to override or
augment the listed behaviors.

fetcher_constructor
  ::

    fetcher_constructor(cache, session)
      (Dict[unicode, unicode], requests.sessions.Session) -> Fetcher

  Construct a Fetcher object with the supplied cache and HTTP session.

resolver
  ::

    resolver(document_loader, document)
      (Loader, Union[Text, dict[Text, Any]]) -> Text

  Resolve a relative document identifier to an absolute one which can be fetched.

The following functions can be set in RuntimeContext to override or
augment the listed behaviors.

construct_tool_object
  ::

    construct_tool_object(toolpath_object, loadingContext)
      (MutableMapping[Text, Any], LoadingContext) -> Process

  Hook to construct a Process object (eg CommandLineTool) object from a document.

select_resources
  ::

    selectResources(request)
      (Dict[str, int], RuntimeContext) -> Dict[Text, int]

  Take a resource request and turn it into a concrete resource assignment.

make_fs_access
  ::

    make_fs_access(basedir)
      (Text) -> StdFsAccess

  Return a file system access object.

In addition, when providing custom subclasses of Process objects, you can override the following methods:

CommandLineTool.make_job_runner
  ::

    make_job_runner(RuntimeContext)
      (RuntimeContext) -> Type[JobBase]

  Create and return a job runner object (this implements concrete execution of a command line tool).

Workflow.make_workflow_step
  ::

    make_workflow_step(toolpath_object, pos, loadingContext, parentworkflowProv)
      (Dict[Text, Any], int, LoadingContext, Optional[ProvenanceProfile]) -> WorkflowStep

  Create and return a workflow step object.



  * [cx_Freeze-5.1.1](https://anthony-tuininga.github.io/cx_Freeze) create standalone executables from Python scripts
  * [cytoolz-0.10.0](https://github.com/pytoolz/cytoolz) CyToolz
=======

|Build Status| |Version Status|

Cython implementation of the
|literal toolz|_ `package, <https://pypi.python.org/pypi/toolz/>`__ which
provides high performance utility functions for iterables, functions,
and dictionaries.

.. |literal toolz| replace:: ``toolz``
.. _literal toolz: https://github.com/pytoolz/toolz

``toolz`` is a pure Python package that borrows heavily from contemporary
functional languanges.  It is designed to interoperate seamlessly with other
libraries including ``itertools``, ``functools``, and third party libraries.
High performance functional data analysis is possible with builtin types
like ``list`` and ``dict``, and user-defined data structures; and low memory
usage is achieved by using the iterator protocol and returning iterators
whenever possible.

``cytoolz`` implements the same API as ``toolz``.  The main differences are
that ``cytoolz`` is faster (typically 2-5x faster with a few spectacular
exceptions) and ``cytoolz`` offers a C API that is accessible to other
projects developed in Cython.  Since ``toolz`` is able to process very
large (potentially infinite) data sets, the performance increase gained by
using ``cytoolz`` can be significant.

See the PyToolz documentation at https://toolz.readthedocs.io and the full
`API Documentation <https://toolz.readthedocs.io/en/latest/api.html>`__
for more details.

LICENSE
-------

New BSD. See `License File <https://github.com/pytoolz/cytoolz/blob/master/LICENSE.txt>`__.


Install
-------

``cytoolz`` is on the Python Package Index (PyPI):

::

    pip install cytoolz

Dependencies
------------

``cytoolz`` supports Python 2.7+ and Python 3.4+ with a common codebase.
It is developed in Cython, but requires no dependecies other than CPython
and a C compiler.  Like ``toolz``, it is a light weight dependency.

Contributions Welcome
---------------------

``toolz`` (and ``cytoolz``) aims to be a repository for utility functions,
particularly those that come from the functional programming and list
processing traditions. We welcome contributions that fall within this scope
and encourage users to scrape their ``util.py`` files for functions that are
broadly useful.

Please take a look at our issue pages for
`toolz <https://github.com/pytoolz/toolz/issues>`__ and
`cytoolz <https://github.com/pytoolz/cytoolz/issues>`__
for contribution ideas.

Community
---------

See our `mailing list <https://groups.google.com/forum/#!forum/pytoolz>`__.
We're friendly.

.. |Build Status| image:: https://travis-ci.org/pytoolz/cytoolz.svg?branch=master
   :target: https://travis-ci.org/pytoolz/cytoolz
.. |Version Status| image:: https://badge.fury.io/py/cytoolz.svg
   :target: http://badge.fury.io/py/cytoolz
  * [daemonize-2.5.0](https://github.com/thesharp/daemonize) 
  * [daiquiri-1.5.0](https://github.com/jd/daiquiri) =======================================
daiquiri -- Python logging setup helper
=======================================

.. image:: https://circleci.com/gh/jd/daiquiri.svg?style=svg
   :target: https://circleci.com/gh/jd/daiquiri

.. image:: https://img.shields.io/pypi/v/daiquiri.svg
    :target: https://pypi.python.org/pypi/daiquiri
    :alt: Latest Version

The daiquiri library provides an easy way to configure logging. It also
provides some custom formatters and handlers.

You can read the whole documentation at http://daiquiri.readthedocs.io/

* Free software: Apache license
* Source: https://github.com/jd/daiquiri


  * [darkslide-4.0.1](https://github.com/ionelmc/python-darkslide) ========
Overview
========



Lightweight markup language (Markdown, ReST, or Textile) slideshow generator. Forked from landslide.

Demo: http://ionelmc.github.io/python-darkslide/

::

    # Darkslide

    ---

    # Overview

    Generate HTML5 slideshows from markdown, ReST, or textile.

    ![python](http://i.imgur.com/bc2xk.png)

    Darkslide is primarily written in Python, but it's themes use:

    - HTML5
    - Javascript
    - CSS

    ---

    # Code Sample

    Darkslide supports code snippets

        !python
        def log(self, message, level='notice'):
            if self.logger and not callable(self.logger):
                raise ValueError(u"Invalid logger set, must be a callable")

            if self.verbose and self.logger:
                self.logger(message, level)

Requirements
============

``python`` and the following modules:

-  ``jinja2``
-  ``pygments`` for code blocks syntax coloration

Markup Conversion
-----------------

-  ``markdown`` for `Markdown <http://en.wikipedia.org/wiki/Markdown>`__
-  ``docutils`` for `reStructured
   Text <http://en.wikipedia.org/wiki/ReStructuredText>`__
-  ``textile`` for
   `Textile <http://en.wikipedia.org/wiki/Textile_(markup_language)>`__

Optional
--------

-  ``watchdog`` for watching/auto-regeneration with the ``-w`` flag

Installation
============

Install the latest stable version of Darkslide with a python package
manager like ``pip``:

::

    $ pip install darkslide

If you want to stay on the edge:

::

    $ git clone https://github.com/adamzap/landslide.git
    $ cd landslide
    $ python setup.py build
    $ sudo python setup.py install

Formatting
==========

Markdown
--------

-  Your Markdown source files must be suffixed by ``.md``, ``.markdn``,
   ``.mdwn``, ``.mdown`` or ``.markdown``
-  To create a title slide, render a single ``h1`` element (eg.
   ``# My Title``)
-  Separate your slides with a horizontal rule (``---`` in markdown)
   except at the end of md files
-  Your other slides should have a heading that renders to an ``h1``
   element
-  To highlight blocks of code, put ``!lang`` where ``lang`` is the
   pygment supported language identifier as the first indented line

ReStructuredText
----------------

-  Your ReST source files must be suffixed by ``.rst`` or ``.rest``
   (**``.txt`` is not supported**)
-  Use headings for slide titles
-  Separate your slides using an horizontal rule (``----`` in RST)
   except at the end of RST files

Textile
-------

-  Separate your slides using ``---``, just like in markdown

Rendering
=========

-  Run ``landslide slides.md`` or ``landslide slides.rst``
-  Enjoy your newly generated ``presentation.html``

Viewing
=======

-  Press ``h`` to toggle display of help
-  Press ``left arrow`` and ``right arrow`` to navigate
-  Press ``t`` to toggle a table of contents for your presentation.
   Slide titles are links
-  Press ``ESC`` to display the presentation overview (Exposé)
-  Press ``n`` to toggle slide number visibility
-  Press ``b`` to toggle screen blanking
-  Press ``c`` to toggle double slide display (current and next
   slides)
-  Press ``S`` to toggle display of link to the source file for each
   slide
-  Press '2' to toggle notes in your slides (specify with the .notes
   macro)
-  Browser zooming is *not* supported

Commandline Options
===================

Usage::

    darkslide [options] input.md ...

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -c, --copy-theme      Copy theme directory into current presentation source
                        directory.
  -b, --debug           Will display any exception trace to stdout.
  -d FILE, --destination=FILE
                        The path to the to the destination html file. Default:
                        presentation.html.
  -e ENCODING, --encoding=ENCODING
                        The encoding of your files. Default: utf8.
  -i, --embed           Embed stylesheet and javascript contents,
                        base64-encoded images and objects in presentation to
                        make a standalone document.
  -l LINENOS, --linenos=LINENOS
                        How to output linenos in source code. Three options
                        available: no (no line numbers); inline (inside <pre>
                        tag); table (lines numbers in another cell, copy-paste
                        friendly).
  -o, --direct-output   Prints the generated HTML code to stdout.
  -P, --no-presenter-notes
                        Don't include presenter notes in the output.
  -q, --quiet           Won't write anything to stdout (silent mode).
  -r, --relative        Make your presentation asset links relative to current
                        working dir; This may be useful if you intend to
                        publish your html presentation online.
  -t THEME, --theme=THEME
                        A theme name, or path to a landlside theme directory
  -v, --verbose         Write informational messages to stdout (enabled by
                        default).
  -x EXTENSIONS, --extensions=EXTENSIONS
                        Comma-separated list of extensions for Markdown.
  -w, --watch           Watch source directory for changes and regenerate
                        slides.

Presentation Configuration
==========================

Darkslide allows to configure your presentation using a ``cfg``
configuration file, therefore easing the aggregation of source
directories and the reuse of them across presentations. Darkslide
configuration files use the ``cfg`` syntax. If you know ``ini`` files,
you get the picture. Below is a sample configuration file:

.. code-block:: ini

    [darkslide]
    ; the old [landslide] is still supported
    theme  = /path/to/my/beautiful/theme
    source = 0_my_first_slides.md
             a_directory
             another_directory
             now_a_slide.markdown
             another_one.rst
    destination = myWonderfulPresentation.html
    css =    my_first_stylesheet.css
             my_other_stylesheet.css
    js =     jquery.js
             my_fancy_javascript.js
    relative = True
    linenos = inline

Don't forget to declare the ``[darkslide]`` section. All configuration
files must end in the .cfg extension.

To generate the presentation as configured, just run:

::

    $ cd /path/to/my/presentation/sources
    $ darkslide config.cfg

Macros
======

You can use macros to enhance your presentation:

Notes
-----

Add notes to your slides using the ``.notes:`` keyword, eg.:

::

    # My Slide Title

    .notes: These are my notes, hidden by default

    My visible content goes here

You can toggle display of notes by pressing the ``2`` key.

Some other macros are also available by default: ``.fx: foo bar`` will
add the ``foo`` and ``bar`` classes to the corresponding slide ``<div>``
element, easing styling of your presentation using CSS.

QR Codes
--------

Add a QR Code to your presentation by using the ``.qr`` keyword:

::

    .qr: 450|https://github.com/ionelmc/python-darkslide

Footnote
--------

Add footnote to the current and all the following presentations

::

    .footnote: Slides available at https://blog.ionelmc.ro/presentations/


Presenter Notes
===============

You can also add presenter notes to each slide by following the slide
content with a heading entitled "Presenter Notes". Press the 'p' key to
open the presenter view.

Registering Macros
==================

Macros are used to transform the HTML contents of your slide.

You can register your own macros by creating ``darkslide.macro.Macro``
derived classes, implementing a ``process(content, source=None)`` method
and returning a tuple containing the modified contents and some css
classes you may be wanting to add to your slide ``<div>`` element. For
example:

::

    !python
    import darkslide

    class MyMacro(darkslide.Macro):
      def process(self, content, source=None):
        return content + '<p>plop</p>', ['plopped_slide']

    g = darkslide.generator.Generator(source='toto.md')
    g.register_macro(MyMacro)
    print g.render()

This will render any slide as below:

::

    !html
    <div class="slide plopped_slide">
      <header><h2>foo</h2></header>
      <section>
        <p>my slide contents</p>
        <p>plop</p>
      </section>
    </div>

Advanced Usage
==============

Setting Custom Destination File
-------------------------------

::

    $ darkslide slides.md -d ~/MyPresentations/presentation.html

Working with Directories
------------------------

::

    $ darkslide slides/

Working with Direct Output
--------------------------

::

    $ darkslide slides.md -o | tidy

Using an Alternate Darkslide Theme
----------------------------------

::

    $ darkslide slides.md -t mytheme
    $ darkslide slides.md -t /path/to/theme/dir

Embedding Base-64-Encoded Images
--------------------------------

::

    $ darkslide slides.md -i

Enabling Markdown Extensions
----------------------------

See documentation on available Markdown extensions
`here <https://pythonhosted.org/Markdown/extensions/index.html>`__:

::

    $ darkslide slides.md -x abbr

Theming
-------

A Darkslide theme is a directory following this simple structure:

::

    mytheme/
    |-- base.html
    |-- css
    |   |-- print.css
    |   `-- screen.css
    `-- js
        `-- slides.js

If a theme does not provide HTML and JS files, those from the default
theme will be used. CSS is not optional.

Last, you can also copy the whole theme directory to your presentation
one by passing the ``--copy-theme`` option to the ``darkslide`` command:

::

    $ darkslide slides.md -t /path/to/some/theme --copy-theme

User stylesheets and Javascripts
================================

If you don't want to bother making your own theme, you can include your
own user css and js files to the generated presentation.

This feature is only available if you use a Darkslide configuration
file, by setting the ``css`` and/or ``js`` flags:

::

    [darkslide]
    ; the old [landslide] is still supported
    theme  = /path/to/my/beautiful/theme
    source = slides.mdown
    css =    custom.css
    js =     jquery.js
             powerpoint.js

These will link the ``custom.css`` stylesheet and both the ``jquery.js``
and ``powerpoint.js`` files within the ``<head>`` section of the
presentation html file.

**NOTE:** Paths to the css and js files must be relative to the
directory you're running the ``darkslide`` command from.

Publishing your Presentation Online
===================================

If you intend to publish your HTML presentation online, you'll have to
use the ``--relative`` option, as well as the ``--copy-theme`` one to
have all asset links relative to the root of your presentation;

::

    $ darkslide slides.md --relative --copy-theme

That way, you'll just have to host the whole presentation directory to a
webserver. Of course, no Python nor PHP nor anything else than a HTTP
webserver (like Apache) is required to host a Darkslide presentation.

`Here's an example <http://www.akei.com/presentations/2011-Djangocong/index.html>`__.

Theme Variables
===============

The ``base.html`` must be a `Jinja2 template
file <http://jinja.pocoo.org/2/documentation/templates>`__ where you can
harness the following template variables:

-  ``css``: the stylesheet contents, available via two keys, ``print``
   and ``screen``, both having:
-  a ``path_url`` key storing the url to the asset file path
-  a ``contents`` key storing the asset contents
-  ``js``: the javascript contents, having:
-  a ``path_url`` key storing the url to the asset file path
-  a ``contents`` key storing the asset contents
-  ``slides``: the slides list, each one having these properties:
-  ``header``: the slide title
-  ``content``: the slide contents
-  ``number``: the slide number
-  ``embed``: is the current document a standalone one?
-  ``num_slides``: the number of slides in current presentation
-  ``toc``: the Table of Contents, listing sections of the document.
   Each section has these properties available:
-  ``title``: the section title
-  ``number``: the slide number of the section
-  ``sub``: subsections, if any

Styles Scope
============

-  To change HTML5 presentation styles, tweak the ``css/screen.css``
   stylesheet bundled with the theme you are using
-  For printing, modify the ``css/print.css``

Authors
=======

The project was originally named Landslide and was authored by
Adam Zapletal (adamzap@gmail.com) and Nicolas Perriault (nperriault@gmail.com)

Slide code is based on html5-slides.

More details: https://github.com/ionelmc/python-darkslide/contributors

=========
Changelog
=========

Darkslide v5.0.1 (2019-10-01)
=============================

* Fixed media for user css to be always be ``all``. Previously it was ``screen, projection`` if embedded.

Darkslide v5.0.0 (2019-09-29)
=============================

* Removed PDF export support. You should just use the PDF export from
  Google Chrome (it works way better than the alternatives).
* Fixed transitions in presenter mode.
* Added support for Up/Down arrow navigation.
  Contributed by Heiko Schlittermann in `#13 <https://github.com/ionelmc/python-darkslide/pull/13>`_.
* Added support for Markdown 3.0+ and Textile 2.3+.
* Changed the broken ``.notes:`` macro to output presenter notes.

Darkslide v4.0.1 (2017-10-19)
=============================

* Fixed print css a bit.
* Fixed missing scrolling to current when changing slides while in overview mode.

Darkslide v4.0.0 (2017-10-17)
=============================

* Dropped MathJax support. Something less to maintain (also, didn't work as expected with ``--embed``). User that need this
  should just use the ``user_js`` option. Or a custom theme.
* Changed themes to use a space-adjusted Alegreya Sans as a fallback.

Darkslide v3.2.0 (2017-10-17)
=============================

* Changed themes to use Rosario as a fallback. For better or worse it's smaller and has same width as Candara.


Darkslide v3.1.0 (2017-10-17)
=============================

* Changed themes to embed a Candara fallback webfont (Alegreya Sans). It's slightly narrower but looks more similar than the other
  alternatives better matching Candara's width (Acme, Galdeano). It even has ligatures.

Darkslide v3.0.1 (2017-10-15)
=============================

* Fixed slightly broken slide class changing.
* Made expose mode scroll to current slide.
* Running presenter mode with no target won't break
  anymore if target window is gone.
* Fixed display of presenter notes.

Darkslide v3.0.0 (2017-10-05)
=============================

* Removed "expanded mode". It was too buggy and doesn't really have a purpose.
* Changed "show context" to be "show next slide" (so two slides at a time). This is way more useful than showing little
  bits of next and prev slides.
* Fixed ``--direct`` on Python 3.
* Fixed glitches when TOC/Help are open.
* Made possible to switch slides when TOC/Help/Overview are open.

Darkslide v2.3.3 (2016-05-15)
=============================

* Fixed height of QR svg elements.

Darkslide v2.3.2 (2016-04-12)
=============================

* Fixed underline occlusion shadows in the footer (for links).
* Fixed missing `presenter_notes` class not being set when notes mode was on.

Darkslide v2.3.1 (2016-02-08)
=============================

* MathJax is loaded on HTTPS.

Darkslide v2.3.0 (2016-02-07)
=============================

* The Darkslide version is shown in the help sidebar.

Darkslide v2.2.1 (2015-10-06)
=============================

* Fixed config file parsing for math_output.

Darkslide v2.2.0 (2015-10-06)
=============================

* Now macro failures abort rendering. Previously they would just log a message that you'd probably woulnd't notice.
* Fixed broken handling where you have css/js in the cfg file.
* Allowed setting the math_output option in the cfg file.
* Fixed encoding issues in the QR macro.
* Added back the old theme with completely black background (as "void").
* Tweak the faux underlines to look better.

Darkslide v2.1.0 (2015-10-05)
=============================

* Added demo links.
* Fixed options handling. Options from command line now will actually work if a cfg file is used.
* Corrected relative paths handling:

  - paths in sources are now relative to the cfg file (previously they were relative to whatever was cwd).
  - relative option now correctly works when destination file is not in cwd.
* Fixed layout of slides with many headering (no more paddings for headings, all root elements are spread out evenly
  anyway).
* Fixed bad styling of ToC (and probably other things in the sidebar).
* Fixed ToC links (contributed by Cyrille Pontvieux).

Darkslide v2.0.4 (2015-09-09)
=============================

* Improved handling for filenames that have non-ascii characters in them.

Darkslide v2.0.3 (2015-09-08)
=============================

* Fixed handling for filenames that have non-ascii characters in them.

Darkslide v2.0.2 (2015-07-20)
=============================

- Added color classes in the abyss theme.
- Fixed link underlines in the presenter notes.

Darkslide v2.0.1 (2015-07-19)
=============================

* Don't use Monaco in the ``base.css`` - it's way bigger than Consolas and the other fonts. And Consolas is nice enough.

Darkslide v2.0.0 (2015-07-17)
=============================

- Fix display of RST image target links.
- Add cmd line option to print version.
- Rewrote the default theme (solarized colors)
- Overhauled the abyss theme, improved the coloring.
- Removed all the other themes (they are ugly and broken anyway) (**backwards incompatible**).
- Fixes for print css.
- Added support for two new css files: ``base.css`` and ``theme.css``. This
  makes reusing styles acros themes and kinds of display (print/screen) more easy.
- Expanded mode is now activated by default.
- Changed macros to use compiled regexes.
- Added a footnote macro.
- Changed QR macro to use ``qrcode`` library. Now it's rendered to SVG. The size is removed (**backwards incompatible**).

Darkslide v1.2.2 (2015-05-22)
=============================

- Fix the blank page issue when generating pdfs (via Chrome's pdf printer).

Darkslide v1.2.1 (2015-05-21)
=============================

- Couple minor improvements to Abyss theme.

Darkslide v1.2.0 (2015-05-19)
=============================

- Modifier keys flag was not cleared propertly (kb shortcuts were not working anymore after
  alt-tab etc); now it's cleared on visibility changes and focus loss.
- Changed expanded mode to automatically hide the context.
- Fixed window resize flickering (for every resize event the expaded flag was toggled).
- Disabled context hiding in presenter view.
- Other small styling improvements.
- Added "abyss" theme.

Landslide v1.1.3
================

-  Identify each slide by a numbered class (#171) (dkg)
-  Fix theme image embedding regex to grab all images (#170)
-  Fix blockquote font size for rst (#161)
-  Fix display of RST image target links (#87)
-  Fix relative path generation (#147)
-  Add command line option for print version (#135)
-  Add use of '---' as a slide separator to textile files (#163)
-  README improvements (#88 and #101)
-  Improve image path regex and replacement (#177)

Landslide v1.1.2
================

-  Add support for Python 3
-  Allow support for copy\_theme argument in CFG files (#139) (syscomet)
-  Improve MathJax rendering for Markdown files
-  Support math output (#144) (davidedelvento)
-  Allow presenter notes in slides with no heading in RST files (#141)
   (regebro)
-  And more...

Landslide v1.1.1
================

Fixes
-----

-  Don't accidentally require watchdog (#134)

Landslide v1.1.0
================

Major Enhancements
------------------

-  Add CHANGELOG
-  Add "ribbon" theme from "shower" presentation tool (#129) (durden)
-  Add ``-w`` flag for watching/auto-regenerating slideshow (#71, #120)
   (jondkoon)

Minor Enhancements
------------------

-  Supress ReST rendering errors
-  CSS pre enhancements (#91) (roktas)
-  Add an example using presenter notes (#106) (netantho)
-  Run macros on headers also, to embed images (#74) (godfat)
-  Allow PHP code snippets to not require <?php (#127) (akrabat)
-  Allow for line numbers and emphasis with reStructuredText (#97)
   (copelco)
-  Add an option to strip presenter notes from output (#107) (aaugustin)

Fixes
-----

-  Firefox offset bug on next slide (#73)
-  Fix base64 encoding issue (#109) (ackdesha)
-  Fix to embed images defined in CSS (#126) (akrabat)
-  Minor documentation fixes (#119, #131) (durden, spin6lock)
-  Use configured encoding when reading all embedded files (#125)
   (iguananaut)
-  Allow pygments lexer names that include special characters (#123)
   (shreyankg)



  * [dask-2.2.0](https://github.com/dask/dask/) Dask
====

|Build Status| |Coverage| |Doc Status| |Gitter| |Version Status| |NumFOCUS|

Dask is a flexible parallel computing library for analytics.  See
documentation_ for more information.


LICENSE
-------

New BSD. See `License File <https://github.com/dask/dask/blob/master/LICENSE.txt>`__.

.. _documentation: https://dask.org
.. |Build Status| image:: https://travis-ci.org/dask/dask.svg?branch=master
   :target: https://travis-ci.org/dask/dask
.. |Coverage| image:: https://coveralls.io/repos/dask/dask/badge.svg
   :target: https://coveralls.io/r/dask/dask
   :alt: Coverage status
.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest
   :target: https://dask.org
   :alt: Documentation Status
.. |Gitter| image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/dask/dask
   :target: https://gitter.im/dask/dask?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
.. |Version Status| image:: https://img.shields.io/pypi/v/dask.svg
   :target: https://pypi.python.org/pypi/dask/
.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
   :target: https://www.numfocus.org/



  * [datacache-1.1.5](https://github.com/openvax/datacache) DataCache
=========

Helpers for transparently downloading datasets

API
---

-  **fetch_file**\ (download_url, filename = *None*, decompress =
   *False*, subdir = *None*)
-  **fetch_and_transform**\ (transformed_filename, transformer, loader,
   source_filename, source_url, subdir = *None*)
-  **fetch_fasta_dict**\ (download_url, filename = *None*, subdir =
   *None*)
-  **fetch_fasta_db**\ (table_name, download_url, fasta_filename =
   *None*, key_column = *‘id’*, value_column = *‘seq’*, subdir = *None*)
-  **fetch_csv_db**\ (table_name, download_url, csv_filename = *None*,
   subdir = *None*, \**pandas_kwargs)
  * [dataclasses-0.6](https://github.com/ericvsmith/dataclasses) .. image:: https://img.shields.io/pypi/v/dataclasses.svg


This is an implementation of PEP 557, Data Classes.  It is a backport
for Python 3.6.  Because dataclasses will be included in Python 3.7,
any discussion of dataclass features should occur on the python-dev
mailing list at https://mail.python.org/mailman/listinfo/python-dev.
At this point this repo should only be used for historical purposes
(it's where the original dataclasses discussions took place) and for
discussion of the actual backport to Python 3.6.

See https://www.python.org/dev/peps/pep-0557/ for the details of how
Data Classes work.

A test file can be found at
https://github.com/ericvsmith/dataclasses/blob/master/test_dataclasses.py,
or in the sdist file.

Installation
-------------

.. code-block::

  pip install dataclasses


Example Usage
-------------

.. code-block:: python

  from dataclasses import dataclass

  @dataclass
  class InventoryItem:
      name: str
      unit_price: float
      quantity_on_hand: int = 0

      def total_cost(self) -> float:
          return self.unit_price * self.quantity_on_hand

  item = InventoryItem('hammers', 10.49, 12)
  print(item.total_cost())

Some additional tools can be found in dataclass_tools.py, included in
the sdist.

Compatibility
-------------

This backport assumes that dict objects retain their sort order.  This
is true in the language spec for Python 3.7 and greater.  Since this
is a backport to Python 3.6, it raises an interesting question: does
that guarantee apply to 3.6?  For CPython 3.6 it does.  As of the time
of this writing, it's also true for all other Python implementations
that claim to be 3.6 compatible, of which there are none.  Any new
3.6 implementations are expected to have ordered dicts.  See the
analysis at the end of this email:

https://mail.python.org/pipermail/python-dev/2017-December/151325.html

As of version 0.4, this code no longer works with Python 3.7. For 3.7,
use the built-in dataclasses module.

Release History
---------------

+---------+------------+-------------------------------------+
| Version | Date       | Description                         |
+=========+============+=====================================+
| 0.6     | 2018-05-17 | Equivalent to Python 3.7.0rc1       |
+---------+------------+-------------------------------------+
| 0.5     | 2018-03-28 | Equivalent to Python 3.7.0b3        |
+---------+------------+-------------------------------------+



  * [datrie-0.8](https://github.com/kmike/datrie) datrie |travis| |appveyor|
==========================

.. |travis| image:: https://travis-ci.org/pytries/datrie.svg
   :target: https://travis-ci.org/pytries/datrie

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/6bpvhllpjhlau7x0?svg=true
   :target: https://ci.appveyor.com/project/superbobry/datrie

Super-fast, efficiently stored Trie for Python (2.x and 3.x).
Uses `libdatrie`_.

.. _libdatrie: https://linux.thai.net/~thep/datrie/datrie.html

Installation
============

::

    pip install datrie

Usage
=====

Create a new trie capable of storing items with lower-case ascii keys::

    >>> import string
    >>> import datrie
    >>> trie = datrie.Trie(string.ascii_lowercase)

``trie`` variable is a dict-like object that can have unicode keys of
certain ranges and Python objects as values.

In addition to implementing the mapping interface, tries facilitate
finding the items for a given prefix, and vice versa, finding the
items whose keys are prefixes of a given string. As a common special
case, finding the longest-prefix item is also supported.

.. warning::

    For efficiency you must define allowed character range(s) while
    creating trie. ``datrie`` doesn't check if keys are in allowed
    ranges at runtime, so be careful! Invalid keys are OK at lookup time
    but values won't be stored correctly for such keys.

Add some values to it (datrie keys must be unicode; the examples
are for Python 2.x)::

    >>> trie[u'foo'] = 5
    >>> trie[u'foobar'] = 10
    >>> trie[u'bar'] = 'bar value'
    >>> trie.setdefault(u'foobar', 15)
    10

Check if u'foo' is in trie::

    >>> u'foo' in trie
    True

Get a value::

    >>> trie[u'foo']
    5

Find all prefixes of a word::

    >>> trie.prefixes(u'foobarbaz')
    [u'foo', u'foobar']

    >>> trie.prefix_items(u'foobarbaz')
    [(u'foo', 5), (u'foobar', 10)]

    >>> trie.iter_prefixes(u'foobarbaz')
    <generator object ...>

    >>> trie.iter_prefix_items(u'foobarbaz')
    <generator object ...>

Find the longest prefix of a word::

    >>> trie.longest_prefix(u'foo')
    u'foo'

    >>> trie.longest_prefix(u'foobarbaz')
    u'foobar'

    >>> trie.longest_prefix(u'gaz')
    KeyError: u'gaz'

    >>> trie.longest_prefix(u'gaz', default=u'vasia')
    u'vasia'

    >>> trie.longest_prefix_item(u'foobarbaz')
    (u'foobar', 10)

Check if the trie has keys with a given prefix::

    >>> trie.has_keys_with_prefix(u'fo')
    True

    >>> trie.has_keys_with_prefix(u'FO')
    False

Get all items with a given prefix from a trie::

    >>> trie.keys(u'fo')
    [u'foo', u'foobar']

    >>> trie.items(u'ba')
    [(u'bar', 'bar value')]

    >>> trie.values(u'foob')
    [10]

Get all suffixes of certain word starting with a given prefix from a trie::

    >>> trie.suffixes()
    [u'pro', u'producer', u'producers', u'product', u'production', u'productivity', u'prof']
    >>> trie.suffixes(u'prod')
    [u'ucer', u'ucers', u'uct', u'uction', u'uctivity']


Save & load a trie (values must be picklable)::

    >>> trie.save('my.trie')
    >>> trie2 = datrie.Trie.load('my.trie')



Trie and BaseTrie
=================

There are two Trie classes in datrie package: ``datrie.Trie`` and
``datrie.BaseTrie``. ``datrie.BaseTrie`` is slightly faster and uses less
memory but it can store only integer numbers -2147483648 <= x <= 2147483647.
``datrie.Trie`` is a bit slower but can store any Python object as a value.

If you don't need values or integer values are OK then use ``datrie.BaseTrie``::

    import datrie
    import string
    trie = datrie.BaseTrie(string.ascii_lowercase)

Custom iteration
================

If the built-in trie methods don't fit you can use ``datrie.State`` and
``datrie.Iterator`` to implement custom traversal.

.. note::

    If you use ``datrie.BaseTrie`` you need ``datrie.BaseState`` and
    ``datrie.BaseIterator`` for custom traversal.


For example, let's find all suffixes of ``'fo'`` for our trie and get
the values::

    >>> state = datrie.State(trie)
    >>> state.walk(u'foo')
    >>> it = datrie.Iterator(state)
    >>> while it.next():
    ...     print(it.key())
    ...     print(it.data))
    o
    5
    obar
    10

Performance
===========

Performance is measured for ``datrie.Trie`` against Python's dict with
100k unique unicode words (English and Russian) as keys and '1' numbers
as values.

``datrie.Trie`` uses about 5M memory for 100k words; Python's dict
uses about 22M for this according to my unscientific tests.

This trie implementation is 2-6 times slower than python's dict
on __getitem__. Benchmark results (macbook air i5 1.8GHz,
"1.000M ops/sec" == "1 000 000 operations per second")::

    Python 2.6:
    dict __getitem__: 7.107M ops/sec
    trie __getitem__: 2.478M ops/sec

    Python 2.7:
    dict __getitem__: 6.550M ops/sec
    trie __getitem__: 2.474M ops/sec

    Python 3.2:
    dict __getitem__: 8.185M ops/sec
    trie __getitem__: 2.684M ops/sec

    Python 3.3:
    dict __getitem__: 7.050M ops/sec
    trie __getitem__: 2.755M ops/sec

Looking for prefixes of a given word is almost as fast as
``__getitem__`` (results are for Python 3.3)::

    trie.iter_prefix_items (hits):      0.461M ops/sec
    trie.prefix_items (hits):           0.743M ops/sec
    trie.prefix_items loop (hits):      0.629M ops/sec
    trie.iter_prefixes (hits):          0.759M ops/sec
    trie.iter_prefixes (misses):        1.538M ops/sec
    trie.iter_prefixes (mixed):         1.359M ops/sec
    trie.has_keys_with_prefix (hits):   1.896M ops/sec
    trie.has_keys_with_prefix (misses): 2.590M ops/sec
    trie.longest_prefix (hits):         1.710M ops/sec
    trie.longest_prefix (misses):       1.506M ops/sec
    trie.longest_prefix (mixed):        1.520M ops/sec
    trie.longest_prefix_item (hits):    1.276M ops/sec
    trie.longest_prefix_item (misses):  1.292M ops/sec
    trie.longest_prefix_item (mixed):   1.379M ops/sec

Looking for all words starting with a given prefix is mostly limited
by overall result count (this can be improved in future because a
lot of time is spent decoding strings from utf_32_le to Python's
unicode)::

    trie.items(prefix="xxx"), avg_len(res)==415:        0.609K ops/sec
    trie.keys(prefix="xxx"), avg_len(res)==415:         0.642K ops/sec
    trie.values(prefix="xxx"), avg_len(res)==415:       4.974K ops/sec
    trie.items(prefix="xxxxx"), avg_len(res)==17:       14.781K ops/sec
    trie.keys(prefix="xxxxx"), avg_len(res)==17:        15.766K ops/sec
    trie.values(prefix="xxxxx"), avg_len(res)==17:      96.456K ops/sec
    trie.items(prefix="xxxxxxxx"), avg_len(res)==3:     75.165K ops/sec
    trie.keys(prefix="xxxxxxxx"), avg_len(res)==3:      77.225K ops/sec
    trie.values(prefix="xxxxxxxx"), avg_len(res)==3:    320.755K ops/sec
    trie.items(prefix="xxxxx..xx"), avg_len(res)==1.4:  173.591K ops/sec
    trie.keys(prefix="xxxxx..xx"), avg_len(res)==1.4:   180.678K ops/sec
    trie.values(prefix="xxxxx..xx"), avg_len(res)==1.4: 503.392K ops/sec
    trie.items(prefix="xxx"), NON_EXISTING:             2023.647K ops/sec
    trie.keys(prefix="xxx"), NON_EXISTING:              1976.928K ops/sec
    trie.values(prefix="xxx"), NON_EXISTING:            2060.372K ops/sec

Random insert time is very slow compared to dict, this is the limitation
of double-array tries; updates are quite fast. If you want to build a trie,
consider sorting keys before the insertion::

    dict __setitem__ (updates):            6.497M ops/sec
    trie __setitem__ (updates):            2.633M ops/sec
    dict __setitem__ (inserts, random):    5.808M ops/sec
    trie __setitem__ (inserts, random):    0.053M ops/sec
    dict __setitem__ (inserts, sorted):    5.749M ops/sec
    trie __setitem__ (inserts, sorted):    0.624M ops/sec
    dict setdefault (updates):             3.455M ops/sec
    trie setdefault (updates):             1.910M ops/sec
    dict setdefault (inserts):             3.466M ops/sec
    trie setdefault (inserts):             0.053M ops/sec

Other results (note that ``len(trie)`` is currently implemented
using trie traversal)::

    dict __contains__ (hits):    6.801M ops/sec
    trie __contains__ (hits):    2.816M ops/sec
    dict __contains__ (misses):  5.470M ops/sec
    trie __contains__ (misses):  4.224M ops/sec
    dict __len__:                334336.269 ops/sec
    trie __len__:                22.900 ops/sec
    dict values():               406.507 ops/sec
    trie values():               20.864 ops/sec
    dict keys():                 189.298 ops/sec
    trie keys():                 2.773 ops/sec
    dict items():                48.734 ops/sec
    trie items():                2.611 ops/sec

Please take this benchmark results with a grain of salt; this
is a very simple benchmark and may not cover your use case.

Current Limitations
===================

* keys must be unicode (no implicit conversion for byte strings
  under Python 2.x, sorry);
* there are no iterator versions of keys/values/items (this is not
  implemented yet);
* it is painfully slow and maybe buggy under pypy;
* library is not tested with narrow Python builds.

Contributing
============

Development happens at github: https://github.com/pytries/datrie.

Feel free to submit ideas, bugs, pull requests.

Running tests and benchmarks
----------------------------

Make sure `tox`_ is installed and run

::

    $ tox

from the source checkout. Tests should pass under Python 2.7 and 3.4+.

::

    $ tox -c tox-bench.ini

runs benchmarks.

If you've changed anything in the source code then
make sure `cython`_ is installed and run

::

    $ update_c.sh

before each ``tox`` command.

Please note that benchmarks are not included in the release
tar.gz's because benchmark data is large and this
saves a lot of bandwidth; use source checkouts from
github or bitbucket for the benchmarks.

.. _cython: https://cython.org/
.. _tox: https://tox.readthedocs.io/

Authors & Contributors
----------------------

* Mikhail Korobov <kmike84@gmail.com>
* Jared Suttles
* Gabi Davar
* Ahmed T. Youssef

This module is based on `libdatrie`_ C library by Theppitak Karoonboonyanan
and is inspired by `fast_trie`_ Ruby bindings, `PyTrie`_ pure
Python implementation and `Tree::Trie`_ Perl implementation;
some docs and API ideas are borrowed from these projects.

.. _fast_trie: https://github.com/tyler/trie
.. _PyTrie: https://github.com/gsakkis/pytrie
.. _Tree::Trie: https://metacpan.org/pod/release/AVIF/Tree-Trie-1.9/Trie.pm

License
=======

Licensed under LGPL v2.1.
CHANGES
=======

0.8 (2019-07-03)
----------------
* Python 3.7 compatibility; extension is rebuilt with Cython 0.29.11.
* Trie.get function;
* Python 2.6 and 3.3 support is dropped;
* removed patch to libdatrie which is no longer required;
* testing and CI fixes.

0.7.1 (2016-03-12)
------------------

* updated the bundled C library to version 0.2.9;
* implemented ``Trie.__len__`` in terms of ``trie_enumerate``;
* rebuilt Cython wrapper with Cython 0.23.4;
* changed ``Trie`` to implement ``collections.abc.MutableMapping``;
* fixed ``Trie`` pickling, which segfaulted on Python2.X.

0.7 (2014-02-18)
----------------

* bundled libdatrie C library is updated to version 0.2.8;
* new `.suffixes()` method (thanks Ahmed T. Youssef);
* wrapper is rebuilt with Cython 0.20.1.

0.6.1 (2013-09-21)
------------------

* fixed build for Visual Studio (thanks Gabi Davar).

0.6 (2013-07-09)
----------------

* datrie is rebuilt with Cython 0.19.1;
* ``iter_prefix_values``, ``prefix_values`` and ``longest_prefix_value``
  methods for ``datrie.BaseTrie`` and ``datrie.Trie`` (thanks Jared Suttles).

0.5.1 (2013-01-30)
------------------

* Recently introduced memory leak in ``longest_prefix``
  and ``longest_prefix_item`` is fixed.

0.5 (2013-01-29)
----------------

* ``longest_prefix`` and ``longest_prefix_item`` methods are fixed;
* datrie is rebuilt with Cython 0.18;
* misleading benchmark results in README are fixed;
* State._walk is renamed to State.walk_char.

0.4.2 (2012-09-02)
------------------

* Update to latest libdatrie; this makes ``.keys()`` method a bit slower but
  removes a keys length limitation.

0.4.1 (2012-07-29)
------------------

* cPickle is used for saving/loading ``datrie.Trie`` if it is available.

0.4 (2012-07-27)
----------------

* ``libdatrie`` improvements and bugfixes, including C iterator API support;
* custom iteration support using ``datrie.State`` and ``datrie.Iterator``.
* speed improvements: ``__length__``, ``keys``, ``values`` and
  ``items`` methods should be up to 2x faster.
* keys longer than 32768 are not supported in this release.


0.3 (2012-07-21)
----------------

There are no new features or speed improvements in this release.

* ``datrie.new`` is deprecated; use ``datrie.Trie`` with the same arguments;
* small test & benchmark improvements.

0.2 (2012-07-16)
----------------

* ``datrie.Trie`` items can have any Python object as a value
  (``Trie`` from 0.1.x becomes ``datrie.BaseTrie``);
* ``longest_prefix`` and ``longest_prefix_items`` are fixed;
* ``save`` & ``load`` are rewritten;
* ``setdefault`` method.


0.1.1 (2012-07-13)
------------------

* Windows support (upstream libdatrie changes are merged);
* license is changed from LGPL v3 to LGPL v2.1 to match the libdatrie license.

0.1 (2012-07-12)
----------------

Initial release.
  * [dbf-0.98.2](https://pypi.python.org/pypi/dbf) Currently supports dBase III, Clipper, FoxPro, and Visual FoxPro tables. Text is returned as unicode, and codepage settings in tables are honored. Memos and Null fields are supported.  Documentation needs work, but author is very responsive to e-mails.

Not supported: index files (but can create tempory non-file indexes), auto-incrementing fields, and Varchar fields.

Installation:  `pip install dbf`



  * [ddt-1.2.1](https://github.com/txels/ddt) A library to multiply test cases
  * [deap-1.2.2](https://www.github.com/deap) DEAP
====

|Build status| |Download| |Join the chat at https://gitter.im/DEAP/deap|

DEAP is a novel evolutionary computation framework for rapid prototyping
and testing of ideas. It seeks to make algorithms explicit and data
structures transparent. It works in perfect harmony with parallelisation
mechanisms such as multiprocessing and
`SCOOP <https://github.com/soravux/scoop>`__.

DEAP includes the following features:

-  Genetic algorithm using any imaginable representation

   -  List, Array, Set, Dictionary, Tree, Numpy Array, etc.

-  Genetic programing using prefix trees

   -  Loosely typed, Strongly typed
   -  Automatically defined functions

-  Evolution strategies (including CMA-ES)
-  Multi-objective optimisation (NSGA-II, SPEA2, MO-CMA-ES)
-  Co-evolution (cooperative and competitive) of multiple populations
-  Parallelization of the evaluations (and more)
-  Hall of Fame of the best individuals that lived in the population
-  Checkpoints that take snapshots of a system regularly
-  Benchmarks module containing most common test functions
-  Genealogy of an evolution (that is compatible with
   `NetworkX <https://github.com/networkx/networkx>`__)
-  Examples of alternative algorithms : Particle Swarm Optimization,
   Differential Evolution, Estimation of Distribution Algorithm

Downloads
---------

Following acceptation of `PEP
438 <http://www.python.org/dev/peps/pep-0438/>`__ by the Python
community, we have moved DEAP’s source releases on
`PyPI <https://pypi.python.org>`__.

You can find the most recent releases at:
https://pypi.python.org/pypi/deap/.

Documentation
-------------

See the `DEAP User’s Guide <http://deap.readthedocs.org/>`__ for DEAP
documentation.

In order to get the tip documentation, change directory to the ``doc``
subfolder and type in ``make html``, the documentation will be under
``_build/html``. You will need `Sphinx <http://sphinx.pocoo.org>`__ to
build the documentation.

Notebooks
~~~~~~~~~

Also checkout our new `notebook
examples <https://github.com/DEAP/notebooks>`__. Using `Jupyter
notebooks <http://jupyter.org>`__ you’ll be able to navigate and execute
each block of code individually and tell what every line is doing.
Either, look at the notebooks online using the notebook viewer links at
the botom of the page or download the notebooks, navigate to the you
download directory and run

.. code:: bash

   jupyter notebook

Installation
------------

We encourage you to use easy_install or pip to install DEAP on your
system. Other installation procedure like apt-get, yum, etc. usually
provide an outdated version.

.. code:: bash

   pip install deap

The latest version can be installed with

.. code:: bash

   pip install git+https://github.com/DEAP/deap@master

If you wish to build from sources, download or clone the repository and
type

.. code:: bash

   python setup.py install

Build Status
------------

DEAP build status is available on Travis-CI
https://travis-ci.org/DEAP/deap.

Requirements
------------

The most basic features of DEAP requires Python2.6. In order to combine
the toolbox and the multiprocessing module Python2.7 is needed for its
support to pickle partial functions. CMA-ES requires Numpy, and we
recommend matplotlib for visualization of results as it is fully
compatible with DEAP’s API.

Since version 0.8, DEAP is compatible out of the box with Python 3. The
installation procedure automatically translates the source to Python 3
with 2to3.

Example
-------

The following code gives a quick overview how simple it is to implement
the Onemax problem optimization with genetic algorithm using DEAP. More
examples are provided
`here <http://deap.readthedocs.org/en/master/examples/index.html>`__.

.. code:: python

   import random
   from deap import creator, base, tools, algorithms

   creator.create("FitnessMax", base.Fitness, weights=(1.0,))
   creator.create("Individual", list, fitness=creator.FitnessMax)

   toolbox = base.Toolbox()

   toolbox.register("attr_bool", random.randint, 0, 1)
   toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=100)
   toolbox.register("population", tools.initRepeat, list, toolbox.individual)

   def evalOneMax(individual):
       return sum(individual),

   toolbox.register("evaluate", evalOneMax)
   toolbox.register("mate", tools.cxTwoPoint)
   toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
   toolbox.register("select", tools.selTournament, tournsize=3)

   population = toolbox.population(n=300)

   NGEN=40
   for gen in range(NGEN):
       offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)
       fits = toolbox.map(toolbox.evaluate, offspring)
       for fit, ind in zip(fits, offspring):
           ind.fitness.values = fit
       population = toolbox.select(offspring, k=len(population))
   top10 = tools.selBest(population, k=10)

How to cite DEAP
----------------

Authors of scientific papers including results generated using DEAP are
encouraged to cite the following paper.

.. code:: xml

   @article{DEAP_JMLR2012, 
       author    = " F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e ",
       title     = { {DEAP}: Evolutionary Algorithms Made Easy },
       pages    = { 2171--2175 },
       volume    = { 13 },
       month     = { jul },
       year      = { 2012 },
       journal   = { Journal of Machine Learning Research }
   }

Publications on DEAP
--------------------

-  François-Michel De Rainville, Félix-Antoine Fortin, Marc-André
   Gardner, Marc Parizeau and Christian Gagné, “DEAP – Enabling Nimbler
   Evolutions”, SIGEVOlution, vol. 6, no 2, pp. 17-26, February 2014.
   `Paper <http://goo.gl/tOrXTp>`__
-  Félix-Antoine Fortin, François-Michel De Rainville, Marc-André
   Gardner, Marc Parizeau and Christian Gagné, “DEAP: Evolutionary
   Algorithms Made Easy”, Journal of Machine Learning Research, vol. 13,
   pp. 2171-2175, jul 2012. `Paper <http://goo.gl/amJ3x>`__
-  François-Michel De Rainville, Félix-Antoine Fortin, Marc-André
   Gardner, Marc Parizeau and Christian Gagné, “DEAP: A Python Framework
   for Evolutionary Algorithms”, in !EvoSoft Workshop, Companion proc.
   of the Genetic and Evolutionary Computation Conference (GECCO 2012),
   July 07-11 2012. `Paper <http://goo.gl/pXXug>`__

Projects using DEAP
-------------------

-  Ribaric, T., & Houghten, S. (2017, June). Genetic programming for
   improved cryptanalysis of elliptic curve cryptosystems. In 2017 IEEE
   Congress on Evolutionary Computation (CEC) (pp. 419-426). IEEE.
-  Ellefsen, Kai Olav, Herman Augusto Lepikson, and Jan C. Albiez.
   “Multiobjective coverage path planning: Enabling automated inspection
   of complex, real-world structures.” Applied Soft Computing 61 (2017):
   264-282.
-  S. Chardon, B. Brangeon, E. Bozonnet, C. Inard (2016), Construction
   cost and energy performance of single family houses : From integrated
   design to automated optimization, Automation in Construction, Volume
   70, p.1-13.
-  B. Brangeon, E. Bozonnet, C. Inard (2016), Integrated refurbishment
   of collective housing and optimization process with real products
   databases, Building Simulation Optimization, pp. 531–538 Newcastle,
   England.
-  Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A.
   Lavender, La Creis Kidd, and Jason H. Moore (2016). Automating
   biomedical data science through tree-based pipeline optimization.
   Applications of Evolutionary Computation, pages 123-137.
-  Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H.
   Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool
   for Automating Data Science. Proceedings of GECCO 2016, pages
   485-492.
-  Van Geit W, Gevaert M, Chindemi G, Rössert C, Courcol J, Muller EB,
   Schürmann F, Segev I and Markram H (2016). BluePyOpt: Leveraging open
   source software and cloud infrastructure to optimise model parameters
   in neuroscience. Front. Neuroinform. 10:17. doi:
   10.3389/fninf.2016.00017 https://github.com/BlueBrain/BluePyOpt
-  Lara-Cabrera, R., Cotta, C. and Fernández-Leiva, A.J. (2014).
   Geometrical vs topological measures for the evolution of aesthetic
   maps in a rts game, Entertainment Computing,
-  Macret, M. and Pasquier, P. (2013). Automatic Tuning of the OP-1
   Synthesizer Using a Multi-objective Genetic Algorithm. In Proceedings
   of the 10th Sound and Music Computing Conference (SMC). (pp 614-621).
-  Fortin, F. A., Grenier, S., & Parizeau, M. (2013, July). Generalizing
   the improved run-time complexity algorithm for non-dominated sorting.
   In Proceeding of the fifteenth annual conference on Genetic and
   evolutionary computation conference (pp. 615-622). ACM.
-  Fortin, F. A., & Parizeau, M. (2013, July). Revisiting the NSGA-II
   crowding-distance computation. In Proceeding of the fifteenth annual
   conference on Genetic and evolutionary computation conference
   (pp. 623-630). ACM.
-  Marc-André Gardner, Christian Gagné, and Marc Parizeau. Estimation of
   Distribution Algorithm based on Hidden Markov Models for
   Combinatorial Optimization. in Comp. Proc. Genetic and Evolutionary
   Computation Conference (GECCO 2013), July 2013.
-  J. T. Zhai, M. A. Bamakhrama, and T. Stefanov. “Exploiting
   Just-enough Parallelism when Mapping Streaming Applications in Hard
   Real-time Systems”. Design Automation Conference (DAC 2013), 2013.
-  V. Akbarzadeh, C. Gagné, M. Parizeau, M. Argany, M. A Mostafavi,
   “Probabilistic Sensing Model for Sensor Placement Optimization Based
   on Line-of-Sight Coverage”, Accepted in IEEE Transactions on
   Instrumentation and Measurement, 2012.
-  M. Reif, F. Shafait, and A. Dengel. “Dataset Generation for
   Meta-Learning”. Proceedings of the German Conference on Artificial
   Intelligence (KI’12). 2012.
-  M. T. Ribeiro, A. Lacerda, A. Veloso, and N. Ziviani.
   “Pareto-Efficient Hybridization for Multi-Objective Recommender
   Systems”. Proceedings of the Conference on Recommanders Systems
   (!RecSys’12). 2012.
-  M. Pérez-Ortiz, A. Arauzo-Azofra, C. Hervás-Martínez, L.
   García-Hernández and L. Salas-Morera. “A system learning user
   preferences for multiobjective optimization of facility layouts”.
   Pr,oceedings on the Int. Conference on Soft Computing Models in
   Industrial and Environmental Applications (SOCO’12). 2012.
-  Lévesque, J.C., Durand, A., Gagné, C., and Sabourin, R.,
   Multi-Objective Evolutionary Optimization for Generating Ensembles of
   Classifiers in the ROC Space, Genetic and Evolutionary Computation
   Conference (GECCO 2012), 2012.
-  Marc-André Gardner, Christian Gagné, and Marc Parizeau, “Bloat
   Control in Genetic Programming with Histogram-based Accept-Reject
   Method”, in Proc. Genetic and Evolutionary Computation Conference
   (GECCO 2011), 2011.
-  Vahab Akbarzadeh, Albert Ko, Christian Gagné, and Marc Parizeau,
   “Topography-Aware Sensor Deployment Optimization with CMA-ES”, in
   Proc. of Parallel Problem Solving from Nature (PPSN 2010), Springer,
   2010.
-  DEAP is used in `TPOT <https://github.com/rhiever/tpot>`__, an open
   source tool that uses genetic programming to optimize machine
   learning pipelines.
-  DEAP is also used in ROS as an optimization package
   http://www.ros.org/wiki/deap.
-  DEAP is an optional dependency for
   `PyXRD <https://github.com/mathijs-dumon/PyXRD>`__, a Python
   implementation of the matrix algorithm developed for the X-ray
   diffraction analysis of disordered lamellar structures.
-  DEAP is used in `glyph <https://github.com/Ambrosys/glyph>`__, a
   library for symbolic regression with applications to
   `MLC <https://en.wikipedia.org/wiki/Machine_learning_control>`__.

If you want your project listed here, send us a link and a brief
description and we’ll be glad to add it.

.. |Build status| image:: https://travis-ci.org/DEAP/deap.svg?branch=master
   :target: https://travis-ci.org/DEAP/deap
.. |Download| image:: https://img.shields.io/pypi/dm/deap.svg
   :target: https://pypi.python.org/pypi/deap
.. |Join the chat at https://gitter.im/DEAP/deap| image:: https://badges.gitter.im/Join%20Chat.svg
   :target: https://gitter.im/DEAP/deap?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge



  * [debtcollector-1.21.0](https://docs.openstack.org/debtcollector/latest) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/debtcollector.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

Debtcollector
=============

.. image:: https://img.shields.io/pypi/v/debtcollector.svg
    :target: https://pypi.org/project/debtcollector/
    :alt: Latest Version

A collection of Python deprecation patterns and strategies that help you
collect your technical debt in a non-destructive manner. The goal of this
library is to provide well documented developer facing deprecation
patterns that start of with a basic set and can expand into a larger
set of patterns as time goes on. The desired output of these patterns
is to apply the warnings module to emit DeprecationWarning or PendingDeprecationWarning
or similar derivative to developers using libraries (or potentially
applications) about future deprecations.


* Free software: Apache license
* Documentation: https://docs.openstack.org/debtcollector/latest
* Source: https://opendev.org/openstack/debtcollector
* Bugs: https://bugs.launchpad.net/debtcollector
* Release Notes: https://docs.openstack.org/releasenotes/debtcollector




  * [decorator-4.3.0](https://github.com/micheles/decorator) Decorator module
=================

The goal of the decorator module is to make it easy to define
signature-preserving function decorators and decorator factories.
It also includes an implementation of multiple dispatch and other niceties
(please check the docs). It is released under a two-clauses
BSD license, i.e. basically you can do whatever you want with it but I am not
responsible.

Installation
-------------

If you are lazy, just perform

``$ pip install decorator``

which will install just the module on your system.

If you prefer to install the full distribution from source, including
the documentation, clone the [GitHub repo](
https://github.com/micheles/decorator) or download the
[tarball](http://pypi.python.org/pypi/decorator), unpack it and run

``$ pip install .``

in the main directory, possibly as superuser.

Testing
--------

If you have the source code installation you can run the tests with

``$ python src/tests/test.py -v``

or (if you have setuptools installed)

``$ python setup.py test``

Notice that you may run into trouble if in your system there
is an older version of the decorator module; in such a case remove the
old version. It is safe even to copy the module `decorator.py` over
an existing one, since we kept backward-compatibility for a long time.

Repository
---------------

The project is hosted on GitHub. You can look at the source here:

https://github.com/micheles/decorator

Documentation
---------------

The documentation has been moved to GitHub: https://raw.githubusercontent.com/micheles/decorator/master/docs/documentation.md

From there you can get a PDF version by simply using the print
functionality of your browser.

For the impatient
-----------------

Here is an example of how to define a family of decorators tracing slow
operations:

```python
from decorator import decorator

@decorator
def warn_slow(func, timelimit=60, *args, **kw):
    t0 = time.time()
    result = func(*args, **kw)
    dt = time.time() - t0
    if dt > timelimit:
           logging.warn('%s took %d seconds', func.__name__, dt)
       else:
           logging.info('%s took %d seconds', func.__name__, dt)
       return result

@warn_slow  # warn if it takes more than 1 minute
def preprocess_input_files(inputdir, tempdir):
    ...

@warn_slow(timelimit=600)  # warn if it takes more than 10 minutes
def run_calculation(tempdir, outdir):
    ...
```

Enjoy!

  * [deepTools-3.3.0](http://pypi.python.org/pypi/deepTools/) ======================================================================
deepTools
======================================================================

User-friendly tools for exploring deep-sequencing data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

deepTools addresses the challenge of handling the large amounts of data
that are now routinely generated from DNA sequencing centers. deepTools
contains useful modules to process the mapped reads data for multiple
quality checks, creating **normalized coverage files** in standard
bedGraph and bigWig file formats, that allow comparison between
different files (for example, treatment and control). Finally, using
such normalized and standardized files, deepTools can create many
publication-ready **visualizations** to identify enrichments and for
functional annotations of the genome.

For support, questions, or feature requests contact:
deeptools@googlegroups.com

For further documentation, please see our `read the docs page <http://deeptools.readthedocs.org/>`__.

Citation:
^^^^^^^^^

Ramírez F, Ryan DP, Grüning B, Bhardwaj V, Kilpert F, Richter AS, Heyne
S, Dündar F, Manke T. `deepTools2: a next generation web server for
deep-sequencing data
analysis. <https://nar.oxfordjournals.org/content/early/2016/04/12/nar.gkw257.abstract>`__
Nucleic Acids Research. 2016 Apr 13:gkw257.
  * [defusedxml-0.6.0](https://github.com/tiran/defusedxml) ===================================================
defusedxml -- defusing XML bombs and other exploits
===================================================

.. image:: https://img.shields.io/pypi/v/defusedxml.svg
    :target: https://pypi.org/project/defusedxml/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/defusedxml.svg
    :target: https://pypi.org/project/defusedxml/
    :alt: Supported Python versions

.. image:: https://travis-ci.org/tiran/defusedxml.svg?branch=master
    :target: https://travis-ci.org/tiran/defusedxml
    :alt: Travis CI

.. image:: https://codecov.io/github/tiran/defusedxml/coverage.svg?branch=master
    :target: https://codecov.io/github/tiran/defusedxml?branch=master
    :alt: codecov

.. image:: https://img.shields.io/pypi/dm/defusedxml.svg
    :target: https://pypistats.org/packages/defusedxml
    :alt: PyPI downloads

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black
    :alt: Code style: black

..

    "It's just XML, what could probably go wrong?"

Christian Heimes <christian@python.org>

Synopsis
========

The results of an attack on a vulnerable XML library can be fairly dramatic.
With just a few hundred **Bytes** of XML data an attacker can occupy several
**Gigabytes** of memory within **seconds**. An attacker can also keep
CPUs busy for a long time with a small to medium size request. Under some
circumstances it is even possible to access local files on your
server, to circumvent a firewall, or to abuse services to rebound attacks to
third parties.

The attacks use and abuse less common features of XML and its parsers. The
majority of developers are unacquainted with features such as processing
instructions and entity expansions that XML inherited from SGML. At best
they know about ``<!DOCTYPE>`` from experience with HTML but they are not
aware that a document type definition (DTD) can generate an HTTP request
or load a file from the file system.

None of the issues is new. They have been known for a long time. Billion
laughs was first reported in 2003. Nevertheless some XML libraries and
applications are still vulnerable and even heavy users of XML are
surprised by these features. It's hard to say whom to blame for the
situation. It's too short sighted to shift all blame on XML parsers and
XML libraries for using insecure default settings. After all they
properly implement XML specifications. Application developers must not rely
that a library is always configured for security and potential harmful data
by default.


.. contents:: Table of Contents
   :depth: 2


Attack vectors
==============

billion laughs / exponential entity expansion
---------------------------------------------

The `Billion Laughs`_ attack -- also known as exponential entity expansion --
uses multiple levels of nested entities. The original example uses 9 levels
of 10 expansions in each level to expand the string ``lol`` to a string of
3 * 10 :sup:`9` bytes, hence the name "billion laughs". The resulting string
occupies 3 GB (2.79 GiB) of memory; intermediate strings require additional
memory. Because most parsers don't cache the intermediate step for every
expansion it is repeated over and over again. It increases the CPU load even
more.

An XML document of just a few hundred bytes can disrupt all services on a
machine within seconds.

Example XML::

    <!DOCTYPE xmlbomb [
    <!ENTITY a "1234567890" >
    <!ENTITY b "&a;&a;&a;&a;&a;&a;&a;&a;">
    <!ENTITY c "&b;&b;&b;&b;&b;&b;&b;&b;">
    <!ENTITY d "&c;&c;&c;&c;&c;&c;&c;&c;">
    ]>
    <bomb>&d;</bomb>


quadratic blowup entity expansion
---------------------------------

A quadratic blowup attack is similar to a `Billion Laughs`_ attack; it abuses
entity expansion, too. Instead of nested entities it repeats one large entity
with a couple of thousand chars over and over again. The attack isn't as
efficient as the exponential case but it avoids triggering countermeasures of
parsers against heavily nested entities. Some parsers limit the depth and
breadth of a single entity but not the total amount of expanded text
throughout an entire XML document.

A medium-sized XML document with a couple of hundred kilobytes can require a
couple of hundred MB to several GB of memory. When the attack is combined
with some level of nested expansion an attacker is able to achieve a higher
ratio of success.

::

    <!DOCTYPE bomb [
    <!ENTITY a "xxxxxxx... a couple of ten thousand chars">
    ]>
    <bomb>&a;&a;&a;... repeat</bomb>


external entity expansion (remote)
----------------------------------

Entity declarations can contain more than just text for replacement. They can
also point to external resources by public identifiers or system identifiers.
System identifiers are standard URIs. When the URI is a URL (e.g. a
``http://`` locator) some parsers download the resource from the remote
location and embed them into the XML document verbatim.

Simple example of a parsed external entity::

    <!DOCTYPE external [
    <!ENTITY ee SYSTEM "http://www.python.org/some.xml">
    ]>
    <root>&ee;</root>

The case of parsed external entities works only for valid XML content. The
XML standard also supports unparsed external entities with a
``NData declaration``.

External entity expansion opens the door to plenty of exploits. An attacker
can abuse a vulnerable XML library and application to rebound and forward
network requests with the IP address of the server. It highly depends
on the parser and the application what kind of exploit is possible. For
example:

* An attacker can circumvent firewalls and gain access to restricted
  resources as all the requests are made from an internal and trustworthy
  IP address, not from the outside.
* An attacker can abuse a service to attack, spy on or DoS your servers but
  also third party services. The attack is disguised with the IP address of
  the server and the attacker is able to utilize the high bandwidth of a big
  machine.
* An attacker can exhaust additional resources on the machine, e.g. with
  requests to a service that doesn't respond or responds with very large
  files.
* An attacker may gain knowledge, when, how often and from which IP address
  an XML document is accessed.
* An attacker could send mail from inside your network if the URL handler
  supports ``smtp://`` URIs.


external entity expansion (local file)
--------------------------------------

External entities with references to local files are a sub-case of external
entity expansion. It's listed as an extra attack because it deserves extra
attention. Some XML libraries such as lxml disable network access by default
but still allow entity expansion with local file access by default. Local
files are either referenced with a ``file://`` URL or by a file path (either
relative or absolute).

An attacker may be able to access and download all files that can be read by
the application process. This may include critical configuration files, too.

::

    <!DOCTYPE external [
    <!ENTITY ee SYSTEM "file:///PATH/TO/simple.xml">
    ]>
    <root>&ee;</root>


DTD retrieval
-------------

This case is similar to external entity expansion, too. Some XML libraries
like Python's xml.dom.pulldom retrieve document type definitions from remote
or local locations. Several attack scenarios from the external entity case
apply to this issue as well.

::

    <?xml version="1.0" encoding="utf-8"?>
    <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
    <html>
        <head/>
        <body>text</body>
    </html>


Python XML Libraries
====================

.. csv-table:: vulnerabilities and features
   :header: "kind", "sax", "etree", "minidom", "pulldom", "xmlrpc", "lxml", "genshi"
   :widths: 24, 7, 8, 8, 7, 8, 8, 8
   :stub-columns: 0

   "billion laughs", "**True**", "**True**", "**True**", "**True**", "**True**", "False (1)", "False (5)"
   "quadratic blowup", "**True**", "**True**", "**True**", "**True**", "**True**", "**True**", "False (5)"
   "external entity expansion (remote)", "**True**", "False (3)", "False (4)", "**True**", "false", "False (1)", "False (5)"
   "external entity expansion (local file)", "**True**", "False (3)", "False (4)", "**True**", "false", "**True**", "False (5)"
   "DTD retrieval", "**True**", "False", "False", "**True**", "false", "False (1)", "False"
   "gzip bomb", "False", "False", "False", "False", "**True**", "**partly** (2)", "False"
   "xpath support (7)", "False", "False", "False", "False", "False", "**True**", "False"
   "xsl(t) support (7)", "False", "False", "False", "False", "False", "**True**", "False"
   "xinclude support (7)", "False", "**True** (6)", "False", "False", "False", "**True** (6)", "**True**"
   "C library", "expat", "expat", "expat", "expat", "expat", "libxml2", "expat"

1. Lxml is protected against billion laughs attacks and doesn't do network
   lookups by default.
2. libxml2 and lxml are not directly vulnerable to gzip decompression bombs
   but they don't protect you against them either.
3. xml.etree doesn't expand entities and raises a ParserError when an entity
   occurs.
4. minidom doesn't expand entities and simply returns the unexpanded entity
   verbatim.
5. genshi.input of genshi 0.6 doesn't support entity expansion and raises a
   ParserError when an entity occurs.
6. Library has (limited) XInclude support but requires an additional step to
   process inclusion.
7. These are features but they may introduce exploitable holes, see
   `Other things to consider`_


Settings in standard library
----------------------------


xml.sax.handler Features
........................

feature_external_ges (http://xml.org/sax/features/external-general-entities)
  disables external entity expansion

feature_external_pes (http://xml.org/sax/features/external-parameter-entities)
  the option is ignored and doesn't modify any functionality

DOM xml.dom.xmlbuilder.Options
..............................

external_parameter_entities
  ignored

external_general_entities
  ignored

external_dtd_subset
  ignored

entities
  unsure


defusedxml
==========

The `defusedxml package`_ (`defusedxml on PyPI`_)
contains several Python-only workarounds and fixes
for denial of service and other vulnerabilities in Python's XML libraries.
In order to benefit from the protection you just have to import and use the
listed functions / classes from the right defusedxml module instead of the
original module. Merely `defusedxml.xmlrpc`_ is implemented as monkey patch.

Instead of::

   >>> from xml.etree.ElementTree import parse
   >>> et = parse(xmlfile)

alter code to::

   >>> from defusedxml.ElementTree import parse
   >>> et = parse(xmlfile)

Additionally the package has an **untested** function to monkey patch
all stdlib modules with ``defusedxml.defuse_stdlib()``.

All functions and parser classes accept three additional keyword arguments.
They return either the same objects as the original functions or compatible
subclasses.

forbid_dtd (default: False)
  disallow XML with a ``<!DOCTYPE>`` processing instruction and raise a
  *DTDForbidden* exception when a DTD processing instruction is found.

forbid_entities (default: True)
  disallow XML with ``<!ENTITY>`` declarations inside the DTD and raise an
  *EntitiesForbidden* exception when an entity is declared.

forbid_external (default: True)
  disallow any access to remote or local resources in external entities
  or DTD and raising an *ExternalReferenceForbidden* exception when a DTD
  or entity references an external resource.


defusedxml (package)
--------------------

DefusedXmlException, DTDForbidden, EntitiesForbidden,
ExternalReferenceForbidden, NotSupportedError

defuse_stdlib() (*experimental*)


defusedxml.cElementTree
-----------------------

parse(), iterparse(), fromstring(), XMLParser


defusedxml.ElementTree
-----------------------

parse(), iterparse(), fromstring(), XMLParser


defusedxml.expatreader
----------------------

create_parser(), DefusedExpatParser


defusedxml.sax
--------------

parse(), parseString(), make_parser()


defusedxml.expatbuilder
-----------------------

parse(), parseString(), DefusedExpatBuilder, DefusedExpatBuilderNS


defusedxml.minidom
------------------

parse(), parseString()


defusedxml.pulldom
------------------

parse(), parseString()


defusedxml.xmlrpc
-----------------

The fix is implemented as monkey patch for the stdlib's xmlrpc package (3.x)
or xmlrpclib module (2.x). The function `monkey_patch()` enables the fixes,
`unmonkey_patch()` removes the patch and puts the code in its former state.

The monkey patch protects against XML related attacks as well as
decompression bombs and excessively large requests or responses. The default
setting is 30 MB for requests, responses and gzip decompression. You can
modify the default by changing the module variable `MAX_DATA`. A value of
`-1` disables the limit.


defusedxml.lxml
---------------

**DEPRECATED** The module is deprecated and will be removed in a future
release.

The module acts as an *example* how you could protect code that uses
lxml.etree. It implements a custom Element class that filters out
Entity instances, a custom parser factory and a thread local storage for
parser instances. It also has a check_docinfo() function which inspects
a tree for internal or external DTDs and entity declarations. In order to
check for entities lxml > 3.0 is required.

parse(), fromstring()
RestrictedElement, GlobalParserTLS, getDefaultParser(), check_docinfo()


defusedexpat
============

The `defusedexpat package`_ (`defusedexpat on PyPI`_)
comes with binary extensions and a
`modified expat`_ library instead of the standard `expat parser`_. It's
basically a stand-alone version of the patches for Python's standard
library C extensions.

Modifications in expat
----------------------

new definitions::

  XML_BOMB_PROTECTION
  XML_DEFAULT_MAX_ENTITY_INDIRECTIONS
  XML_DEFAULT_MAX_ENTITY_EXPANSIONS
  XML_DEFAULT_RESET_DTD

new XML_FeatureEnum members::

  XML_FEATURE_MAX_ENTITY_INDIRECTIONS
  XML_FEATURE_MAX_ENTITY_EXPANSIONS
  XML_FEATURE_IGNORE_DTD

new XML_Error members::

  XML_ERROR_ENTITY_INDIRECTIONS
  XML_ERROR_ENTITY_EXPANSION

new API functions::

  int XML_GetFeature(XML_Parser parser,
                     enum XML_FeatureEnum feature,
                     long *value);
  int XML_SetFeature(XML_Parser parser,
                     enum XML_FeatureEnum feature,
                     long value);
  int XML_GetFeatureDefault(enum XML_FeatureEnum feature,
                            long *value);
  int XML_SetFeatureDefault(enum XML_FeatureEnum feature,
                            long value);

XML_FEATURE_MAX_ENTITY_INDIRECTIONS
   Limit the amount of indirections that are allowed to occur during the
   expansion of a nested entity. A counter starts when an entity reference
   is encountered. It resets after the entity is fully expanded. The limit
   protects the parser against exponential entity expansion attacks (aka
   billion laughs attack). When the limit is exceeded the parser stops and
   fails with `XML_ERROR_ENTITY_INDIRECTIONS`.
   A value of 0 disables the protection.

   Supported range
     0 .. UINT_MAX
   Default
     40

XML_FEATURE_MAX_ENTITY_EXPANSIONS
   Limit the total length of all entity expansions throughout the entire
   document. The lengths of all entities are accumulated in a parser variable.
   The setting protects against quadratic blowup attacks (lots of expansions
   of a large entity declaration). When the sum of all entities exceeds
   the limit, the parser stops and fails with `XML_ERROR_ENTITY_EXPANSION`.
   A value of 0 disables the protection.

   Supported range
     0 .. UINT_MAX
   Default
     8 MiB

XML_FEATURE_RESET_DTD
   Reset all DTD information after the <!DOCTYPE> block has been parsed. When
   the flag is set (default: false) all DTD information after the
   endDoctypeDeclHandler has been called. The flag can be set inside the
   endDoctypeDeclHandler. Without DTD information any entity reference in
   the document body leads to `XML_ERROR_UNDEFINED_ENTITY`.

   Supported range
     0, 1
   Default
     0


How to avoid XML vulnerabilities
================================

Best practices
--------------

* Don't allow DTDs
* Don't expand entities
* Don't resolve externals
* Limit parse depth
* Limit total input size
* Limit parse time
* Favor a SAX or iterparse-like parser for potential large data
* Validate and properly quote arguments to XSL transformations and
  XPath queries
* Don't use XPath expression from untrusted sources
* Don't apply XSL transformations that come untrusted sources

(based on Brad Hill's `Attacking XML Security`_)


Other things to consider
========================

XML, XML parsers and processing libraries have more features and possible
issue that could lead to DoS vulnerabilities or security exploits in
applications. I have compiled an incomplete list of theoretical issues that
need further research and more attention. The list is deliberately pessimistic
and a bit paranoid, too. It contains things that might go wrong under daffy
circumstances.


attribute blowup / hash collision attack
----------------------------------------

XML parsers may use an algorithm with quadratic runtime O(n :sup:`2`) to
handle attributes and namespaces. If it uses hash tables (dictionaries) to
store attributes and namespaces the implementation may be vulnerable to
hash collision attacks, thus reducing the performance to O(n :sup:`2`) again.
In either case an attacker is able to forge a denial of service attack with
an XML document that contains thousands upon thousands of attributes in
a single node.

I haven't researched yet if expat, pyexpat or libxml2 are vulnerable.


decompression bomb
------------------

The issue of decompression bombs (aka `ZIP bomb`_) apply to all XML libraries
that can parse compressed XML stream like gzipped HTTP streams or LZMA-ed
files. For an attacker it can reduce the amount of transmitted data by three
magnitudes or more. Gzip is able to compress 1 GiB zeros to roughly 1 MB,
lzma is even better::

    $ dd if=/dev/zero bs=1M count=1024 | gzip > zeros.gz
    $ dd if=/dev/zero bs=1M count=1024 | lzma -z > zeros.xy
    $ ls -sh zeros.*
    1020K zeros.gz
     148K zeros.xy

None of Python's standard XML libraries decompress streams except for
``xmlrpclib``. The module is vulnerable <https://bugs.python.org/issue16043>
to decompression bombs.

lxml can load and process compressed data through libxml2 transparently.
libxml2 can handle even very large blobs of compressed data efficiently
without using too much memory. But it doesn't protect applications from
decompression bombs. A carefully written SAX or iterparse-like approach can
be safe.


Processing Instruction
----------------------

`PI`_'s like::

  <?xml-stylesheet type="text/xsl" href="style.xsl"?>

may impose more threats for XML processing. It depends if and how a
processor handles processing instructions. The issue of URL retrieval with
network or local file access apply to processing instructions, too.


Other DTD features
------------------

`DTD`_ has more features like ``<!NOTATION>``. I haven't researched how
these features may be a security threat.


XPath
-----

XPath statements may introduce DoS vulnerabilities. Code should never execute
queries from untrusted sources. An attacker may also be able to create an XML
document that makes certain XPath queries costly or resource hungry.


XPath injection attacks
-----------------------

XPath injeciton attacks pretty much work like SQL injection attacks.
Arguments to XPath queries must be quoted and validated properly, especially
when they are taken from the user. The page `Avoid the dangers of XPath injection`_
list some ramifications of XPath injections.

Python's standard library doesn't have XPath support. Lxml supports
parameterized XPath queries which does proper quoting. You just have to use
its xpath() method correctly::

   # DON'T
   >>> tree.xpath("/tag[@id='%s']" % value)

   # instead do
   >>> tree.xpath("/tag[@id=$tagid]", tagid=name)


XInclude
--------

`XML Inclusion`_ is another way to load and include external files::

   <root xmlns:xi="http://www.w3.org/2001/XInclude">
     <xi:include href="filename.txt" parse="text" />
   </root>

This feature should be disabled when XML files from an untrusted source are
processed. Some Python XML libraries and libxml2 support XInclude but don't
have an option to sandbox inclusion and limit it to allowed directories.


XMLSchema location
------------------

A validating XML parser may download schema files from the information in a
``xsi:schemaLocation`` attribute.

::

  <ead xmlns="urn:isbn:1-931666-22-9"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="urn:isbn:1-931666-22-9 http://www.loc.gov/ead/ead.xsd">
  </ead>


XSL Transformation
------------------

You should keep in mind that XSLT is a Turing complete language. Never
process XSLT code from unknown or untrusted source! XSLT processors may
allow you to interact with external resources in ways you can't even imagine.
Some processors even support extensions that allow read/write access to file
system, access to JRE objects or scripting with Jython.

Example from `Attacking XML Security`_ for Xalan-J::

    <xsl:stylesheet version="1.0"
     xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
     xmlns:rt="http://xml.apache.org/xalan/java/java.lang.Runtime"
     xmlns:ob="http://xml.apache.org/xalan/java/java.lang.Object"
     exclude-result-prefixes= "rt ob">
     <xsl:template match="/">
       <xsl:variable name="runtimeObject" select="rt:getRuntime()"/>
       <xsl:variable name="command"
         select="rt:exec($runtimeObject, &apos;c:\Windows\system32\cmd.exe&apos;)"/>
       <xsl:variable name="commandAsString" select="ob:toString($command)"/>
       <xsl:value-of select="$commandAsString"/>
     </xsl:template>
    </xsl:stylesheet>


Related CVEs
============

CVE-2013-1664
  Unrestricted entity expansion induces DoS vulnerabilities in Python XML
  libraries (XML bomb)

CVE-2013-1665
  External entity expansion in Python XML libraries inflicts potential
  security flaws and DoS vulnerabilities


Other languages / frameworks
=============================

Several other programming languages and frameworks are vulnerable as well. A
couple of them are affected by the fact that libxml2 up to 2.9.0 has no
protection against quadratic blowup attacks. Most of them have potential
dangerous default settings for entity expansion and external entities, too.

Perl
----

Perl's XML::Simple is vulnerable to quadratic entity expansion and external
entity expansion (both local and remote).


Ruby
----

Ruby's REXML document parser is vulnerable to entity expansion attacks
(both quadratic and exponential) but it doesn't do external entity
expansion by default. In order to counteract entity expansion you have to
disable the feature::

  REXML::Document.entity_expansion_limit = 0

libxml-ruby and hpricot don't expand entities in their default configuration.


PHP
---

PHP's SimpleXML API is vulnerable to quadratic entity expansion and loads
entities from local and remote resources. The option ``LIBXML_NONET`` disables
network access but still allows local file access. ``LIBXML_NOENT`` seems to
have no effect on entity expansion in PHP 5.4.6.


C# / .NET / Mono
----------------

Information in `XML DoS and Defenses (MSDN)`_ suggest that .NET is
vulnerable with its default settings. The article contains code snippets
how to create a secure XML reader::

  XmlReaderSettings settings = new XmlReaderSettings();
  settings.ProhibitDtd = false;
  settings.MaxCharactersFromEntities = 1024;
  settings.XmlResolver = null;
  XmlReader reader = XmlReader.Create(stream, settings);


Java
----

Untested. The documentation of Xerces and its `Xerces SecurityMananger`_
sounds like Xerces is also vulnerable to billion laugh attacks with its
default settings. It also does entity resolving when an
``org.xml.sax.EntityResolver`` is configured. I'm not yet sure about the
default setting here.

Java specialists suggest to have a custom builder factory::

  DocumentBuilderFactory builderFactory = DocumentBuilderFactory.newInstance();
  builderFactory.setXIncludeAware(False);
  builderFactory.setExpandEntityReferences(False);
  builderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, True);
  # either
  builderFactory.setFeature("http://apache.org/xml/features/disallow-doctype-decl", True);
  # or if you need DTDs
  builderFactory.setFeature("http://xml.org/sax/features/external-general-entities", False);
  builderFactory.setFeature("http://xml.org/sax/features/external-parameter-entities", False);
  builderFactory.setFeature("http://apache.org/xml/features/nonvalidating/load-external-dtd", False);
  builderFactory.setFeature("http://apache.org/xml/features/nonvalidating/load-dtd-grammar", False);


TODO
====

* DOM: Use xml.dom.xmlbuilder options for entity handling
* SAX: take feature_external_ges and feature_external_pes (?) into account
* test experimental monkey patching of stdlib modules
* improve documentation


License
=======

Copyright (c) 2013-2017 by Christian Heimes <christian@python.org>

Licensed to PSF under a Contributor Agreement.

See https://www.python.org/psf/license for licensing details.


Acknowledgements
================

Brett Cannon (Python Core developer)
  review and code cleanup

Antoine Pitrou (Python Core developer)
  code review

Aaron Patterson, Ben Murphy and Michael Koziarski (Ruby community)
  Many thanks to Aaron, Ben and Michael from the Ruby community for their
  report and assistance.

Thierry Carrez (OpenStack)
  Many thanks to Thierry for his report to the Python Security Response
  Team on behalf of the OpenStack security team.

Carl Meyer (Django)
  Many thanks to Carl for his report to PSRT on behalf of the Django security
  team.

Daniel Veillard (libxml2)
  Many thanks to Daniel for his insight and assistance with libxml2.

semantics GmbH (https://www.semantics.de/)
  Many thanks to my employer semantics for letting me work on the issue
  during working hours as part of semantics's open source initiative.


References
==========

* `XML DoS and Defenses (MSDN)`_
* `Billion Laughs`_ on Wikipedia
* `ZIP bomb`_ on Wikipedia
* `Configure SAX parsers for secure processing`_
* `Testing for XML Injection`_

.. _defusedxml package: https://github.com/tiran/defusedxml
.. _defusedxml on PyPI: https://pypi.python.org/pypi/defusedxml
.. _defusedexpat package: https://github.com/tiran/defusedexpat
.. _defusedexpat on PyPI: https://pypi.python.org/pypi/defusedexpat
.. _modified expat: https://github.com/tiran/expat
.. _expat parser: http://expat.sourceforge.net/
.. _Attacking XML Security: https://www.isecpartners.com/media/12976/iSEC-HILL-Attacking-XML-Security-bh07.pdf
.. _Billion Laughs: https://en.wikipedia.org/wiki/Billion_laughs
.. _XML DoS and Defenses (MSDN): https://msdn.microsoft.com/en-us/magazine/ee335713.aspx
.. _ZIP bomb: https://en.wikipedia.org/wiki/Zip_bomb
.. _DTD: https://en.wikipedia.org/wiki/Document_Type_Definition
.. _PI: https://en.wikipedia.org/wiki/Processing_Instruction
.. _Avoid the dangers of XPath injection: http://www.ibm.com/developerworks/xml/library/x-xpathinjection/index.html
.. _Configure SAX parsers for secure processing: http://www.ibm.com/developerworks/xml/library/x-tipcfsx/index.html
.. _Testing for XML Injection: https://www.owasp.org/index.php/Testing_for_XML_Injection_(OWASP-DV-008)
.. _Xerces SecurityMananger: https://xerces.apache.org/xerces2-j/javadocs/xerces2/org/apache/xerces/util/SecurityManager.html
.. _XML Inclusion: https://www.w3.org/TR/xinclude/#include_element

Changelog
=========

defusedxml 0.6.0
----------------

*Release date: 17-Apr-2019*

- Increase test coverage.
- Add badges to README.


defusedxml 0.6.0rc1
-------------------

*Release date: 14-Apr-2019*

- Test on Python 3.7 stable and 3.8-dev
- Drop support for Python 3.4
- No longer pass *html* argument to XMLParse. It has been deprecated and
  ignored for a long time. The DefusedXMLParser still takes a html argument.
  A deprecation warning is issued when the argument is False and a TypeError
  when it's True.
- defusedxml now fails early when pyexpat stdlib module is not available or
  broken.
- defusedxml.ElementTree.__all__ now lists ParseError as public attribute.
- The defusedxml.ElementTree and defusedxml.cElementTree modules had a typo
  and used XMLParse instead of XMLParser as an alias for DefusedXMLParser.
  Both the old and fixed name are now available.


defusedxml 0.5.0
----------------

*Release date: 07-Feb-2017*

- No changes


defusedxml 0.5.0.rc1
--------------------

*Release date: 28-Jan-2017*

- Add compatibility with Python 3.6
- Drop support for Python 2.6, 3.1, 3.2, 3.3
- Fix lxml tests (XMLSyntaxError: Detected an entity reference loop)


defusedxml 0.4.1
----------------

*Release date: 28-Mar-2013*

- Add more demo exploits, e.g. python_external.py and Xalan XSLT demos.
- Improved documentation.


defusedxml 0.4
--------------

*Release date: 25-Feb-2013*

- As per http://seclists.org/oss-sec/2013/q1/340 please REJECT
  CVE-2013-0278, CVE-2013-0279 and CVE-2013-0280 and use CVE-2013-1664,
  CVE-2013-1665 for OpenStack/etc.
- Add missing parser_list argument to sax.make_parser(). The argument is
  ignored, though. (thanks to Florian Apolloner)
- Add demo exploit for external entity attack on Python's SAX parser, XML-RPC
  and WebDAV.


defusedxml 0.3
--------------

*Release date: 19-Feb-2013*

- Improve documentation


defusedxml 0.2
--------------

*Release date: 15-Feb-2013*

- Rename ExternalEntitiesForbidden to ExternalReferenceForbidden
- Rename defusedxml.lxml.check_dtd() to check_docinfo()
- Unify argument names in callbacks
- Add arguments and formatted representation to exceptions
- Add forbid_external argument to all functions and classes
- More tests
- LOTS of documentation
- Add example code for other languages (Ruby, Perl, PHP) and parsers (Genshi)
- Add protection against XML and gzip attacks to xmlrpclib

defusedxml 0.1
--------------

*Release date: 08-Feb-2013*

- Initial and internal release for PSRT review



  * [deprecation-2.0.6](http://deprecation.readthedocs.io/) deprecation
===========

.. image:: https://readthedocs.org/projects/deprecation/badge/?version=latest
   :target: http://deprecation.readthedocs.io/en/latest/
   :alt: Documentation Status

.. image:: https://travis-ci.org/briancurtin/deprecation.svg?branch=master
    :target: https://travis-ci.org/briancurtin/deprecation

.. image:: https://codecov.io/gh/briancurtin/deprecation/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/briancurtin/deprecation

The ``deprecation`` library provides a ``deprecated`` decorator and a
``fail_if_not_removed`` decorator for your tests. Together, the two
enable the automation of several things:

1. The docstring of a deprecated method gets the deprecation details
   appended to the end of it. If you generate your API docs direct
   from your source, you don't need to worry about writing your own
   notification. You also don't need to worry about forgetting to
   write it. It's done for you.
2. Rather than having code live on forever because you only deprecated
   it but never actually moved on from it, you can have your tests
   tell you when it's time to remove the code. The ``@deprecated``
   decorator can be told when it's time to entirely remove the code,
   which causes ``@fail_if_not_removed`` to raise an ``AssertionError``,
   causing either your unittest or py.test tests to fail.

See http://deprecation.readthedocs.io/ for the full documentation.

Installation
============

 ::

    pip install deprecation

Usage
=====

 ::

    import deprecation

    @deprecation.deprecated(deprecated_in="1.0", removed_in="2.0",
                            current_version=__version__,
                            details="Use the bar function instead")
    def foo():
        """Do some stuff"""
        return 1

...but doesn't Python ignore ``DeprecationWarning``?
====================================================

Yes, by default since 2.7—and for good reason [#]_ —and this works fine
with that.

1. It often makes sense for you to run your tests with a ``-W`` flag or
   the ``PYTHONWARNINGS`` environment variable so you catch warnings
   in development and handle them appropriately. The warnings raised by
   this library show up there, as they're subclasses of the built-in
   ``DeprecationWarning``. See the `Command Line
   <https://docs.python.org/2/using/cmdline.html#cmdoption-W>`_
   and `Environment Variable
   <https://docs.python.org/2/using/cmdline.html#envvar-PYTHONWARNINGS>`_
   documentation for more details.
2. Even if you don't enable those things, the behavior of this library
   remains the same. The docstrings will still be updated and the tests
   will still fail when they need to. You'll get the benefits regardless
   of what Python cares about ``DeprecationWarning``.

----

.. [#] Exposing application users to ``DeprecationWarning``\s that are
       emitted by lower-level code needlessly involves end-users in
       "how things are done." It often leads to users raising issues
       about warnings they're presented, which on one hand is done
       rightfully so, as it's been presented to them as some sort of
       issue to resolve. However, at the same time, the warning could
       be well known and planned for. From either side, loud
       ``DeprecationWarning``\s can be seen as noise that isn't
       necessary outside of development.



  * [dill-0.3.0](https://pypi.org/project/dill) -----------------------------
dill: serialize all of python
-----------------------------

About Dill
==========

``dill`` extends python's ``pickle`` module for serializing and de-serializing
python objects to the majority of the built-in python types. Serialization
is the process of converting an object to a byte stream, and the inverse
of which is converting a byte stream back to on python object hierarchy.

``dill`` provides the user the same interface as the ``pickle`` module, and
also includes some additional features. In addition to pickling python
objects, ``dill`` provides the ability to save the state of an interpreter
session in a single command.  Hence, it would be feasable to save a
interpreter session, close the interpreter, ship the pickled file to
another computer, open a new interpreter, unpickle the session and
thus continue from the 'saved' state of the original interpreter
session.

``dill`` can be used to store python objects to a file, but the primary
usage is to send python objects across the network as a byte stream.
``dill`` is quite flexible, and allows arbitrary user defined classes
and functions to be serialized.  Thus ``dill`` is not intended to be
secure against erroneously or maliciously constructed data. It is
left to the user to decide whether the data they unpickle is from
a trustworthy source.

``dill`` is part of ``pathos``, a python framework for heterogeneous computing.
``dill`` is in active development, so any user feedback, bug reports, comments,
or suggestions are highly appreciated.  A list of known issues is maintained
at http://trac.mystic.cacr.caltech.edu/project/pathos/query.html, with a public
ticket list at https://github.com/uqfoundation/dill/issues.


Major Features
==============

``dill`` can pickle the following standard types:

    - none, type, bool, int, long, float, complex, str, unicode,
    - tuple, list, dict, file, buffer, builtin,
    - both old and new style classes,
    - instances of old and new style classes,
    - set, frozenset, array, functions, exceptions

``dill`` can also pickle more 'exotic' standard types:

    - functions with yields, nested functions, lambdas,
    - cell, method, unboundmethod, module, code, methodwrapper,
    - dictproxy, methoddescriptor, getsetdescriptor, memberdescriptor,
    - wrapperdescriptor, xrange, slice,
    - notimplemented, ellipsis, quit

``dill`` cannot yet pickle these standard types:

    - frame, generator, traceback

``dill`` also provides the capability to:

    - save and load python interpreter sessions
    - save and extract the source code from functions and classes
    - interactively diagnose pickling errors


Current Release
===============

This documentation is for version ``dill-0.3.1.1``.

The latest released version of ``dill`` is available from:

    https://pypi.org/project/dill

``dill`` is distributed under a 3-clause BSD license.

    >>> import dill
    >>> print (dill.license())


Development Version 
===================

You can get the latest development version with all the shiny new features at:

    https://github.com/uqfoundation

If you have a new contribution, please submit a pull request.


Installation
============

``dill`` is packaged to install from source, so you must
download the tarball, unzip, and run the installer::

    [download]
    $ tar -xvzf dill-0.3.1.1.tar.gz
    $ cd dill-0.3.1.1
    $ python setup py build
    $ python setup py install

You will be warned of any missing dependencies and/or settings
after you run the "build" step above. 

Alternately, ``dill`` can be installed with ``pip`` or ``easy_install``::

    $ pip install dill


Requirements
============

``dill`` requires:

    - ``python``, **version >= 2.6** or **version >= 3.1**, or ``pypy``

Optional requirements:

    - ``setuptools``, **version >= 0.6**
    - ``pyreadline``, **version >= 1.7.1** (on windows)
    - ``objgraph``, **version >= 1.7.2**


More Information
================

Probably the best way to get started is to look at the documentation at
http://dill.rtfd.io. Also see ``dill.tests`` for a set of scripts that
demonstrate how ``dill`` can serialize different python objects. You can
run the test suite with ``python -m dill.tests``. The contents of any
pickle file can be examined with ``undill``.  As ``dill`` conforms to
the ``pickle`` interface, the examples and documentation found at
http://docs.python.org/library/pickle.html also apply to ``dill``
if one will ``import dill as pickle``. The source code is also generally
well documented, so further questions may be resolved by inspecting the
code itself. Please feel free to submit a ticket on github, or ask a
question on stackoverflow (**@Mike McKerns**).
If you would like to share how you use ``dill`` in your work, please send
an email (to **mmckerns at uqfoundation dot org**).


Citation
========

If you use ``dill`` to do research that leads to publication, we ask that you
acknowledge use of ``dill`` by citing the following in your publication::

    M.M. McKerns, L. Strand, T. Sullivan, A. Fang, M.A.G. Aivazis,
    "Building a framework for predictive science", Proceedings of
    the 10th Python in Science Conference, 2011;
    http://arxiv.org/pdf/1202.1056

    Michael McKerns and Michael Aivazis,
    "pathos: a framework for heterogeneous computing", 2010- ;
    http://trac.mystic.cacr.caltech.edu/project/pathos

Please see http://trac.mystic.cacr.caltech.edu/project/pathos or
http://arxiv.org/pdf/1202.1056 for further information.
  * [distlib-0.2.9.post0](https://bitbucket.org/pypa/distlib) Low-level components of distutils2/packaging, augmented with higher-level APIs for making packaging easier.
  * [distributed-2.2.0](https://distributed.dask.org) Distributed
===========

A library for distributed computation.  See documentation_ for more details.


.. _documentation: https://distributed.readthedocs.io/en/latest



  * [django-configurations-2.1](https://django-configurations.readthedocs.io/) django-configurations
=====================

.. image:: https://travis-ci.org/jazzband/django-configurations.svg?branch=master
   :alt: Build Status
   :target: https://travis-ci.org/jazzband/django-configurations

.. image:: https://jazzband.co/static/img/badge.svg
   :alt: Jazzband
   :target: https://jazzband.co/

.. image:: https://codecov.io/github/jazzband/django-configurations/coverage.svg?branch=master
   :alt: Codecov
   :target: https://codecov.io/github/jazzband/django-configurations?branch=master

django-configurations eases Django project configuration by relying
on the composability of Python classes. It extends the notion of
Django's module based settings loading with well established
object oriented programming patterns.

Check out the `documentation`__ for more complete examples.

.. __: https://django-configurations.readthedocs.io/en/latest/


Quickstart
----------

Install django-configurations:

.. code-block:: console

    pip install django-configurations

Then subclass the included ``configurations.Configuration`` class in your
project's **settings.py** or any other module you're using to store the
settings constants, e.g.:

.. code-block:: python

    # mysite/settings.py

    from configurations import Configuration

    class Dev(Configuration):
        DEBUG = True

Set the ``DJANGO_CONFIGURATION`` environment variable to the name of the class
you just created, e.g. in bash:

.. code-block:: console

    export DJANGO_CONFIGURATION=Dev

and the ``DJANGO_SETTINGS_MODULE`` environment variable to the module
import path as usual, e.g. in bash:

.. code-block:: console

    export DJANGO_SETTINGS_MODULE=mysite.settings

*Alternatively* supply the ``--configuration`` option when using Django
management commands along the lines of Django's default ``--settings``
command line option, e.g.::

    python manage.py runserver --settings=mysite.settings --configuration=Dev

To enable Django to use your configuration you now have to modify your
**manage.py** or **wsgi.py** script to use django-configurations's versions
of the appropriate starter functions, e.g. a typical **manage.py** using
django-configurations would look like this:

.. code-block:: python

    #!/usr/bin/env python

    import os
    import sys

    if __name__ == "__main__":
        os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mysite.settings')
        os.environ.setdefault('DJANGO_CONFIGURATION', 'Dev')

        from configurations.management import execute_from_command_line

        execute_from_command_line(sys.argv)

Notice in line 10 we don't use the common tool
``django.core.management.execute_from_command_line`` but instead
``configurations.management.execute_from_command_line``.

The same applies to your **wsgi.py** file, e.g.:

.. code-block:: python

    import os

    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mysite.settings')
    os.environ.setdefault('DJANGO_CONFIGURATION', 'Dev')

    from configurations.wsgi import get_wsgi_application

    application = get_wsgi_application()

Here we don't use the default ``django.core.wsgi.get_wsgi_application``
function but instead ``configurations.wsgi.get_wsgi_application``.

That's it! You can now use your project with ``manage.py`` and your favorite
WSGI enabled server.



  * [dnaio-0.3](https://github.com/marcelm/dnaio/) [![Travis](https://travis-ci.org/marcelm/dnaio.svg?branch=master)](https://travis-ci.org/marcelm/dnaio)
[![PyPI](https://img.shields.io/pypi/v/dnaio.svg?branch=master)](https://pypi.python.org/pypi/dnaio)
[![Codecov](https://codecov.io/gh/marcelm/dnaio/branch/master/graph/badge.svg)](https://codecov.io/gh/marcelm/dnaio)

# dnaio parses FASTQ and FASTA

`dnaio` is a Python 3 library for fast parsing of FASTQ and also FASTA files. The code was previously part of the
[Cutadapt](https://cutadapt.readthedocs.io/) tool and has been improved since it has been split out.


## Example usage

The main interface is the `dnaio.open` function:

    import dnaio

    with dnaio.open('reads.fastq.gz') as f:
        bp = 0
        for record in f:
            bp += len(record)
    print(f'The input file contains {bp/1E6:.1f} Mbp')


## Features and supported file types

- FASTQ input and output
- FASTA input and output
- Compressed input and output (`.gz`, `.bz2` and `.xz`, detected automatically)
- Paired-end data in two files
- Interleaved paired-end data in a single file
- Files with DOS/Windows linebreaks can be read
- FASTQ files with a second header line (after the `+`) are supported


# Limitations

- Multi-line FASTQ files are not supported. You shouldn’t use them anyway.
- FASTQ parsing is the focus of this library. The FASTA parser is not as optimized.


# Links

* [Source code](https://github.com/marcelm/dnaio/)
* [Report an issue](https://github.com/marcelm/dnaio/issues)
* [Project page on PyPI](https://pypi.python.org/pypi/dnaio/)



  * [dnspython-1.16.0](http://www.dnspython.org) dnspython is a DNS toolkit for Python. It supports almost all
record types. It can be used for queries, zone transfers, and dynamic
updates.  It supports TSIG authenticated messages and EDNS0.

dnspython provides both high and low level access to DNS. The high
level classes perform queries for data of a given name, type, and
class, and return an answer set.  The low level classes allow
direct manipulation of DNS zones, messages, names, and records.


  * [doc8-0.8.0](https://launchpad.net/doc8) ====
Doc8
====

Doc8 is an *opinionated* style checker for `rst`_ (with basic support for
plain text) styles of documentation.

QuickStart
==========

::

    pip install doc8

To run doc8 just invoke it against any doc directory::

    $ doc8 coolproject/docs

Usage
=====

Command line usage
******************

::

    $ doc8  -h

    usage: doc8 [-h] [--config path] [--allow-long-titles] [--ignore code]
                [--no-sphinx] [--ignore-path path] [--ignore-path-errors path]
                [--default-extension extension] [--file-encoding encoding]
                [--max-line-length int] [-e extension] [-v] [--version]
                [path [path ...]]

    Check documentation for simple style requirements.

    What is checked:
        - invalid rst format - D000
        - lines should not be longer than 79 characters - D001
          - RST exception: line with no whitespace except in the beginning
          - RST exception: lines with http or https urls
          - RST exception: literal blocks
          - RST exception: rst target directives
        - no trailing whitespace - D002
        - no tabulation for indentation - D003
        - no carriage returns (use unix newlines) - D004
        - no newline at end of file - D005

    positional arguments:
      path                  Path to scan for doc files (default: current
                            directory).

    optional arguments:
      -h, --help            show this help message and exit
      --config path         user config file location (default: doc8.ini, tox.ini,
                            pep8.ini, setup.cfg).
      --allow-long-titles   allow long section titles (default: false).
      --ignore code         ignore the given error code(s).
      --no-sphinx           do not ignore sphinx specific false positives.
      --ignore-path path    ignore the given directory or file (globs are
                            supported).
      --ignore-path-errors path
                            ignore the given specific errors in the provided file.
      --default-extension extension
                            default file extension to use when a file is found
                            without a file extension.
      --file-encoding encoding
                            override encoding to use when attempting to determine
                            an input files text encoding (providing this avoids
                            using `chardet` to automatically detect encoding/s)
      --max-line-length int
                            maximum allowed line length (default: 79).
      -e extension, --extension extension
                            check file extensions of the given type (default:
                            .rst, .txt).
      -q, --quiet           only print violations
      -v, --verbose         run in verbose mode.
      --version             show the version and exit.

Ini file usage
**************

Instead of using the CLI for options the following files will also be examined
for ``[doc8]`` sections that can also provided the same set of options. If
the ``--config path`` option is used these files will **not** be scanned for
the current working directory and that configuration path will be used
instead.

* ``$CWD/doc8.ini``
* ``$CWD/tox.ini``
* ``$CWD/pep8.ini``
* ``$CWD/setup.cfg``

An example section that can be placed into one of these files::

    [doc8]

    ignore-path=/tmp/stuff,/tmp/other_stuff
    max-line-length=99
    verbose=1
    ignore-path-errors=/tmp/other_thing.rst;D001;D002

**Note:** The option names are the same as the command line ones (with the
only variation of this being the ``no-sphinx`` option which from
configuration file will be ``sphinx`` instead).

Option conflict resolution
**************************

When the same option is passed on the command line and also via configuration
files the following strategies are applied to resolve these types
of conflicts.

======================   ===========  ========
Option                   Overrides    Merges
======================   ===========  ========
``allow-long-titles``    Yes          No
``ignore-path-errors``   No           Yes
``default-extension``    Yes          No
``extension``            No           Yes
``ignore-path``          No           Yes
``ignore``               No           Yes
``max-line-length``      Yes          No
``file-encoding``        Yes          No
``sphinx``               Yes          No
======================   ===========  ========

**Note:** In the above table the configuration file option when specified as
*overrides* will replace the same option given via the command line. When
*merges* is stated then the option will be combined with the command line
option (for example by becoming a larger list or set of values that contains
the values passed on the command line *and* the values passed via
configuration).

.. _rst: http://docutils.sourceforge.net/docs/ref/rst/introduction.html




  * [docopt-0.6.2](http://docopt.org) ``docopt`` creates *beautiful* command-line interfaces
======================================================================

Video introduction to **docopt**: `PyCon UK 2012: Create *beautiful*
command-line interfaces with Python <http://youtu.be/pXhcPJK5cMc>`_

    New in version 0.6.1:

    - Fix issue `#85 <https://github.com/docopt/docopt/issues/85>`_
      which caused improper handling of ``[options]`` shortcut
      if it was present several times.

    New in version 0.6.0:

    - New argument ``options_first``, disallows interspersing options
      and arguments.  If you supply ``options_first=True`` to
      ``docopt``, it will interpret all arguments as positional
      arguments after first positional argument.

    - If option with argument could be repeated, its default value
      will be interpreted as space-separated list. E.g. with
      ``[default: ./here ./there]`` will be interpreted as
      ``['./here', './there']``.

    Breaking changes:

    - Meaning of ``[options]`` shortcut slightly changed. Previously
      it ment *"any known option"*. Now it means *"any option not in
      usage-pattern"*.  This avoids the situation when an option is
      allowed to be repeated unintentionaly.

    - ``argv`` is ``None`` by default, not ``sys.argv[1:]``.
      This allows ``docopt`` to always use the *latest* ``sys.argv``,
      not ``sys.argv`` during import time.

Isn't it awesome how ``optparse`` and ``argparse`` generate help
messages based on your code?!

*Hell no!*  You know what's awesome?  It's when the option parser *is*
generated based on the beautiful help message that you write yourself!
This way you don't need to write this stupid repeatable parser-code,
and instead can write only the help message--*the way you want it*.

**docopt** helps you create most beautiful command-line interfaces
*easily*:

.. code:: python

    """Naval Fate.

    Usage:
      naval_fate.py ship new <name>...
      naval_fate.py ship <name> move <x> <y> [--speed=<kn>]
      naval_fate.py ship shoot <x> <y>
      naval_fate.py mine (set|remove) <x> <y> [--moored | --drifting]
      naval_fate.py (-h | --help)
      naval_fate.py --version

    Options:
      -h --help     Show this screen.
      --version     Show version.
      --speed=<kn>  Speed in knots [default: 10].
      --moored      Moored (anchored) mine.
      --drifting    Drifting mine.

    """
    from docopt import docopt


    if __name__ == '__main__':
        arguments = docopt(__doc__, version='Naval Fate 2.0')
        print(arguments)

Beat that! The option parser is generated based on the docstring above
that is passed to ``docopt`` function.  ``docopt`` parses the usage
pattern (``"Usage: ..."``) and option descriptions (lines starting
with dash "``-``") and ensures that the program invocation matches the
usage pattern; it parses options, arguments and commands based on
that. The basic idea is that *a good help message has all necessary
information in it to make a parser*.

Also, `PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ recommends
putting help message in the module docstrings.

Installation
======================================================================

Use `pip <http://pip-installer.org>`_ or easy_install::

    pip install docopt==0.6.2

Alternatively, you can just drop ``docopt.py`` file into your
project--it is self-contained.

**docopt** is tested with Python 2.5, 2.6, 2.7, 3.2, 3.3 and PyPy.

API
======================================================================

.. code:: python

    from docopt import docopt

.. code:: python

    docopt(doc, argv=None, help=True, version=None, options_first=False)

``docopt`` takes 1 required and 4 optional arguments:

- ``doc`` could be a module docstring (``__doc__``) or some other
  string that contains a **help message** that will be parsed to
  create the option parser.  The simple rules of how to write such a
  help message are given in next sections.  Here is a quick example of
  such a string:

.. code:: python

    """Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]

    -h --help    show this
    -s --sorted  sorted output
    -o FILE      specify output file [default: ./test.txt]
    --quiet      print less text
    --verbose    print more text

    """

- ``argv`` is an optional argument vector; by default ``docopt`` uses
  the argument vector passed to your program (``sys.argv[1:]``).
  Alternatively you can supply a list of strings like ``['--verbose',
  '-o', 'hai.txt']``.

- ``help``, by default ``True``, specifies whether the parser should
  automatically print the help message (supplied as ``doc``) and
  terminate, in case ``-h`` or ``--help`` option is encountered
  (options should exist in usage pattern, more on that below). If you
  want to handle ``-h`` or ``--help`` options manually (as other
  options), set ``help=False``.

- ``version``, by default ``None``, is an optional argument that
  specifies the version of your program. If supplied, then, (assuming
  ``--version`` option is mentioned in usage pattern) when parser
  encounters the ``--version`` option, it will print the supplied
  version and terminate.  ``version`` could be any printable object,
  but most likely a string, e.g. ``"2.1.0rc1"``.

    Note, when ``docopt`` is set to automatically handle ``-h``,
    ``--help`` and ``--version`` options, you still need to mention
    them in usage pattern for this to work. Also, for your users to
    know about them.

- ``options_first``, by default ``False``.  If set to ``True`` will
  disallow mixing options and positional argument.  I.e. after first
  positional argument, all arguments will be interpreted as positional
  even if the look like options.  This can be used for strict
  compatibility with POSIX, or if you want to dispatch your arguments
  to other programs.

The **return** value is a simple dictionary with options, arguments
and commands as keys, spelled exactly like in your help message.  Long
versions of options are given priority. For example, if you invoke the
top example as::

    naval_fate.py ship Guardian move 100 150 --speed=15

the return dictionary will be:

.. code:: python

    {'--drifting': False,    'mine': False,
     '--help': False,        'move': True,
     '--moored': False,      'new': False,
     '--speed': '15',        'remove': False,
     '--version': False,     'set': False,
     '<name>': ['Guardian'], 'ship': True,
     '<x>': '100',           'shoot': False,
     '<y>': '150'}

Help message format
======================================================================

Help message consists of 2 parts:

- Usage pattern, e.g.::

    Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]

- Option descriptions, e.g.::

    -h --help    show this
    -s --sorted  sorted output
    -o FILE      specify output file [default: ./test.txt]
    --quiet      print less text
    --verbose    print more text

Their format is described below; other text is ignored.

Usage pattern format
----------------------------------------------------------------------

**Usage pattern** is a substring of ``doc`` that starts with
``usage:`` (case *insensitive*) and ends with a *visibly* empty line.
Minimum example:

.. code:: python

    """Usage: my_program.py

    """

The first word after ``usage:`` is interpreted as your program's name.
You can specify your program's name several times to signify several
exclusive patterns:

.. code:: python

    """Usage: my_program.py FILE
              my_program.py COUNT FILE

    """

Each pattern can consist of the following elements:

- **<arguments>**, **ARGUMENTS**. Arguments are specified as either
  upper-case words, e.g. ``my_program.py CONTENT-PATH`` or words
  surrounded by angular brackets: ``my_program.py <content-path>``.
- **--options**.  Options are words started with dash (``-``), e.g.
  ``--output``, ``-o``.  You can "stack" several of one-letter
  options, e.g. ``-oiv`` which will be the same as ``-o -i -v``. The
  options can have arguments, e.g.  ``--input=FILE`` or ``-i FILE`` or
  even ``-iFILE``. However it is important that you specify option
  descriptions if you want for option to have an argument, a default
  value, or specify synonymous short/long versions of option (see next
  section on option descriptions).
- **commands** are words that do *not* follow the described above
  conventions of ``--options`` or ``<arguments>`` or ``ARGUMENTS``,
  plus two special commands: dash "``-``" and double dash "``--``"
  (see below).

Use the following constructs to specify patterns:

- **[ ]** (brackets) **optional** elements.  e.g.: ``my_program.py
  [-hvqo FILE]``
- **( )** (parens) **required** elements.  All elements that are *not*
  put in **[ ]** are also required, e.g.: ``my_program.py
  --path=<path> <file>...`` is the same as ``my_program.py
  (--path=<path> <file>...)``.  (Note, "required options" might be not
  a good idea for your users).
- **|** (pipe) **mutualy exclusive** elements. Group them using **(
  )** if one of the mutually exclusive elements is required:
  ``my_program.py (--clockwise | --counter-clockwise) TIME``. Group
  them using **[ ]** if none of the mutually-exclusive elements are
  required: ``my_program.py [--left | --right]``.
- **...** (ellipsis) **one or more** elements. To specify that
  arbitrary number of repeating elements could be accepted, use
  ellipsis (``...``), e.g.  ``my_program.py FILE ...`` means one or
  more ``FILE``-s are accepted.  If you want to accept zero or more
  elements, use brackets, e.g.: ``my_program.py [FILE ...]``. Ellipsis
  works as a unary operator on the expression to the left.
- **[options]** (case sensitive) shortcut for any options.  You can
  use it if you want to specify that the usage pattern could be
  provided with any options defined below in the option-descriptions
  and do not want to enumerate them all in usage-pattern.  -
  "``[--]``". Double dash "``--``" is used by convention to separate
  positional arguments that can be mistaken for options. In order to
  support this convention add "``[--]``" to you usage patterns.  -
  "``[-]``". Single dash "``-``" is used by convention to signify that
  ``stdin`` is used instead of a file. To support this add "``[-]``"
  to you usage patterns. "``-``" act as a normal command.

If your pattern allows to match argument-less option (a flag) several
times::

    Usage: my_program.py [-v | -vv | -vvv]

then number of occurences of the option will be counted. I.e.
``args['-v']`` will be ``2`` if program was invoked as ``my_program
-vv``. Same works for commands.

If your usage patterns allows to match same-named option with argument
or positional argument several times, the matched arguments will be
collected into a list::

    Usage: my_program.py <file> <file> --path=<path>...

I.e. invoked with ``my_program.py file1 file2 --path=./here
--path=./there`` the returned dict will contain ``args['<file>'] ==
['file1', 'file2']`` and ``args['--path'] == ['./here', './there']``.


Option descriptions format
----------------------------------------------------------------------

**Option descriptions** consist of a list of options that you put
below your usage patterns.

It is necessary to list option descriptions in order to specify:

- synonymous short and long options,
- if an option has an argument,
- if option's argument has a default value.

The rules are as follows:

- Every line in ``doc`` that starts with ``-`` or ``--`` (not counting
  spaces) is treated as an option description, e.g.::

    Options:
      --verbose   # GOOD
      -o FILE     # GOOD
    Other: --bad  # BAD, line does not start with dash "-"

- To specify that option has an argument, put a word describing that
  argument after space (or equals "``=``" sign) as shown below. Follow
  either <angular-brackets> or UPPER-CASE convention for options'
  arguments.  You can use comma if you want to separate options. In
  the example below, both lines are valid, however you are recommended
  to stick to a single style.::

    -o FILE --output=FILE       # without comma, with "=" sign
    -i <file>, --input <file>   # with comma, wihtout "=" sing

- Use two spaces to separate options with their informal description::

    --verbose More text.   # BAD, will be treated as if verbose option had
                           # an argument "More", so use 2 spaces instead
    -q        Quit.        # GOOD
    -o FILE   Output file. # GOOD
    --stdout  Use stdout.  # GOOD, 2 spaces

- If you want to set a default value for an option with an argument,
  put it into the option-description, in form ``[default:
  <my-default-value>]``::

    --coefficient=K  The K coefficient [default: 2.95]
    --output=FILE    Output file [default: test.txt]
    --directory=DIR  Some directory [default: ./]

- If the option is not repeatable, the value inside ``[default: ...]``
  will be interpeted as string.  If it *is* repeatable, it will be
  splited into a list on whitespace::

    Usage: my_program.py [--repeatable=<arg> --repeatable=<arg>]
                         [--another-repeatable=<arg>]...
                         [--not-repeatable=<arg>]

    # will be ['./here', './there']
    --repeatable=<arg>          [default: ./here ./there]

    # will be ['./here']
    --another-repeatable=<arg>  [default: ./here]

    # will be './here ./there', because it is not repeatable
    --not-repeatable=<arg>      [default: ./here ./there]

Examples
----------------------------------------------------------------------

We have an extensive list of `examples
<https://github.com/docopt/docopt/tree/master/examples>`_ which cover
every aspect of functionality of **docopt**.  Try them out, read the
source if in doubt.

Subparsers, multi-level help and *huge* applications (like git)
----------------------------------------------------------------------

If you want to split your usage-pattern into several, implement
multi-level help (whith separate help-screen for each subcommand),
want to interface with existing scripts that don't use **docopt**, or
you're building the next "git", you will need the new ``options_first``
parameter (described in API section above). To get you started quickly
we implemented a subset of git command-line interface as an example:
`examples/git
<https://github.com/docopt/docopt/tree/master/examples/git>`_


Data validation
----------------------------------------------------------------------

**docopt** does one thing and does it well: it implements your
command-line interface.  However it does not validate the input data.
On the other hand there are libraries like `python schema
<https://github.com/halst/schema>`_ which make validating data a
breeze.  Take a look at `validation_example.py
<https://github.com/docopt/docopt/tree/master/examples/validation_example.py>`_
which uses **schema** to validate data and report an error to the
user.

Development
======================================================================

We would *love* to hear what you think about **docopt** on our `issues
page <http://github.com/docopt/docopt/issues>`_

Make pull requrests, report bugs, suggest ideas and discuss
**docopt**. You can also drop a line directly to
<vladimir@keleshev.com>.

Porting ``docopt`` to other languages
======================================================================

We think **docopt** is so good, we want to share it beyond the Python
community!

The follosing ports are available:

- `Ruby port <http://github.com/docopt/docopt.rb>`_
- `CoffeeScript port <http://github.com/docopt/docopt.coffee>`_
- `Lua port <http://github.com/docopt/docopt.lua>`_
- `PHP port <http://github.com/docopt/docopt.php>`_

But you can always create a port for your favorite language!  You are
encouraged to use the Python version as a reference implementation.  A
Language-agnostic test suite is bundled with `Python implementation
<http://github.com/docopt/docopt>`_.

Porting discussion is on `issues page
<http://github.com/docopt/docopt/issues>`_.

Changelog
======================================================================

**docopt** follows `semantic versioning <http://semver.org>`_.  The
first release with stable API will be 1.0.0 (soon).  Until then, you
are encouraged to specify explicitly the version in your dependency
tools, e.g.::

    pip install docopt==0.6.2

- 0.6.2 `Wheel <http://pythonwheels.com/>`_ support.
- 0.6.1 Bugfix release.
- 0.6.0 ``options_first`` parameter.
  **Breaking changes**: Corrected ``[options]`` meaning.
  ``argv`` defaults to ``None``.
- 0.5.0 Repeated options/commands are counted or accumulated into a
  list.
- 0.4.2 Bugfix release.
- 0.4.0 Option descriptions become optional,
  support for "``--``" and "``-``" commands.
- 0.3.0 Support for (sub)commands like `git remote add`.
  Introduce ``[options]`` shortcut for any options.
  **Breaking changes**: ``docopt`` returns dictionary.
- 0.2.0 Usage pattern matching. Positional arguments parsing based on
  usage patterns.
  **Breaking changes**: ``docopt`` returns namespace (for arguments),
  not list. Usage pattern is formalized.
- 0.1.0 Initial release. Options-parsing only (based on options
  description).
  * [docutils-0.15.2](http://docutils.sourceforge.net/) Docutils is a modular system for processing documentation
into useful formats, such as HTML, XML, and LaTeX.  For
input Docutils supports reStructuredText, an easy-to-read,
what-you-see-is-what-you-get plaintext markup syntax.


  * [docutils-stubs-0.0.21](https://github.com/tk0miya/docutils-stubs) docutils-stubs
===============

`PEP 561`_ based Type information for docutils_.

.. _PEP 561: https://www.python.org/dev/peps/pep-0561
.. _docutils: http://docutils.sourceforge.net/



  * [dogpile.cache-0.7.1](https://github.com/sqlalchemy/dogpile.cache) dogpile
=======

Dogpile consists of two subsystems, one building on top of the other.

``dogpile`` provides the concept of a "dogpile lock", a control structure
which allows a single thread of execution to be selected as the "creator" of
some resource, while allowing other threads of execution to refer to the previous
version of this resource as the creation proceeds; if there is no previous
version, then those threads block until the object is available.

``dogpile.cache`` is a caching API which provides a generic interface to
caching backends of any variety, and additionally provides API hooks which
integrate these cache backends with the locking mechanism of ``dogpile``.

Overall, dogpile.cache is intended as a replacement to the `Beaker
<http://beaker.groovie.org>`_ caching system, the internals of which are
written by the same author.   All the ideas of Beaker which "work" are re-
implemented in dogpile.cache in a more efficient and succinct manner, and all
the cruft (Beaker's internals were first written in 2005) relegated to the
trash heap.

Documentation
-------------

See dogpile.cache's full documentation at
`dogpile.cache documentation <http://dogpilecache.sqlalchemy.org>`_.  The
sections below provide a brief synopsis of the ``dogpile`` packages.

Features
--------

* A succinct API which encourages up-front configuration of pre-defined
  "regions", each one defining a set of caching characteristics including
  storage backend, configuration options, and default expiration time.
* A standard get/set/delete API as well as a function decorator API is
  provided.
* The mechanics of key generation are fully customizable.   The function
  decorator API features a pluggable "key generator" to customize how
  cache keys are made to correspond to function calls, and an optional
  "key mangler" feature provides for pluggable mangling of keys
  (such as encoding, SHA-1 hashing) as desired for each region.
* The dogpile lock, first developed as the core engine behind the Beaker
  caching system, here vastly simplified, improved, and better tested.
  Some key performance
  issues that were intrinsic to Beaker's architecture, particularly that
  values would frequently be "double-fetched" from the cache, have been fixed.
* Backends implement their own version of a "distributed" lock, where the
  "distribution" matches the backend's storage system.  For example, the
  memcached backends allow all clients to coordinate creation of values
  using memcached itself.   The dbm file backend uses a lockfile
  alongside the dbm file.  New backends, such as a Redis-based backend,
  can provide their own locking mechanism appropriate to the storage
  engine.
* Writing new backends or hacking on the existing backends is intended to be
  routine - all that's needed are basic get/set/delete methods. A distributed
  lock tailored towards the backend is an optional addition, else dogpile uses
  a regular thread mutex. New backends can be registered with dogpile.cache
  directly or made available via setuptools entry points.
* Included backends feature three memcached backends (python-memcached, pylibmc,
  bmemcached), a Redis backend, a backend based on Python's
  anydbm, and a plain dictionary backend.
* Space for third party plugins, including one which provides the
  dogpile.cache engine to Mako templates.
  * [dominate-2.4.0](https://github.com/Knio/dominate/) Dominate
========

`Dominate` is a Python library for creating and manipulating HTML documents using an elegant DOM API.
It allows you to write HTML pages in pure Python very concisely, which eliminates the need to learn another template language, and lets you take advantage of the more powerful features of Python.

![Python version](https://img.shields.io/pypi/pyversions/dominate.svg?style=flat)
[![Build status](https://img.shields.io/travis/Knio/dominate/master.svg?style=flat)](https://travis-ci.org/Knio/dominate)
[![Coverage status](https://img.shields.io/coveralls/github/Knio/dominate/master.svg?style=flat)](https://coveralls.io/r/Knio/dominate?branch=master)

Python:

```python
import dominate
from dominate.tags import *

doc = dominate.document(title='Dominate your HTML')

with doc.head:
    link(rel='stylesheet', href='style.css')
    script(type='text/javascript', src='script.js')

with doc:
    with div(id='header').add(ol()):
        for i in ['home', 'about', 'contact']:
            li(a(i.title(), href='/%s.html' % i))

    with div():
        attr(cls='body')
        p('Lorem ipsum..')

print(doc)
```

Output:

```html
<!DOCTYPE html>
<html>
  <head>
    <title>Dominate your HTML</title>
    <link href="style.css" rel="stylesheet">
    <script src="script.js" type="text/javascript"></script>
  </head>
  <body>
    <div id="header">
      <ol>
        <li>
          <a href="/home.html">Home</a>
        </li>
        <li>
          <a href="/about.html">About</a>
        </li>
        <li>
          <a href="/contact.html">Contact</a>
        </li>
      </ol>
    </div>
    <div class="body">
      <p>Lorem ipsum..</p>
    </div>
  </body>
</html>
```


Installation
------------

The recommended way to install `dominate` is with
[`pip`](http://pypi.python.org/pypi/pip/):

    sudo pip install dominate

[![PyPI version](https://img.shields.io/pypi/v/dominate.svg?style=flat)](https://pypi.org/project/dominate/)
[![PyPI downloads](https://img.shields.io/pypi/dm/dominate.svg?style=flat)](https://pypi.org/project/dominate/)



Developed By
------------

* Tom Flanagan - <tom@zkpq.ca>
* Jake Wharton - <jakewharton@gmail.com>
* [Brad Janke](//github.com/bradj)

Git repository located at
[github.com/Knio/dominate](//github.com/Knio/dominate)


Examples
========

All examples assume you have imported the appropriate tags or entire tag set:

```python
from dominate.tags import *
```


Hello, World!
-------------

The most basic feature of `dominate` exposes a class for each HTML element, where the constructor
accepts child elements, text, or keyword attributes. `dominate` nodes return their HTML representation
from the `__str__`, `__unicode__`, and `render()` methods.

```python
print(html(body(h1('Hello, World!'))))
```
```html
<html>
    <body>
        <h1>Hello, World!</h1>
    </body>
</html>
```

Attributes
----------

`Dominate` can also use keyword arguments to append attributes onto your tags. Most of the attributes are a direct copy from the HTML spec with a few variations.

For attributes `class` and `for` which conflict with Python's [reserved keywords](http://docs.python.org/2/reference/lexical_analysis.html#keywords "Reserved Keywords"), you can use the following aliases:

| class | for |
|-------|-----|
|_class | _for |
|cls | fr |
|className|htmlFor|
|class_name|html_for|


```python
test = label(cls='classname anothername', fr='someinput')
print(test)
```
```html
<label class="classname anothername" for="someinput"></label>
```

Use `data_*` for [custom HTML5 data attributes](http://www.w3.org/html/wg/drafts/html/master/dom.html#embedding-custom-non-visible-data-with-the-data-*-attributes "HTML5 Data Attributes").

```python
test = div(data_employee='101011')
print(test)
```
```html
<div data-employee="101011"></div>
```

You can also modify the attributes of tags through a dictionary-like interface:

```python
header = div()
header['id'] = 'header'
print(header)
```
```html
<div id="header"></div>
```

Complex Structures
------------------

Through the use of the `+=` operator and the `.add()` method you can easily create more advanced structures.

Create a simple list:

```python
list = ul()
for item in range(4):
    list += li('Item #', item)
print(list)
```
```html
<ul>
    <li>Item #0</li>
    <li>Item #1</li>
    <li>Item #2</li>
    <li>Item #3</li>
</ul>
```

`dominate` supports iterables to help streamline your code:

```python
print(ul(li(a(name, href=link), __pretty=False) for name, link in menu_items))
```
```html
<ul>
    <li><a href="/home/">Home</a></li>
    <li><a href="/about/">About</a></li>
    <li><a href="/downloads/">Downloads</a></li>
    <li><a href="/links/">Links</a></li>
</ul>
```

A simple document tree:

```python
_html = html()
_body = _html.add(body())
header  = _body.add(div(id='header'))
content = _body.add(div(id='content'))
footer  = _body.add(div(id='footer'))
print(_html)
```
```html
<html>
    <body>
        <div id="header"></div>
        <div id="content"></div>
        <div id="footer"></div>
    </body>
</html>
```

For clean code, the `.add()` method returns children in tuples. The above example can be cleaned up and expanded like this:

```python
_html = html()
_head, _body = _html.add(head(title('Simple Document Tree')), body())
names = ['header', 'content', 'footer']
header, content, footer = _body.add([div(id=name) for name in names])
print(_html)
```
```html
<html>
    <head>
       <title>Simple Document Tree</title>
    </head>
    <body>
        <div id="header"></div>
        <div id="content"></div>
        <div id="footer"></div>
    </body>
</html>
```

You can modify the attributes of tags through a dictionary-like interface:

```python
header = div()
header['id'] = 'header'
print(header)
```
```html
<div id="header"></div>
```

Or the children of a tag though an array-line interface:

```python
header = div('Test')
header[0] = 'Hello World'
print(header)
```
```html
<div>Hello World</div>
```

Comments can be created using objects too!

```python
print(comment('BEGIN HEADER'))
```
```html
<!--BEGIN HEADER-->
```

```python
print(comment(p('Upgrade to newer IE!'), condition='lt IE9'))
```
```html
<!--[if lt IE9]>
  <p>Upgrade to newer IE!</p>
<![endif]-->
```

Rendering
---------

By default, `render()` tries to make all output human readable, with one HTML
element per line and two spaces of indentation.

This behavior can be controlled by the `__pretty` (default: `True` except for
certain element types like `pre`) attribute when creating an element, and by
the `pretty` (default: `True`), `indent` (default: `  `) and `xhtml` (default: `False`)
 arguments to `render()`. Rendering options propagate to all descendant nodes.

```python
a = div(span('Hello World'))
print(a.render())
```
```html
<div>
  <span>Hello World</span>
</div>
```
```python
print(a.render(pretty=False))
```
```html
<div><span>Hello World</span></div>
```
```python
print(a.render(indent='\t'))
```
```html
<div>
	<span>Hello World</span>
</div>
```
```python
a = div(span('Hello World'), __pretty=False)
print(a.render())
```
```html
<div><span>Hello World</span></div>
```
```python
d = div()
with d:
    hr()
    p("Test")
    br()
print(d.render())
print(d.render(xhtml=True))
```
```html
<div>
  <hr>
  <p>Test</p><br>
</div>
<div>
  <hr />
  <p>Test</p><br />
</div>
```


Context Managers
----------------

You can also add child elements using Python's `with` statement:

```python
h = ul()
with h:
    li('One')
    li('Two')
    li('Three')

print(h)
```
```html
<ul>
    <li>One</li>
    <li>Two</li>
    <li>Three</li>
</ul>
```


You can use this along with the other mechanisms of adding children elements, including nesting `with` statements, and it works as expected:

```python
h = html()
with h.add(body()).add(div(id='content')):
    h1('Hello World!')
    p('Lorem ipsum ...')
    with table().add(tbody()):
        l = tr()
        l += td('One')
        l.add(td('Two'))
        with l:
            td('Three')

print(h)
```
```html
<html>
    <body>
        <div id="content">
            <h1>Hello World!</h1>
            <p>Lorem ipsum ...</p>
            <table>
                <tbody>
                    <tr>
                        <td>One</td>
                        <td>Two</td>
                        <td>Three</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </body>
</html>
```

When the context is closed, any nodes that were not already added to something get added to the current context.

Attributes can be added to the current context with the `attr` function:

```python
d = div()
with d:
    attr(id='header')

 print(d)
 ```
 ```html
<div id="header"></div>
```

And text nodes can be added with the `dominate.util.text` function:

```python
from dominate.util import text
para = p(__pretty=False)
with para:
    text('Have a look at our ')
    a('other products', href='/products')

print(para)
```
```html
<p>Have a look at our <a href="/products">other products</a></p>
```


Decorators
----------

`Dominate` is great for creating reusable widgets for parts of your page. Consider this example:

```python
def greeting(name):
    with div() as d:
        p('Hello, %s' % name)
    return d

print(greeting('Bob'))
```
```html
<div>
    <p>Hello, Bob</p>
</div>
```

You can see the following pattern being repeated here:

```python
def widget(parameters):
    with tag() as t:
        ...
    return t
```

This boilerplate can be avoided by using tags (objects and instances) as decorators

```python
@div
def greeting(name):
    p('Hello %s' % name)
print(greeting('Bob'))
```
```html
<div>
    <p>Hello Bob</p>
</div>
```

The decorated function will return a new instance of the tag used to decorate it, and execute in a `with` context which will collect all the nodes created inside it.

You can also use instances of tags as decorators, if you need to add attributes or other data to the root node of the widget.
Each call to the decorated function will return a copy of the node used to decorate it.

```python
@div(h2('Welcome'), cls='greeting')
def greeting(name):
    p('Hello %s' % name)

print(greeting('Bob'))
```
```html

<div class="greeting">
    <h2>Welcome</h2>
    <p>Hello Bob</p>
</div>
```

Creating Documents
------------------

Since creating the common structure of an HTML document everytime would be excessively tedious dominate provides a class to create and manage them for you: `document`.

When you create a new document, the basic HTML tag structure is created for you.

```python
d = document()
print(d)
```
```html
<!DOCTYPE html>
<html>
    <head>
       <title>Dominate</title>
    </head>
    <body></body>
</html>
```

The `document` class accepts `title`, `doctype`, and `request` keyword arguments.
The default values for these arguments are `Dominate`, `<!DOCTYPE html>`, and `None` respectively.

The `document` class also provides helpers to allow you to access the `title`, `head`, and `body` nodes directly.

```python
d = document()
```

```python
>>> d.head
<dominate.tags.head: 0 attributes, 1 children>
>>> d.body
<dominate.tags.body: 0 attributes, 0 children>
>>> d.title
u'Dominate'
```


The `document` class also provides helpers to allow you to directly add nodes to the `body` tag.

```python
d = document()
d += h1('Hello, World!')
d += p('This is a paragraph.')
print(d)
```
```html
<!DOCTYPE html>
<html>
    <head>
       <title>Dominate</title>
    </head>
    <body>
        <h1>Hello, World!</h1>
        <p>This is a paragraph.</p>
    </body>
</html>
```

Embedding HTML
--------------

If you need to embed a node of pre-formed HTML coming from a library such as markdown or the like, you can avoid escaped HTML by using the raw method from the dominate.util package:

```
from dominate.util import raw
...
td(raw('<a href="example.html">Example</a>'))
```

Without the raw call, this code would render escaped HTML with lt, etc.


SVG
---

The `dominate.svg` module contains SVG tags similar to how `dominate.tags` contains HTML tags. SVG elements will automatically convert `_` to `-` for dashed elements. For example:

```python
from dominate.svg import *
print(circle(stroke_width=5))
```

```html
<circle stroke-width="5"></circle>
```




  * [dpcontracts-0.6.0](https://github.com/deadpixi/contracts) Introduction
============
This module provides a collection of decorators that makes it easy to
write software using contracts.

Contracts are a debugging and verification tool.  They are declarative
statements about what states a program must be in to be considered
"correct" at runtime.  They are similar to assertions, and are verified
automatically at various well-defined points in the program.  Contracts can
be specified on functions and on classes.

Contracts serve as a form of documentation and a way of formally
specifying program behavior.  Good practice often includes writing all of
the contracts first, with these contract specifying the exact expected
state before and after each function or method call and the things that
should always be true for a given class of object.

Contracts consist of two parts: a description and a condition.  The
description is simply a human-readable string that describes what the
contract is testing, while the condition is a single function that tests
that condition.  The condition is executed automatically and passed certain
arguments (which vary depending on the type of contract), and must return
a boolean value: True if the condition has been met, and False otherwise.

Legacy Python Support
=====================
This module supports versions of Python >= 3.5; that is, versions with
support for "async def" functions.  There is a branch of this module that
is kept compatible to the greatest possible degree for versions of Python
earlier than 3.5 (including Python 2.7).

The Python 2 and <= 3.5 branch is available at
https://github.com/deadpixi/contracts/tree/python2

This legacy-compatible version is also distributed on PyPI along the 0.5.x
branch; this branch will kept compatible with newer versions to the greatest
extent possible.

That branch is a drop-in replacement for this module and includes all
functionality except support for "async def" functions.

Preconditions and Postconditions
================================
Contracts on functions consist of preconditions and postconditions.
A precondition is declared using the `requires` decorator, and describes
what must be true upon entrance to the function. The condition function
is passed an arguments object, which as as its attributes the arguments
to the decorated function:

    >>> @require("`i` must be an integer", lambda args: isinstance(args.i, int))
    ... @require("`j` must be an integer", lambda args: isinstance(args.j, int))
    ... def add2(i, j):
    ...   return i + j

Note that an arbitrary number of preconditions can be stacked on top of
each other.

These decorators have declared that the types of both arguments must be
integers.  Calling the `add2` function with the correct types of arguments
works:

    >>> add2(1, 2)
    3

But calling with incorrect argument types (violating the contract) fails
with a PreconditionError (a subtype of AssertionError):

    >>> add2("foo", 2)
    Traceback (most recent call last):
    PreconditionError: `i` must be an integer

Functions can also have postconditions, specified using the `ensure`
decorator.  Postconditions describe what must be true after the function
has successfully returned.  Like the `require` decorator, the `ensure`
decorator is passed an argument object.  It is also passed an additional
argument, which is the result of the function invocation.  For example:

    >>> @require("`i` must be a positive integer",
    ...          lambda args: isinstance(args.i, int) and args.i > 0)
    ... @require("`j` must be a positive integer",
    ...          lambda args: isinstance(args.j, int) and args.j > 0)
    ... @ensure("the result must be greater than either `i` or `j`",
    ...         lambda args, result: result > args.i and result > args.j)
    ... def add2(i, j):
    ...     if i == 7:
    ...        i = -7 # intentionally broken for purposes of example
    ...     return i + j

We can now call the function and ensure that everything is working correctly:

    >>> add2(1, 3)
    4

Except that the function is broken in unexpected ways:

    >>> add2(7, 4)
    Traceback (most recent call last):
    PostconditionError: the result must be greater than either `i` or `j`

The function specifying the condition doesn't have to be a lambda; it can be
any function, and pre- and postconditions don't have to actually reference
the arguments or results of the function at all.  They can simply check
the function's environments and effects:

    >>> names = set()
    >>> def exists_in_database(x):
    ...   return x in names
    >>> @require("`name` must be a string", lambda args: isinstance(args.name, str))
    ... @require("`name` must not already be in the database",
    ...          lambda args: not exists_in_database(args.name.strip()))
    ... @ensure("the normalized version of the name must be added to the database",
    ...         lambda args, result: exists_in_database(args.name.strip()))
    ... def add_to_database(name):
    ...     if name not in names and name != "Rob": # intentionally broken
    ...         names.add(name.strip())

    >>> add_to_database("James")
    >>> add_to_database("Marvin")
    >>> add_to_database("Marvin")
    Traceback (most recent call last):
    PreconditionError: `name` must not already be in the database
    >>> add_to_database("Rob")
    Traceback (most recent call last):
    PostconditionError: the normalized version of the name must be added to the database

All of the various calling conventions of Python are supported:

    >>> @require("`a` is an integer", lambda args: isinstance(args.a, int))
    ... @require("`b` is a string", lambda args: isinstance(args.b, str))
    ... @require("every member of `c` should be a boolean",
    ...          lambda args: all(isinstance(x, bool) for x in args.c))
    ... def func(a, b="Foo", *c):
    ...     pass

    >>> func(1, "foo", True, True, False)
    >>> func(b="Foo", a=7)
    >>> args = {"a": 8, "b": "foo"}
    >>> func(**args)
    >>> args = (1, "foo", True, True, False)
    >>> func(*args)
    >>> args = {"a": 9}
    >>> func(**args)
    >>> func(10)

A common contract is to validate the types of arguments. To that end,
there is an additional decorator, `types`, that can be used
to validate arguments' types:

    >>> class ExampleClass:
    ...     pass

    >>> @types(a=int, b=str, c=(type(None), ExampleClass)) # or types.NoneType, if you prefer
    ... @require("a must be nonzero", lambda args: args.a != 0)
    ... def func(a, b, c=38):
    ...     return " ".join(str(x) for x in [a, b])

    >>> func(1, "foo", ExampleClass())
    '1 foo'

    >>> func(1.0, "foo", ExampleClass) # invalid type for `a`
    Traceback (most recent call last):
    PreconditionError: the types of arguments must be valid

    >>> func(1, "foo") # invalid type (the default) for `c`
    Traceback (most recent call last):
    PreconditionError: the types of arguments must be valid

Contracts on Classes
====================
The `require` and `ensure` decorators can be used on class methods too,
not just bare functions:

    >>> class Foo:
    ...     @require("`name` should be nonempty", lambda args: len(args.name) > 0)
    ...     def __init__(self, name):
    ...         self.name = name

    >>> foo = Foo()
    Traceback (most recent call last):
    TypeError: __init__ missing required positional argument: 'name'

    >>> foo = Foo("")
    Traceback (most recent call last):
    PreconditionError: `name` should be nonempty

Classes may also have an additional sort of contract specified over them:
the invariant.  An invariant, created using the `invariant` decorator,
specifies a condition that must always be true for instances of that class.
In this case, "always" means "before invocation of any method and after
its return" -- methods are allowed to violate invariants so long as they
are restored prior to return.

Invariant contracts are passed a single variable, a reference to the
instance of the class. For example:

    >>> @invariant("inner list can never be empty", lambda self: len(self.lst) > 0)
    ... @invariant("inner list must consist only of integers",
    ...            lambda self: all(isinstance(x, int) for x in self.lst))
    ... class NonemptyList:
    ...     @require("initial list must be a list", lambda args: isinstance(args.initial, list))
    ...     @require("initial list cannot be empty", lambda args: len(args.initial) > 0)
    ...     @ensure("the list instance variable is equal to the given argument",
    ...             lambda args, result: args.self.lst == args.initial)
    ...     @ensure("the list instance variable is not an alias to the given argument",
    ...             lambda args, result: args.self.lst is not args.initial)
    ...     def __init__(self, initial):
    ...         self.lst = initial[:]
    ...
    ...     def get(self, i):
    ...         return self.lst[i]
    ...
    ...     def pop(self):
    ...         self.lst.pop()
    ...
    ...     def as_string(self):
    ...         # Build up a string representation using the `get` method,
    ...         # to illustrate methods calling methods with invariants.
    ...         return ",".join(str(self.get(i)) for i in range(0, len(self.lst)))

    >>> nl = NonemptyList([1,2,3])
    >>> nl.pop()
    >>> nl.pop()
    >>> nl.pop()
    Traceback (most recent call last):
    PostconditionError: inner list can never be empty

    >>> nl = NonemptyList(["a", "b", "c"])
    Traceback (most recent call last):
    PostconditionError: inner list must consist only of integers

Violations of invariants are ignored in the following situations:

    - before calls to __init__ and __new__ (since the object is still
      being initialized)

    - before and after calls to any method whose name begins with "__",
      except for methods implementing arithmetic and comparison operations
      and container type emulation (because such methods are private and
      expected to manipulate the object's inner state, plus things get hairy
      with certain applications of `__getattr(ibute)?__`)

    - before and after calls to methods added from outside the initial
      class definition (because invariants are processed only at class
      definition time)

    - before and after calls to classmethods, since they apply to the class
      as a whole and not any particular instance

For example:

    >>> @invariant("`always` should be True", lambda self: self.always)
    ... class Foo:
    ...     always = True
    ...
    ...     def get_always(self):
    ...         return self.always
    ...
    ...     @classmethod
    ...     def break_everything(cls):
    ...         cls.always = False

    >>> x = Foo()
    >>> x.get_always()
    True
    >>> x.break_everything()
    >>> x.get_always()
    Traceback (most recent call last):
    PreconditionError: `always` should be True

Also note that if a method invokes another method on the same object,
all of the invariants will be tested again:

    >>> nl = NonemptyList([1,2,3])
    >>> nl.as_string() == '1,2,3'
    True

Transforming Data in Contracts
==============================
In general, you should avoid transforming data inside a contract; contracts
themselves are supposed to be side-effect-free.

However, this is not always possible in Python.

Take, for example, iterables passed as arguments. We might want to verify
that a given set of properties hold for every item in the iterable. The
obvious solution would be to do something like this:

    >>> @require("every item in `l` must be > 0", lambda args: all(x > 0 for x in args.l))
    ... def my_func(l):
    ...     return sum(l)

This works well in most situations:

    >>> my_func([1, 2, 3])
    6
    >>> my_func([0, -1, 2])
    Traceback (most recent call last):
    PreconditionError: every item in `l` must be > 0

But it fails in the case of a generator:

    >>> def iota(n):
    ...     for i in range(1, n):
    ...         yield i

    >>> sum(iota(5))
    10
    >>> my_func(iota(5))
    0

The call to `my_func` has a result of 0 because the generator was consumed
inside the `all` call inside the contract. Obviously, this is problematic.

Sadly, there is no generic solution to this problem. In a statically-typed
language, the compiler can verify that some properties of infinite lists
(though not all of them, and what exactly depends on the type system).

We get around that limitation here using an additional decorator, called
`transform` that transforms the arguments to a function, and a function
called `rewrite` that rewrites argument tuples.

For example:

    >>> @transform(lambda args: rewrite(args, l=list(args.l)))
    ... @require("every item in `l` must be > 0", lambda args: all(x > 0 for x in args.l))
    ... def my_func(l):
    ...     return sum(l)
    >>> my_func(iota(5))
    10

Note that this does not completely solve the problem of infinite sequences,
but it does allow for verification of any desired prefix of such a sequence.

This works for class methods too, of course:

    >>> class TestClass:
    ...     @transform(lambda args: rewrite(args, l=list(args.l)))
    ...     @require("every item in `l` must be > 0", lambda args: all(x > 0 for x in args.l))
    ...     def my_func(self, l):
    ...         return sum(l)
    >>> TestClass().my_func(iota(5))
    10

Contracts on Asynchronous Functions (aka coroutine functions)
=============================================================
Contracts can be placed on coroutines (that is, async functions):

    >>> import asyncio
    >>> @require("`a` is an integer", lambda args: isinstance(args.a, int))
    ... @require("`b` is a string", lambda args: isinstance(args.b, str))
    ... @require("every member of `c` should be a boolean",
    ...          lambda args: all(isinstance(x, bool) for x in args.c))
    ... async def func(a, b="Foo", *c):
    ...     await asyncio.sleep(1)

    >>> asyncio.get_event_loop().run_until_complete(
    ...     func( 1, "foo", True, True, False))

Predicates functions themselves cannot be coroutines, as this could
influence the run loop:

    >>> async def coropred_aisint(e):
    ...     await asyncio.sleep(1)
    ...     return isinstance(getattr(e, 'a'), int)
    >>> @require("`a` is an integer", coropred_aisint)
    ... @require("`b` is a string", lambda args: isinstance(args.b, str))
    ... @require("every member of `c` should be a boolean",
    ...          lambda args: all(isinstance(x, bool) for x in args.c))
    ... async def func(a, b="Foo", *c):
    ...     await asyncio.sleep(1)
    Traceback (most recent call last):
    AssertionError: contract predicates cannot be coroutines

Contracts and Debugging
=======================
Contracts are a documentation and testing tool; they are not intended
to be used to validate user input or implement program logic.  Indeed,
running Python with `__debug__` set to False (e.g. by calling the Python
intrepreter with the "-O" option) disables contracts.

Testing This Module
===================
This module has embedded doctests that are run with the module is invoked
from the command line.  Simply run the module directly to run the tests.

Contact Information and Licensing
=================================
This module has a home page at `GitHub <https://github.com/deadpixi/contracts>`_.

This module was written by Rob King (jking@deadpixi.com).

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
  * [drmaa-0.7.9](https://github.com/pygridtools/drmaa-python) DRMAA Python
------------

.. image:: https://img.shields.io/travis/pygridtools/drmaa-python/stable.svg
   :alt: Build status
   :target: https://travis-ci.org/pygridtools/drmaa-python

.. image:: https://img.shields.io/coveralls/pygridtools/drmaa-python/stable.svg
    :target: https://coveralls.io/r/pygridtools/drmaa-python

.. image:: https://img.shields.io/pypi/dm/drmaa.svg
   :target: https://warehouse.python.org/project/drmaa/
   :alt: PyPI downloads

.. image:: https://img.shields.io/pypi/v/drmaa.svg
   :target: https://warehouse.python.org/project/drmaa/
   :alt: Latest version on PyPI

.. image:: https://img.shields.io/pypi/l/drmaa.svg
   :alt: License

`Distributed Resource Management Application API <http://en.wikipedia.org/wiki/DRMAA>`__
(DRMAA) bindings for Python. For more information
`read the docs <http://drmaa-python.readthedocs.org>`__.  

If you simply want to run Python functions on a DRMAA-compatible grid, use
`GridMap <https://github.com/pygridtools/gridmap>`__.

Requirements
~~~~~~~~~~~~

-  Python 2.7+
-  A DRMAA-compatible cluster (e.g., Grid Engine)

Installation
~~~~~~~~~~~~

To use the DRMAA Python library, you need to install it via ``pip``:


.. code-block:: bash

   pip install drmaa


and then setup any environment variables that are necessary for your particular DRM system.
For SGE, this means ``SGE_ROOT`` and ``SGE_CELL``, which should be set as follows:


.. code-block:: bash
   
   export SGE_ROOT=/path/to/gridengine
   export SGE_CELL=default


where ``/path/to/gridengine/`` is replaced with the actual path to your Grid Engine installation, 
and ``default`` is replaced with your installation's actual cell. The path is typically 
``/var/lib/gridengine``.

You will also need access to the ``libdrmaa.so.1.0`` C library, which can often be installed as
part of the ``libdrmaa-dev`` package on most Unixes. Once you have installed that, you may need to 
tell DRMAA Python where it is installed by setting the ``DRMAA_LIBRARY_PATH`` environment variable,
if it is not installed in a location that Python usually looks for libraries.


.. code-block:: bash

   export DRMAA_LIBRARY_PATH=/usr/lib/libdrmaa.so.1.0

Acknowledgments
~~~~~~~~~~~~~~~

Thank you to `StatPro <http://www.statpro.com/>`__ and 
`Educational Testing Service <https://github.com/EducationalTestingService>`__ for
funding the development of DRMAA Python.

Changelog
~~~~~~~~~

`See GitHub releases <https://github.com/drmaa-python/drmaa-python/releases>`__.

  * [dulwich-0.19.11](https://www.dulwich.io/) .. image:: https://travis-ci.org/dulwich/dulwich.png?branch=master
  :alt: Build Status
  :target: https://travis-ci.org/dulwich/dulwich

.. image:: https://ci.appveyor.com/api/projects/status/mob7g4vnrfvvoweb?svg=true
  :alt: Windows Build Status
  :target: https://ci.appveyor.com/project/jelmer/dulwich/branch/master

This is the Dulwich project.

It aims to provide an interface to git repos (both local and remote) that
doesn't call out to git directly but instead uses pure Python.

**Main website**: <https://www.dulwich.io/>

**License**: Apache License, version 2 or GNU General Public License, version 2 or later.

The project is named after the part of London that Mr. and Mrs. Git live in
in the particular Monty Python sketch.

Installation
------------

By default, Dulwich' setup.py will attempt to build and install the optional C
extensions. The reason for this is that they significantly improve the performance
since some low-level operations that are executed often are much slower in CPython.

If you don't want to install the C bindings, specify the --pure argument to setup.py::

    $ python setup.py --pure install

or if you are installing from pip::

    $ pip install dulwich --global-option="--pure"

Note that you can also specify --global-option in a
`requirements.txt <https://pip.pypa.io/en/stable/reference/pip_install/#requirement-specifiers>`_
file, e.g. like this::

    dulwich --global-option=--pure

Getting started
---------------

Dulwich comes with both a lower-level API and higher-level plumbing ("porcelain").

For example, to use the lower level API to access the commit message of the
last commit::

    >>> from dulwich.repo import Repo
    >>> r = Repo('.')
    >>> r.head()
    '57fbe010446356833a6ad1600059d80b1e731e15'
    >>> c = r[r.head()]
    >>> c
    <Commit 015fc1267258458901a94d228e39f0a378370466>
    >>> c.message
    'Add note about encoding.\n'

And to print it using porcelain::

    >>> from dulwich import porcelain
    >>> porcelain.log('.', max_entries=1)
    --------------------------------------------------
    commit: 57fbe010446356833a6ad1600059d80b1e731e15
    Author: Jelmer Vernooĳ <jelmer@jelmer.uk>
    Date:   Sat Apr 29 2017 23:57:34 +0000

    Add note about encoding.

Further documentation
---------------------

The dulwich documentation can be found in docs/ and built by running ``make
doc``. It can also be found `on the web <https://www.dulwich.io/docs/>`_.

Help
----

There is a *#dulwich* IRC channel on the `Freenode <https://www.freenode.net/>`_, and
`dulwich-announce <https://groups.google.com/forum/#!forum/dulwich-announce>`_
and `dulwich-discuss <https://groups.google.com/forum/#!forum/dulwich-discuss>`_
mailing lists.

Contributing
------------

For a full list of contributors, see the git logs or `AUTHORS <AUTHORS>`_.

If you'd like to contribute to Dulwich, see the `CONTRIBUTING <CONTRIBUTING.rst>`_
file and `list of open issues <https://github.com/dulwich/dulwich/issues>`_.

Supported versions of Python
----------------------------

At the moment, Dulwich supports (and is tested on) CPython 2.7, 3.4, 3.5, 3.6,
3.7 and Pypy.
  * [easygui-0.98.1](https://github.com/robertlugg/easygui) ABOUT EASYGUI
=============

EasyGui provides an easy-to-use interface for simple GUI interaction
with a user.  It does not require the programmer to know anything about
tkinter, frames, widgets, callbacks or lambda.  All GUI interactions are
invoked by simple function calls that return results.

Example Usage
-------------

    >>> import easygui
    >>> easygui.ynbox('Shall I continue?', 'Title', ('Yes', 'No'))
    1
    >>> easygui.msgbox('This is a basic message box.', 'Title Goes Here')
    'OK'
    >>> easygui.buttonbox('Click on your favorite flavor.', 'Favorite Flavor', ('Chocolate', 'Vanilla', 'Strawberry'))
    'Chocolate'

Full documentation is always available.

For the most-recent production version:
<http://easygui.readthedocs.org/en/master/>.


LICENSE INFORMATION
===================
EasyGui version |version|

Copyright (c) 2015, Easygui developers and Stephen Raymond Ferg

All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

    1. Redistributions of source code must retain the above copyright notice,
       this list of conditions and the following disclaimer.

    2. Redistributions in binary form must reproduce the above copyright notice,
       this list of conditions and the following disclaimer in the documentation and/or
       other materials provided with the distribution.

    3. The name of the author may not be used to endorse or promote products derived
       from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


ABOUT THE EASYGUI LICENSE
-------------------------
| This license is what is generally known as the "modified BSD license",
| aka "revised BSD", "new BSD", "3-clause BSD".
| See http://www.opensource.org/licenses/bsd-license.php
|
| This license is GPL-compatible.
| See `<http://en.wikipedia.org/wiki/License_compatibility>`_
| See http://www.gnu.org/licenses/license-list.html#GPLCompatibleLicenses
|
| The BSD License is less restrictive than GPL.
| It allows software released under the license to be incorporated into proprietary products.
| Works based on the software may be released under a proprietary license or as closed source software.
| `<http://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_.28.22New_BSD_License.22.29>`_
  * [ecdsa-0.13](http://github.com/warner/python-ecdsa) # Pure-Python ECDSA

[![build status](https://travis-ci.org/warner/python-ecdsa.png)](http://travis-ci.org/warner/python-ecdsa)
[![Coverage Status](https://coveralls.io/repos/warner/python-ecdsa/badge.svg)](https://coveralls.io/r/warner/python-ecdsa)
[![Latest Version](https://pypip.in/version/ecdsa/badge.svg?style=flat)](https://pypi.python.org/pypi/ecdsa/)


This is an easy-to-use implementation of ECDSA cryptography (Elliptic Curve
Digital Signature Algorithm), implemented purely in Python, released under
the MIT license. With this library, you can quickly create keypairs (signing
key and verifying key), sign messages, and verify the signatures. The keys
and signatures are very short, making them easy to handle and incorporate
into other protocols.

## Features

This library provides key generation, signing, and verifying, for five
popular NIST "Suite B" GF(p) curves, with key lengths of 192, 224, 256, 384,
and 521 bits. The "short names" for these curves, as known by the OpenSSL
tool (`openssl ecparam --list_curves`), are: prime192v1, secp224r1,
prime256v1, secp384r1, and secp521r1. It also includes the 256-bit curve used
by Bitcoin, whose short name is secp256k1. No other curves are included, but
it would not be too hard to add more.

## Dependencies

This library uses only Python. It requires python2.6 or later versions of the
python2.x series. It is also compatible with python3.2 and 3.3.

To run the OpenSSL compatibility tests, the 'openssl' tool must be on your
$PATH. This release has been tested successfully against both OpenSSL 0.9.8o
and 1.0.0a .

## Speed

The following table shows how long this library takes to generate keypairs
(keygen=), to sign data (sign=), and to verify those signatures (verify=), on
my 2008 Mac laptop. All times are in seconds. It also shows the length of a
signature (in bytes): the verifying ("public") key is typically the same
length as the signature, and the signing ("private") key is half that length. Use "python setup.py speed" to generate this table on your own computer.

* NIST192p: siglen= 48, keygen=0.160s, sign=0.058s, verify=0.116s
* NIST224p: siglen= 56, keygen=0.230s, sign=0.086s, verify=0.165s
* NIST256p: siglen= 64, keygen=0.305s, sign=0.112s, verify=0.220s
* NIST384p: siglen= 96, keygen=0.801s, sign=0.289s, verify=0.558s
* NIST521p: siglen=132, keygen=1.582s, sign=0.584s, verify=1.152s

For comparison, a quality C++ implementation of ECDSA (Crypto++) typically
computes a NIST256p signature in 2.88ms and a verification in 8.53ms, about
30-40x faster.

Keys and signature can be serialized in different ways (see Usage, below).
For a NIST192p key, the three basic representations require strings of the
following lengths (in bytes):

    to_string:  signkey= 24, verifykey= 48, signature=48
    DER:        signkey=106, verifykey= 80, signature=55
    PEM:        signkey=278, verifykey=162, (no support for PEM signatures)

## History

In 2006, Peter Pearson announced his pure-python implementation of ECDSA in a
[message to sci.crypt][1], available from his [download site][2]. In 2010,
Brian Warner wrote a wrapper around this code, to make it a bit easier and
safer to use. You are looking at the README for this wrapper.

[1]: http://www.derkeiler.com/Newsgroups/sci.crypt/2006-01/msg00651.html
[2]: http://webpages.charter.net/curryfans/peter/downloads.html

## Testing

There are four test suites, three for the original Pearson module, and one
more for the wrapper. To run them all, do this:

    python setup.py test
    tox -e coverage

On my 2014 Mac Mini, the combined tests take about 20 seconds to run. On a
2.4GHz P4 Linux box, they take 81 seconds.

One component of `test_pyecdsa.py` checks compatibility with OpenSSL, by
running the "openssl" CLI tool. If this tool is not on your $PATH, you may
want to comment out this test (the easiest way is to add a line that says
"del OpenSSL" to the end of test_pyecdsa.py).

## Security

This library does not protect against timing attacks. Do not allow attackers
to measure how long it takes you to generate a keypair or sign a message.
This library depends upon a strong source of random numbers. Do not use it on
a system where os.urandom() is weak.

## Usage

You start by creating a SigningKey. You can use this to sign data, by passing
in a data string and getting back the signature (also a string). You can also
ask a SigningKey to give you the corresponding VerifyingKey. The VerifyingKey
can be used to verify a signature, by passing it both the data string and the
signature string: it either returns True or raises BadSignatureError.

    from ecdsa import SigningKey
    sk = SigningKey.generate() # uses NIST192p
    vk = sk.get_verifying_key()
    signature = sk.sign("message")
    assert vk.verify(signature, "message")

Each SigningKey/VerifyingKey is associated with a specific curve, like
NIST192p (the default one). Longer curves are more secure, but take longer to
use, and result in longer keys and signatures.

    from ecdsa import SigningKey, NIST384p
    sk = SigningKey.generate(curve=NIST384p)
    vk = sk.get_verifying_key()
    signature = sk.sign("message")
    assert vk.verify(signature, "message")

The SigningKey can be serialized into several different formats: the shortest
is to call `s=sk.to_string()`, and then re-create it with
`SigningKey.from_string(s, curve)` . This short form does not record the
curve, so you must be sure to tell from_string() the same curve you used for
the original key. The short form of a NIST192p-based signing key is just 24
bytes long. If the point encoding is invalid or it does not lie on the
specified curve, `from_string()` will raise MalformedPointError.

    from ecdsa import SigningKey, NIST384p
    sk = SigningKey.generate(curve=NIST384p)
    sk_string = sk.to_string()
    sk2 = SigningKey.from_string(sk_string, curve=NIST384p)
    # sk and sk2 are the same key

`sk.to_pem()` and `sk.to_der()` will serialize the signing key into the same
formats that OpenSSL uses. The PEM file looks like the familiar ASCII-armored
`"-----BEGIN EC PRIVATE KEY-----"` base64-encoded format, and the DER format
is a shorter binary form of the same data.
`SigningKey.from_pem()/.from_der()` will undo this serialization. These
formats include the curve name, so you do not need to pass in a curve
identifier to the deserializer. In case the file is malformed `from_der()`
and `from_pem()` will raise UnexpectedDER or MalformedPointError.

    from ecdsa import SigningKey, NIST384p
    sk = SigningKey.generate(curve=NIST384p)
    sk_pem = sk.to_pem()
    sk2 = SigningKey.from_pem(sk_pem)
    # sk and sk2 are the same key

Likewise, the VerifyingKey can be serialized in the same way:
`vk.to_string()/VerifyingKey.from_string()`, `to_pem()/from_pem()`, and
`to_der()/from_der()`. The same curve= argument is needed for
`VerifyingKey.from_string()`.

    from ecdsa import SigningKey, VerifyingKey, NIST384p
    sk = SigningKey.generate(curve=NIST384p)
    vk = sk.get_verifying_key()
    vk_string = vk.to_string()
    vk2 = VerifyingKey.from_string(vk_string, curve=NIST384p)
    # vk and vk2 are the same key

    from ecdsa import SigningKey, VerifyingKey, NIST384p
    sk = SigningKey.generate(curve=NIST384p)
    vk = sk.get_verifying_key()
    vk_pem = vk.to_pem()
    vk2 = VerifyingKey.from_pem(vk_pem)
    # vk and vk2 are the same key

There are a couple of different ways to compute a signature. Fundamentally,
ECDSA takes a number that represents the data being signed, and returns a
pair of numbers that represent the signature. The hashfunc= argument to
`sk.sign()` and `vk.verify()` is used to turn an arbitrary string into
fixed-length digest, which is then turned into a number that ECDSA can sign,
and both sign and verify must use the same approach. The default value is
hashlib.sha1, but if you use NIST256p or a longer curve, you can use
hashlib.sha256 instead.

There are also multiple ways to represent a signature. The default
`sk.sign()` and `vk.verify()` methods present it as a short string, for
simplicity and minimal overhead. To use a different scheme, use the
`sk.sign(sigencode=)` and `vk.verify(sigdecode=)` arguments. There are helper
funcions in the "ecdsa.util" module that can be useful here.

It is also possible to create a SigningKey from a "seed", which is
deterministic. This can be used in protocols where you want to derive
consistent signing keys from some other secret, for example when you want
three separate keys and only want to store a single master secret. You should
start with a uniformly-distributed unguessable seed with about curve.baselen
bytes of entropy, and then use one of the helper functions in ecdsa.util to
convert it into an integer in the correct range, and then finally pass it
into `SigningKey.from_secret_exponent()`, like this:

    from pyecdsa import NIST384p, SigningKey
    from pyecdsa.util import randrange_from_seed__trytryagain

    def make_key(seed):
      secexp = randrange_from_seed__trytryagain(seed, NIST384p.order)
      return SigningKey.from_secret_exponent(secexp, curve=NIST384p)

    seed = os.urandom(NIST384p.baselen) # or other starting point
    sk1a = make_key(seed)
    sk1b = make_key(seed)
    # note: sk1a and sk1b are the same key
    sk2 = make_key("2-"+seed)  # different key

## OpenSSL Compatibility

To produce signatures that can be verified by OpenSSL tools, or to verify
signatures that were produced by those tools, use:

    # openssl ecparam -name secp224r1 -genkey -out sk.pem
    # openssl ec -in sk.pem -pubout -out vk.pem
    # openssl dgst -ecdsa-with-SHA1 -sign sk.pem -out data.sig data
    # openssl dgst -ecdsa-with-SHA1 -verify vk.pem -signature data.sig data
    # openssl dgst -ecdsa-with-SHA1 -prverify sk.pem -signature data.sig data

    sk.sign(msg, hashfunc=hashlib.sha1, sigencode=ecdsa.util.sigencode_der)
    vk.verify(sig, msg, hashfunc=hashlib.sha1, sigdecode=ecdsa.util.sigdecode_der)

The keys that openssl handles can be read and written as follows:

    sk = SigningKey.from_pem(open("sk.pem").read())
    open("sk.pem","w").write(sk.to_pem())

    vk = VerifyingKey.from_pem(open("vk.pem").read())
    open("vk.pem","w").write(vk.to_pem())

## Entropy

Creating a signing key with `SigningKey.generate()` requires some form of
entropy (as opposed to `from_secret_exponent/from_string/from_der/from_pem`,
which are deterministic and do not require an entropy source). The default
source is `os.urandom()`, but you can pass any other function that behaves
like os.urandom as the entropy= argument to do something different. This may
be useful in unit tests, where you want to achieve repeatable results. The
ecdsa.util.PRNG utility is handy here: it takes a seed and produces a strong
pseudo-random stream from it:

    from ecdsa.util import PRNG
    from ecdsa import SigningKey
    rng1 = PRNG("seed")
    sk1 = SigningKey.generate(entropy=rng1)
    rng2 = PRNG("seed")
    sk2 = SigningKey.generate(entropy=rng2)
    # sk1 and sk2 are the same key

Likewise, ECDSA signature generation requires a random number, and each
signature must use a different one (using the same number twice will
immediately reveal the private signing key). The `sk.sign()` method takes an
entropy= argument which behaves the same as `SigningKey.generate(entropy=)`.

## Deterministic Signatures

If you call `SigningKey.sign_deterministic(data)` instead of `.sign(data)`,
the code will generate a deterministic signature instead of a random one.
This uses the algorithm from RFC6979 to safely generate a unique `k` value,
derived from the private key and the message being signed. Each time you sign
the same message with the same key, you will get the same signature (using
the same `k`).

This may become the default in a future version, as it is not vulnerable to
failures of the entropy source.

## Examples

Create a NIST192p keypair and immediately save both to disk:

    from ecdsa import SigningKey
    sk = SigningKey.generate()
    vk = sk.get_verifying_key()
    open("private.pem","w").write(sk.to_pem())
    open("public.pem","w").write(vk.to_pem())

Load a signing key from disk, use it to sign a message, and write the
signature to disk:

    from ecdsa import SigningKey
    sk = SigningKey.from_pem(open("private.pem").read())
    message = open("message","rb").read()
    sig = sk.sign(message)
    open("signature","wb").write(sig)

Load the verifying key, message, and signature from disk, and verify the
signature:

    from ecdsa import VerifyingKey, BadSignatureError
    vk = VerifyingKey.from_pem(open("public.pem").read())
    message = open("message","rb").read()
    sig = open("signature","rb").read()
    try:
      vk.verify(sig, message)
      print "good signature"
    except BadSignatureError:
      print "BAD SIGNATURE"

Create a NIST521p keypair

    from ecdsa import SigningKey, NIST521p
    sk = SigningKey.generate(curve=NIST521p)
    vk = sk.get_verifying_key()

Create three independent signing keys from a master seed:

    from pyecdsa import NIST192p, SigningKey
    from pyecdsa.util import randrange_from_seed__trytryagain

    def make_key_from_seed(seed, curve=NIST192p):
      secexp = randrange_from_seed__trytryagain(seed, curve.order)
      return SigningKey.from_secret_exponent(secexp, curve)

    sk1 = make_key_from_seed("1:%s" % seed)
    sk2 = make_key_from_seed("2:%s" % seed)
    sk3 = make_key_from_seed("3:%s" % seed)



  * [elasticsearch-7.0.2](https://github.com/elastic/elasticsearch-py) Python Elasticsearch Client
===========================

Official low-level client for Elasticsearch. Its goal is to provide common
ground for all Elasticsearch-related code in Python; because of this it tries
to be opinion-free and very extendable.

For a more high level client library with more limited scope, have a look at
`elasticsearch-dsl`_ - a more pythonic library sitting on top of
``elasticsearch-py``.

It provides a more convenient and idiomatic way to write and manipulate
`queries`_. It stays close to the Elasticsearch JSON DSL, mirroring its
terminology and structure while exposing the whole range of the DSL from Python
either directly using defined classes or a queryset-like expressions.

It also provides an optional `persistence layer`_ for working with documents as
Python objects in an ORM-like fashion: defining mappings, retrieving and saving
documents, wrapping the document data in user-defined classes.

.. _elasticsearch-dsl: https://elasticsearch-dsl.readthedocs.io/
.. _queries: https://elasticsearch-dsl.readthedocs.io/en/latest/search_dsl.html
.. _persistence layer: https://elasticsearch-dsl.readthedocs.io/en/latest/persistence.html#doctype

Compatibility
-------------

The library is compatible with all Elasticsearch versions since ``0.90.x`` but you
**have to use a matching major version**:

For **Elasticsearch 7.0** and later, use the major version 7 (``7.x.y``) of the
library.

For **Elasticsearch 6.0** and later, use the major version 6 (``6.x.y``) of the
library.

For **Elasticsearch 5.0** and later, use the major version 5 (``5.x.y``) of the
library.

For **Elasticsearch 2.0** and later, use the major version 2 (``2.x.y``) of the
library, and so on.

The recommended way to set your requirements in your `setup.py` or
`requirements.txt` is::

    # Elasticsearch 7.x
    elasticsearch>=7.0.0,<8.0.0

    # Elasticsearch 6.x
    elasticsearch6>=6.0.0,<7.0.0

    # Elasticsearch 5.x
    elasticsearch>=5.0.0,<6.0.0

    # Elasticsearch 2.x
    elasticsearch>=2.0.0,<3.0.0

If you have a need to have multiple versions installed at the same time older
versions are also released as ``elasticsearch2`` and ``elasticsearch5``.

Installation
------------

Install the ``elasticsearch`` package with `pip
<https://pypi.python.org/pypi/elasticsearch>`_::

    pip install elasticsearch


Example use
-----------

Simple use-case::

    >>> from datetime import datetime
    >>> from elasticsearch import Elasticsearch

    # by default we connect to localhost:9200
    >>> es = Elasticsearch()

    # create an index in elasticsearch, ignore status code 400 (index already exists)
    >>> es.indices.create(index='my-index', ignore=400)
    {'acknowledged': True, 'shards_acknowledged': True, 'index': 'my-index'}

    # datetimes will be serialized
    >>> es.index(index="my-index", id=42, body={"any": "data", "timestamp": datetime.now()})
    {'_index': 'my-index',
     '_type': '_doc',
     '_id': '42',
     '_version': 1,
     'result': 'created',
     '_shards': {'total': 2, 'successful': 1, 'failed': 0},
     '_seq_no': 0,
     '_primary_term': 1}

    # but not deserialized
    >>> es.get(index="my-index", id=42)['_source']
    {'any': 'data', 'timestamp': '2019-05-17T17:28:10.329598'}

`Full documentation`_.

.. _Full documentation: https://elasticsearch-py.readthedocs.io/

Elastic Cloud (and SSL) use-case::

    >>> from elasticsearch import Elasticsearch
    >>> es = Elasticsearch(cloud_id="<some_long_cloud_id>", http_auth=('elastic','yourpassword'))
    >>> es.info()

Using SSL Context with a self-signed cert use-case::

    >>> from elasticsearch import Elasticsearch
    >>> from ssl import create_default_context

    >>> context = create_default_context(cafile="path/to/cafile.pem")
    >>> es = Elasticsearch("https://elasticsearch.url:port", ssl_context=context, http_auth=('elastic','yourpassword'))
    >>> es.info()



Features
--------

The client's features include:

 * translating basic Python data types to and from json (datetimes are not
   decoded for performance reasons)
 * configurable automatic discovery of cluster nodes
 * persistent connections
 * load balancing (with pluggable selection strategy) across all available nodes
 * failed connection penalization (time based - failed connections won't be
   retried until a timeout is reached)
 * support for ssl and http authentication
 * thread safety
 * pluggable architecture


License
-------

Copyright 2019 Elasticsearch

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Build status
------------
.. image:: https://readthedocs.org/projects/elasticsearch-py/badge/?version=latest&style=flat
   :target: https://elasticsearch-py.readthedocs.io/en/master/

.. image:: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/badge/icon
   :target: https://clients-ci.elastic.co/job/elastic+elasticsearch-py+master/
  * [elementpath-1.2.0](https://github.com/sissaschool/elementpath) ***********
elementpath
***********

.. elementpath-introduction

The proposal of this package is to provide XPath 1.0 and 2.0 selectors for Python's ElementTree XML
data structures, both for the standard ElementTree library and for the
`lxml.etree <http://lxml.de>`_ library.

For `lxml.etree <http://lxml.de>`_ this package can be useful for providing XPath 2.0 selectors,
because `lxml.etree <http://lxml.de>`_ already has it's own implementation of XPath 1.0.


Installation and usage
======================

You can install the package with *pip* in a Python 2.7 or Python 3.5+ environment::

    pip install elementpath

For using it import the package and apply the selectors on ElementTree nodes:

>>> import elementpath
>>> from xml.etree import ElementTree
>>> root = ElementTree.XML('<A><B1/><B2><C1/><C2/><C3/></B2></A>')
>>> elementpath.select(root, '/A/B2/*')
[<Element 'C1' at ...>, <Element 'C2' at ...>, <Element 'C3' at ...>]

The *select* API provides the standard XPath result format that is a list or an elementary
datatype's value. If you want only to iterate over results you can use the generator function
*iter_select* that accepts the same arguments of *select*.

The selectors API works also using XML data trees based on the `lxml.etree <http://lxml.de>`_
library:

>>> import elementpath
>>> import lxml.etree as etree
>>> root = etree.XML('<A><B1/><B2><C1/><C2/><C3/></B2></A>')
>>> elementpath.select(root, '/A/B2/*')
[<Element C1 at ...>, <Element C2 at ...>, <Element C3 at ...>]

When you need to apply the same XPath expression to several XML data you can also use the
*Selector* class, creating an instance and then using it to apply the path on distinct XML
data:

>>> import elementpath
>>> import lxml.etree as etree
>>> selector = elementpath.Selector('/A/*/*')
>>> root = etree.XML('<A><B1/><B2><C1/><C2/><C3/></B2></A>')
>>> selector.select(root)
[<Element C1 at ...>, <Element C2 at ...>, <Element C3 at ...>]
>>> root = etree.XML('<A><B1><C0/></B1><B2><C1/><C2/><C3/></B2></A>')
>>> selector.select(root)
[<Element C0 at ...>, <Element C1 at ...>, <Element C2 at ...>, <Element C3 at ...>]

Public API classes and functions are described into the
`elementpath manual on the "Read the Docs" site <http://elementpath.readthedocs.io/en/latest/>`_.

Contributing
============

You can contribute to this package reporting bugs, using the issue tracker or by a pull request.
In case you open an issue please try to provide a test or test data for reproducing the wrong
behaviour. The provided testing code shall be added to the tests of the package.

The XPath parsers are based on an implementation of the Pratt's Top Down Operator Precedence parser.
The implemented parser includes some lookup-ahead features, helpers for registering tokens and for
extending language implementations. Also the token class has been generalized using a `MutableSequence`
as base class. See *tdop_parser.py* for the basic internal classes and *xpath1_parser.py* for extensions
and for a basic usage of the parser.

If you like you can use the basic parser and tokens provided by the *tdop_parser.py* module to
implement other types of parsers (I think it could be also a funny exercise!).


License
=======

This software is distributed under the terms of the MIT License.
See the file 'LICENSE' in the root directory of the present
distribution, or http://opensource.org/licenses/MIT.



  * [entrypoints-0.3](https://github.com/takluyver/entrypoints) Entry points are a way for Python packages to advertise objects with some
common interface. The most common examples are ``console_scripts`` entry points,
which define shell commands by identifying a Python function to run.

*Groups* of entry points, such as ``console_scripts``, point to objects with
similar interfaces. An application might use a group to find its plugins, or
multiple groups if it has different kinds of plugins.

The **entrypoints** module contains functions to find and load entry points.
You can install it from PyPI with ``pip install entrypoints``.

To advertise entry points when distributing a package, see
`entry_points in the Python Packaging User Guide
<https://packaging.python.org/en/latest/distributing.html#entry-points>`_.

  * [ephem-3.7.6.0](http://rhodesmill.org/pyephem/) ==============
PyEphem README
==============

.. image:: http://bottlepy.org/docs/dev/_static/Gittip.png
   :target: https://www.gittip.com/brandon-rhodes/

.. image:: https://travis-ci.org/brandon-rhodes/pyephem.png
   :target: https://travis-ci.org/brandon-rhodes/pyephem

.. _ephem: http://pypi.python.org/pypi/ephem/
.. _pyephem: http://pypi.python.org/pypi/pyephem/
.. _XEphem: http://www.clearskyinstitute.com/xephem/
.. _Quick Reference: http://rhodesmill.org/pyephem/quick
.. _Tutorial: http://rhodesmill.org/pyephem/tutorial
.. _PyEphem web site: http://rhodesmill.org/pyephem/

PyEphem provides an ``ephem`` Python package
for performing high-precision astronomy computations.
The underlying numeric routines are coded in C
and are the same ones that drive the popular `XEphem`_ astronomy application,
whose author, Elwood Charles Downey,
generously gave permission for their use in PyEphem.
The name *ephem* is short for the word *ephemeris*,
which is the traditional term for a table
giving the position of a planet, asteroid, or comet for a series of dates.

The `PyEphem web site`_ offers documentation
and also links to the project bug tracker, user support forum,
and source code repository.
If you have a C compiler and the
`pip Python installer tool <https://pip.pypa.io/en/latest/installing.html>`_
on your system,
then installing PyEphem should be as easy as::

  pip install ephem

There are also Windows installers in the downloads section below.

The design of PyEphem emphasizes convenience and ease of use.
Both celestial bodies and the observer's location on Earth
are represented by Python objects,
while dates and angles automatically print themselves
in standard astronomical formats::

 >>> import ephem
 >>> mars = ephem.Mars()
 >>> mars.compute('2008/1/1')
 >>> print(mars.ra)
 5:59:27.35
 >>> print(mars.dec)
 26:56:27.4

The documentation includes both a `Quick Reference`_ and a `Tutorial`_,
which are included in text files within the module itself
as well as being available on the `PyEphem web site`_.

The features provided by PyEphem include:

* Find where a planet, comet, or asteroid is in the sky.

  * High-precision orbital routines are provdied
    for the Moon, Sun, planets, and the major planet moons.
  * The user can supply the orbital elements of a comet, asteroid,
    or Earth-orbiting satellite, and have its location computed.
  * The positions of 94 bright stars come built-in,
    and the user can create further fixed objects as needed
    for their calculations.

* Determine where in the sky an object appears for a particular observer.

  * The user can supply the longitude, latitude, and altitude
    of the location from which they will be observing.
  * For convenience, a small database of longitudes and latitudes
    for 122 world cities is included.
  * For specified weather conditions (temperature and pressure),
    PyEphem will compensate for atmospheric refraction
    by adjusting the positions of bodies near the horizon.

* Compute when a body will rise, transit overhead, and set
  from a particular location.

* Parse and use orbital data in either the traditional XEphem file format,
  or the standard TLE format used for tracking Earth-orbiting satellites.

* Determine the dates of the equinoxes and solstices.

* Compute the dates of the various phases of the Moon.

* Convert from the Greenwich Time (more precisely, Ephemeris Time)
  which PyEphem uses to the local time of the user.

* Convert positions between the equatorial, ecliptic, and galactic
  coordinate systems.

* Determine on which page of the Uranometria or the Millennium Star Atlas
  a particular star should appear.

* Return the Julian Date corresponding to any calendar date.

Developers
----------

If you are interested in learning about how PyEphem works or in
exploring its source code, check out this repository from GitHub.  It is
hosted at:

https://github.com/brandon-rhodes/pyephem

If you lack expertise with version control, you can instead simply
download a static copy of the most recent source code using this link:

https://github.com/brandon-rhodes/pyephem/archive/master.zip

To run its source code in place, create a `virtual environment
<http://docs.python-guide.org/en/latest/dev/virtualenvs/>`_, activate
it, change directory to the root of the PyEphem source code, and run::

    python setup.py build_ext -i

You can then run the PyEphem test suite to see whether all of its
features are working correctly on your operating system and platform::

    python -m unittest discover ephem

PyEphem’s documentation is organized as a standard `Sphinx
<http://www.sphinx-doc.org/en/master/>`_ document project.  You can
build the documentation either with the Sphinx command line::

    sphinx-build -b html pyephem/ephem/doc/ ./my_documentation_directory/

— or, more typically, by invoking one of the targets in the
documentation’s Makefile::

    make -C ephem/doc html



  * [epydoc-3.0.1](http://epydoc.sourceforge.net) Epydoc is a tool for generating API documentation documentation for
Python modules, based on their docstrings.  For an example of epydoc's
output, see the API documentation for epydoc itself (`html
<http://epydoc.sf.net/api/>`__\ , `pdf
<http://epydoc.sf.net/epydoc.pdf>`__\ ).  A lightweight markup
language called `epytext <http://epydoc.sf.net/epytextintro.html>`__
can be used to format docstrings, and to add information about
specific fields, such as parameters and instance variables.  Epydoc
also understands docstrings written in `reStructuredText
<http://docutils.sourceforge.net/rst.html>`__\ , Javadoc, and
plaintext. For a more extensive example of epydoc's output, see the
API documentation for `Python 2.5
<http://epydoc.sourceforge.net/stdlib/>`__\ .
  * [eventlet-0.25.0](http://eventlet.net) Eventlet is a concurrent networking library for Python that allows you to change how you run your code, not how you write it.

It uses epoll or libevent for highly scalable non-blocking I/O.  Coroutines ensure that the developer uses a blocking style of programming that is similar to threading, but provide the benefits of non-blocking I/O.  The event dispatch is implicit, which means you can easily use Eventlet from the Python interpreter, or as a small part of a larger application.

It's easy to get started using Eventlet, and easy to convert existing
applications to use it.  Start off by looking at the `examples`_,
`common design patterns`_, and the list of `basic API primitives`_.

.. _examples: http://eventlet.net/doc/examples.html
.. _common design patterns: http://eventlet.net/doc/design_patterns.html
.. _basic API primitives: http://eventlet.net/doc/basic_usage.html


Quick Example
===============

Here's something you can try right on the command line::

    % python
    >>> import eventlet
    >>> from eventlet.green import urllib2
    >>> gt = eventlet.spawn(urllib2.urlopen, 'http://eventlet.net')
    >>> gt2 = eventlet.spawn(urllib2.urlopen, 'http://secondlife.com')
    >>> gt2.wait()
    >>> gt.wait()


Getting Eventlet
==================

The easiest way to get Eventlet is to use pip::

  pip install -U eventlet

To install latest development version once::

  pip install -U https://github.com/eventlet/eventlet/archive/master.zip


Building the Docs Locally
=========================

To build a complete set of HTML documentation, you must have Sphinx, which can be found at http://sphinx.pocoo.org/ (or installed with `pip install Sphinx`)::

  cd doc
  make html

The built html files can be found in doc/_build/html afterward.


Twisted
=======

Eventlet had Twisted hub in the past, but community interest to this integration has dropped over time,
now it is not supported, so with apologies for any inconvenience we discontinue Twisted integration.

If you have a project that uses Eventlet with Twisted, your options are:

* use last working release eventlet==0.14
* start a new project with only Twisted hub code, identify and fix problems. As of eventlet 0.13, `EVENTLET_HUB` environment variable can point to external modules.
* fork Eventlet, revert Twisted removal, identify and fix problems. This work may be merged back into main project.

Apologies for any inconvenience.


Flair
=====

.. image:: https://travis-ci.org/eventlet/eventlet.svg?branch=master
    :target: https://travis-ci.org/eventlet/eventlet

.. image:: https://codecov.io/gh/eventlet/eventlet/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/eventlet/eventlet



  * [execnet-1.6.1](https://execnet.readthedocs.io/en/latest/) execnet: distributed Python deployment and communication
========================================================

Important
---------

**execnet currently is in maintenance-only mode, mostly because it is still the backend
of the pytest-xdist plugin. Do not use in new projects.**

.. image:: https://img.shields.io/pypi/v/execnet.svg
    :target: https://pypi.org/project/execnet/

.. image:: https://anaconda.org/conda-forge/execnet/badges/version.svg
    :target: https://anaconda.org/conda-forge/execnet

.. image:: https://img.shields.io/pypi/pyversions/execnet.svg
    :target: https://pypi.org/project/execnet/

.. image:: https://travis-ci.org/pytest-dev/execnet.svg?branch=master
    :target: https://travis-ci.org/pytest-dev/execnet

.. image:: https://ci.appveyor.com/api/projects/status/n9qy8df16my4gds9/branch/master?svg=true
    :target: https://ci.appveyor.com/project/pytestbot/execnet

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/python/black

.. _execnet: http://codespeak.net/execnet

execnet_ provides carefully tested means to ad-hoc interact with Python
interpreters across version, platform and network barriers.  It provides
a minimal and fast API targetting the following uses:

* distribute tasks to local or remote processes
* write and deploy hybrid multi-process applications
* write scripts to administer multiple hosts

Features
------------------

* zero-install bootstrapping: no remote installation required!

* flexible communication: send/receive as well as
  callback/queue mechanisms supported

* simple serialization of python builtin types (no pickling)

* grouped creation and robust termination of processes

* well tested between CPython 2.7, 3.4+, Jython 2.5.1 and PyPy 2.2
  interpreters.

* interoperable between Windows and Unix-ish systems.

* integrates with different threading models, including standard
  os threads, eventlet and gevent based systems.



  * [extras-1.0.0](https://github.com/testing-cabal/extras) ======
extras
======

extras is a set of extensions to the Python standard library, originally
written to make the code within testtools cleaner, but now split out for
general use outside of a testing context.


Documentation
-------------

pydoc extras is your friend. extras currently contains the following functions:

* try_import

* try_imports

* safe_hasattr

Which do what their name suggests.


Licensing
---------

This project is distributed under the MIT license and copyright is owned by
the extras authors. See LICENSE for details.


Required Dependencies
---------------------

 * Python 2.6+ or 3.0+


Bug reports and patches
-----------------------

Please report bugs using github issues at <https://github.com/testing-cabal/extras>.
Patches can also be submitted via github.  You can mail the authors directly
via the mailing list testtools-dev@lists.launchpad.net. (Note that Launchpad
discards email from unknown addresses - be sure to sign up for a Launchpad
account before mailing the list, or your mail will be silently discarded).


History
-------

extras used to be testtools.helpers, and was factored out when folk wanted to
use it separately.


Thanks
------

 * Martin Pool
  * [fastcache-1.1.0](https://github.com/pbrady/fastcache) C implementation of Python 3 functools.lru_cache.  Provides speedup of 10-30x
over standard library.  Passes test suite from standard library for lru_cache.

Provides 2 Least Recently Used caching function decorators:

  clru_cache - built-in (faster)
             >>> from fastcache import clru_cache, __version__
             >>> __version__
             '1.1.0'
             >>> @clru_cache(maxsize=325, typed=False)
             ... def fib(n):
             ...     """Terrible Fibonacci number generator."""
             ...     return n if n < 2 else fib(n-1) + fib(n-2)
             ...
             >>> fib(300)
             222232244629420445529739893461909967206666939096499764990979600
             >>> fib.cache_info()
             CacheInfo(hits=298, misses=301, maxsize=325, currsize=301)
             >>> print(fib.__doc__)
             Terrible Fibonacci number generator.
             >>> fib.cache_clear()
             >>> fib.cache_info()
             CacheInfo(hits=0, misses=0, maxsize=325, currsize=0)
             >>> fib.__wrapped__(300)
             222232244629420445529739893461909967206666939096499764990979600
             >>> type(fib)
             >>> <class 'fastcache.clru_cache'>

  lru_cache  - python wrapper around clru_cache
             >>> from fastcache import lru_cache
             >>> @lru_cache(maxsize=128, typed=False)
             ... def f(a, b):
             ...     pass
             ...
             >>> type(f)
             >>> <class 'function'>


  (c)lru_cache(maxsize=128, typed=False, state=None, unhashable='error')

      Least-recently-used cache decorator.

      If *maxsize* is set to None, the LRU features are disabled and the cache
      can grow without bound.

      If *typed* is True, arguments of different types will be cached separately.
      For example, f(3.0) and f(3) will be treated as distinct calls with
      distinct results.

      If *state* is a list or dict, the items will be incorporated into the
      argument hash.

      The result of calling the cached function with unhashable (mutable)
      arguments depends on the value of *unhashable*:

          If *unhashable* is 'error', a TypeError will be raised.

          If *unhashable* is 'warning', a UserWarning will be raised, and
          the wrapped function will be called with the supplied arguments.
          A miss will be recorded in the cache statistics.

          If *unhashable* is 'ignore', the wrapped function will be called
          with the supplied arguments. A miss will will be recorded in
          the cache statistics.

      View the cache statistics named tuple (hits, misses, maxsize, currsize)
      with f.cache_info().  Clear the cache and statistics with f.cache_clear().
      Access the underlying function with f.__wrapped__.

      See:  http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used
  * [fasteners-0.15](https://github.com/harlowja/fasteners) Fasteners
=========

.. image:: https://travis-ci.org/harlowja/fasteners.png?branch=master
   :target: https://travis-ci.org/harlowja/fasteners

.. image:: https://ci.appveyor.com/api/projects/status/7d7aku32pimpadiv
   :target: https://ci.appveyor.com/project/JoshuaHarlow/fasteners

.. image:: https://readthedocs.org/projects/fasteners/badge/?version=latest
   :target: https://readthedocs.org/projects/fasteners/?badge=latest
   :alt: Documentation Status

.. image:: https://img.shields.io/pypi/dm/fasteners.svg
   :target: https://pypi.python.org/pypi/fasteners/
   :alt: Downloads

.. image:: https://img.shields.io/pypi/v/fasteners.svg
    :target: https://pypi.python.org/pypi/fasteners/
    :alt: Latest Version

Overview
--------

A python `package`_ that provides useful locks.

It includes the following.

Locking decorator
*****************

* Helpful ``locked`` decorator (that acquires instance
  objects lock(s) and acquires on method entry and
  releases on method exit).

Reader-writer locks
*******************

* Multiple readers (at the same time).
* Single writers (blocking any readers).
* Helpful ``read_locked`` and ``write_locked`` decorators.

Inter-process locks
*******************

* Single writer using file based locking (these automatically
  release on process exit, even if ``__release__`` or
  ``__exit__`` is never called).
* Helpful ``interprocess_locked`` decorator.

Generic helpers
***************

* A ``try_lock`` helper context manager that will attempt to
  acquire a given lock and provide back whether the attempt
  passed or failed (if it passes, then further code in the
  context manager will be ran **with** the lock acquired).

.. _package: https://pypi.python.org/pypi/fasteners



  * [fastnumbers-2.2.1](https://github.com/SethMMorton/fastnumbers) fastnumbers
===========

.. image:: https://img.shields.io/pypi/v/fastnumbers.svg
    :target: https://pypi.org/project/fastnumbers/

.. image:: https://img.shields.io/pypi/pyversions/fastnumbers.svg
    :target: https://pypi.org/project/fastnumbers/

.. image:: https://img.shields.io/pypi/l/fastnumbers.svg
    :target: https://github.com/SethMMorton/fastnumbers/blob/master/LICENSE

.. image:: https://img.shields.io/travis/SethMMorton/fastnumbers/master.svg?label=travis-ci
    :target: https://travis-ci.org/SethMMorton/fastnumbers

.. image:: https://ci.appveyor.com/api/projects/status/5ahtcvmt3aoui3mw/branch/master?svg=true
    :target: https://ci.appveyor.com/project/SethMMorton/fastnumbers/branch/master

.. image:: https://codecov.io/gh/SethMMorton/fastnumbers/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/SethMMorton/fastnumbers

.. image:: https://api.codacy.com/project/badge/Grade/7221f3d2be3147e9a975d604f1770cfb
    :target: https://www.codacy.com/app/SethMMorton/fastnumbers

Super-fast and clean conversions to numbers.

    - Source Code: https://github.com/SethMMorton/fastnumbers
    - Downloads: https://pypi.org/project/fastnumbers/
    - Documentation: http://fastnumbers.readthedocs.io/

``fastnumbers`` is a module with the following three objectives:

    #. Provide drop-in replacements for the Python built-in ``int`` and
       ``float`` that on average are up to 2x faster. These functions
       should behave *identically* to the Python built-ins except for a few
       specific corner-cases as mentioned in the
       `API documentation <http://fastnumbers.readthedocs.io/en/master/api.html>`_.
    #. Provide a set of convenience functions that wrap the above
       ``int`` and ``float`` replacements and provides easy, concise,
       powerful, fast and flexible error handling.
    #. Provide a set of functions that can be used to rapidly identify if
       an input *could* be converted to *int* or *float*.

**NOTICE**: The first major version release after Jan 1, 2020 will drop support for Python 2.7.

Examples
--------

The below examples showcase the ``fast_float`` function, which is
a fast conversion function with error-handling.
Please see the
`API Documentation <http://fastnumbers.readthedocs.io/en/master/api.html>`_
for other functions that are available from ``fastnumbers``.

.. code-block:: python

    >>> from fastnumbers import fast_float, float as fnfloat
    >>> # Convert string to a float
    >>> fast_float('56.07')
    56.07
    >>> # Unconvertable string returned as-is by default
    >>> fast_float('bad input')
    'bad input'
    >>> # Unconvertable strings can trigger a default value
    >>> fast_float('bad input', default=0)
    0
    >>> # 'default' is also the first optional positional arg
    >>> fast_float('bad input', 0)
    0
    >>> # Integers are converted to floats
    >>> fast_float(54)
    54.0
    >>> # One can ask inf or nan to be substituted with another value
    >>> fast_float('nan')
    nan
    >>> fast_float('nan', nan=0.0)
    0.0
    >>> fast_float(float('nan'), nan=0.0)
    0.0
    >>> fast_float('56.07', nan=0.0)
    56.07
    >>> # The default built-in float behavior can be triggered with
    >>> # "raise_on_invalid" set to True. 
    >>> fast_float('bad input', raise_on_invalid=True) #doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
      ...
    ValueError: invalid literal for float(): bad input
    >>> # A key function can be used to return an alternate value for invalid input
    >>> fast_float('bad input', key=len)
    9
    >>> fast_float(54, key=len)
    54.0
    >>> # Single unicode characters can be converted.
    >>> fast_float(u'\u2164')  # Roman numeral 5 (V)
    5.0
    >>> fast_float(u'\u2466')  # 7 enclosed in a circle
    7.0

**NOTE**: If you need locale-dependent conversions, supply the ``fastnumbers``
function of your choice to ``locale.atof``.

.. code-block:: python

    import locale
    locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')
    print(atof('468,5', func=fast_float))  # Prints 468.5

Timing
------

Just how much faster is ``fastnumbers`` than a pure python implementation?
Please see the following Jupyter notebooks for timing information on various Python versions.

    - https://nbviewer.jupyter.org/github/SethMMorton/fastnumbers/blob/master/TIMING_27.ipynb
    - https://nbviewer.jupyter.org/github/SethMMorton/fastnumbers/blob/master/TIMING_35.ipynb
    - https://nbviewer.jupyter.org/github/SethMMorton/fastnumbers/blob/master/TIMING_36.ipynb

How Is ``fastnumbers`` So Fast?
-------------------------------

CPython goes to great lengths to ensure that your string input is converted to a
number *correctly* (you can prove this to yourself by examining the source code for
`integer conversions <https://github.com/python/cpython/blob/e349bf23584eef20e0d1e1b2989d9b1430f15507/Objects/longobject.c#L2213>`_
and for
`float conversions <https://github.com/python/cpython/blob/e349bf23584eef20e0d1e1b2989d9b1430f15507/Python/dtoa.c#L1434>`_),
but this extra effort is only needed for very large
integers or for floats with many digits or large exponents. For integers, if the
result could fit into a C ``long`` then a naive algorithm of < 10 lines of C code
is sufficient. For floats, if the number does not require high precision or does not
have a large exponent (such as "-123.45e6") then a short naive algorithm is also
possible.

These naive algorithms are quite fast, but the performance improvement comes at the
expense of being unsafe (no protection against overflow or round-off errors).
``fastnumbers`` uses a heuristic to determine if the input can be safely converted
with the much faster naive algorithm. These heuristics are extremely conservative -
if there is *any* chance that the naive result would not give *exactly* the same
result as the built-in functions then it will fall back on CPython's conversion
function. For this reason, ``fastnumbers`` is aways *at least as fast* as CPython's
built-in ``float`` and ``int`` functions, and oftentimes is significantly faster
because most real-world numbers pass the heuristic.

Installation
------------

Use ``pip``!

.. code-block::

    $ pip install fastnumbers

How to Run Tests
----------------

Please note that ``fastnumbers`` is NOT set-up to support ``python setup.py test``.

The recommended way to run tests is with `tox <https://tox.readthedocs.io/en/latest/>`_.
Suppose you want to run tests for Python 3.6 - you can run tests by simply executing the
following:

.. code-block:: sh

    $ tox -e py36

``tox`` will create virtual a virtual environment for your tests and install all the
needed testing requirements for you.

If you want to run testing on all of Python 2.7, 3.4, 3.5, 3.6, and 3.7 you can simply
execute

.. code-block:: sh

    $ tox

If you do not wish to use ``tox``, you can install the testing dependencies with the
``dev-requirements.txt`` file and then run the tests manually using
`pytest <https://docs.pytest.org/en/latest/>`_.

.. code-block:: sh

    $ pip install -r dev/requirements.txt
    $ pytest

Author
------

Seth M. Morton

History
-------

Please visit the `changelog <http://fastnumbers.readthedocs.io/en/master/changelog.html>`_.
  * [feather-format-0.4.0](http://github.com/wesm/feather) ## Python interface to the Apache Arrow-based Feather File Format

Feather efficiently stores pandas DataFrame objects on disk. It depends on the
Apache Arrow for Python

## Installing

```shell
pip install feather-format
```

pip users note: ``feather-format`` depends on ``pyarrow`` and may not be
available on your platform via pip. If that does not work try conda-forge.

From [conda-forge][1]:

```shell
conda install feather-format -c conda-forge
```

## Limitations

Some features of pandas are not supported in Feather:

* Non-string column names
* Row indexes
* Object-type columns with non-homogeneous data

[1]: https://conda-forge.github.io
  * [fields-5.0.0](https://github.com/ionelmc/python-fields) ======
Fields
======

.. list-table::
    :stub-columns: 1

    * - docs
      - |docs|
    * - tests
      - | |travis| |appveyor| |requires|
        | |coveralls| |codecov|
        | |landscape| |scrutinizer| |codacy| |codeclimate|
    * - package
      - |version| |downloads| |wheel| |supported-versions| |supported-implementations|

.. |docs| image:: https://readthedocs.org/projects/python-fields/badge/?style=flat
    :target: https://readthedocs.org/projects/python-fields
    :alt: Documentation Status

.. |travis| image:: https://travis-ci.org/ionelmc/python-fields.svg?branch=master
    :alt: Travis-CI Build Status
    :target: https://travis-ci.org/ionelmc/python-fields

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/ionelmc/python-fields?branch=master&svg=true
    :alt: AppVeyor Build Status
    :target: https://ci.appveyor.com/project/ionelmc/python-fields

.. |requires| image:: https://requires.io/github/ionelmc/python-fields/requirements.svg?branch=master
    :alt: Requirements Status
    :target: https://requires.io/github/ionelmc/python-fields/requirements/?branch=master

.. |coveralls| image:: https://coveralls.io/repos/ionelmc/python-fields/badge.svg?branch=master&service=github
    :alt: Coverage Status
    :target: https://coveralls.io/r/ionelmc/python-fields

.. |codecov| image:: https://codecov.io/github/ionelmc/python-fields/coverage.svg?branch=master
    :alt: Coverage Status
    :target: https://codecov.io/github/ionelmc/python-fields

.. |landscape| image:: https://landscape.io/github/ionelmc/python-fields/master/landscape.svg?style=flat
    :target: https://landscape.io/github/ionelmc/python-fields/master
    :alt: Code Quality Status

.. |codacy| image:: https://img.shields.io/codacy/REPLACE_WITH_PROJECT_ID.svg?style=flat
    :target: https://www.codacy.com/app/ionelmc/python-fields
    :alt: Codacy Code Quality Status

.. |codeclimate| image:: https://codeclimate.com/github/ionelmc/python-fields/badges/gpa.svg
   :target: https://codeclimate.com/github/ionelmc/python-fields
   :alt: CodeClimate Quality Status
.. |version| image:: https://img.shields.io/pypi/v/fields.svg?style=flat
    :alt: PyPI Package latest release
    :target: https://pypi.python.org/pypi/fields

.. |downloads| image:: https://img.shields.io/pypi/dm/fields.svg?style=flat
    :alt: PyPI Package monthly downloads
    :target: https://pypi.python.org/pypi/fields

.. |wheel| image:: https://img.shields.io/pypi/wheel/fields.svg?style=flat
    :alt: PyPI Wheel
    :target: https://pypi.python.org/pypi/fields

.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/fields.svg?style=flat
    :alt: Supported versions
    :target: https://pypi.python.org/pypi/fields

.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/fields.svg?style=flat
    :alt: Supported implementations
    :target: https://pypi.python.org/pypi/fields

.. |scrutinizer| image:: https://img.shields.io/scrutinizer/g/ionelmc/python-fields/master.svg?style=flat
    :alt: Scrutinizer Status
    :target: https://scrutinizer-ci.com/g/ionelmc/python-fields/

Container class boilerplate killer.

Features:

* Human-readable ``__repr__``
* Complete set of comparison methods
* Keyword and positional argument support. Works like a normal class - you can override just about anything in the
  subclass (eg: a custom ``__init__``). In contrast, `hynek/characteristic <https://github.com/hynek/characteristic>`_
  forces different call schematics and calls your ``__init__`` with different arguments.


Installation
============

::

    pip install fields

Usage & examples
================

A class that has 2 attributes, ``name`` and ``size``:

.. code:: pycon

    >>> from fields import Fields
    >>> class Pizza(Fields.name.size):
    ...     pass
    ...
    >>> p = Pizza("Pepperoni", "large")
    >>> p
    Pizza(name='Pepperoni', size='large')
    >>> p.size
    'large'
    >>> p.name
    'Pepperoni'

You can also use keyword arguments:

.. code:: pycon

    >>> Pizza(size="large", name="Pepperoni")
    Pizza(name='Pepperoni', size='large')

You can have as many attributes as you want:

.. code:: pycon

    >>> class Pizza(Fields.name.ingredients.crust.size):
    ...     pass
    ...
    >>> Pizza("Funghi", ["mushrooms", "mozarella"], "thin", "large")
    Pizza(name='Funghi', ingredients=['mushrooms', 'mozarella'], crust='thin', size='large')


A class that has one required attribute ``value`` and two attributes (``left`` and ``right``) with default value
``None``:

.. code:: pycon

    >>> class Node(Fields.value.left[None].right[None]):
    ...     pass
    ...
    >>> Node(1, Node(2), Node(3, Node(4)))
    Node(value=1, left=Node(value=2, left=None, right=None), right=Node(value=3, left=Node(value=4, left=None, right=None), right=None))
    >>> Node(1, right=Node(2))
    Node(value=1, left=None, right=Node(value=2, left=None, right=None))

You can also use it *inline*:

.. code:: pycon

    >>> Fields.name.size("Pepperoni", "large")
    FieldsBase(name='Pepperoni', size='large')

Want tuples?
------------

An alternative to ``namedtuple``:

.. code:: python

    >>> from fields import Tuple
    >>> class Pair(Tuple.a.b):
    ...     pass
    ...
    >>> issubclass(Pair, tuple)
    True
    >>> p = Pair(1, 2)
    >>> p.a
    1
    >>> p.b
    2
    >>> tuple(p)
    (1, 2)
    >>> a, b = p
    >>> a
    1
    >>> b
    2

Tuples are *fast*!

::

    benchmark: 9 tests, min 5 rounds (of min 25.00us), 1.00s max time, timer: time.perf_counter

    Name (time in us)                 Min        Max     Mean   StdDev  Rounds  Iterations
    --------------------------------------------------------------------------------------
    test_characteristic            6.0100  1218.4800  11.7102  34.3158   15899          10
    test_fields                    6.8000  1850.5250   9.8448  33.8487    5535           4
    test_slots_fields              6.3500   721.0300   8.6120  14.8090   15198          10
    test_super_dumb                7.0111  1289.6667  11.6881  31.6012   15244           9
    test_dumb                      3.7556   673.8444   5.8010  15.0514   14246          18
    test_tuple                     3.1750   478.7750   5.1974   9.1878   14642          12
    test_namedtuple                3.2778   538.1111   5.0403   9.9177   14105           9
    test_attrs_decorated_class     4.2062   540.5125   5.3618  11.6708   14266          16
    test_attrs_class               3.7889   316.1056   4.7731   6.0656   14026          18
    --------------------------------------------------------------------------------------

Documentation
=============

https://python-fields.readthedocs.org/

Development
===========

To run all the tests run ``tox`` in your shell (``pip install tox`` if you don't have it)::

    tox

FAQ
===

Why should I use this?
----------------------

It's less to type, why have quotes around when the names need to be valid symbols anyway. In fact, this is one of the
shortest forms possible to specify a container with fields.

But you're abusing a very well known syntax. You're using attribute access instead of a list of strings. Why?
--------------------------------------------------------------------------------------------------------------

Symbols should be symbols. Why validate strings so they are valid symbols when you can avoid that? Just use symbols.
Save on both typing and validation code.

The use of language constructs is not that surprising or confusing in the sense that semantics precede conventional
syntax use. For example, if we have ``class Person(Fields.first_name.last_name.height.weight): pass`` then it's going to
be clear we're talking about a *Person* object with *first_name*, *last_name*, *height* and *width* fields: the words
have clear meaning.

Again, you should not name your variables as `f1`, `f2` or any other non-semantic symbols anyway.

Semantics precede syntax: it's like looking at a cake resembling a dog, you won't expect the cake to bark and run
around.



Is this stable? Is it tested?
-------------------------------

Yes. Mercilessly tested on `Travis <https://travis-ci.org/ionelmc/python-fields>`_ and `AppVeyor
<https://ci.appveyor.com/project/ionelmc/python-fields>`_.

Is the API stable?
-------------------

Yes, ofcourse.

Why not ``namedtuple``?
------------------------

It's ugly, repetivive and unflexible. Compare this:

.. code:: python

    >>> from collections import namedtuple
    >>> class MyContainer(namedtuple("MyContainer", ["field1", "field2"])):
    ...     pass
    >>> MyContainer(1, 2)
    MyContainer(field1=1, field2=2)

To this:

.. code:: python

    >>> class MyContainer(Tuple.field1.field2):
    ...     pass
    >>> MyContainer(1, 2)
    MyContainer(field1=1, field2=2)

Why not ``characteristic``?
----------------------------

Ugly, inconsistent - you don't own the class:

    Lets try this:

    .. code:: python

        >>> import characteristic
        >>> @characteristic.attributes(["field1", "field2"])
        ... class MyContainer(object):
        ...     def __init__(self, a, b):
        ...         if a > b:
        ...             raise ValueError("Expected %s < %s" % (a, b))
        >>> MyContainer(1, 2)
        Traceback (most recent call last):
            ...
        ValueError: Missing keyword value for 'field1'.

    WHAT !? Ok, lets write some more code:

    .. code:: python

        >>> MyContainer(field1=1, field2=2)
        Traceback (most recent call last):
            ...
        TypeError: __init__() ... arguments...

    This is bananas. You have to write your class *around* these quirks.

Lets try this:

.. code:: python

    >>> class MyContainer(Fields.field1.field2):
    ...     def __init__(self, a, b):
    ...         if a > b:
    ...             raise ValueError("Expected %s < %s" % (a, b))
    ...         super(MyContainer, self).__init__(a, b)

Just like a normal class, works as expected:

.. code:: python

    >>> MyContainer(1, 2)
    MyContainer(field1=1, field2=2)

Why not ``attrs``?
------------------

Now this is a very difficult question.

Consider this typical use-case::

.. sourcecode:: pycon

    >>> import attr
    >>> @attr.s
    ... class Point(object):
    ...     x = attr.ib()
    ...     y = attr.ib()

Worth noting:

* attrs_ is faster because it doesn't allow your class to be
  used as a mixin (it doesn't do any ``super(cls, self).__init__(...)`` for you).
* the typical use-case doesn't allow you to have a custom ``__init__``. If you define a custom
  ``__init__``, it will get overridden by the one attrs_ generates.
* It works better with IDEs and source code analysis tools because of the
  attributes defined on the class.

All in all, attrs_ is a fast and minimal container library with no support for
subclasses. Definitely worth considering.

.. _attrs: <https://pypi.python.org/pypi/attrs

Won't this confuse ``pylint``?
------------------------------

Normaly it would, but there's a plugin that makes pylint understand it, just like any other class:
`pylint-fields <https://github.com/ionelmc/pylint-fields>`_.

Testimonials
============

..

    Diabolical. Can't be unseen.

    -- `David Beazley <https://twitter.com/dabeaz/status/670237225104355328>`_

..

    I think that's the saddest a single line of python has ever made me.

    -- Someone on IRC (#python)

..

    Don't speak around saying that I like it.

    -- A PyPy contributor

..

    Fields is completey bat-shit insane, but kind of cool.

    -- Someone on IRC (#python)

..

    WHAT?!?!

    -- Unsuspecting victim at EuroPython 2015

.. 

    I don't think it should work ...

    -- Unsuspecting victim at EuroPython 2015

..

    Is it some Ruby thing?

    -- Unsuspecting victim at EuroPython 2015

..

    Are Python programmers that lazy?

    -- Some Java developer

..

    I'm going to use this in my next project. You're a terrible person.

    -- `Isaac Dickinson <https://github.com/sundwarf>`_

Apologies
=========

I tried my best at `EuroPython <https://youtu.be/nofEnPqj0cE?t=2554>`_ ...


Changelog
=========

5.0.0 (2016-04-13)
------------------

* Added the ``fields.InheritableFields`` base. It allows subclassing and it's intended for multiple inheritance scenarios. Yes, 
  yes, this enables lots pain and suffering, but some people just like it that way.
* Simplified the interfaces for the builtin sealers (the required argument is gone as it was redundant, and the remaining 
  arguments are swapped). Now they must be a function that take just two arguments: ``fields, defaults``.

4.0.0 (2016-01-28)
------------------

* Added ``__all__`` and ``factory`` conveniences. Removed ``fields.Factory`` from the public API since it need some special
  care with it's use (it's a damn metaclass after all).
* Added ``make_init_func`` into public API for advanced uses (combine with ``factory`` and ``class_sealer``).

3.0.0 (2015-10-04)
------------------

* Disallowed creating containers with fields with "dunder" names. E.g.: ``class Foo(Fields.__foo__):`` is disallowed.

2.4.0 (2015-06-13)
------------------

* Similarly to ``fields.Fields``, added three new bases:

  * ``fields.BareFields`` (implements ``__init__``).
  * ``fields.ComparableMixin`` (implements ``__eq__``, ``__ne__``, ``__lt__``, ``__gt__``, ``__le__``, ``__ge__`` and ``__hash__``).
  * ``fields.PrintableMixin`` (implements ``__repr__``).

* Improved reference section in the docs.
* Added ``fields.ConvertibleFields`` and ``fields.ConvertibleMixin``. They have two convenience properties: ``as_dict`` and `as_tuple``.

2.3.0 (2015-01-20)
------------------

* Allowed overriding ``__slots__`` in ``SlotsFields`` subclasses.

2.2.0 (2015-01-19)
------------------

* Added ``make_init_func`` as an optional argument to ``class_sealer``. Rename the ``__base__`` option to just ``base``.

2.1.1 (2015-01-19)
------------------

* Removed bogus ``console_scripts`` entrypoint.

2.1.0 (2015-01-09)
------------------

* Added ``SlotsFields`` (same as ``Fields`` but automatically adds ``__slots__`` for memory efficiency on CPython).
* Added support for default argument to Tuple.

2.0.0 (2014-10-16)
------------------

* Made the __init__ in the FieldsBase way faster (used for ``fields.Fields``).
* Moved ``RegexValidate`` in ``fields.extras``.

1.0.0 (2014-10-05)
------------------

* Lots of internal changes, the metaclass is not created in a closure anymore. No more closures.
* Added ``RegexValidate`` container creator (should be taken as an example on using the Factory metaclass).
* Added support for using multiple containers as baseclasses.
* Added a ``super()`` `sink` so that ``super().__init__(*args, **kwargs)`` always works. Everything inherits from a
  baseclass that has an ``__init__`` that can take any argument (unlike ``object.__init__``). This allows for flexible
  usage.
* Added validation so that you can't use conflicting field layout when using multiple containers as the baseclass.
* Changed the __init__ function in the class container so it works like a python function w.r.t. positional and keyword
  arguments. Example: ``class MyContainer(Fields.a.b.c[1].d[2])`` will function the same way as ``def func(a, b, c=1,
  d=2)`` would when arguments are passed in. You can now use ``MyContainer(1, 2, 3, 4)`` (everything positional) or
  ``MyContainer(1, 2, 3, d=4)`` (mixed).

0.3.0 (2014-07-19)
------------------

* Corrected string repr.

0.2.0 (2014-06-28)
------------------

* Lots of breaking changes. Switched from __call__ to __getitem__ for default value assignment.

0.1.0 (2014-06-27)
------------------

* Alpha release.
  * [filelock-3.0.12](https://github.com/benediktschmitt/py-filelock) # py-filelock

![travis-ci](https://travis-ci.org/benediktschmitt/py-filelock.svg?branch=master)

This package contains a single module, which implements a platform independent
file lock in Python, which provides a simple way of inter-process communication:

```Python
from filelock import Timeout, FileLock

lock = FileLock("high_ground.txt.lock")
with lock:
    open("high_ground.txt", "a").write("You were the chosen one.")        
```

**Don't use** a *FileLock* to lock the file you want to write to, instead create
a separate *.lock* file as shown above.

![animated example](https://raw.githubusercontent.com/benediktschmitt/py-filelock/master/example/example.gif)


## Similar libraries

Perhaps you are looking for something like

*   https://pypi.python.org/pypi/pid/2.1.1
*   https://docs.python.org/3.6/library/msvcrt.html#msvcrt.locking
*   or https://docs.python.org/3/library/fcntl.html#fcntl.flock


## Installation

*py-filelock* is available via PyPi:

```
$ pip3 install filelock
```


## Documentation

The documentation for the API is available on
[readthedocs.org](https://filelock.readthedocs.io/).


### Examples

A *FileLock* is used to indicate another process of your application that a
resource or working
directory is currently used. To do so, create a *FileLock* first:

```Python
from filelock import Timeout, FileLock

file_path = "high_ground.txt"
lock_path = "high_ground.txt.lock"

lock = FileLock(lock_path, timeout=1)
```

The lock object supports multiple ways for acquiring the lock, including the
ones used to acquire standard Python thread locks:

```Python
with lock:
    open(file_path, "a").write("Hello there!")

lock.acquire()
try:
    open(file_path, "a").write("General Kenobi!")
finally:
    lock.release()
```

The *acquire()* method accepts also a *timeout* parameter. If the lock cannot be
acquired within *timeout* seconds, a *Timeout* exception is raised:

```Python
try:
    with lock.acquire(timeout=10):
        open(file_path, "a").write("I have a bad feeling about this.")
except Timeout:
    print("Another instance of this application currently holds the lock.")
```

The lock objects are recursive locks, which means that once acquired, they will
not block on successive lock requests:

```Python
def cite1():
    with lock:
        open(file_path, "a").write("I hate it when he does that.")

def cite2():
    with lock:
        open(file_path, "a").write("You don't want to sell me death sticks.")

# The lock is acquired here.
with lock:
    cite1()
    cite2()

# And released here.
```


## FileLock vs SoftFileLock

The *FileLock* is platform dependent while the *SoftFileLock* is not. Use the
*FileLock* if all instances of your application are running on the same host and
a *SoftFileLock* otherwise.

The *SoftFileLock* only watches the existence of the lock file. This makes it
ultra portable, but also more prone to dead locks if the application crashes.
You can simply delete the lock file in such cases.


## Contributions

Contributions are always welcome, please make sure they pass all tests before
creating a pull request. Never hesitate to open a new issue, although it may
take some time for me to respond.


## License

This package is [public domain](./LICENSE.rst).



  * [findspark-1.3.0](https://github.com/minrk/findspark) 
Provides findspark.init() to make pyspark importable as a regular library.



  * [first-2.0.2](http://github.com/hynek/first/) first: The function you always missed in Python
===============================================

.. image:: https://travis-ci.org/hynek/first.svg?branch=master
   :target: https://travis-ci.org/hynek/first
   :alt: CI Status

``first`` is an MIT-licensed Python package with a simple function that returns the first true value from an iterable, or ``None`` if there is none.
If you need more power, you can also supply a ``key`` function that is used to judge the truth value of the element or a ``default`` value if ``None`` doesn’t fit your use case.

N.B. I’m using the term “true” consistently with Python docs for ``any()`` and ``all()`` — it means that the value evaluates to true like: ``True``, ``1``, ``"foo"``, or ``[None]``.
But **not**: ``None``, ``False``, ``[]``, or ``0``.
In JavaScript, they call this “truthy”.


Examples
========

A simple example to get started:

.. code-block:: pycon

   >>> from first import first
   >>> first([0, None, False, [], (), 42])
   42

However, it’s especially useful for dealing with regular expressions in ``if/elif/else`` branches:

.. code-block:: python

   import re

   from first import first


   re1 = re.compile('b(.*)')
   re2 = re.compile('a(.*)')

   m = first(regexp.match('abc') for regexp in [re1, re2])
   if not m:
      print('no match!')
   elif m.re is re1:
      print('re1', m.group(1))
   elif m.re is re2:
      print('re2', m.group(1))

The optional ``key`` function gives you even *more* selection power.
If you want to return the first even number from a list, just do the following:

.. code-block:: pycon

   >>> from first import first
   >>> first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0)
   4

``default`` on the other hand allows you to specify a value that is returned if none of the elements is true:

.. code-block:: pycon

   >>> from first import first
   >>> first([0, None, False, [], ()], default=42)
   42


Usage
=====

The package consists of one module consisting of one function:

.. code-block:: python

   from first import first

   first(iterable, default=None, key=None)

This function returns the first element of ``iterable`` that is true if ``key`` is ``None``.
If there is no true element, the value of ``default`` is returned, which is ``None`` by default.

If a callable is supplied in ``key``, the result of ``key(element)`` is used to judge the truth value of the element, but the element itself is returned.

``first`` has no dependencies and should work with any Python available.


Alternatives
============

``first`` brings nothing to the table that wasn’t possible before.
However the existing solutions aren’t very idiomatic for such a common and simple problem.

The following constructs are equivalent to ``first(seq)`` and work since Python 2.6:

.. code-block:: python

   next(itertools.ifilter(None, seq), None)
   next(itertools.ifilter(bool, seq), None)
   next((x for x in seq if x), None)

None of them is as pretty as I’d like them to be.
The ``re`` example from above would look like the following:

.. code-block:: python

   next(itertools.ifilter(None, (regexp.match('abc') for regexp in [re1, re2])), None)
   next((regexp.match('abc') for regexp in [re1, re2] if regexp.match('abc')), None)
   next((match for match in itertools.imap(
       operator.methodcaller('match', 'abc'), [re1, re2]) if match), None)

Note that in the second case you have to call ``regexp.match()`` *twice*.
The third example "fixes" that problem but also summons Cthulhu.

For comparison, one more time the ``first``-version:

.. code-block:: python

   first(regexp.match('abc') for regexp in [re1, re2])

Idiomatic, clear and readable. Pythonic. :)

----

As of version 0.6.5 from 2015, the excellent `boltons package <https://boltons.readthedocs.io/>`_ contains a ``first``-like function as part of its `iterutils module <https://boltons.readthedocs.io/en/latest/iterutils.html#boltons.iterutils.first>`_.


Background
==========

The idea for ``first`` goes back to a discussion I had with `Łukasz Langa`_ about how the ``re`` example above is painful in Python.
We figured such a function is missing Python, however it’s rather unlikely we’d get it in and even if, it wouldn’t get in before 3.4 anyway, which is years away as of yours truly is writing this.

So I decided to release it as a package for now.  If it proves popular enough, it may even make it into Python’s stdlib in the end.


.. _`Łukasz Langa`: https://github.com/ambv


.. :changelog:

History
-------

2.0.2 (2019-03-07)
++++++++++++++++++

- Package tests as part of the dist.
- Update docs.
- Drop unsupported Python versions from CI.
  N.B. The code hasn't changed and ``first`` continues to work as before.


2.0.1 (2013-08-04)
++++++++++++++++++

- Make installable on systems that don’t support UTF-8 by default.
- *Backward incompatible*: Drop support for Python older than 2.6, the previous fix gets too convoluted otherwise.
  Please don’t use Python < 2.6 anyway.
  I beg you.
  N.B. that this is a *pure packaging/QA matter*: the module still works perfectly with ancient Python versions.


2.0.0 (2012-10-13)
++++++++++++++++++

- `pred` proved to be rather useless.  Changed to `key` which is just a selector.  This is a *backward incompatible* change and the reason for going 2.0.
- Add `default` argument which is returned instead of `None` if no true element is found.

1.0.2 (2012-10-09)
++++++++++++++++++

- Fix packaging. I get this never right the first time. :-/

1.0.1 (2012-10-09)
++++++++++++++++++

- Documentation fixes only.

1.0.0 (2012-10-09)
++++++++++++++++++

- Initial release.


Credits
=======

“first” is written and maintained by Hynek Schlawack and various contributors:

- Artem Bezsmertnyi
- Łukasz Langa
- Nick Coghlan
- Vincent Driessen



  * [fixtures-3.0.0](https://launchpad.net/python-fixtures) *************************************************************
fixtures: Fixtures with cleanups for testing and convenience.
*************************************************************

  Copyright (c) 2010, Robert Collins <robertc@robertcollins.net>
  
  Licensed under either the Apache License, Version 2.0 or the BSD 3-clause
  license at the users choice. A copy of both licenses are available in the
  project source as Apache-2.0 and BSD. You may not use this file except in
  compliance with one of these two licences.
  
  Unless required by applicable law or agreed to in writing, software
  distributed under these licenses is distributed on an "AS IS" BASIS, WITHOUT
  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
  license you chose for the specific language governing permissions and
  limitations under that license.


Fixtures defines a Python contract for reusable state / support logic,
primarily for unit testing. Helper and adaption logic is included to make it
easy to write your own fixtures using the fixtures contract. Glue code is
provided that makes using fixtures that meet the Fixtures contract in unittest
compatible test cases easy and straight forward.

Dependencies
============

* Python 2.6+, or 3.3+
  This is the base language fixtures is written in and for.

* pbr
  Used for version and release management of fixtures.

* testtools <https://launchpad.net/testtools> 0.9.22 or newer.
  testtools provides helpful glue functions for the details API used to report
  information about a fixture (whether its used in a testing or production
  environment).

For use in a unit test suite using the included glue, one of:

* Python 2.7+

* unittest2

* bzrlib.tests

* Or any other test environment that supports TestCase.addCleanup.

Writing your own glue code is easy, or you can simply use Fixtures directly
without any support code.

To run the test suite for fixtures, testtools is needed.

Why Fixtures
============

Standard Python unittest.py provides no obvious method for making and reusing
state needed in a test case other than by adding a method on the test class.
This scales poorly - complex helper functions propagating up a test class
hierarchy is a regular pattern when this is done. Mocking while a great tool
doesn't itself prevent this (and helpers to mock complex things can accumulate
in the same way if placed on the test class).

By defining a uniform contract where helpers have no dependency on the test
class we permit all the regular code hygiene activities to take place without
the distorting influence of being in a class hierarchy that is modelling an
entirely different thing - which is what helpers on a TestCase suffer from.

About Fixtures
==============

A Fixture represents some state. Each fixture has attributes on it that are
specific to the fixture. For instance, a fixture representing a directory that
can be used for temporary files might have a attribute 'path'.

Most fixtures have complete ``pydoc`` documentation, so be sure to check
``pydoc fixtures`` for usage information.

Creating Fixtures
=================

Minimally, subclass Fixture, define _setUp to initialize your state and schedule
a cleanup for when cleanUp is called and you're done::

  >>> import unittest
  >>> import fixtures
  >>> class NoddyFixture(fixtures.Fixture):
  ...     def _setUp(self):
  ...         self.frobnozzle = 42
  ...         self.addCleanup(delattr, self, 'frobnozzle')

This will initialize frobnozzle when ``setUp`` is called, and when ``cleanUp``
is called get rid of the frobnozzle attribute. Prior to version 1.3.0 fixtures
recommended overriding ``setUp``. This is still supported, but since it is
harder to write leak-free fixtures in this fashion, it is not recommended.

If your fixture has diagnostic data - for instance the log file of an
application server, or log messages, it can expose that by creating a content
object (``testtools.content.Content``) and calling ``addDetail``.

  >>> from testtools.content import text_content
  >>> class WithLog(fixtures.Fixture):
  ...     def _setUp(self):
  ...         self.addDetail('message', text_content('foo bar baz'))

The method ``useFixture`` will use another fixture, call ``setUp`` on it, call
``self.addCleanup(thefixture.cleanUp)``, attach any details from it and return
the fixture. This allows simple composition of different fixtures.

  >>> class ReusingFixture(fixtures.Fixture):
  ...     def _setUp(self):
  ...         self.noddy = self.useFixture(NoddyFixture())

There is a helper for adapting a function or function pair into Fixtures. it
puts the result of the function in fn_result::

  >>> import os.path
  >>> import shutil
  >>> import tempfile
  >>> def setup_function():
  ...     return tempfile.mkdtemp()
  >>> def teardown_function(fixture):
  ...     shutil.rmtree(fixture)
  >>> fixture = fixtures.FunctionFixture(setup_function, teardown_function)
  >>> fixture.setUp()
  >>> print (os.path.isdir(fixture.fn_result))
  True
  >>> fixture.cleanUp()

This can be expressed even more pithily:

  >>> fixture = fixtures.FunctionFixture(tempfile.mkdtemp, shutil.rmtree)
  >>> fixture.setUp()
  >>> print (os.path.isdir(fixture.fn_result))
  True
  >>> fixture.cleanUp()

Another variation is MethodFixture which is useful for adapting alternate
fixture implementations to Fixture::

  >>> class MyServer:
  ...    def start(self):
  ...        pass
  ...    def stop(self):
  ...        pass
  >>> server = MyServer()
  >>> fixture = fixtures.MethodFixture(server, server.start, server.stop)

You can also combine existing fixtures using ``CompoundFixture``::

  >>> noddy_with_log = fixtures.CompoundFixture([NoddyFixture(),
  ...                                            WithLog()])
  >>> with noddy_with_log as x:
  ...     print (x.fixtures[0].frobnozzle)
  42

The Fixture API
===============

The example above introduces some of the Fixture API. In order to be able to
clean up after a fixture has been used, all fixtures define a ``cleanUp``
method which should be called when a fixture is finished with.

Because it's nice to be able to build a particular set of related fixtures in
advance of using them, fixtures also have a ``setUp`` method which should be
called before trying to use them.

One common desire with fixtures that are expensive to create is to reuse them
in many test cases; to support this the base Fixture also defines a ``reset``
which calls ``self.cleanUp(); self.setUp()``. Fixtures that can more
efficiently make themselves reusable should override this method. This can then
be used with multiple test state via things like ``testresources``,
``setUpClass``, or ``setUpModule``.

When using a fixture with a test you can manually call the setUp and cleanUp
methods. More convenient though is to use the included glue from
``fixtures.TestWithFixtures`` which provides a mixin defining
``useFixture`` (camel case because unittest is camel case throughout) method.
It will call setUp on the fixture, call self.addCleanup(fixture) to schedule a
cleanup, and return the fixture. This lets one write::

  >>> import testtools
  >>> import unittest

Note that we use testtools TestCase here as we need to guarantee a
TestCase.addCleanup method in this doctest. Unittest2 - Python2.7 and above -
also have ``addCleanup``. testtools has it's own implementation of
``useFixture`` so there is no need to use ``fixtures.TestWithFixtures`` with
``testtools.TestCase``.

  >>> class NoddyTest(testtools.TestCase, fixtures.TestWithFixtures):
  ...     def test_example(self):
  ...         fixture = self.useFixture(NoddyFixture())
  ...         self.assertEqual(42, fixture.frobnozzle)
  >>> result = unittest.TestResult()
  >>> _ = NoddyTest('test_example').run(result)
  >>> print (result.wasSuccessful())
  True

Fixtures implement the context protocol, so you can also use a fixture as a
context manager::

  >>> with fixtures.FunctionFixture(setup_function, teardown_function) as fixture:
  ...    print (os.path.isdir(fixture.fn_result))
  True

When multiple cleanups error, fixture.cleanUp() will raise a wrapper exception
rather than choosing an arbitrary single exception to raise::

  >>> import sys
  >>> from fixtures.fixture import MultipleExceptions
  >>> class BrokenFixture(fixtures.Fixture):
  ...     def _setUp(self):
  ...         self.addCleanup(lambda:1/0)
  ...         self.addCleanup(lambda:1/0)
  >>> fixture = BrokenFixture()
  >>> fixture.setUp()
  >>> try:
  ...    fixture.cleanUp()
  ... except MultipleExceptions:
  ...    exc_info = sys.exc_info()
  >>> print (exc_info[1].args[0][0].__name__)
  ZeroDivisionError

Fixtures often expose diagnostic details that can be useful for tracking down
issues. The ``getDetails`` method will return a dict of all the attached
details, but can only be called before ``cleanUp`` is called. Each detail
object is an instance of ``testtools.content.Content``.

  >>> with WithLog() as l:
  ...     print(l.getDetails()['message'].as_text())
  foo bar baz

Errors in setUp
+++++++++++++++

The examples above used ``_setUp`` rather than ``setUp`` because the base
class implementation of ``setUp`` acts to reduce the chance of leaking
external resources if an error is raised from ``_setUp``. Specifically,
``setUp`` contains a try:/except: block which catches all exceptions, captures
any registered detail objects, and calls ``self.cleanUp`` before propagating
the error. As long as you take care to register any cleanups before calling
the code that may fail, this will cause them to be cleaned up. The captured
detail objects are provided to the args of the raised exception.

If the error that occured was a subclass of ``Exception`` then ``setUp`` will
raise ``MultipleExceptions`` with the last element being a ``SetupError`` that
contains the detail objects. Otherwise, to prevent causing normally
uncatchable errors like KeyboardInterrupt being caught inappropriately in the
calling layer, the original exception will be raised as-is and no diagnostic
data other than that from the original exception will be available.

Shared Dependencies
+++++++++++++++++++

A common use case within complex environments is having some fixtures shared by
other ones.

Consider the case of testing using a ``TempDir`` with two fixtures built on top
of it; say a small database and a web server. Writing either one is nearly
trivial. However handling ``reset()`` correctly is hard: both the database and
web server would reasonably expect to be able to discard operating system
resources they may have open within the temporary directory before its removed.
A recursive ``reset()`` implementation would work for one, but not both.
Calling ``reset()`` on the ``TempDir`` instance between each test is probably
desirable but we don't want to have to do a complete ``cleanUp`` of the higher
layer fixtures (which would make the ``TempDir`` be unused and trivially
resettable. We have a few options available to us.

Imagine that the webserver does not depend on the DB fixture in any way - we
just want the webserver and DB fixture to coexist in the same tempdir.

A simple option is to just provide an explicit dependency fixture for the
higher layer fixtures to use.  This pushes complexity out of the core and onto
users of fixtures::

  >>> class WithDep(fixtures.Fixture):
  ...     def __init__(self, tempdir, dependency_fixture):
  ...         super(WithDep, self).__init__()
  ...         self.tempdir = tempdir
  ...         self.dependency_fixture = dependency_fixture
  ...     def setUp(self):
  ...         super(WithDep, self).setUp()
  ...         self.addCleanup(self.dependency_fixture.cleanUp)
  ...         self.dependency_fixture.setUp()
  ...         # we assume that at this point self.tempdir is usable.
  >>> DB = WithDep
  >>> WebServer = WithDep
  >>> tempdir = fixtures.TempDir()
  >>> db = DB(tempdir, tempdir)
  >>> server = WebServer(tempdir, db)
  >>> server.setUp()
  >>> server.cleanUp()

Another option is to write the fixtures to gracefully handle a dependency
being reset underneath them. This is insufficient if the fixtures would
block the dependency resetting (for instance by holding file locks open
in a tempdir - on Windows this will prevent the directory being deleted).

Another approach which ``fixtures`` neither helps nor hinders is to raise
a signal of some sort for each user of a fixture before it is reset. In the
example here, ``TempDir`` might offer a subscribers attribute that both the
DB and web server would be registered in. Calling ``reset`` or ``cleanUp``
on the tempdir would trigger a callback to all the subscribers; the DB and
web server reset methods would look something like:

  >>> def reset(self):
  ...     if not self._cleaned:
  ...         self._clean()

(Their action on the callback from the tempdir would be to do whatever work
was needed and set ``self._cleaned``.) This approach has the (perhaps)
suprising effect that resetting the webserver may reset the DB - if the
webserver were to be depending on ``tempdir.reset`` as a way to reset the
webservers state.

Another approach which is not currently implemented is to provide an object
graph of dependencies and a reset mechanism that can traverse that, along with
a separation between 'reset starting' and 'reset finishing' - the DB and
webserver would both have their ``reset_starting`` methods called, then the
tempdir would be reset, and finally the DB and webserver would have
``reset_finishing`` called.

Stock Fixtures
==============

In addition to the Fixture, FunctionFixture and MethodFixture classes fixtures
includes a number of precanned fixtures. The API docs for fixtures will list
the complete set of these, should the dcs be out of date or not to hand. For
the complete feature set of each fixture please see the API docs.

ByteStream
++++++++++

Trivial adapter to make a BytesIO (though it may in future auto-spill to disk
for large content) and expose that as a detail object, for automatic inclusion
in test failure descriptions. Very useful in combination with MonkeyPatch.

  >>> fixture = fixtures.StringStream('my-content')
  >>> fixture.setUp()
  >>> with fixtures.MonkeyPatch('sys.something', fixture.stream):
  ...     pass
  >>> fixture.cleanUp()

EnvironmentVariable
+++++++++++++++++++

Isolate your code from environmental variables, delete them or set them to a
new value.

  >>> fixture = fixtures.EnvironmentVariable('HOME')

FakeLogger
++++++++++

Isolate your code from an external logging configuration - so that your test
gets the output from logged messages, but they don't go to e.g. the console.

  >>> fixture = fixtures.FakeLogger()

FakePopen
+++++++++

Pretend to run an external command rather than needing it to be present to run
tests.

  >>> from testtools.compat import BytesIO
  >>> fixture = fixtures.FakePopen(lambda _:{'stdout': BytesIO('foobar')})

MockPatchObject
+++++++++++++++

Adapts ``mock.patch.object`` to be used as a Fixture.

  >>> class Fred:
  ...     value = 1
  >>> fixture = fixtures.MockPatchObject(Fred, 'value', 2)
  >>> with fixture:
  ...     Fred().value
  2
  >>> Fred().value
  1

MockPatch
+++++++++

Adapts ``mock.patch`` to be used as a Fixture.

  >>> fixture = fixtures.MockPatch('subprocess.Popen.returncode', 3)

MockPatchMultiple
+++++++++++++++++

Adapts ``mock.patch.multiple`` to be used as a Fixture.

  >>> fixture = fixtures.MockPatchMultiple('subprocess.Popen', returncode=3)

MonkeyPatch
+++++++++++

Control the value of a named python attribute.

  >>> def fake_open(path, mode):
  ...     pass
  >>> fixture = fixtures.MonkeyPatch('__builtin__.open', fake_open)

Note that there are some complexities when patching methods - please see the
API documentation for details.

NestedTempfile
++++++++++++++

Change the default directory that the tempfile module places temporary files
and directories in. This can be useful for containing the noise created by
code which doesn't clean up its temporary files. This does not affect
temporary file creation where an explicit containing directory was provided.

  >>> fixture = fixtures.NestedTempfile()

PackagePathEntry
++++++++++++++++

Adds a single directory to the path for an existing python package. This adds
to the package.__path__ list. If the directory is already in the path, nothing
happens, if it isn't then it is added on setUp and removed on cleanUp.

  >>> fixture = fixtures.PackagePathEntry('package/name', '/foo/bar')

PythonPackage
+++++++++++++

Creates a python package directory. Particularly useful for testing code that
dynamically loads packages/modules, or for mocking out the command line entry
points to Python programs.

  >>> fixture = fixtures.PythonPackage('foo.bar', [('quux.py', '')])

PythonPathEntry
+++++++++++++++

Adds a single directory to sys.path. If the directory is already in the path,
nothing happens, if it isn't then it is added on setUp and removed on cleanUp.

  >>> fixture = fixtures.PythonPathEntry('/foo/bar')

StringStream
++++++++++++

Trivial adapter to make a StringIO (though it may in future auto-spill to disk
for large content) and expose that as a detail object, for automatic inclusion
in test failure descriptions. Very useful in combination with MonkeyPatch.

  >>> fixture = fixtures.StringStream('stdout')
  >>> fixture.setUp()
  >>> with fixtures.MonkeyPatch('sys.stdout', fixture.stream):
  ...     pass
  >>> fixture.cleanUp()

TempDir
+++++++

Create a temporary directory and clean it up later.

  >>> fixture = fixtures.TempDir()

The created directory is stored in the ``path`` attribute of the fixture after
setUp.

TempHomeDir
+++++++++++

Create a temporary directory and set it as $HOME in the environment.

  >>> fixture = fixtures.TempHomeDir()

The created directory is stored in the ``path`` attribute of the fixture after
setUp.

The environment will now have $HOME set to the same path, and the value
will be returned to its previous value after tearDown.

Timeout
+++++++

Aborts if the covered code takes more than a specified number of whole wall-clock
seconds.

There are two possibilities, controlled by the 'gentle' argument: when gentle,
an exception will be raised and the test (or other covered code) will fail.
When not gentle, the entire process will be terminated, which is less clean,
but more likely to break hangs where no Python code is running.  

*Caution:* Only one timeout can be active at any time across all threads in a
single process.  Using more than one has undefined results.  (This could be
improved by chaining alarms.)

*Note:* Currently supported only on Unix because it relies on the ``alarm``
system call.

Contributing
============

Fixtures has its project homepage on Launchpad
<https://launchpad.net/python-fixtures>. Source code is hosted on GitHub
<https://github.com/testing-cabal/fixtures>.
  * [flake8-3.7.8](https://gitlab.com/pycqa/flake8) ========
 Flake8
========

Flake8 is a wrapper around these tools:

- PyFlakes
- pycodestyle
- Ned Batchelder's McCabe script

Flake8 runs all the tools by launching the single ``flake8`` command.
It displays the warnings in a per-file, merged output.

It also adds a few features:

- files that contain this line are skipped::

    # flake8: noqa

- lines that contain a ``# noqa`` comment at the end will not issue warnings.
- you can ignore specific errors on a line with ``# noqa: <error>``, e.g.,
  ``# noqa: E234``. Multiple codes can be given, separated by comma. The ``noqa`` token is case insensitive, the colon before the list of codes is required otherwise the part after ``noqa`` is ignored
- Git and Mercurial hooks
- extendable through ``flake8.extension`` and ``flake8.formatting`` entry
  points


Quickstart
==========

See our `quickstart documentation
<http://flake8.pycqa.org/en/latest/index.html#quickstart>`_ for how to install
and get started with Flake8.


Frequently Asked Questions
==========================

Flake8 maintains an `FAQ <http://flake8.pycqa.org/en/latest/faq.html>`_ in its
documentation.


Questions or Feedback
=====================

If you have questions you'd like to ask the developers, or feedback you'd like
to provide, feel free to use the mailing list: code-quality@python.org

We would love to hear from you. Additionally, if you have a feature you'd like
to suggest, the mailing list would be the best place for it.


Links
=====

* `Flake8 Documentation <http://flake8.pycqa.org/en/latest/>`_

* `GitLab Project <https://gitlab.com/pycqa/flake8>`_

* `All (Open and Closed) Issues
  <https://gitlab.com/pycqa/flake8/issues?scope=all&sort=updated_desc&state=all>`_

* `Code-Quality Archives
  <https://mail.python.org/mailman/listinfo/code-quality>`_

* `Code of Conduct
  <http://flake8.pycqa.org/en/latest/internal/contributing.html#code-of-conduct>`_

* `Getting Started Contributing
  <http://flake8.pycqa.org/en/latest/internal/contributing.html>`_


Maintenance
===========

Flake8 was created by Tarek Ziadé and is currently maintained by `Ian Cordasco
<http://www.coglib.com/~icordasc/>`_



  * [flake8-docstrings-1.3.1](https://gitlab.com/pycqa/flake8-docstrings) flake8-docstrings
=================

A simple module that adds an extension for the fantastic pydocstyle_ tool to
flake8_.

Simply install this extension::

    pip install flake8-docstrings

and run flake8.

You can set the pydocstyle convention_ at the command line using::

    $ flake8 --docstring-convention numpy ...

Or, adding ``docstring-convention=numpy`` to your flake8 configuration file.
The available set of conventions depends on the version of pydocstyle installed.
The default is ``pep257``, pydocstyle v2.0.0 added ``numpy`` (for the numpydoc
standard), while pydocstyle v4.0.0 added ``google``.

Report any issues on our `bug tracker`_.

.. _pydocstyle: https://github.com/pycqa/pydocstyle
.. _flake8: https://gitlab.com/pycqa/flake8
.. _convention: http://www.pydocstyle.org/en/latest/error_codes.html#default-conventions
.. _bug tracker: https://gitlab.com/pycqa/flake8-docstrings/issues


History/Changelog
=================

1.5.0
-----

- Add ``--ignore-decorators`` option which allows functions with a specific
  decorator to ignore error codes.

1.4.0
-----

- Add ``--docstring-convention`` option which allows selection of conventions
  besides the default ``pep257``.  Available options are based on those
  available from ``pydocstyle`` and are currently ``pep257``, ``google``, and
  ``numpy``.  ``flake8-docstrings`` also adds a special ``all`` docstring
  convention which will enable all rules from ``pydocstyle``.  Note that
  ``pydocstyle`` defines some conflicting rules so you'll want to use
  ``ignore`` / ``extend-ignore`` when selecting ``docstring-convention = all``

- Bump minimum flake8 version to 3

- Fix proper handling of ``stdin`` via ``--stdin-display-name``

1.3.1
-----

- Fix incompatibility with pydocstyle 4.x

1.3.0
-----

- Bump minimum pydocstyle version to 2.1.0

1.2.0
-----

- Fix EnvironError and AllError invocations

- Avoid Flake8 warning for requesting ``builtins``

1.1.0
-----

- Upgrade dependency on pydocstyle to 2.0.0

1.0.3
-----

- Use flake8-polyfill to get standard-in to handle Flake8 3.x and 2.x

1.0.2
-----

- Use pycodestyle to get standard-in.

1.0.1
-----

- Make sure this works out of the box (is enabled by default) with Flake8 3.0

1.0.0
-----

- Switch dependency name to pydocstyle. pep257 was renamed to pydocstyle, this
  update switches the requirement to that new package name. Since we're
  swapping out dependencies, we've issued a major version bump.

0.2.7
-----

- Try to import pydocstyle (not pycodestyle) as pep257

0.2.6
-----

- Respect pep257's default ignore list

- Handle AllError and other exceptions from pep257

0.2.5
-----

- Use pep257's ``tokenize_open`` function to pass input to the tool.

- Use pep257's conventions so any error codes that are ignored by default
  using ``pep257`` are also ignored by default with this plugin.

0.2.4
-----

- Fix bug introduced in 0.2.2 where the file source was always None causing
  D100 and D104 errors for all files and no other errors to be found.

0.2.3
-----

- Remove extraneous space in error message.

- Fix up how the plugin displays with ``flake8 --version``.

0.2.2
-----

- Better support for input provided via stdin.

0.2.1
-----

- Prevent AllError or EnvironmentErrors from being raised. Thanks Alex
  Pyrgiotis.

0.2.0
-----

- Upgrade to pep257 0.3.0

0.1.4
-----

- Stop truncating error messages

0.1.3
-----

- Really fix the installation issue this time.

0.1.2
-----

- Actually fix the PyPI release. **Ugh**

0.1.1
-----

- Fix the PyPI release.

0.1.0
-----

- Initial Release!



  * [flake8-import-order-0.18.1](https://github.com/PyCQA/flake8-import-order) flake8-import-order
===================

|Build Status|

A `flake8 <http://flake8.readthedocs.org/en/latest/>`__ and `Pylama
<https://github.com/klen/pylama>`__ plugin that checks the ordering of
your imports. It does not check anything else about the
imports. Merely that they are grouped and ordered correctly.

In general stdlib comes first, then 3rd party, then local packages,
and that each group is individually alphabetized, however this depends
on the style used. Flake8-Import-Order supports a number of `styles
<#styles>`_ and is extensible allowing for `custom styles
<#extending-styles>`_.

This plugin was originally developed to match the style preferences of
the `cryptography <https://github.com/pyca/cryptography>`__ project,
with this style remaining the default.

Warnings
--------

This package adds 4 new flake8 warnings

-  ``I100``: Your import statements are in the wrong order.
-  ``I101``: The names in your from import are in the wrong order.
-  ``I201``: Missing newline between import groups.
-  ``I202``: Additional newline in a group of imports.

Styles
------

The following styles are directly supported,

* ``cryptography`` - see an `example <https://github.com/PyCQA/flake8-import-order/blob/master/tests/test_cases/complete_cryptography.py>`__
* ``google`` - style described in `Google Style Guidelines <https://google.github.io/styleguide/pyguide.html?showone=Imports_formatting#Imports_formatting>`__, see an `example <https://github.com/PyCQA/flake8-import-order/blob/master/tests/test_cases/complete_google.py>`__
* ``smarkets`` - style as ``google`` only with `import` statements before `from X import ...` statements, see an `example <https://github.com/PyCQA/flake8-import-order/blob/master/tests/test_cases/complete_smarkets.py>`__
* ``appnexus`` - style as ``google`` only with `import` statements for packages local to your company or organisation coming after `import` statements for third-party packages, see an `example <https://github.com/PyCQA/flake8-import-order/blob/master/tests/test_cases/complete_appnexus.py>`__
* ``edited`` - see an `example <https://github.com/PyCQA/flake8-import-order/blob/master/tests/test_cases/complete_edited.py>`__
* ``pycharm`` - style as ``smarkets`` only with case sensitive sorting imported names
* ``pep8`` - style that only enforces groups without enforcing the order within the groups

You can also `add your own style <#extending-styles>`_ by extending ``Style``
class.

Configuration
-------------

You will want to set the ``application-import-names`` option to a
comma separated list of names that should be considered local to your
application. These will be used to help categorise your import
statements into the correct groups. Note that relative imports are
always considered local.

You will want to set the ``application-package-names`` option to a
comma separated list of names that should be considered local to your
company or organisation, but which are obtained using some sort of
package manager like Pip, Apt, or Yum.  Typically, code representing
the values listed in this option is located in a different repository
than the code being developed.  This option is only accepted in the
supported ``appnexus`` or ``edited`` styles or in any style that
accepts application package names.

The ``application-import-names`` and ``application-package-names`` can
contain namespaced packages or even exact nested module names. (This
is possible with 0.16 onwards).

``import-order-style`` controls what style the plugin follows
(``cryptography`` is the default).

Limitations
-----------

Currently these checks are limited to module scope imports only.
Conditional imports in module scope will also be ignored.

Classification of an imported module is achieved by checking the
module against a stdlib list and then if there is no match against the
``application-import-names`` list and ``application-package-names`` if
the style accepts application-package names. Only if none of these
lists contain the imported module will it be classified as third
party.

These checks only consider an import against its previous import,
rather than considering all the imports together. This means that
``I100`` errors are only raised for the latter of adjacent imports out
of order. For example,

.. code-block:: python

    import X.B
    import X  # I100
    import X.A

only ``import X`` raises an ``I100`` error, yet ``import X.A`` is also
out of order compared with the ``import X.B``.

Imported modules are classified as stdlib if the module is in a
vendored list of stdlib modules. This list is based on the latest
release of Python and hence the results can be misleading. This list
is also the same for all Python versions because otherwise it would
be impossible to write programs that work under both Python 2 and 3
*and* pass the import order check.

The ``I202`` check will consider any blank line between imports to
count, even if the line is not contextually related to the
imports. For example,

.. code-block:: python

    import logging
    try:
        from logging import NullHandler
    except ImportError:
        class NullHandler(logging.Handler):
            """Shim for version of Python < 2.7."""

            def emit(self, record):
                pass
    import sys  # I202 due to the blank line before the 'def emit'

will trigger a ``I202`` error despite the blank line not being
contextually related.

Extending styles
----------------

You can add your own style by extending ``flake8_import_order.styles.Style``
class. Here's an example:

.. code-block:: python

    from flake8_import_order.styles import Cryptography


    class ReversedCryptography(Cryptography):
        # Note that Cryptography is a subclass of Style.

        @staticmethod
        def sorted_names(names):
            return reversed(Cryptography.sorted_names(names))

By default there are five import groupings or sections; future,
stdlib, third party, application, and relative imports. A style can
choose to accept another grouping, application-package, by setting the
``Style`` class variable ``accepts_application_package_names`` to
True, e.g.

.. code-block:: python

    class PackageNameCryptography(Cryptography):
        accepts_application_package_names = True

To make flake8-import-order able to discover your extended style, you need to
register it as ``flake8_import_order.styles`` using setuptools' `entry points
<https://setuptools.readthedocs.io/en/latest/pkg_resources.html#entry-points>`__
mechanism:

.. code-block:: python

    # setup.py of your style package
    setup(
        name='flake8-import-order-reversed-cryptography',
        ...,
        entry_points={
            'flake8_import_order.styles': [
                'reversed = reversedcryptography:ReversedCryptography',
                # 'reversed' is a style name.  You can pass it to
                # --import-order-style option
                # 'reversedcryptography:ReversedCryptography' is an import path
                # of your extended style class.
            ]
        }
    )

.. |Build Status| image:: https://travis-ci.org/PyCQA/flake8-import-order.svg?branch=master
   :target: https://travis-ci.org/PyCQA/flake8-import-order



  * [flake8-polyfill-1.0.2](https://gitlab.com/pycqa/flake8-polyfill) =============================
 Polyfill for Flake8 Plugins
=============================

``flake8-polyfill`` is a package that provides some compatibility helpers for
Flake8 plugins that intend to support Flake8 2.x and 3.x simultaneously.


Installation
============

.. code-block:: bash

    pip install flake8-polyfill


Usage
=====

Option Handling
---------------

One problem area with compatibility with Flake8 2.x and 3.x is the registering
options and receiving the parsed values.

Flake8 3.0 added extra parameters to the ``add_option`` method which don't
have the same effect on Flake8 2.x. To accomodate the change, this polyfill
module allows you to do:

.. code-block:: python

    from flake8_polyfill import options

    class MyFlake8Plugin(object):
        @classmethod
        def add_options(cls, parser):
            options.register(parser, '--my-long-option-name',
                             parse_from_config=True,
                             comma_separated_list=True,
                             default='...',
                             help='...')
            options.register(parser, '-m', '--my-other-long-option-name',
                             parse_from_config=True,
                             normalize_paths=True,
                             default='...',
                             help='...')

        @classmethod
        def parse_options(cls, values):
            cls.my_long_option_name = values.my_long_option_name
            cls.my_other_long_option_name = values.my_other_long_option_name

And have the code work the same way on both versions.

Retrieving Standard In
----------------------

Until Flake8 2.6, getting the code on standard in from a plugin has been
simple:

.. code-block:: python

    import pep8

    stdin = pep8.get_stdin_value()

In 2.6 you now have to know whether to use ``pep8`` or ``pycodestyle`` since
Flake8 2.6 made a hard change to ``pycodestyle``.

The reason you need to know which module to use is because standard in can be
exhausted and Flake8 does some work to cache the value so that call always
returns the desired data.

In 3.0, Flake8 no longer monkey-patches those modules.

To accommodate this, this package provides:

.. code-block:: python

    from flake8_polyfill import stdin

    stdin.monkey_patch('all')
    stdin.monkey_patch('pep8')
    stdin.monkey_patch('pycodestyle')

This allows you to have the polyfill module monkey-patch what you want so it
is always monkey-patched. It will also do so in an intelligent way.

Version Comparison
------------------

Flake8 2.x did not include an object that would allow for easy version
comparison. Flake8 3.0, however, added a ``__version_info__`` attribute. For
consistency, Flake8 Polyfill will turn 2.x's version string into a tuple
suitable for comparison.

.. code-block:: python

    from flake8_polyfill import version

    if (2, 4) <= version.version_info < (2, 6):
        # ...
    elif (2, 6) <= version.version_info < (3, 0):
        # ...
    elif (3, 0) <= version.version_info < (4, 0):
        # ...


License
=======

MIT


Creator
=======

Ian Cordasco



  * [flaky-3.6.0](https://github.com/box/flaky) flaky
=====

.. image:: http://opensource.box.com/badges/active.svg
    :target: http://opensource.box.com/badges

.. image:: https://travis-ci.org/box/flaky.svg?branch=master
    :target: https://travis-ci.org/box/flaky

.. image:: https://img.shields.io/pypi/v/flaky.svg
    :target: https://pypi.python.org/pypi/flaky

About
-----

Flaky is a plugin for nose or pytest that automatically reruns flaky tests.

Ideally, tests reliably pass or fail, but sometimes test fixtures must rely on components that aren't 100%
reliable. With flaky, instead of removing those tests or marking them to @skip, they can be automatically
retried.

For more information about flaky, see `this presentation <http://opensource.box.com/flaky/>`_.

Marking tests flaky
~~~~~~~~~~~~~~~~~~~

To mark a test as flaky, simply import flaky and decorate the test with @flaky:

.. code-block:: python

    from flaky import flaky

.. code-block:: python

    @flaky
    def test_something_that_usually_passes(self):
        value_to_double = 21
        result = get_result_from_flaky_doubler(value_to_double)
        self.assertEqual(result, value_to_double * 2, 'Result doubled incorrectly.')

By default, flaky will retry a failing test once, but that behavior can be overridden by passing values to the
flaky decorator. It accepts two parameters: max_runs, and min_passes; flaky will run tests up to max_runs times, until
it has succeeded min_passes times. Once a test passes min_passes times, it's considered a success; once it has been
run max_runs times without passing min_passes times, it's considered a failure.

.. code-block:: python

    @flaky(max_runs=3, min_passes=2)
    def test_something_that_usually_passes(self):
        """This test must pass twice, and it can be run up to three times."""
        value_to_double = 21
        result = get_result_from_flaky_doubler(value_to_double)
        self.assertEqual(result, value_to_double * 2, 'Result doubled incorrectly.')

Marking a class flaky
+++++++++++++++++++++

In addition to marking a single test flaky, entire test cases can be marked flaky:

.. code-block:: python

    @flaky
    class TestMultipliers(TestCase):
        def test_flaky_doubler(self):
            value_to_double = 21
            result = get_result_from_flaky_doubler(value_to_double)
            self.assertEqual(result, value_to_double * 2, 'Result doubled incorrectly.')

        @flaky(max_runs=3)
        def test_flaky_tripler(self):
            value_to_triple = 14
            result = get_result_from_flaky_tripler(value_to_triple)
            self.assertEqual(result, value_to_triple * 3, 'Result tripled incorrectly.')

The @flaky class decorator will mark test_flaky_doubler as flaky, but it won't override the 3 max_runs
for test_flaky_tripler (from the decorator on that test method).

Pytest marker
+++++++++++++

When using ``pytest``, ``@pytest.mark.flaky`` can be used in place of ``@flaky``.

Don't rerun certain types of failures
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Depending on your tests, some failures are obviously not due to flakiness. Instead of rerunning
after those failures, you can specify a filter function that can tell flaky to fail the test right away.

.. code-block:: python

    def is_not_crash(err, *args):
        return not issubclass(err[0], ProductCrashedError)

    @flaky
    def test_something():
        raise ProductCrashedError

    @flaky(rerun_filter=is_not_crash)
    def test_something_else():
        raise ProductCrashedError

Flaky will run ``test_something`` twice, but will only run ``test_something_else`` once.

It can also be used to incur a delay between test retries:

.. code-block:: python

    import time

    def delay_rerun(*args):
        time.sleep(1)
        return True

    @flaky(rerun_filter=delay_rerun)
    def test_something_else():
        ...

Activating the plugin
~~~~~~~~~~~~~~~~~~~~~

Like any nose plugin, flaky can be activated via the command line:

.. code-block:: console

    nosetests --with-flaky

With pytest, flaky will automatically run. It can, however be disabled via the command line:

.. code-block:: console

    pytest -p no:flaky

Command line arguments
~~~~~~~~~~~~~~~~~~~~~~

No Flaky Report
+++++++++++++++

Pass ``--no-flaky-report`` to suppress the report at the end of the run detailing flaky test results.

Shorter Flaky Report
++++++++++++++++++++

Pass ``--no-success-flaky-report`` to suppress information about successful flaky tests.

Force Flaky
+++++++++++

Pass ``--force-flaky`` to treat all tests as flaky.

Pass ``--max-runs=MAX_RUNS`` and/or ``--min-passes=MIN_PASSES`` to control the behavior of flaky if ``--force-flaky``
is specified. Flaky decorators on individual tests will override these defaults.


*Additional usage examples are in the code - see test/test_nose/test_nose_example.py and test/test_pytest/test_pytest_example.py*

Installation
------------

To install, simply:

.. code-block:: console

    pip install flaky


Compatibility
-------------

Flaky is tested with the following test runners and options:

- Nosetests. Doctests cannot be marked flaky.

- Py.test. Works with ``pytest-xdist`` but not with the ``--boxed`` option. Doctests cannot be marked flaky.


Contributing
------------

See `CONTRIBUTING.rst <https://github.com/box/flaky/blob/master/CONTRIBUTING.rst>`_.


Setup
~~~~~

Create a virtual environment and install packages -

.. code-block:: console

    mkvirtualenv flaky
    pip install -r requirements-dev.txt


Testing
~~~~~~~

Run all tests using -

.. code-block:: console

    tox

The tox tests include code style checks via pycodestyle and pylint.


Copyright and License
---------------------

::

 Copyright 2015 Box, Inc. All rights reserved.

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.



  * [flask-appconfig-0.11.1](http://github.com/mbr/flask-appconfig) Flask-AppConfig
===============

Allows you to configure an application using pre-set methods.

.. code-block:: python

    from flask_appconfig import AppConfig

    def create_app(configfile=None):
        app = Flask('myapp')
        AppConfig(app, configfile)
        return app

The application returned by ``create_app`` will, in order:

1. Load default settings from a module called ``myapp.default_config``, if it
   exists. (method described in
   http://flask.pocoo.org/docs/config/#configuring-from-files )
2. Load settings from a configuration file whose name is given in the
   environment variable ``MYAPP_CONFIG`` (see link from 1.).
3. Load json or string values directly from environment variables that start
   with a prefix of ``MYAPP_``, i.e. setting ``MYAPP_SQLALCHEMY_ECHO=true``
   will cause the setting of ``SQLALCHEMY_ECHO`` to be ``True``.

Any of these behaviors can be altered or disabled by passing the appropriate
options to the constructor or ``init_app()``.


Heroku support
--------------

Flask-AppConfig supports configuring a number of services through
``HerokuConfig``:

.. code-block:: python

    from flask_appconfig import HerokuConfig

    def create_app(configfile=None):
        app = Flask('myapp')
        HerokuConfig(app, configfile)
        return app

Works like the example above, but environment variables set by various Heroku
addons will be parsed as json and converted to configuration variables
accordingly. Forexample, when enabling `Mailgun
<https://addons.heroku.com/mailgun>`_, the configuration of `Flask-Mail
<http://pythonhosted.org/Flask-Mail/>`_ will be automatically be set correctly.


Using "ENV-only"
----------------

If you only want to use the environment-parsing functions of Flask-AppConfig,
the appropriate functions are exposed:

.. code-block:: python

    from flask_appconfig.heroku import from_heroku_envvars
    from flask_appconfig.env import from_envvars

    # from environment variables. note that you need to set the prefix, as
    # no auto-detection can be done without an app object
    from_envvars(app.config, prefix=app.name.upper() + '_')

    # also possible: parse heroku configuration values
    # any dict-like object will do as the first parameter
    from_heroku_envvars(app.config)


Installation
------------

Via `PyPI <http://pypi.python.org/pypi/flask-appconfig>`_::

    $ pip install flask-appconfig

Requires Python 2.7.


flask utility
-------------

If you want to get started quickly without thinking a lot about writing a run
script, the ``flask`` utility supports the ``create_app``/factory pattern::

    $ flask --app=myapp dev

This will import a module ``myapp``, and call ``myapp.run(debug=True)``.

Other options can come in handy as well::

    $ flask --app=myapp dev -S -p 8000

Runs the app on port 8080, with SSL enabled. You can also set the ``FLASK_APP``
environment variable or set ``FLASK_APP`` inside ``.env`` and omit the
``--app`` parameter.

Note that the ``flask`` utility is subject to change, as it will conflict with
the CLI functionality of Flask 1.0. The API is currently kept close, but it
will see changes once Flask 1.0 is released.


Flask-Debug and Flask-DebugToolbar support
******************************************

``flask`` automatically activates Flask-Debug_ and Flask-DebugToolbar_ on
your application; this allows to have it installed locally while not having to
install any debug code in production. You can suppress this behavior with the
``-E``/``--no-flask-debug`` flag.

Note that these features are only enabled if you install either of these
extensions manually; they are not dependencies of Flask-Appconfig.

.. _Flask-Debug: https://github.com/mbr/flask-debug
.. _Flask-DebugToolbar: https://flask-debugtoolbar.readthedocs.org/


Thoughts on Configuration
-------------------------

There is a lot of ways to configure a Flask application and often times,
less-than-optimal ones are chosen in a hurry.

This extension aims to do three things:

1. Set a "standard" of doing configuration that is flexible and in-line with
   the official docs and (what I consider) good practices.
2. Make it as convenient as possible to provide these configuration methods in
   an application.
3. Auto-configure on Heroku as much as possible without sacrificing 1. and 2.

`12factor.net <http://12factor.net/>`_ seems to capture a good amount of good
thoughts on the issue and Flask-Appconfig should aid you in writing an
application that follows the principles laid out there.

Providing defaults
******************

Defaults should be included and overridable, without altering the file
containing the defaults.

Separate code and configuration
*******************************

It should be possible to install the app to a read-only (possibly system-wide)
location, without having to store configuration files (or, even worse,
configuration modules) inside its folders.

Environment variables and instance folders make this possible. As an added
benefit, configuration does not need to be stored alongside the code in version
control.

No code necessary for most deployments using the factory-method pattern
***********************************************************************

When deploying with gunicorn, passing ``myapp:create_app()`` suffices to create
an app instance, no boilerplate code to create the WSGI app should be necessary.

Multiple instances
******************

Running multiple apps inside the same interpreter should also be possible. While
this is slightly more complicated and may occasionally violate the "no-code"
guideline above, it's still straightforward by using configuration file
parameters.


Development
-----------
Flask-AppConfig is under "conceptional development". The API or semantics
may change in the future.

Send pull requests for more Heroku-apps to be supported. Send feedback via mail.

Changelog
---------

Backwards-incompatible changes, as they were introduced:

0.11
****
* The ``flaskdev`` tool has been replaced with ``flask``.
* Using the new ``flask`` tool auto-reloading will also change by default. If a
  syntax error is introduced to the code, the app will try to restart after two
  seconds by default, instead of crashing. This can be suppressed with the
  '--extended-reload 0' flag.
* If the app import fails, ``flask`` will add ``.`` to ``sys.path`` and try to
  to import once again.
* Experimental commands ``serve`` and ``db`` have been added.

0.4
***
* Environment variables are no longer prefixed with ``FLASK_`` by default, but
  rather use ``APPNAME_`` (with ``APPNAME`` being the applications name in
  uppercase).
* ``MYAPP_SETTINGS`` became ``MYAPP_CONFIG``, ``default_settings`` became
  ``default_config``.
  * [flask-nav-0.6](http://github.com/mbr/flask-nav) Flask-Nav
=========

Flask-Nav is a `Flask <http://flask.pocoo.org>`_-Extension to ease the creation
of navigational Elements in Applications. It provides means to Express the
Navigational structure and different ways to render these, making it easy to
custom tailor it for your application.

A motivating example:

.. code-block:: python

    from flask import Flask, render_template
    from flask_nav import Nav
    from flask_nav.elements import *

    nav = Nav()

    # registers the "top" menubar
    nav.register_element('top', Navbar(
        View('Widgits, Inc.', 'index'),
        View('Our Mission', 'about'),
        Subgroup(
            'Products',
            View('Wg240-Series', 'products', product='wg240'),
            View('Wg250-Series', 'products', product='wg250'),
            Separator(),
            Label('Discontinued Products'),
            View('Wg10X', 'products', product='wg10x'),
        ),
        Link('Tech Support', href='http://techsupport.invalid/widgits_inc'),
    ))


    app = Flask(__name__)
    # [...] (view definitions)

    nav.init_app(app)

You can find a small, runnable example application inside the ``example``
folder. To run it, install `Flask-Appconfig
<https://github.com/mbr/flask-appconfig>`_ and execute::

    $ flask --app=example dev

The `full documentation <http://pythonhosted.org/flask-nav/>`_ can be found on PyPI.
  * [flup-1.0.3](http://www.saddi.com/software/flup/) 
  * [freezer-7.1.0](https://docs.openstack.org/freezer/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/freezer.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

OpenStack Freezer
=================

.. image:: freezer_logo.jpg

Freezer is a Backup Restore DR as a Service platform that helps you to automate
the data backup and restore process.

The following features are available:

-  Backup file system using point-in-time snapshot
-  Strong encryption supported: AES-256-CFB
-  Backup file system tree directly (without volume snapshot)
-  Backup journalled MongoDB directory tree using lvm snapshot to Swift
-  Backup MySQL with lvm snapshot
-  Restore data from a specific date automatically to file system
-  Low storage consumption as the backup are uploaded as a stream
-  Flexible backup policy (incremental and differential)
-  Data is archived in GNU Tar format for file based incremental
-  Multiple compression algorithm support (zlib, bzip2, xz)
-  Remove old backup automatically according to the provided parameters
-  Multiple storage media support (Swift, local file system, or ssh)
-  Flush kernel buffered memory to disk
-  Multi-platform (Linux, Windows, \*BSD, OSX)
-  Manage multiple jobs (I.e., multiple backups on the same node)
-  Synchronize backups and restore on multiple nodes
-  Web user interface integrated with OpenStack Horizon
-  Execute scripts/commands before or after a job execution
-  More ...

To learn how to use Freezer's API, consult the documentation available online
at:

- `Backup API Reference <https://docs.openstack.org/api-ref/backup/>`__
- `Freezer API <https://github.com/openstack/freezer-api>`__

Freezer Disaster Recovery:
- `Freezer DR <https://github.com/openstack/freezer-dr>`__

Freezer Horizon plugin:
- `Freezer Web UI <https://github.com/openstack/freezer-web-ui>`__

For more information on OpenStack APIs, SDKs and CLIs in general, refer to:

- `OpenStack for App Developers <https://www.openstack.org/appdev/>`__
- `Development resources for OpenStack clouds
  <https://developer.openstack.org/>`__

Operators
---------

To learn how to deploy and configure OpenStack Freezer, consult the
documentation available online at:

- `OpenStack Freezer <https://docs.openstack.org/freezer/latest/>`__

In the unfortunate event that bugs are discovered, they should be reported to
the appropriate bug tracker. If you obtained the software from a 3rd party
operating system vendor, it is often wise to use their own bug tracker for
reporting problems. In all other cases use the master OpenStack bug tracker,
available at:

- `Bug Tracker <https://storyboard.openstack.org/#!/project/openstack/freezer>`__

Troubleshooting
---------------

When errors occure, these are good places to check:

* freezer-api log: `$HOME/log/freezer-api.log`
                   `/var/log/apache2/freezer-api.log`
* freezer-agent log: `$HOME/.freezer/freezer.log`
* freezer-scheduler log:`/var/log/freezer/scheduler.log`

Developers
----------

Any new code must follow the development guidelines detailed in the HACKING.rst
file and OpenStack general development guidelines, and pass all unit tests.

Further developer focused documentation is available at:

- `Official Freezer Documentation <https://docs.openstack.org/freezer/latest/>`__
- `Official Client Documentation
  <https://docs.openstack.org/python-freezerclient/latest/>`__

Contributors are encouraged to join IRC (``#openstack-freezer`` on freenode):

- `IRC <https://wiki.openstack.org/wiki/IRC>`__

Other Information
-----------------

Release notes for the project can be found at:

- `Release notes
  <https://docs.openstack.org/releasenotes/freezer/>`__

During each `Summit`_ and `Project Team Gathering`_, we agree on what the whole
community wants to focus on for the upcoming release. The plans for freezer can
be found at:

- `Freezer Old README <https://github.com/openstack/freezer/tree/master/doc/README.rst>`__

- `Freezer Specs <http://specs.openstack.org/openstack/freezer-specs/>`__

.. _Summit: https://www.openstack.org/summit/
.. _Project Team Gathering: https://www.openstack.org/ptg/




  * [fsspec-0.4.1](http://github.com/intake/filesystem_spec) # filesystem_spec

[![Build Status](https://travis-ci.org/intake/filesystem_spec.svg?branch=master)](https://travis-ci.org/martindurant/filesystem_spec)
[![Docs](https://readthedocs.org/projects/filesystem-spec/badge/?version=latest)](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest)

A specification for pythonic filesystems.

## Install

```bash
pip install fsspec
```
or
```bash
conda install -c conda-forge fsspec
```

## Purpose

To produce a template or specification for a file-system interface, that specific implementations should follow,
so that applications making use of them can rely on a common behaviour and not have to worry about the specific
internal implementation decisions with any given backend. Many such implementations are included in this package,
or in sister projects such as `s3fs` and `gcsfs`.

In addition, if this is well-designed, then additional functionality, such as a key-value store or FUSE
mounting of the file-system implementation may be available for all implementations "for free".

## Documentation

Please refer to [RTD](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest)

## Develop

fsspec uses [tox](https://tox.readthedocs.io/en/latest/) and
[tox-conda](https://github.com/tox-dev/tox-conda) to manage dev and test
environments. First, install conda with tox and tox-conda in a base environment
(eg. `conda install -c conda-forge tox tox-conda`). Calls to `tox` can then be
used to configure a development environment and run tests.

First, setup a development conda environment via `tox -e dev`. This will
install fspec dependencies, test & dev tools, and install fsspec in develop
mode. Then, activate the dev environment under `.tox/dev` via `conda activate .tox/dev`.

### Testing

Tests can be run directly in the activated dev environment via `pytest fsspec`.

The full fsspec test suite can be run via `tox`, which will setup and execute
tests against multiple dependency versions in isolated environment. Run `tox
-av` to list available test environments, select environments via `tox -e <env>`.

The full fsspec suite requires a system-level docker, docker-compose, and fuse
installation. See `ci/install.sh` for a detailed installation example.

### Code Formatting

fsspec uses [Black](https://black.readthedocs.io/en/stable) to ensure
a consistent code format throughout the project. ``black`` is automatically
installed in the tox dev env, activated via `conda activate .tox/dev`.

Then, run `black fsspec` from the root of the filesystem_spec repository to
auto-format your code. Additionally, many editors have plugins that will apply
`black` as you edit files.

Optionally, you may wish to setup [pre-commit hooks](https://pre-commit.com) to
automatically run `black` when you make a git commit. ``black`` is automatically
installed in the tox dev env, activated via `conda activate .tox/dev`.

Then, run `pre-commit install --install-hooks` from the root of the
filesystem_spec repository to setup pre-commit hooks. `black` will now be run
before you commit, reformatting any changed files. You can format without
committing via `pre-commit run` or skip these checks with `git commit
--no-verify`.
  * [funcsigs-1.0.2](http://funcsigs.readthedocs.org) .. funcsigs documentation master file, created by
   sphinx-quickstart on Fri Apr 20 20:27:52 2012.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Introducing funcsigs
====================

The Funcsigs Package
--------------------

``funcsigs`` is a backport of the `PEP 362`_ function signature features from
Python 3.3's `inspect`_ module. The backport is compatible with Python 2.6, 2.7
as well as 3.3 and up. 3.2 was supported by version 0.4, but with setuptools and
pip no longer supporting 3.2, we cannot make any statement about 3.2
compatibility.

Compatibility
`````````````

The ``funcsigs`` backport has been tested against:

* CPython 2.6
* CPython 2.7
* CPython 3.3
* CPython 3.4
* CPython 3.5
* CPython nightlies
* PyPy and PyPy3(currently failing CI)

Continuous integration testing is provided by `Travis CI`_.

Under Python 2.x there is a compatibility issue when a function is assigned to
the ``__wrapped__`` property of a class after it has been constructed.
Similiarily there under PyPy directly passing the ``__call__`` method of a
builtin is also a compatibility issues.  Otherwise the functionality is
believed to be uniform between both Python2 and Python3.

Issues
``````

Source code for ``funcsigs`` is hosted on `GitHub`_. Any bug reports or feature
requests can be made using GitHub's `issues system`_. |build_status| |coverage|

Example
-------

To obtain a `Signature` object, pass the target function to the
``funcsigs.signature`` function.

.. code-block:: python

    >>> from funcsigs import signature
    >>> def foo(a, b=None, *args, **kwargs):
    ...     pass
    ...
    >>> sig = signature(foo)
    >>> sig
    <funcsigs.Signature object at 0x...>
    >>> sig.parameters
    OrderedDict([('a', <Parameter at 0x... 'a'>), ('b', <Parameter at 0x... 'b'>), ('args', <Parameter at 0x... 'args'>), ('kwargs', <Parameter at 0x... 'kwargs'>)])
    >>> sig.return_annotation
    <class 'funcsigs._empty'>

Introspecting callables with the Signature object
-------------------------------------------------

.. note::

   This section of documentation is a direct reproduction of the Python
   standard library documentation for the inspect module.

The Signature object represents the call signature of a callable object and its
return annotation.  To retrieve a Signature object, use the :func:`signature`
function.

.. function:: signature(callable)

   Return a :class:`Signature` object for the given ``callable``::

      >>> from funcsigs import signature
      >>> def foo(a, *, b:int, **kwargs):
      ...     pass

      >>> sig = signature(foo)

      >>> str(sig)
      '(a, *, b:int, **kwargs)'

      >>> str(sig.parameters['b'])
      'b:int'

      >>> sig.parameters['b'].annotation
      <class 'int'>

   Accepts a wide range of python callables, from plain functions and classes to
   :func:`functools.partial` objects.

   .. note::

      Some callables may not be introspectable in certain implementations of
      Python.  For example, in CPython, built-in functions defined in C provide
      no metadata about their arguments.


.. class:: Signature

   A Signature object represents the call signature of a function and its return
   annotation.  For each parameter accepted by the function it stores a
   :class:`Parameter` object in its :attr:`parameters` collection.

   Signature objects are *immutable*.  Use :meth:`Signature.replace` to make a
   modified copy.

   .. attribute:: Signature.empty

      A special class-level marker to specify absence of a return annotation.

   .. attribute:: Signature.parameters

      An ordered mapping of parameters' names to the corresponding
      :class:`Parameter` objects.

   .. attribute:: Signature.return_annotation

      The "return" annotation for the callable.  If the callable has no "return"
      annotation, this attribute is set to :attr:`Signature.empty`.

   .. method:: Signature.bind(*args, **kwargs)

      Create a mapping from positional and keyword arguments to parameters.
      Returns :class:`BoundArguments` if ``*args`` and ``**kwargs`` match the
      signature, or raises a :exc:`TypeError`.

   .. method:: Signature.bind_partial(*args, **kwargs)

      Works the same way as :meth:`Signature.bind`, but allows the omission of
      some required arguments (mimics :func:`functools.partial` behavior.)
      Returns :class:`BoundArguments`, or raises a :exc:`TypeError` if the
      passed arguments do not match the signature.

   .. method:: Signature.replace(*[, parameters][, return_annotation])

      Create a new Signature instance based on the instance replace was invoked
      on.  It is possible to pass different ``parameters`` and/or
      ``return_annotation`` to override the corresponding properties of the base
      signature.  To remove return_annotation from the copied Signature, pass in
      :attr:`Signature.empty`.

      ::

         >>> def test(a, b):
         ...     pass
         >>> sig = signature(test)
         >>> new_sig = sig.replace(return_annotation="new return anno")
         >>> str(new_sig)
         "(a, b) -> 'new return anno'"


.. class:: Parameter

   Parameter objects are *immutable*.  Instead of modifying a Parameter object,
   you can use :meth:`Parameter.replace` to create a modified copy.

   .. attribute:: Parameter.empty

      A special class-level marker to specify absence of default values and
      annotations.

   .. attribute:: Parameter.name

      The name of the parameter as a string.  Must be a valid python identifier
      name (with the exception of ``POSITIONAL_ONLY`` parameters, which can have
      it set to ``None``).

   .. attribute:: Parameter.default

      The default value for the parameter.  If the parameter has no default
      value, this attribute is set to :attr:`Parameter.empty`.

   .. attribute:: Parameter.annotation

      The annotation for the parameter.  If the parameter has no annotation,
      this attribute is set to :attr:`Parameter.empty`.

   .. attribute:: Parameter.kind

      Describes how argument values are bound to the parameter.  Possible values
      (accessible via :class:`Parameter`, like ``Parameter.KEYWORD_ONLY``):

      +------------------------+----------------------------------------------+
      |    Name                | Meaning                                      |
      +========================+==============================================+
      | *POSITIONAL_ONLY*      | Value must be supplied as a positional       |
      |                        | argument.                                    |
      |                        |                                              |
      |                        | Python has no explicit syntax for defining   |
      |                        | positional-only parameters, but many built-in|
      |                        | and extension module functions (especially   |
      |                        | those that accept only one or two parameters)|
      |                        | accept them.                                 |
      +------------------------+----------------------------------------------+
      | *POSITIONAL_OR_KEYWORD*| Value may be supplied as either a keyword or |
      |                        | positional argument (this is the standard    |
      |                        | binding behaviour for functions implemented  |
      |                        | in Python.)                                  |
      +------------------------+----------------------------------------------+
      | *VAR_POSITIONAL*       | A tuple of positional arguments that aren't  |
      |                        | bound to any other parameter. This           |
      |                        | corresponds to a ``*args`` parameter in a    |
      |                        | Python function definition.                  |
      +------------------------+----------------------------------------------+
      | *KEYWORD_ONLY*         | Value must be supplied as a keyword argument.|
      |                        | Keyword only parameters are those which      |
      |                        | appear after a ``*`` or ``*args`` entry in a |
      |                        | Python function definition.                  |
      +------------------------+----------------------------------------------+
      | *VAR_KEYWORD*          | A dict of keyword arguments that aren't bound|
      |                        | to any other parameter. This corresponds to a|
      |                        | ``**kwargs`` parameter in a Python function  |
      |                        | definition.                                  |
      +------------------------+----------------------------------------------+

      Example: print all keyword-only arguments without default values::

         >>> def foo(a, b, *, c, d=10):
         ...     pass

         >>> sig = signature(foo)
         >>> for param in sig.parameters.values():
         ...     if (param.kind == param.KEYWORD_ONLY and
         ...                        param.default is param.empty):
         ...         print('Parameter:', param)
         Parameter: c

   .. method:: Parameter.replace(*[, name][, kind][, default][, annotation])

      Create a new Parameter instance based on the instance replaced was invoked
      on.  To override a :class:`Parameter` attribute, pass the corresponding
      argument.  To remove a default value or/and an annotation from a
      Parameter, pass :attr:`Parameter.empty`.

      ::

         >>> from funcsigs import Parameter
         >>> param = Parameter('foo', Parameter.KEYWORD_ONLY, default=42)
         >>> str(param)
         'foo=42'

         >>> str(param.replace()) # Will create a shallow copy of 'param'
         'foo=42'

         >>> str(param.replace(default=Parameter.empty, annotation='spam'))
         "foo:'spam'"


.. class:: BoundArguments

   Result of a :meth:`Signature.bind` or :meth:`Signature.bind_partial` call.
   Holds the mapping of arguments to the function's parameters.

   .. attribute:: BoundArguments.arguments

      An ordered, mutable mapping (:class:`collections.OrderedDict`) of
      parameters' names to arguments' values.  Contains only explicitly bound
      arguments.  Changes in :attr:`arguments` will reflect in :attr:`args` and
      :attr:`kwargs`.

      Should be used in conjunction with :attr:`Signature.parameters` for any
      argument processing purposes.

      .. note::

         Arguments for which :meth:`Signature.bind` or
         :meth:`Signature.bind_partial` relied on a default value are skipped.
         However, if needed, it is easy to include them.

      ::

        >>> def foo(a, b=10):
        ...     pass

        >>> sig = signature(foo)
        >>> ba = sig.bind(5)

        >>> ba.args, ba.kwargs
        ((5,), {})

        >>> for param in sig.parameters.values():
        ...     if param.name not in ba.arguments:
        ...         ba.arguments[param.name] = param.default

        >>> ba.args, ba.kwargs
        ((5, 10), {})


   .. attribute:: BoundArguments.args

      A tuple of positional arguments values.  Dynamically computed from the
      :attr:`arguments` attribute.

   .. attribute:: BoundArguments.kwargs

      A dict of keyword arguments values.  Dynamically computed from the
      :attr:`arguments` attribute.

   The :attr:`args` and :attr:`kwargs` properties can be used to invoke
   functions::

      def test(a, *, b):
         ...

      sig = signature(test)
      ba = sig.bind(10, b=20)
      test(*ba.args, **ba.kwargs)


.. seealso::

   :pep:`362` - Function Signature Object.
      The detailed specification, implementation details and examples.

Copyright
---------

*funcsigs* is a derived work of CPython under the terms of the `PSF License
Agreement`_. The original CPython inspect module, its unit tests and
documentation are the copyright of the Python Software Foundation. The derived
work is distributed under the `Apache License Version 2.0`_.

.. _PSF License Agreement: http://docs.python.org/3/license.html#terms-and-conditions-for-accessing-or-otherwise-using-python
.. _Apache License Version 2.0: http://opensource.org/licenses/Apache-2.0
.. _GitHub: https://github.com/testing-cabal/funcsigs
.. _PSF License Agreement: http://docs.python.org/3/license.html#terms-and-conditions-for-accessing-or-otherwise-using-python
.. _Travis CI: http://travis-ci.org/
.. _Read The Docs: http://funcsigs.readthedocs.org/
.. _PEP 362: http://www.python.org/dev/peps/pep-0362/
.. _inspect: http://docs.python.org/3/library/inspect.html#introspecting-callables-with-the-signature-object
.. _issues system: https://github.com/testing-cabal/funcsigs/issues

.. |build_status| image:: https://secure.travis-ci.org/aliles/funcsigs.png?branch=master
   :target: http://travis-ci.org/#!/aliles/funcsigs
   :alt: Current build status

.. |coverage| image:: https://coveralls.io/repos/aliles/funcsigs/badge.png?branch=master
   :target: https://coveralls.io/r/aliles/funcsigs?branch=master
   :alt: Coverage status

.. |pypi_version| image:: https://pypip.in/v/funcsigs/badge.png
   :target: https://crate.io/packages/funcsigs/
   :alt: Latest PyPI version
  * [future-0.17.1](https://python-future.org) future: Easy, safe support for Python 2/3 compatibility
=======================================================

``future`` is the missing compatibility layer between Python 2 and Python
3. It allows you to use a single, clean Python 3.x-compatible codebase to
support both Python 2 and Python 3 with minimal overhead.

It is designed to be used as follows::

    from __future__ import (absolute_import, division,
                            print_function, unicode_literals)
    from builtins import (
             bytes, dict, int, list, object, range, str,
             ascii, chr, hex, input, next, oct, open,
             pow, round, super,
             filter, map, zip)

followed by predominantly standard, idiomatic Python 3 code that then runs
similarly on Python 2.6/2.7 and Python 3.3+.

The imports have no effect on Python 3. On Python 2, they shadow the
corresponding builtins, which normally have different semantics on Python 3
versus 2, to provide their Python 3 semantics.


Standard library reorganization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``future`` supports the standard library reorganization (PEP 3108) through the
following Py3 interfaces:

    >>> # Top-level packages with Py3 names provided on Py2:
    >>> import html.parser
    >>> import queue
    >>> import tkinter.dialog
    >>> import xmlrpc.client
    >>> # etc.

    >>> # Aliases provided for extensions to existing Py2 module names:
    >>> from future.standard_library import install_aliases
    >>> install_aliases()

    >>> from collections import Counter, OrderedDict   # backported to Py2.6
    >>> from collections import UserDict, UserList, UserString
    >>> import urllib.request
    >>> from itertools import filterfalse, zip_longest
    >>> from subprocess import getoutput, getstatusoutput


Automatic conversion
--------------------

An included script called `futurize
<http://python-future.org/automatic_conversion.html>`_ aids in converting
code (from either Python 2 or Python 3) to code compatible with both
platforms. It is similar to ``python-modernize`` but goes further in
providing Python 3 compatibility through the use of the backported types
and builtin functions in ``future``.


Documentation
-------------

See: http://python-future.org


Credits
-------

:Author:  Ed Schofield, Jordan M. Adler, et al
:Sponsor: Python Charmers Pty Ltd, Australia, and Python Charmers Pte
          Ltd, Singapore. http://pythoncharmers.com
:Others:  See docs/credits.rst or http://python-future.org/credits.html


Licensing
---------
Copyright 2013-2019 Python Charmers Pty Ltd, Australia.
The software is distributed under an MIT licence. See LICENSE.txt.
  * [future-fstrings-1.2.0](https://github.com/asottile/future-fstrings) [![Build Status](https://asottile.visualstudio.com/asottile/_apis/build/status/asottile.future-fstrings?branchName=master)](https://asottile.visualstudio.com/asottile/_build/latest?definitionId=15&branchName=master)
[![Azure DevOps coverage](https://img.shields.io/azure-devops/coverage/asottile/asottile/15/master.svg)](https://dev.azure.com/asottile/asottile/_build/latest?definitionId=15&branchName=master)

future-fstrings
===============

A backport of fstrings to python<3.6.


## Installation

`pip install future-fstrings`


## Usage

Include the following encoding cookie at the top of your file (this replaces
the utf-8 cookie if you already have it):

```python
# -*- coding: future_fstrings -*-
```

And then write python3.6 fstring code as usual!

```python
# -*- coding: future_fstrings -*-
thing = 'world'
print(f'hello {thing}')
```

```console
$ python2.7 main.py
hello world
```

## Showing transformed source

`future-fstrings` also includes a cli to show transformed source.

```console
$ future-fstrings-show main.py
# -*- coding: future_fstrings -*-
thing = 'world'
print('hello {}'.format((thing)))
```

## Transform source for micropython

The `future-fstrings-show` command can be used to transform source before
distributing.  This can allow you to write f-string code but target platforms
which do not support f-strings, such as [micropython].

To use this on modern versions of python, install using:

```bash
pip install future-fstrings[rewrite]
```

and then use `future-fstrings-show` as above.

For instance:

```bash
future-fstrings-show code.py > code_rewritten.py
```

[micropython]: https://github.com/micropython/micropython

## How does this work?

`future-fstrings` has two parts:

1. A utf-8 compatible `codec` which performs source manipulation
    - The `codec` first decodes the source bytes using the UTF-8 codec
    - The `codec` then leverages
      [tokenize-rt](https://github.com/asottile/tokenize-rt) to rewrite
      f-strings.
2. A `.pth` file which registers a codec on interpreter startup.

## you may also like

- [future-breakpoint](https://github.com/asottile/future-breakpoint)



  * [galaxy-lib-19.5.2](https://github.com/galaxyproject/galaxy-lib) .. image:: https://readthedocs.org/projects/galaxy-lib/badge/?version=latest
   :target: http://galaxy-lib.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status

.. image:: https://badge.fury.io/py/galaxy-lib.svg
   :target: https://pypi.python.org/pypi/galaxy-lib/

.. image:: https://travis-ci.org/galaxyproject/galaxy-lib.png?branch=master
   :target: https://travis-ci.org/galaxyproject/galaxy-lib

Overview
--------

A small subset of the Galaxy_ project for reuse outside the core. This subset has minimal dependencies and is Python 3 compatible.

* Free software: Academic Free License version 3.0
* Documentation: https://galaxy-lib.readthedocs.org.
* Code: https://github.com/galaxyproject/galaxy-lib


.. _Galaxy: http://galaxyproject.org/
.. _GitHub: https://github.com/
.. _Docker: https://www.docker.com/
.. _Homebrew: http://brew.sh/
.. _linuxbrew: https://github.com/Homebrew/linuxbrew
.. _Vagrant: https://www.vagrantup.com/
.. _Travis CI: http://travis-ci.org/
.. _`tools-devteam`: https://github.com/galaxyproject/tools-devteam
.. _`tools-iuc`: https://github.com/galaxyproject/tools-iuc
.. _Publishing to the Tool Shed: http://planemo.readthedocs.org/en/latest/publishing.html
.. _Common Workfow Language: http://common-workflow-language.github.io




History
-------

.. to_doc

---------------------
19.5.2 (2019-04-28)
---------------------

* Update to latest Miniconda version.

---------------------
19.5.1 (2019-03-03)
---------------------

* Fix galaxy.tools.verify API method interface change breaking Planemo compatibility.

---------------------
19.5.0 (2019-03-01)
---------------------

* Synchronize with the development tip of Galaxy.
* Fix Travis CI configuration (thanks to @nsoranzo).

---------------------
18.9.2 (2019-01-02)
---------------------

* Implement option for uploading local test data.
* Bring in the latest Galaxy changes, which provide Python 3.7 support.

---------------------
18.9.1 (2018-09-15)
---------------------

* More work improving channel handling for mulled containers.

---------------------
18.9.0 (2018-09-14)
---------------------

* Update Conda channel order (thanks to @bgruening).
* Bring in the latest Galaxy changes and add Galaxy XSD to the project.
* Various cwltool fixes (thanks to @nsoranzo).

---------------------
18.5.15 (2018-09-12)
---------------------

* More tweaks to CWL tool loading to support newer versions of cwltool.

---------------------
18.5.14 (2018-09-12)
---------------------

* Improved tool linting and reporting thanks to @natefoo and @nsoranzo.
* Add support for newer cwltool thanks to @nsoranzo.
* Improved mulled building thanks to @pcm32.

---------------------
18.5.13 (2018-05-23)
---------------------

* Small updates to test parsing to support Galaxy workflow testing.

---------------------
18.5.12 (2018-05-22)
---------------------

* Update test data processing to allow URIs in Galaxy workflow tests.

---------------------
18.5.11 (2018-05-16)
---------------------

* Parse CWL SoftwareRequirements to Galaxy requirements (required to fix various Planemo functionality
  for CWL tools).

---------------------
18.5.10 (2018-05-10)
---------------------

* Docker logging API fix for Planemo.

---------------------
18.5.9 (2018-05-07)
---------------------

* Update CWL linting to target CWL 1.0.

---------------------
18.5.8 (2018-05-06)
---------------------

* Better error handling for Conda searching (thanks to @bgruening).
* Update against the latest Galaxy codebase.
* Add Galaxy tool linting to ensure versions are PEP 440 compliant (thanks to @davebx).

---------------------
18.5.7 (2018-03-12)
---------------------

* More tool testing client fixes, this time for ephemeris.

---------------------
18.5.6 (2018-03-12)
---------------------

* Bring in the latest Galaxy dev branch - includes code cleanup and many Python 3 fixes from
  @nsoranzo as well as client code for executing tool tests against external Galaxy instances.
* Extend tool testing client from Galaxy's dev branch with even more data collection for compatiblity
  with Planemo.

---------------------
18.5.5 (2018-03-06)
---------------------

* Fix mulled to use shlex.quote to escape single quotes in test command
  (thanks to @mbargull).
* Make markupsafe a dependency since it is import unconditionally in galaxy.tools.toolbox
  (thanks to @mbargull).
* Python 3 fix for assertion testing.

---------------------
18.5.4 (2018-03-01)
---------------------

* Make conda image for mulled builds configurable via an environment variable
  (thanks to @mbargull).

---------------------
18.5.3 (2018-02-28)
---------------------

* Fix path module for import on Windows for Pulsar.

---------------------
18.5.2 (2018-02-28)
---------------------

* Various fixes for library usage mostly related to Conda (with help from @nsoranzo).

---------------------
18.5.1 (2018-02-26)
---------------------

* Redo last release - pushed to PyPI without actually including the desired fix.

---------------------
18.5.0 (2018-02-26)
---------------------

* Another Python 3 fix for Planemo.
* Fix galaxy-lib version - this has actually been tracking the 18.05 release of Galaxy for the last two releases.

---------------------
18.1.0 (2018-02-26)
---------------------

* More Python 3 fixes for Planemo thanks to @nsoranzo.
* Bring in the latest Galaxy development branch.

---------------------
17.9.12 (2018-02-22)
---------------------

* Python 3 fix for Planemo thanks to @nsoranzo.
* Fix bad merge of miniconda update for mulled work.

---------------------
17.9.11 (2018-02-22)
---------------------

* Update to the latest Galaxy dev just prior to the branch of 18.01.
* Python 3 fixes.

---------------------
17.9.10 (2017-11-23)
---------------------

* Added docs for using mulled-build with your own quay.io account
  (thanks to @jerowe).
* Catch errors in Conda search if nothing is found (preventing planemo-monitor
  from functioning properly) (thanks to @bgruening).
* Make multi-requirement container building via mulled more stable
  (thanks to @bgruening).

---------------------
17.9.9 (2017-09-27)
---------------------

* Bring in latest updates from the 17.09 branch of Galaxy - including updating the default target Conda version and fixes for module resolution.

---------------------
17.9.8 (2017-09-26)
---------------------

* Bring in updated CWL utilities from the upstream work on CWL integration.

---------------------
17.9.7 (2017-09-19)
---------------------

* Bring in updated CWL utilities from the upstream work on CWL integration.

---------------------
17.9.6 (2017-09-15)
---------------------

* Remove ``command`` lint check that is no longer valid.

---------------------
17.9.5 (2017-09-06)
---------------------

* Bring in updated CWL utilities from the upstream work on CWL integration.

---------------------
17.9.4 (2017-09-06)
---------------------

* Bring in various Galaxy updates including numerous Conda fixes and changes (thanks to @nsoranzo).
* Improved error handling when parsing tool reStructuredText (thanks to @erasche).
* Updated CWL utilities.

---------------------
17.9.3 (2017-06-27)
---------------------

* Bug fix in from_dict parsing of tool dependency specs.

---------------------
17.9.2 (2017-06-22)
---------------------

* Sync with mulled enhancements and fixes from Galaxy's development branch.

---------------------
17.9.1 (2017-06-17)
---------------------

* Various small Singularity fixes and enhancements.

---------------------
17.9.0 (2017-06-11)
---------------------

* Bring in latest Galaxy dev changes.
* Implement support for building Singularity mulled containers.
* Implement mulled version 2 package hashing.
* Fix default namespace for mulled operations from mulled to biocontainers.

---------------------
17.5.11 (2017-05-31)
---------------------

* Fix HISTORY.rst formatting to properly render release on PyPI.
* Fix bug in new offline Conda search function introduced in 17.5.10.

---------------------
17.5.10 (2017-05-30)
---------------------

* Always clean up build directory in mulled commands (thanks to @johanneskoester).
* Expose offline mode in Conda search utility (for repeated fast searches).
* When publishing mulled containers - use quay.io API to publish them as public.
* Add explicit option ``--check-published`` to ``mulled-build-*``.
* Fix auto-installation of Involucro on first attempt.
* Handle explicit tags in ``mulled-build-files`` and add an implicit tag of ``0`` if none found.
* Fix tab parsing in ``mulled-build-files``.

---------------------
17.5.9 (2017-05-16)
---------------------

* Make mulled-search to search biocontainers instead of mulled repository by default
  (thanks to @tom-tan).
* Allow setting a new base image with mulled-build.

---------------------
17.5.8 (2017-04-23)
---------------------

* Fix mulled image cleanup. #55.

---------------------
17.5.7 (2017-03-15)
---------------------

* Updates to CWL library functionality for several months worth of CWL tool updates.
* Allow finding tools by a URI-like strings (e.g. ``file://``, ``http://``, ``dockstore://``).
* Bring in latest Galaxy updates.

---------------------
17.5.6 (2017-03-01)
---------------------

* Expanded options for mulled CLI tools and library functionality.
  Fixes #49.

---------------------
17.5.5 (2017-02-26)
---------------------

* Fix bug in 17.5.4 where under certain conditions conda-build would attempt to be setup
  with the conda --use-local flag - which is not allowed.

---------------------
17.5.4 (2017-02-26)
---------------------

* Fix local builds Conda support to reflect conda-build is required.
* Fix default target path for miniconda installs.

---------------------
17.5.3 (2017-02-24)
---------------------

* Update against the latest Galaxy dev branch changes.
* Update Conda utilities to allow using locally built packages.

---------------------
17.5.2 (2017-02-21)
---------------------

* Conda utility enhancements to fix a Planemo bug.

---------------------
17.5.1 (2017-02-21)
---------------------

* Various improvements to Galaxy tool linting.

---------------------
17.5.0 (2017-02-16)
---------------------

* Bring in the last of the Galaxy dev changes.
* Allow Conda installs to target global Conda config (for Planemo)

---------------------
17.1.2 (2017-01-23)
---------------------

* Bring in the last of the Galaxy dev changes before branch of release_17.01.
* Improvements to mulled testing thanks to @mvdbeek.

---------------------
17.1.1 (2016-12-14)
---------------------

* Revert changes to shell command execution in Galaxy that had unintended consequences for Planemo.    

---------------------
17.1.0 (2016-12-12)
---------------------

* Improved mulled logging thanks to @bgruening.
* Bring in the latest Galaxy dev changes.

---------------------
16.10.10 (2016-10-24)
---------------------

* Fix mulled package data fetching for Mac OS X (thanks to @dannon).

---------------------
16.10.9 (2016-10-21)
---------------------

* Small fixes including to reflect mulled name on quay.io changed to biocontainers.

---------------------
16.10.8 (2016-10-10)
---------------------

* More mulled enhancements and bug fixes thanks to @bgruening and @daler.

---------------------
16.10.7 (2016-10-08)
---------------------

* More mulled enhancements and bug fixes thanks to @bgruening.
* Fix bioconda support by adding conda-forge to list of default channels.

---------------------
16.10.6 (2016-10-07)
---------------------

* More mulled enhancements thanks to @bgruening.

---------------------
16.10.5 (2016-10-04)
---------------------

* Some docstring cleanup and minor tweaks to Conda support for downstream planemo mulled work.

---------------------
16.10.4 (2016-10-03)
---------------------

* More mulled fixes and enhancements.

---------------------
16.10.3 (2016-10-02)
---------------------

* Small mulled and Conda related fix and enhancements.

---------------------
16.10.2 (2016-09-30)
---------------------

* Fix setup.py for features in 16.10.1.

---------------------
16.10.1 (2016-09-29)
---------------------

* Updates for recents changes to Galaxy and initial mulled scripts and container resolver.

---------------------
16.10.0 (2016-08-31)
---------------------

* Updates for recent changes to Galaxy.

---------------------
16.7.10 (2016-08-04)
---------------------

* Updates for recent change to Galaxy.    

---------------------
16.7.9 (2016-06-13)
---------------------

* Updates for recent changes to Galaxy and cwltool.

---------------------
16.7.8 (2016-06-05)
---------------------

* Updates to include Galaxy library for verifying test outputs
  and the latest dev changes to Galaxy.

---------------------
16.7.7 (2016-05-23)
---------------------

* Fixes to CWL and Docker libraries for Planemo.

---------------------
16.7.6 (2016-05-11)
---------------------

* Fixes to cwl processing for Planemo.

---------------------
16.7.5 (2016-05-11)
---------------------

* Updates to cwl processing for Planemo.

---------------------
16.7.4 (2016-05-10)
---------------------

* Updates to cwl processing for Planemo.

---------------------
16.7.3 (2016-05-07)
---------------------

* Updates to cwltool_deps for Planemo.

---------------------
16.7.2 (2016-05-06)
---------------------

* Updates to tool parsing and linting for Planemo.

---------------------
16.7.1 (2016-05-02)
---------------------

* Update against the latest development branch of Galaxy.

---------------------
16.7.0 (2016-04-21)
---------------------

* Update against the latest development branch of Galaxy.

---------------------
16.4.1 (2016-04-08)
---------------------

* Update against the latest development branch of Galaxy.

---------------------
16.4.0 (2016-02-15)
---------------------

* Update against the latest development branch of Galaxy.

---------------------
16.1.9 (2016-01-14)
---------------------

* Fix a bug in the source distribution of galaxy-lib.

---------------------
16.1.8 (2016-01-12)
---------------------

* Update against Galaxy's release_16.01 branch.

---------------------
16.1.7 (2016-01-03)
---------------------

* Update against Galaxy's dev branch - including conda updates,
  dependency resolution changes, and toolbox updates.

---------------------
16.1.6 (2015-12-28)
---------------------

* Additional fixes to setup.py and updates for recent changes to
  Galaxy's dev branch.

---------------------
16.1.5 (2015-12-22)
---------------------

* Fix another bug that was preventing dependency resolution from
  working in Pulsar.

---------------------
16.1.4 (2015-12-22)
---------------------

* Another setup.py fix for job metrics module.

---------------------
16.1.3 (2015-12-22)
---------------------

* Python 3 fixes and updates for recent Galaxy dev commits.

---------------------
16.1.2 (2015-12-21)
---------------------

* Fix for missing galaxy.tools.parser package in setup.py.
* Fix LICENSE in repository.

---------------------
16.1.1 (2015-12-20)
---------------------

* Fix small issues with dependencies, naming, and versioning with first release.

---------------------
16.1.0 (2015-12-20)
---------------------

* Setup project.

.. _bioblend: https://github.com/galaxyproject/bioblend/
.. _XSD: http://www.w3schools.com/schema/
.. _lxml: http://lxml.de/
.. _xmllint: http://xmlsoft.org/xmllint.html
.. _nose: https://nose.readthedocs.org/en/latest/



  * [gast-0.2.2](https://github.com/serge-sans-paille/gast/) A generic AST to represent Python2 and Python3's Abstract Syntax Tree(AST).

GAST provides a compatibility layer between the AST of various Python versions,
as produced by ``ast.parse`` from the standard ``ast`` module.
  * [get_version-2.0.6](https://github.com/flying-sheep/get_version) get_version |b-pypi| |b-travis| |b-cover| |b-black|
===================================================
Automatically use the latest “vX.X.X” Git tag as version in your Python package.

It also supports getting the version from Python source distributions (``sdist``) or,
once your package is installed, via ``pkg_resources`` (part of ``setuptools``).

usage
-----
Add ``setuptools`` to the dependencies of your package, and the following into ``yourpackage.py`` (or ``__init__.py``):

.. code-block:: python

    from get_version import get_version
    __version__ = get_version(__file__)
    del get_version

contributing
------------
Use |black|_ to ensure proper code style. In order to not forget you can use |pre-commit|_.

.. |b-travis| image:: https://travis-ci.com/flying-sheep/get_version.svg?branch=master
   :target: https://travis-ci.com/flying-sheep/get_version
.. |b-cover| image:: https://coveralls.io/repos/github/flying-sheep/get_version/badge.svg
   :target: https://coveralls.io/github/flying-sheep/get_version
.. |b-pypi| image:: https://img.shields.io/pypi/v/get_version.svg
   :target: https://pypi.org/project/get_version
.. |b-black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black

.. |black| replace:: ``black .``
.. _black: https://black.readthedocs.io/en/stable/
.. |pre-commit| replace:: ``pre-commit install``
.. _pre-commit: https://pre-commit.com/

  * [gevent-1.4.0](http://www.gevent.org/) ========
 gevent
========

.. image:: https://travis-ci.org/gevent/gevent.svg?branch=master
   :target: https://travis-ci.org/gevent/gevent

.. image:: https://ci.appveyor.com/api/projects/status/q4kl21ng2yo2ixur?svg=true
   :target: https://ci.appveyor.com/project/denik/gevent

.. image:: https://coveralls.io/repos/gevent/gevent/badge.svg?branch=master&service=github
   :target: https://coveralls.io/github/gevent/gevent?branch=master

..
  This file is included in README.rst from the top-level
  so it is limited to pure ReST markup, not Sphinx.



gevent is a coroutine_ -based Python_ networking library that uses
`greenlet <https://greenlet.readthedocs.io>`_ to provide a high-level synchronous API on top of the `libev`_
or `libuv`_ event loop.

Features include:


* Fast event loop based on `libev`_ or `libuv`_.
* Lightweight execution units based on greenlets.
* API that re-uses concepts from the Python standard library (for
  examples there are `events`_ and
  `queues`_).
* `Cooperative sockets with SSL support <http://www.gevent.org/api/index.html#networking>`_
* `Cooperative DNS queries <http://www.gevent.org/dns.html>`_ performed through a threadpool,
  dnspython, or c-ares.
* `Monkey patching utility <http://www.gevent.org/intro.html#monkey-patching>`_ to get 3rd party modules to become cooperative
* TCP/UDP/HTTP servers
* Subprocess support (through `gevent.subprocess`_)
* Thread pools

gevent is `inspired by eventlet`_ but features a more consistent API,
simpler implementation and better performance. Read why others `use
gevent`_ and check out the list of the `open source projects based on
gevent`_.

gevent was written by `Denis Bilenko <http://denisbilenko.com/>`_.

Since version 1.1, gevent is maintained by Jason Madden for
`NextThought <https://nextthought.com>`_ with help from the
`contributors <https://github.com/gevent/gevent/graphs/contributors>`_
and is licensed under the MIT license.

See `what's new`_ in the latest major release.

Check out the detailed changelog_ for this version.

.. _events: http://www.gevent.org/api/gevent.event.html#gevent.event.Event
.. _queues: http://www.gevent.org/api/gevent.queue.html#gevent.queue.Queue
.. _gevent.subprocess: http://www.gevent.org/api/gevent.subprocess.html#module-gevent.subprocess

.. _coroutine: https://en.wikipedia.org/wiki/Coroutine
.. _Python: http://python.org
.. _libev: http://software.schmorp.de/pkg/libev.html
.. _libuv: http://libuv.org
.. _inspired by eventlet: http://blog.gevent.org/2010/02/27/why-gevent/
.. _use gevent: http://groups.google.com/group/gevent/browse_thread/thread/4de9703e5dca8271
.. _open source projects based on gevent: https://github.com/gevent/gevent/wiki/Projects
.. _what's new: http://www.gevent.org/whatsnew_1_3.html
.. _changelog: http://www.gevent.org/changelog.html


Read the documentation online at http://www.gevent.org.

Post feedback and issues on the `bug tracker`_, `mailing list`_, blog_
and `twitter (@gevent)`_.


===============================
 Installation and Requirements
===============================

.. _installation:

..
  This file is included in README.rst so it is limited to plain
  ReST markup, not Sphinx.

Supported Platforms
===================

`gevent 1.3`_ runs on Python 2.7 and Python 3. Releases 3.4, 3.5 and
3.6 of Python 3 are supported. (Users of older versions of Python 2
need to install gevent 1.0.x (2.5), 1.1.x (2.6) or 1.2.x (<=2.7.8);
gevent 1.2 can be installed on Python 3.3.) gevent requires the
`greenlet <https://greenlet.readthedocs.io>`_ library and will install
the `cffi`_ library by default on Windows.

gevent 1.3 also runs on PyPy 5.5 and above, although 5.9 or above is
strongly recommended. On PyPy, there are no external dependencies.

gevent is tested on Windows, OS X, and Linux, and should run on most
other Unix-like operating systems (e.g., FreeBSD, Solaris, etc.)

.. note:: On Windows using the libev backend, gevent is
          limited to a maximum of 1024 open sockets due to
          `limitations in libev`_. This limitation should not exist
          with the default libuv backend.

Installation
============

.. note::

   This section is about installing released versions of gevent
   as distributed on the `Python Package Index`_

.. _Python Package Index: http://pypi.org/project/gevent

gevent and greenlet can both be installed with `pip`_, e.g., ``pip
install gevent``. Installation using `buildout
<http://docs.buildout.org/en/latest/>`_ is also supported.

On Windows, OS X, and Linux, both gevent and greenlet are
distributed as binary `wheels`_.

.. tip::

   You need Pip 8.0 or later, or buildout 2.10.0 to install the
   binary wheels.

.. tip::

   On Linux, you'll need to install gevent from source if you wish to
   use the libuv loop implementation. This is because the `manylinux1
   <https://www.python.org/dev/peps/pep-0513/>`_ specification for the
   distributed wheels does not support libuv. The `cffi`_ library
   *must* be installed at build time.


Installing From Source
----------------------

If you are unable to use the binary wheels (for platforms where no
pre-built wheels are available or if wheel installation is disabled,
e.g., for libuv support on Linux), here are some things you need to know.

- You can install gevent from source with ``pip install --no-binary
  gevent gevent``.

- You'll need a working C compiler that can build Python extensions.
  On some platforms, you may need to install Python development
  packages.

- Installing from source requires ``setuptools``. This is installed
  automatically in virtual environments and by buildout. However,
  gevent uses :pep:`496` environment markers in ``setup.py``.
  Consequently, you'll need a version of setuptools newer than 25
  (mid 2016) to install gevent from source; a version that's too old
  will produce a ``ValueError``. Older versions of pipenv may also
  `have issues installing gevent for this reason
  <https://github.com/pypa/pipenv/issues/2113>`_.

- To build the libuv backend (which is required on Windows and
  optional elsewhere), or the CFFI-based libev backend, you must
  install `cffi`_ before attempting to install gevent on CPython (on
  PyPy this step is not necessary).


Common Installation Issues
--------------------------

The following are some common installation problems and solutions for
those compiling gevent from source.

- Some Linux distributions are now mounting their temporary
  directories with the ``noexec`` option. This can cause a standard
  ``pip install gevent`` to fail with an error like ``cannot run C
  compiled programs``. One fix is to mount the temporary directory
  without that option. Another may be to use the ``--build`` option to
  ``pip install`` to specify another directory. See `issue #570
  <https://github.com/gevent/gevent/issues/570>`_ and `issue #612
  <https://github.com/gevent/gevent/issues/612>`_ for examples.

- Also check for conflicts with environment variables like ``CFLAGS``. For
  example, see `Library Updates <http://www.gevent.org/whatsnew_1_1.html#library-updates-label>`_.

- Users of a recent SmartOS release may need to customize the
  ``CPPFLAGS`` (the environment variable containing the default
  options for the C preprocessor) if they are using the libev shipped
  with gevent. See `Operating Systems
  <http://www.gevent.org/whatsnew_1_1.html#operating-systems-label>`_
  for more information.

- If you see ``ValueError: ("Expected ',' or end-of-list in", "cffi >=
  1.11.5 ; sys_platform == 'win32' and platform_python_implementation
  == 'CPython'", 'at', " ; sys_platform == 'win32' and
  platform_python_implementation == 'CPython'")``, the version of
  setuptools is too old. Install a more recent version of setuptools.


Extra Dependencies
==================

gevent has no runtime dependencies outside the standard library,
greenlet and (on some platforms) `cffi`_. However, there are a
number of additional libraries that extend gevent's functionality and
will be used if they are available.

The `psutil <https://pypi.org/project/psutil>`_ library is needed to
monitor memory usage.

`zope.event <https://pypi.org/project/zope.event>`_ is highly
recommended for configurable event support; it can be installed with
the ``events`` extra, e.g., ``pip install gevent[events]``.

`dnspython <https://pypi.org/project/dnspython>`_ is required for the
new pure-Python resolver, and on Python 2, so is `idna
<https://pypi.org/project/idna>`_. They can be installed with the
``dnspython`` extra.


Development
===========

To install the latest development version::

  pip install setuptools cffi 'cython>=0.28' git+git://github.com/gevent/gevent.git#egg=gevent

.. note::

   You will not be able to run gevent's test suite using that method.

To hack on gevent (using a virtualenv)::

  $ git clone https://github.com/gevent/gevent.git
  $ cd gevent
  $ virtualenv env
  $ source env/bin/activate
  (env) $ pip install -r dev-requirements.txt

.. note::

   The notes above about installing from source apply here as well.
   The ``dev-requirements.txt`` file takes care of the library
   prerequisites (CFFI, Cython), but having a working C compiler that
   can create Python extensions is up to you.


Running Tests
-------------

There are a few different ways to run the tests. To simply run the
tests on one version of Python during development, begin with the
above instructions to install gevent in a virtual environment and then
run::

  (env) $ python -mgevent.tests

Before submitting a pull request, it's a good idea to run the tests
across all supported versions of Python, and to check the code quality
using prospector. This is what is done on Travis CI. Locally it
can be done using tox::

  pip install tox
  tox

The testrunner accepts a ``--coverage`` argument to enable code
coverage metrics through the `coverage.py`_ package. That would go
something like this::

  python -m gevent.tests --coverage
  coverage combine
  coverage html -i
  <open htmlcov/index.html>

Continuous integration
----------------------

A test suite is run for every push and pull request submitted. Travis
CI is used to test on Linux, and `AppVeyor`_ runs the builds on
Windows.

.. image:: https://travis-ci.org/gevent/gevent.svg?branch=master
   :target: https://travis-ci.org/gevent/gevent

.. image:: https://ci.appveyor.com/api/projects/status/q4kl21ng2yo2ixur?svg=true
   :target: https://ci.appveyor.com/project/denik/gevent


Builds on Travis CI automatically submit updates to `coveralls.io`_ to
monitor test coverage.

.. image:: https://coveralls.io/repos/gevent/gevent/badge.svg?branch=master&service=github
   :target: https://coveralls.io/github/gevent/gevent?branch=master

.. note:: On Debian, you will probably need ``libpythonX.Y-testsuite``
          installed to run all the tests.

.. _coverage.py: https://pypi.python.org/pypi/coverage/
.. _coveralls.io: https://coveralls.io/github/gevent/gevent
.. _`pip`: https://pip.pypa.io/en/stable/installing/
.. _`wheels`: http://pythonwheels.com
.. _`gevent 1.3`: whatsnew_1_3.html

.. _`cffi`: https://cffi.readthedocs.io
.. _`limitations in libev`: http://pod.tst.eu/http://cvs.schmorp.de/libev/ev.pod#WIN32_PLATFORM_LIMITATIONS_AND_WORKA
.. _AppVeyor: https://ci.appveyor.com/project/denik/gevent


.. _bug tracker: https://github.com/gevent/gevent/wiki/Projects
.. _mailing list: http://groups.google.com/group/gevent
.. _blog: http://blog.gevent.org
.. _twitter (@gevent): http://twitter.com/gevent



  * [gitdb2-2.0.5](https://github.com/gitpython-developers/gitdb) GitDB is a pure-Python git object database



  * [gnureadline-8.0.0](http://github.com/ludwigschwardt/python-gnureadline) Stand-alone GNU readline module
===============================

First... STOP
-------------

Consider this: do you really need this package in 2019? You typically don't if

- you use the Python provided by a standard Linux distribution like Ubuntu
  Debian, CentOS, etc. *(It already uses the proper readline.)*
- you run **Windows**
  *(It won't work! Try* `pyreadline`_ *instead.)*
- you use the Python provided by **Homebrew** or Fink on macOS
  *(It has real readline already!)*
- you want it for `IPython`_
  *(It switched to* `prompt_toolkit`_ *in version 5.0.)*
- you use a Python distribution like Anaconda or Enthought / Canopy
  *(Again, real readline.)*

You might need it if

- you use Python provided by MacPorts or the system on macOS
  *(Python compiled against libedit.)*
- you use a Python distribution like ActivePython on Linux or macOS
  *(This used to ship without readline.)*
- you want to get the latest bug fixes and features in either the readline
  library or its Python module *(Typically when stuck on older systems.)*

Still interested?
-----------------

Some platforms, such as macOS, do not ship with `GNU readline`_ installed.
The readline extension module in the standard library of Mac "system" Python
uses NetBSD's `editline`_ (libedit) library instead, which is a readline
replacement with a less restrictive software license.

As the alternatives to GNU readline do not have fully equivalent functionality,
it is useful to add proper readline support to these platforms. This module
achieves this by bundling the standard Python readline module with the GNU
readline source code, which is compiled and statically linked to it. The end
result is a package which is simple to install and requires no extra shared
libraries.

The module is called *gnureadline* so as not to clash with the readline module
in the standard library. This keeps polite installers such as `pip`_ happy and
is sufficient for shells such as `IPython`_. **Please take note that IPython
does not depend on gnureadline anymore since version 5.0 as it now uses**
`prompt_toolkit`_ **instead**.

In order to use this module in the standard Python shell it has to be installed
with the more impolite easy_install from `setuptools`_. **It is recommended that
you use pip >= 8.0 together with setuptools >= 0.8 to install gnureadline.**
This will download a binary wheel from PyPI if available, thereby bypassing the
need for compilation and its slew of potential problems (especially on macOS).

The module can be used with both Python 2.x and 3.x, and has been tested with
Python versions 2.6, 2.7, and 3.2 to 3.8. The first three numbers of the module
version reflect the version of the underlying GNU readline library (major,
minor and patch level), while any additional fourth number distinguishes
different module updates based on the same readline library.

This module is usually unnecessary on Linux and other Unix systems with default
readline support. An exception is if you have a Python distribution that does
not include GNU readline due to licensing restrictions (such as ActiveState's
`ActivePython`_). If you are using Windows, which also ships without GNU
readline, you might want to consider using the `pyreadline`_ module instead,
which is a readline replacement written in pure Python that interacts with the
Windows clipboard.

The latest development version is available from the `GitHub repository`_.

.. _GNU readline: http://www.gnu.org/software/readline/
.. _editline: http://www.thrysoee.dk/editline/
.. _pip: http://www.pip-installer.org/
.. _IPython: http://ipython.org/
.. _prompt_toolkit: http://python-prompt-toolkit.readthedocs.io/en/stable/
.. _setuptools: https://pypi.python.org/pypi/setuptools
.. _ActivePython: http://community.activestate.com/faq/why-doesnt-activepython-u
.. _pyreadline: http://pypi.python.org/pypi/pyreadline
.. _GitHub repository: http://github.com/ludwigschwardt/python-gnureadline


History
=======

8.0.0 (2019-07-10)
------------------

* Uses Python 3.7.3 readline.c (commit ef10f88, 2019-03-20), also OK for 3.8
* Uses Python 3.6.8 readline.c (commit 25555e0, 2018-12-08)
* Uses Python 3.4.4 readline.c (commit 7462b64, 2015-11-02)
* Uses Python 2.7.16 readline.c (commit 89b5ea2, 2018-12-19)
* Updated to build against readline 8.0

6.3.8 (2017-10-20)
------------------

* #42, #44: Address compiler issues (avoid Cygwin, fix multi-arch on gcc)
* #40: Make GPLv3 license explicit
* #39: Look for bash shell in more places
* Uses Python 2.x readline.c from hg 2.7 branch (95814:192f9efe4a38)
* Uses Python 3.x readline.c from hg 3.4 / 3.5 branch (95813:ec6ed10d611e)
* Updated to build against readline 6.3 (patch-level 8)

6.3.3 (2014-04-08)
------------------

* Major rework of OS X build process (detect arches, no custom flags)
* #20, #22, #28: Various issues addressed by new streamlined build
* #28: Use $CC or cc to compile libreadline instead of default gcc
* #35: Workaround for clang from Xcode 5.1 and Mac OS X 10.9.2
* Uses Python 3.4 readline.c from hg 3.4 branch (89086:3110fb3095a2)
* Updated to build against readline 6.3 (patch-level 3)

6.2.5 (2014-02-19)
------------------

* Renamed module to *gnureadline* to improve installation with pip
* #23, #25-27, #29-33: Tweaks and package reworked to gnureadline
* Uses Python 2.x readline.c from hg 2.7 branch (89084:6b10943a5916)
* Uses Python 3.x readline.c from hg 3.3 branch (89085:6adac0d9b933)
* Updated to build against readline 6.2 (patch-level 5)

6.2.4.1 (2012-10-22)
--------------------

* #21: Fixed building on Python.org 3.3 / Mac OS 10.8

6.2.4 (2012-10-17)
------------------

* #15: Improved detection of compilers before Xcode 4.3
* Uses Python 3.x readline.c from v3.3.0 tag (changeset 73997)
* Updated to build against readline 6.2 (patch-level 4)

6.2.2 (2012-02-24)
------------------

* #14: Fixed compilation with Xcode 4.3 on Mac OS 10.7
* Updated to build against readline 6.2 (patch-level 2)

6.2.1 (2011-08-31)
------------------

* #10: Fixed '_emacs_meta_keymap' missing symbol on Mac OS 10.7
* #7: Fixed SDK version check to work with Mac OS 10.7 and later
* Uses Python 2.x readline.c from release27-maint branch (r87358)
* Uses Python 3.x readline.c from release32-maint branch (r88446)

6.2.0 (2011-06-02)
------------------

* #5: Removed '-arch ppc' on Mac OS 10.6, as Snow Leopard supports Intel only
* Updated to build against readline 6.2 (patch-level 1)

6.1.0 (2010-09-20)
------------------

* Changed version number to reflect readline version instead of Python version
* #4: Updated to build against readline 6.1 (patch-level 2)
* #2: Python 3 support
* Uses Python 2.x readline.c from release27-maint branch (r83672)
* Uses Python 3.x readline.c from r32a2 tag (r84541)
* Source code moved to GitHub
* Additional maintainer: Sridhar Ratnakumar

2.6.4 (2009-11-26)
------------------

* Added -fPIC to compiler flags to fix linking error on 64-bit Ubuntu
* Enabled all readline functionality specified in pyconfig.h macros
* Uses readline.c from Python svn trunk (r75725), which followed 2.6.4 release
* Patched readline.c to replace Py_XDECREF calls with the safer Py_CLEAR
* Fixed compilation error on Mac OS 10.4 with XCode older than version 2.4

2.6.1 (2009-11-18)
------------------

* Updated package to work with Mac OS 10.6 (Snow Leopard), which ships with
  Python 2.6.1
* Uses readline.c from Python 2.6.1 release
* Backported "spurious trailing space" bugfix from Python svn trunk (see e.g.
  https://bugs.launchpad.net/python/+bug/470824 for details on bug)
* Updated to build against readline 6.0 (patch-level 4)
* Now builds successfully on Linux (removed Mac-specific flags in this case),
  and still supports Mac OS 10.4 and 10.5

2.5.1 (2008-05-28)
------------------

* Updated package to work with Mac OS 10.5 (Leopard), which ships with Python
  2.5.1
* Uses readline.c from Python 2.5.1 release
* Updated to build against readline 5.2 (patch-level 12)
* New maintainer: Ludwig Schwardt

2.4.2 (2005-12-26)
------------------

* Original package by Bob Ippolito, supporting Python 2.3 / 2.4 on Mac OS 10.3
  (Panther) and 10.4 (Tiger)
* Builds against readline 5.1
  * [google-2.0.2](http://breakingcode.wordpress.com/) googlesearch
============

Google search from Python.

https://python-googlesearch.readthedocs.io/en/latest/

Usage example
-------------

    # Get the first 20 hits for: "Breaking Code" WordPress blog
    from googlesearch import search
    for url in search('"Breaking Code" WordPress blog', stop=20):
        print(url)

Installing
----------

    pip install google

  * [google-pasta-0.1.7]() 
  * [graphviz-0.11.1](https://github.com/xflr6/graphviz) Graphviz
========

|PyPI version| |License| |Supported Python| |Format| |Docs|

|Travis| |Codecov|

This package facilitates the creation and rendering of graph descriptions in
the DOT_ language of the Graphviz_ graph drawing software (`master repo`_) from
Python.

Create a graph object, assemble the graph by adding nodes and edges, and
retrieve its DOT source code string. Save the source code to a file and render
it with the Graphviz installation of your system.

Use the ``view`` option/method to directly inspect the resulting (PDF, PNG,
SVG, etc.) file with its default application. Graphs can also be rendered
and displayed within `Jupyter notebooks`_ (formerly known as
`IPython notebooks`_, example_) as well as the `Jupyter Qt Console`_.


Links
-----

- GitHub: https://github.com/xflr6/graphviz
- PyPI: https://pypi.org/project/graphviz/
- Documentation: https://graphviz.readthedocs.io
- Changelog: https://graphviz.readthedocs.io/en/latest/changelog.html
- Issue Tracker: https://github.com/xflr6/graphviz/issues
- Download: https://pypi.org/project/graphviz/#files


Installation
------------

This package runs under Python 2.7, and 3.5+, use pip_ to install:

.. code:: bash

    $ pip install graphviz

To render the generated DOT source code, you also need to install Graphviz
(`download page`_).

Make sure that the directory containing the ``dot`` executable is on your
systems' path.


Quickstart
----------

Create a graph object:

.. code:: python

    >>> from graphviz import Digraph

    >>> dot = Digraph(comment='The Round Table')

    >>> dot  #doctest: +ELLIPSIS
    <graphviz.dot.Digraph object at 0x...>

Add nodes and edges:

.. code:: python

    >>> dot.node('A', 'King Arthur')
    >>> dot.node('B', 'Sir Bedevere the Wise')
    >>> dot.node('L', 'Sir Lancelot the Brave')

    >>> dot.edges(['AB', 'AL'])
    >>> dot.edge('B', 'L', constraint='false')

Check the generated source code:

.. code:: python

    >>> print(dot.source)  # doctest: +NORMALIZE_WHITESPACE
    // The Round Table
    digraph {
        A [label="King Arthur"]
        B [label="Sir Bedevere the Wise"]
        L [label="Sir Lancelot the Brave"]
        A -> B
        A -> L
        B -> L [constraint=false]
    }

Save and render the source code, optionally view the result:

.. code:: python

    >>> dot.render('test-output/round-table.gv', view=True)  # doctest: +SKIP
    'test-output/round-table.gv.pdf'

.. image:: https://raw.github.com/xflr6/graphviz/master/docs/round-table.png
    :align: center


See also
--------

- pygraphviz_ |--| full-blown interface wrapping the Graphviz C library with SWIG
- graphviz-python_ |--| official Python bindings (documentation_)
- pydot_ |--| stable pure-Python approach, requires pyparsing


License
-------

This package is distributed under the `MIT license`_.


.. _pip: https://pip.readthedocs.io
.. _Graphviz:  https://www.graphviz.org
.. _master repo: https://gitlab.com/graphviz/graphviz/
.. _download page: https://www.graphviz.org/download/
.. _DOT: https://www.graphviz.org/doc/info/lang.html
.. _Jupyter notebooks: https://jupyter.org
.. _IPython notebooks: https://ipython.org/notebook.html
.. _example: https://nbviewer.jupyter.org/github/xflr6/graphviz/blob/master/examples/notebook.ipynb
.. _Jupyter Qt Console: https://qtconsole.readthedocs.io

.. _pygraphviz: https://pypi.org/project/pygraphviz/
.. _graphviz-python: https://pypi.org/project/graphviz-python/
.. _documentation: https://www.graphviz.org/pdf/gv.3python.pdf
.. _pydot: https://pypi.org/project/pydot/

.. _MIT license: https://opensource.org/licenses/MIT


.. |--| unicode:: U+2013


.. |PyPI version| image:: https://img.shields.io/pypi/v/graphviz.svg
    :target: https://pypi.org/project/graphviz/
    :alt: Latest PyPI Version
.. |License| image:: https://img.shields.io/pypi/l/graphviz.svg
    :target: https://pypi.org/project/graphviz/
    :alt: License
.. |Supported Python| image:: https://img.shields.io/pypi/pyversions/graphviz.svg
    :target: https://pypi.org/project/graphviz/
    :alt: Supported Python Versions
.. |Format| image:: https://img.shields.io/pypi/format/graphviz.svg
    :target: https://pypi.org/project/graphviz/
    :alt: Format
.. |Docs| image:: https://readthedocs.org/projects/graphviz/badge/?version=stable
    :target: https://graphviz.readthedocs.io/en/stable/
    :alt: Readthedocs
.. |Travis| image:: https://img.shields.io/travis/xflr6/graphviz.svg
    :target: https://travis-ci.org/xflr6/graphviz
    :alt: Travis
.. |Codecov| image:: https://codecov.io/gh/xflr6/graphviz/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/xflr6/graphviz
    :alt: Codecov



  * [greenlet-0.4.15](https://github.com/python-greenlet/greenlet) .. image:: https://secure.travis-ci.org/python-greenlet/greenlet.png
   :target: http://travis-ci.org/python-greenlet/greenlet

The greenlet package is a spin-off of Stackless, a version of CPython
that supports micro-threads called "tasklets". Tasklets run
pseudo-concurrently (typically in a single or a few OS-level threads)
and are synchronized with data exchanges on "channels".

A "greenlet", on the other hand, is a still more primitive notion of
micro-thread with no implicit scheduling; coroutines, in other
words. This is useful when you want to control exactly when your code
runs. You can build custom scheduled micro-threads on top of greenlet;
however, it seems that greenlets are useful on their own as a way to
make advanced control flow structures. For example, we can recreate
generators; the difference with Python's own generators is that our
generators can call nested functions and the nested functions can
yield values too. Additionally, you don't need a "yield" keyword. See
the example in tests/test_generator.py.

Greenlets are provided as a C extension module for the regular
unmodified interpreter.

Greenlets are lightweight coroutines for in-process concurrent
programming.

Who is using Greenlet?
======================

There are several libraries that use Greenlet as a more flexible
alternative to Python's built in coroutine support:

 - `Concurrence`_
 - `Eventlet`_
 - `Gevent`_

.. _Concurrence: http://opensource.hyves.org/concurrence/
.. _Eventlet: http://eventlet.net/
.. _Gevent: http://www.gevent.org/

Getting Greenlet
================

The easiest way to get Greenlet is to install it with pip or
easy_install::

  pip install greenlet
  easy_install greenlet


Source code archives and windows installers are available on the
python package index at https://pypi.python.org/pypi/greenlet

The source code repository is hosted on github:
https://github.com/python-greenlet/greenlet

Documentation is available on readthedocs.org:
https://greenlet.readthedocs.io
  * [grpcio-1.22.0](https://grpc.io) gRPC Python
===========

|compat_check_pypi|

Package for gRPC Python.

.. |compat_check_pypi| image:: https://python-compatibility-tools.appspot.com/one_badge_image?package=grpcio
   :target: https://python-compatibility-tools.appspot.com/one_badge_target?package=grpcio

Supported Python Versions
-------------------------
Python >= 3.5

Deprecated Python Versions
--------------------------
Python == 2.7. Python 2.7 support will be removed on January 1, 2020.

Installation
------------

gRPC Python is available for Linux, macOS, and Windows.

Installing From PyPI
~~~~~~~~~~~~~~~~~~~~

If you are installing locally...

::

  $ pip install grpcio

Else system wide (on Ubuntu)...

::

  $ sudo pip install grpcio

If you're on Windows make sure that you installed the :code:`pip.exe` component
when you installed Python (if not go back and install it!) then invoke:

::

  $ pip.exe install grpcio

Windows users may need to invoke :code:`pip.exe` from a command line ran as
administrator.

n.b. On Windows and on Mac OS X one *must* have a recent release of :code:`pip`
to retrieve the proper wheel from PyPI. Be sure to upgrade to the latest
version!

Installing From Source
~~~~~~~~~~~~~~~~~~~~~~

Building from source requires that you have the Python headers (usually a
package named :code:`python-dev`).

::

  $ export REPO_ROOT=grpc  # REPO_ROOT can be any directory of your choice
  $ git clone -b $(curl -L https://grpc.io/release) https://github.com/grpc/grpc $REPO_ROOT
  $ cd $REPO_ROOT
  $ git submodule update --init

  # For the next two commands do `sudo pip install` if you get permission-denied errors
  $ pip install -rrequirements.txt
  $ GRPC_PYTHON_BUILD_WITH_CYTHON=1 pip install .

You cannot currently install Python from source on Windows. Things might work
out for you in MSYS2 (follow the Linux instructions), but it isn't officially
supported at the moment.

Troubleshooting
~~~~~~~~~~~~~~~

Help, I ...

* **... see a** :code:`pkg_resources.VersionConflict` **when I try to install
  grpc**

  This is likely because :code:`pip` doesn't own the offending dependency,
  which in turn is likely because your operating system's package manager owns
  it. You'll need to force the installation of the dependency:

  :code:`pip install --ignore-installed $OFFENDING_DEPENDENCY`

  For example, if you get an error like the following:

  ::

    Traceback (most recent call last):
    File "<string>", line 17, in <module>
     ...
    File "/usr/lib/python2.7/dist-packages/pkg_resources.py", line 509, in find
      raise VersionConflict(dist, req)
    pkg_resources.VersionConflict: (six 1.8.0 (/usr/lib/python2.7/dist-packages), Requirement.parse('six>=1.10'))

  You can fix it by doing:

  ::

    sudo pip install --ignore-installed six

* **... see the following error on some platforms**

  ::

    /tmp/pip-build-U8pSsr/cython/Cython/Plex/Scanners.c:4:20: fatal error: Python.h: No such file or directory
    #include "Python.h"
                    ^
    compilation terminated.

  You can fix it by installing `python-dev` package. i.e

  ::

    sudo apt-get install python-dev




  * [gtfparse-1.2.0](https://github.com/openvax/gtfparse) |Build Status| |Coverage Status|

gtfparse
========

Parsing tools for GTF (gene transfer format) files.

Example usage
=============

Parsing all rows of a GTF file into a Pandas DataFrame
------------------------------------------------------

.. code:: python

   from gtfparse import read_gtf

   # returns GTF with essential columns such as "feature", "seqname", "start", "end"
   # alongside the names of any optional keys which appeared in the attribute column
   df = read_gtf("gene_annotations.gtf")

   # filter DataFrame to gene entries on chrY
   df_genes = df[df["feature"] == "gene"]
   df_genes_chrY = df_genes[df_genes["seqname"] == "Y"]

Getting gene FPKM values from a StringTie GTF file
--------------------------------------------------

.. code:: python

   from gtfparse import read_gtf

   df = read_gtf(
       "stringtie-output.gtf",
       column_converters={"FPKM": float})

   gene_fpkms = {
       gene_name: fpkm
       for (gene_name, fpkm, feature)
       in zip(df["gene_name"], df["FPKM"], df["feature"])
       if feature == "gene"
   }

.. |Build Status| image:: https://travis-ci.org/openvax/gtfparse.svg?branch=master
   :target: https://travis-ci.org/openvax/gtfparse
.. |Coverage Status| image:: https://coveralls.io/repos/openvax/gtfparse/badge.svg?branch=master&service=github
   :target: https://coveralls.io/github/openvax/gtfparse?branch=master
  * [h5py-2.9.0](http://www.h5py.org) The h5py package provides both a high- and low-level interface to the HDF5
library from Python. The low-level interface is intended to be a complete
wrapping of the HDF5 API, while the high-level component supports  access to
HDF5 files, datasets and groups using established Python and NumPy concepts.

A strong emphasis on automatic conversion between Python (Numpy) datatypes and
data structures and their HDF5 equivalents vastly simplifies the process of
reading and writing data from Python.

Supports HDF5 versions 1.8.4 and higher.  On Windows, HDF5 is included with
the installer.
  * [hacking-1.1.0](https://docs.openstack.org/hacking/latest/) Introduction
============

hacking is a set of flake8 plugins that test and enforce the :ref:`StyleGuide`.

Hacking pins its dependencies, as a new release of some dependency can break
hacking based gating jobs. This is because new versions of dependencies can
introduce new rules, or make existing rules stricter.

Installation
============

hacking is available from pypi, so just run::

  pip install hacking

This will install specific versions of ``flake8`` with the ``hacking``,
``pep8``, ``mccabe`` and ``pyflakes`` plugins.

Origin
======

Hacking started its life out as a text file in Nova's first commit. It was
initially based on the `Google Python Style Guide`_, and over time more
OpenStack specific rules were added. Hacking serves several purposes:

1. Agree on a common style guide so reviews don't get bogged down on style
   nit picks. (example: docstring guidelines)
2. Make code written by many different authors easier to read by making the
   style more uniform. (example: unix vs windows newlines)
3. Call out dangerous patterns and avoid them. (example: shadowing built-in
   or reserved words)

Initially the hacking style guide was enforced manually by reviewers, but this
was a big waste of time so hacking, the tool, was born to automate
the process and remove the extra burden from human reviewers.

.. _`Google Python Style Guide`: https://google.github.io/styleguide/pyguide.html

Versioning
==========

hacking uses the ``major.minor.maintenance`` release notation, where maintenance
releases cannot contain new checks.  This way projects can gate on hacking
by pinning on the ``major.minor`` number while accepting maintenance updates
without being concerned that a new version will break the gate with a new
check.

For example a project can depend on ``hacking>=0.10.0,<0.11.0``, and can know
that ``0.10.1`` will not fail in places where ``0.10.0`` passed.


Adding additional checks
========================

Each check is a pep8 plugin so read

- https://github.com/jcrocholl/pep8/blob/master/docs/developer.rst#contribute

The focus of new or changed rules should be to do one of the following

- Substantially increase the reviewability of the code (eg: H301, H303)
  as they make it easy to understand where symbols come from)
- Catch a common programming error that may arise in the future (H201)
- Prevent a situation that would 100% of the time be -1ed by
  developers (H903)

But, as always, remember that these are Guidelines. Treat them as
such. There are always times for exceptions. All new rules should
support noqa.

If a check needs to be staged in, or it does not apply to every project or its
branch, it can be added as off by default.

Requirements
------------
- The check must already have community support. We do not want to dictate
  style, only enforce it.
- The canonical source of the OpenStack Style Guidelines is :ref:`StyleGuide`,
  and hacking just enforces
  them; so when adding a new check, it must be in ``HACKING.rst``
- False negatives are ok, but false positives are not
- Cannot be project specific, project specific checks should be `Local Checks`_
- Include extensive tests
- Registered as entry_points in ``setup.cfg``
- Error code must be in the relevant ``Hxxx`` group
- The check should not attempt to import modules from the code being checked.
  Importing random modules, has caused all kinds of trouble for us in the past.


Enabling off-by-default checks
==============================

Some of the available checks are disabled by default. These checks are:

- [H106] Don't put vim configuration in source files.
- [H203] Use assertIs(Not)None to check for None.
- [H204] Use assert(Not)Equal to check for equality.
- [H205] Use assert(Greater|Less)(Equal) for comparison.
- [H210] Require 'autospec', 'spec', or 'spec_set' in
  mock.patch/mock.patch.object calls
- [H904] Delay string interpolations at logging calls.

To enable these checks, edit the ``flake8`` section of the ``tox.ini`` file.
For example to enable H106 and H203:

.. code-block:: ini

  [flake8]
  enable-extensions = H106,H203

Local Checks
============

hacking supports having local changes in a source tree. They can be configured
to run in two different ways. They can be registered individually, or with
a factory function.

For individual registration, put a comma separated list of pep8 compatible
check functions into the hacking section of tox.ini. E.g.:

.. code-block:: ini

  [hacking]
  local-check = nova.tests.hacking.bad_code_is_terrible

Alternately, you can specify the location of a callable that will be called
at registration time and will be passed the registration function. The callable
should expect to call the passed in function on everything if wants to
register. Such as:

.. code-block:: ini

  [hacking]
  local-check-factory = nova.tests.hacking.factory




  * [hdmedians-0.13](http://github.com/daleroberts/hdmedians) 
  * [hiredis-1.0.0](https://github.com/redis/hiredis-py) # hiredis-py

[![Build Status](https://travis-ci.org/redis/hiredis-py.svg?branch=master)](https://travis-ci.org/redis/hiredis-py)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/muso9gbe316tjsac/branch/master?svg=true)](https://ci.appveyor.com/project/duyue/hiredis-py/)

Python extension that wraps protocol parsing code in [hiredis][hiredis].
It primarily speeds up parsing of multi bulk replies.

[hiredis]: http://github.com/redis/hiredis

## Install

hiredis-py is available on [PyPI](https://pypi.org/project/hiredis/), and can
be installed with:

```
pip install hiredis
```

### Requirements

hiredis-py requires **Python 2.7 or 3.4+**.

Make sure Python development headers are available when installing hiredis-py.
On Ubuntu/Debian systems, install them with `apt-get install python-dev` for Python 2
or `apt-get install python3-dev` for Python 3.

## Usage

The `hiredis` module contains the `Reader` class. This class is responsible for
parsing replies from the stream of data that is read from a Redis connection.
It does not contain functionality to handle I/O.

### Reply parser

The `Reader` class has two methods that are used when parsing replies from a
stream of data. `Reader.feed` takes a string argument that is appended to the
internal buffer. `Reader.gets` reads this buffer and returns a reply when the
buffer contains a full reply. If a single call to `feed` contains multiple
replies, `gets` should be called multiple times to extract all replies.

Example:

```python
>>> reader = hiredis.Reader()
>>> reader.feed("$5\r\nhello\r\n")
>>> reader.gets()
'hello'
```

When the buffer does not contain a full reply, `gets` returns `False`. This
means extra data is needed and `feed` should be called again before calling
`gets` again:

```python
>>> reader.feed("*2\r\n$5\r\nhello\r\n")
>>> reader.gets()
False
>>> reader.feed("$5\r\nworld\r\n")
>>> reader.gets()
['hello', 'world']
```

#### Unicode

`hiredis.Reader` is able to decode bulk data to any encoding Python supports.
To do so, specify the encoding you want to use for decoding replies when
initializing it:

```python
>>> reader = hiredis.Reader(encoding="utf-8", errors="strict")
>>> reader.feed("$3\r\n\xe2\x98\x83\r\n")
>>> reader.gets()
u'☃'
```

Decoding of bulk data will be attempted using the specified encoding and
error handler. If the error handler is `'strict'` (the default), a
`UnicodeDecodeError` is raised when data cannot be dedcoded. This is identical
to Python's default behavior. Other valid values to `errors` include
`'replace'`, `'ignore'`, and `'backslashreplace'`. More information on the
behavior of these error handlers can be found
[here](https://docs.python.org/3/howto/unicode.html#the-string-type).


When the specified encoding cannot be found, a `LookupError` will be raised
when calling `gets` for the first reply with bulk data.

#### Error handling

When a protocol error occurs (because of multiple threads using the same
socket, or some other condition that causes a corrupt stream), the error
`hiredis.ProtocolError` is raised. Because the buffer is read in a lazy
fashion, it will only be raised when `gets` is called and the first reply in
the buffer contains an error. There is no way to recover from a faulty protocol
state, so when this happens, the I/O code feeding data to `Reader` should
probably reconnect.

Redis can reply with error replies (`-ERR ...`). For these replies, the custom
error class `hiredis.ReplyError` is returned, **but not raised**.

When other error types should be used (so existing code doesn't have to change
its `except` clauses), `Reader` can be initialized with the `protocolError` and
`replyError` keywords. These keywords should contain a *class* that is a
subclass of `Exception`. When not provided, `Reader` will use the default
error types.

## Benchmarks

The repository contains a benchmarking script in the `benchmark` directory,
which uses [gevent](http://gevent.org/) to have non-blocking I/O and redis-py
to handle connections. These benchmarks are done with a patched version of
redis-py that uses hiredis-py when it is available.

All benchmarks are done with 10 concurrent connections.

* SET key value + GET key
  * redis-py: 11.76 Kops
  * redis-py *with* hiredis-py: 13.40 Kops
  * improvement: **1.1x**

List entries in the following tests are 5 bytes.

* LRANGE list 0 **9**:
  * redis-py: 4.78 Kops
  * redis-py *with* hiredis-py: 12.94 Kops
  * improvement: **2.7x**
* LRANGE list 0 **99**:
  * redis-py: 0.73 Kops
  * redis-py *with* hiredis-py: 11.90 Kops
  * improvement: **16.3x**
* LRANGE list 0 **999**:
  * redis-py: 0.07 Kops
  * redis-py *with* hiredis-py: 5.83 Kops
  * improvement: **83.2x**

Throughput improvement for simple SET/GET is minimal, but the larger multi bulk replies
get, the larger the performance improvement is.

## License

This code is released under the BSD license, after the license of hiredis.



  * [html5lib-1.0.1](https://github.com/html5lib/html5lib-python) html5lib
========

.. image:: https://travis-ci.org/html5lib/html5lib-python.png?branch=master
  :target: https://travis-ci.org/html5lib/html5lib-python

html5lib is a pure-python library for parsing HTML. It is designed to
conform to the WHATWG HTML specification, as is implemented by all major
web browsers.


Usage
-----

Simple usage follows this pattern:

.. code-block:: python

  import html5lib
  with open("mydocument.html", "rb") as f:
      document = html5lib.parse(f)

or:

.. code-block:: python

  import html5lib
  document = html5lib.parse("<p>Hello World!")

By default, the ``document`` will be an ``xml.etree`` element instance.
Whenever possible, html5lib chooses the accelerated ``ElementTree``
implementation (i.e. ``xml.etree.cElementTree`` on Python 2.x).

Two other tree types are supported: ``xml.dom.minidom`` and
``lxml.etree``. To use an alternative format, specify the name of
a treebuilder:

.. code-block:: python

  import html5lib
  with open("mydocument.html", "rb") as f:
      lxml_etree_document = html5lib.parse(f, treebuilder="lxml")

When using with ``urllib2`` (Python 2), the charset from HTTP should be
pass into html5lib as follows:

.. code-block:: python

  from contextlib import closing
  from urllib2 import urlopen
  import html5lib

  with closing(urlopen("http://example.com/")) as f:
      document = html5lib.parse(f, transport_encoding=f.info().getparam("charset"))

When using with ``urllib.request`` (Python 3), the charset from HTTP
should be pass into html5lib as follows:

.. code-block:: python

  from urllib.request import urlopen
  import html5lib

  with urlopen("http://example.com/") as f:
      document = html5lib.parse(f, transport_encoding=f.info().get_content_charset())

To have more control over the parser, create a parser object explicitly.
For instance, to make the parser raise exceptions on parse errors, use:

.. code-block:: python

  import html5lib
  with open("mydocument.html", "rb") as f:
      parser = html5lib.HTMLParser(strict=True)
      document = parser.parse(f)

When you're instantiating parser objects explicitly, pass a treebuilder
class as the ``tree`` keyword argument to use an alternative document
format:

.. code-block:: python

  import html5lib
  parser = html5lib.HTMLParser(tree=html5lib.getTreeBuilder("dom"))
  minidom_document = parser.parse("<p>Hello World!")

More documentation is available at https://html5lib.readthedocs.io/.


Installation
------------

html5lib works on CPython 2.7+, CPython 3.3+ and PyPy.  To install it,
use:

.. code-block:: bash

    $ pip install html5lib


Optional Dependencies
---------------------

The following third-party libraries may be used for additional
functionality:

- ``datrie`` can be used under CPython to improve parsing performance
  (though in almost all cases the improvement is marginal);

- ``lxml`` is supported as a tree format (for both building and
  walking) under CPython (but *not* PyPy where it is known to cause
  segfaults);

- ``genshi`` has a treewalker (but not builder); and

- ``chardet`` can be used as a fallback when character encoding cannot
  be determined.


Bugs
----

Please report any bugs on the `issue tracker
<https://github.com/html5lib/html5lib-python/issues>`_.


Tests
-----

Unit tests require the ``pytest`` and ``mock`` libraries and can be
run using the ``py.test`` command in the root directory.

Test data are contained in a separate `html5lib-tests
<https://github.com/html5lib/html5lib-tests>`_ repository and included
as a submodule, thus for git checkouts they must be initialized::

  $ git submodule init
  $ git submodule update

If you have all compatible Python implementations available on your
system, you can run tests on all of them using the ``tox`` utility,
which can be found on PyPI.


Questions?
----------

There's a mailing list available for support on Google Groups,
`html5lib-discuss <http://groups.google.com/group/html5lib-discuss>`_,
though you may get a quicker response asking on IRC in `#whatwg on
irc.freenode.net <http://wiki.whatwg.org/wiki/IRC>`_.

Change Log
----------

1.0.1
~~~~~

Released on December 7, 2017

Breaking changes:

* Drop support for Python 2.6. (#330) (Thank you, Hugo, Will Kahn-Greene!)
* Remove ``utils/spider.py`` (#353) (Thank you, Jon Dufresne!)

Features:

* Improve documentation. (#300, #307) (Thank you, Jon Dufresne, Tom Most,
  Will Kahn-Greene!)
* Add iframe seamless boolean attribute. (Thank you, Ritwik Gupta!)
* Add itemscope as a boolean attribute. (#194) (Thank you, Jonathan Vanasco!)
* Support Python 3.6. (#333) (Thank you, Jon Dufresne!)
* Add CI support for Windows using AppVeyor. (Thank you, John Vandenberg!)
* Improve testing and CI and add code coverage (#323, #334), (Thank you, Jon
  Dufresne, John Vandenberg, Geoffrey Sneddon, Will Kahn-Greene!)
* Semver-compliant version number.

Bug fixes:

* Add support for setuptools < 18.5 to support environment markers. (Thank you,
  John Vandenberg!)
* Add explicit dependency for six >= 1.9. (Thank you, Eric Amorde!)
* Fix regexes to work with Python 3.7 regex adjustments. (#318, #379) (Thank
  you, Benedikt Morbach, Ville Skyttä, Mark Vasilkov!)
* Fix alphabeticalattributes filter namespace bug. (#324) (Thank you, Will
  Kahn-Greene!)
* Include license file in generated wheel package. (#350) (Thank you, Jon
  Dufresne!)
* Fix annotation-xml typo. (#339) (Thank you, Will Kahn-Greene!)
* Allow uppercase hex chararcters in CSS colour check. (#377) (Thank you,
  Komal Dembla, Hugo!)


1.0
~~~

Released and unreleased on December 7, 2017. Badly packaged release.


0.999999999/1.0b10
~~~~~~~~~~~~~~~~~~

Released on July 15, 2016

* Fix attribute order going to the tree builder to be document order
  instead of reverse document order(!).


0.99999999/1.0b9
~~~~~~~~~~~~~~~~

Released on July 14, 2016

* **Added ordereddict as a mandatory dependency on Python 2.6.**

* Added ``lxml``, ``genshi``, ``datrie``, ``charade``, and ``all``
  extras that will do the right thing based on the specific
  interpreter implementation.

* Now requires the ``mock`` package for the testsuite.

* Cease supporting DATrie under PyPy.

* **Remove PullDOM support, as this hasn't ever been properly
  tested, doesn't entirely work, and as far as I can tell is
  completely unused by anyone.**

* Move testsuite to ``py.test``.

* **Fix #124: move to webencodings for decoding the input byte stream;
  this makes html5lib compliant with the Encoding Standard, and
  introduces a required dependency on webencodings.**

* **Cease supporting Python 3.2 (in both CPython and PyPy forms).**

* **Fix comments containing double-dash with lxml 3.5 and above.**

* **Use scripting disabled by default (as we don't implement
  scripting).**

* **Fix #11, avoiding the XSS bug potentially caused by serializer
  allowing attribute values to be escaped out of in old browser versions,
  changing the quote_attr_values option on serializer to take one of
  three values, "always" (the old True value), "legacy" (the new option,
  and the new default), and "spec" (the old False value, and the old
  default).**

* **Fix #72 by rewriting the sanitizer to apply only to treewalkers
  (instead of the tokenizer); as such, this will require amending all
  callers of it to use it via the treewalker API.**

* **Drop support of charade, now that chardet is supported once more.**

* **Replace the charset keyword argument on parse and related methods
  with a set of keyword arguments: override_encoding, transport_encoding,
  same_origin_parent_encoding, likely_encoding, and default_encoding.**

* **Move filters._base, treebuilder._base, and treewalkers._base to .base
  to clarify their status as public.**

* **Get rid of the sanitizer package. Merge sanitizer.sanitize into the
  sanitizer.htmlsanitizer module and move that to sanitizer. This means
  anyone who used sanitizer.sanitize or sanitizer.HTMLSanitizer needs no
  code changes.**

* **Rename treewalkers.lxmletree to .etree_lxml and
  treewalkers.genshistream to .genshi to have a consistent API.**

* Move a whole load of stuff (inputstream, ihatexml, trie, tokenizer,
  utils) to be underscore prefixed to clarify their status as private.


0.9999999/1.0b8
~~~~~~~~~~~~~~~

Released on September 10, 2015

* Fix #195: fix the sanitizer to drop broken URLs (it threw an
  exception between 0.9999 and 0.999999).


0.999999/1.0b7
~~~~~~~~~~~~~~

Released on July 7, 2015

* Fix #189: fix the sanitizer to allow relative URLs again (as it did
  prior to 0.9999/1.0b5).


0.99999/1.0b6
~~~~~~~~~~~~~

Released on April 30, 2015

* Fix #188: fix the sanitizer to not throw an exception when sanitizing
  bogus data URLs.


0.9999/1.0b5
~~~~~~~~~~~~

Released on April 29, 2015

* Fix #153: Sanitizer fails to treat some attributes as URLs. Despite how
  this sounds, this has no known security implications.  No known version
  of IE (5.5 to current), Firefox (3 to current), Safari (6 to current),
  Chrome (1 to current), or Opera (12 to current) will run any script
  provided in these attributes.

* Pass error message to the ParseError exception in strict parsing mode.

* Allow data URIs in the sanitizer, with a whitelist of content-types.

* Add support for Python implementations that don't support lone
  surrogates (read: Jython). Fixes #2.

* Remove localization of error messages. This functionality was totally
  unused (and untested that everything was localizable), so we may as
  well follow numerous browsers in not supporting translating technical
  strings.

* Expose treewalkers.pprint as a public API.

* Add a documentEncoding property to HTML5Parser, fix #121.


0.999
~~~~~

Released on December 23, 2013

* Fix #127: add work-around for CPython issue #20007: .read(0) on
  http.client.HTTPResponse drops the rest of the content.

* Fix #115: lxml treewalker can now deal with fragments containing, at
  their root level, text nodes with non-ASCII characters on Python 2.


0.99
~~~~

Released on September 10, 2013

* No library changes from 1.0b3; released as 0.99 as pip has changed
  behaviour from 1.4 to avoid installing pre-release versions per
  PEP 440.


1.0b3
~~~~~

Released on July 24, 2013

* Removed ``RecursiveTreeWalker`` from ``treewalkers._base``. Any
  implementation using it should be moved to
  ``NonRecursiveTreeWalker``, as everything bundled with html5lib has
  for years.

* Fix #67 so that ``BufferedStream`` to correctly returns a bytes
  object, thereby fixing any case where html5lib is passed a
  non-seekable RawIOBase-like object.


1.0b2
~~~~~

Released on June 27, 2013

* Removed reordering of attributes within the serializer. There is now
  an ``alphabetical_attributes`` option which preserves the previous
  behaviour through a new filter. This allows attribute order to be
  preserved through html5lib if the tree builder preserves order.

* Removed ``dom2sax`` from DOM treebuilders. It has been replaced by
  ``treeadapters.sax.to_sax`` which is generic and supports any
  treewalker; it also resolves all known bugs with ``dom2sax``.

* Fix treewalker assertions on hitting bytes strings on
  Python 2. Previous to 1.0b1, treewalkers coped with mixed
  bytes/unicode data on Python 2; this reintroduces this prior
  behaviour on Python 2. Behaviour is unchanged on Python 3.


1.0b1
~~~~~

Released on May 17, 2013

* Implementation updated to implement the `HTML specification
  <http://www.whatwg.org/specs/web-apps/current-work/>`_ as of 5th May
  2013 (`SVN <http://svn.whatwg.org/webapps/>`_ revision r7867).

* Python 3.2+ supported in a single codebase using the ``six`` library.

* Removed support for Python 2.5 and older.

* Removed the deprecated Beautiful Soup 3 treebuilder.
  ``beautifulsoup4`` can use ``html5lib`` as a parser instead. Note that
  since it doesn't support namespaces, foreign content like SVG and
  MathML is parsed incorrectly.

* Removed ``simpletree`` from the package. The default tree builder is
  now ``etree`` (using the ``xml.etree.cElementTree`` implementation if
  available, and ``xml.etree.ElementTree`` otherwise).

* Removed the ``XHTMLSerializer`` as it never actually guaranteed its
  output was well-formed XML, and hence provided little of use.

* Removed default DOM treebuilder, so ``html5lib.treebuilders.dom`` is no
  longer supported. ``html5lib.treebuilders.getTreeBuilder("dom")`` will
  return the default DOM treebuilder, which uses ``xml.dom.minidom``.

* Optional heuristic character encoding detection now based on
  ``charade`` for Python 2.6 - 3.3 compatibility.

* Optional ``Genshi`` treewalker support fixed.

* Many bugfixes, including:

  * #33: null in attribute value breaks XML AttValue;

  * #4: nested, indirect descendant, <button> causes infinite loop;

  * `Google Code 215
    <http://code.google.com/p/html5lib/issues/detail?id=215>`_: Properly
    detect seekable streams;

  * `Google Code 206
    <http://code.google.com/p/html5lib/issues/detail?id=206>`_: add
    support for <video preload=...>, <audio preload=...>;

  * `Google Code 205
    <http://code.google.com/p/html5lib/issues/detail?id=205>`_: add
    support for <video poster=...>;

  * `Google Code 202
    <http://code.google.com/p/html5lib/issues/detail?id=202>`_: Unicode
    file breaks InputStream.

* Source code is now mostly PEP 8 compliant.

* Test harness has been improved and now depends on ``nose``.

* Documentation updated and moved to https://html5lib.readthedocs.io/.


0.95
~~~~

Released on February 11, 2012


0.90
~~~~

Released on January 17, 2010


0.11.1
~~~~~~

Released on June 12, 2008


0.11
~~~~

Released on June 10, 2008


0.10
~~~~

Released on October 7, 2007


0.9
~~~

Released on March 11, 2007


0.2
~~~

Released on January 8, 2007



  * [httpie-1.0.2](http://httpie.org/) HTTPie: a CLI, cURL-like tool for humans
########################################

HTTPie (pronounced *aitch-tee-tee-pie*) is a command line HTTP client.
Its goal is to make CLI interaction with web services as human-friendly
as possible. It provides a simple ``http`` command that allows for sending
arbitrary HTTP requests using a simple and natural syntax, and displays
colorized output. HTTPie can be used for testing, debugging, and
generally interacting with HTTP servers.


.. class:: no-web

    .. image:: https://raw.githubusercontent.com/jakubroztocil/httpie/master/httpie.gif
        :alt: HTTPie in action
        :width: 100%
        :align: center


    .. image:: https://raw.githubusercontent.com/jakubroztocil/httpie/master/httpie.png
        :alt: HTTPie compared to cURL
        :width: 100%
        :align: center


.. class:: no-web no-pdf

|pypi| |unix_build| |coverage| |gitter|



.. contents::

.. section-numbering::



Main features
=============

* Expressive and intuitive syntax
* Formatted and colorized terminal output
* Built-in JSON support
* Forms and file uploads
* HTTPS, proxies, and authentication
* Arbitrary request data
* Custom headers
* Persistent sessions
* Wget-like downloads
* Python 2.7 and 3.x support
* Linux, macOS and Windows support
* Plugins
* Documentation
* Test coverage


Installation
============


macOS
-----


On macOS, HTTPie can be installed via `Homebrew <http://brew.sh/>`_
(recommended):

.. code-block:: bash

    $ brew install httpie


A MacPorts *port* is also available:

.. code-block:: bash

    $ port install httpie

Linux
-----

Most Linux distributions provide a package that can be installed using the
system package manager, for example:

.. code-block:: bash

    # Debian, Ubuntu, etc.
    $ apt-get install httpie

.. code-block:: bash

    # Fedora
    $ dnf install httpie

.. code-block:: bash

    # CentOS, RHEL, ...
    $ yum install httpie

.. code-block:: bash

    # Arch Linux
    $ pacman -S httpie


Windows, etc.
-------------

A universal installation method (that works on Windows, Mac OS X, Linux, …,
and always provides the latest version) is to use `pip`_:


.. code-block:: bash

    # Make sure we have an up-to-date version of pip and setuptools:
    $ pip install --upgrade pip setuptools

    $ pip install --upgrade httpie


(If ``pip`` installation fails for some reason, you can try
``easy_install httpie`` as a fallback.)


Python version
--------------

Although Python 2.7 is supported as well, it is strongly recommended to
install HTTPie against the latest Python 3.x whenever possible. That will
ensure that some of the newer HTTP features, such as
`SNI (Server Name Indication)`_, work out of the box.
Python 3 is the default for Homebrew installations starting with version 0.9.4.
To see which version HTTPie uses, run ``http --debug``.


Unstable version
----------------

You can also install the latest unreleased development version directly from
the ``master`` branch on GitHub.  It is a work-in-progress of a future stable
release so the experience might be not as smooth.


.. class:: no-pdf

|unix_build|


On macOS you can install it with Homebrew:

.. code-block:: bash

    $ brew install httpie --HEAD


Otherwise with ``pip``:

.. code-block:: bash

    $ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz


Verify that now we have the
`current development version identifier <https://github.com/jakubroztocil/httpie/blob/0af6ae1be444588bbc4747124e073423151178a0/httpie/__init__.py#L5>`_
with the ``-dev`` suffix, for example:

.. code-block:: bash

    $ http --version
    1.0.0-dev


Usage
=====


Hello World:


.. code-block:: bash

    $ http httpie.org


Synopsis:

.. code-block:: bash

    $ http [flags] [METHOD] URL [ITEM [ITEM]]


See also ``http --help``.


Examples
--------

Custom `HTTP method`_, `HTTP headers`_ and `JSON`_ data:

.. code-block:: bash

    $ http PUT example.org X-API-Token:123 name=John


Submitting `forms`_:

.. code-block:: bash

    $ http -f POST example.org hello=World


See the request that is being sent using one of the `output options`_:

.. code-block:: bash

    $ http -v example.org


Use `Github API`_ to post a comment on an
`issue <https://github.com/jakubroztocil/httpie/issues/83>`_
with `authentication`_:

.. code-block:: bash

    $ http -a USERNAME POST https://api.github.com/repos/jakubroztocil/httpie/issues/83/comments body='HTTPie is awesome! :heart:'


Upload a file using `redirected input`_:

.. code-block:: bash

    $ http example.org < file.json


Download a file and save it via `redirected output`_:

.. code-block:: bash

    $ http example.org/file > file


Download a file ``wget`` style:

.. code-block:: bash

    $ http --download example.org/file

Use named `sessions`_ to make certain aspects or the communication persistent
between requests to the same host:

.. code-block:: bash

    $ http --session=logged-in -a username:password httpbin.org/get API-Key:123

    $ http --session=logged-in httpbin.org/headers


Set a custom ``Host`` header to work around missing DNS records:

.. code-block:: bash

    $ http localhost:8000 Host:example.com

..


HTTP method
===========

The name of the HTTP method comes right before the URL argument:

.. code-block:: bash

    $ http DELETE example.org/todos/7


Which looks similar to the actual ``Request-Line`` that is sent:

.. code-block:: http

    DELETE /todos/7 HTTP/1.1


When the ``METHOD`` argument is omitted from the command, HTTPie defaults to
either ``GET`` (with no request data) or ``POST`` (with request data).


Request URL
===========

The only information HTTPie needs to perform a request is a URL.
The default scheme is, somewhat unsurprisingly, ``http://``,
and can be omitted from the argument – ``http example.org`` works just fine.


Querystring parameters
----------------------

If you find yourself manually constructing URLs with querystring parameters
on the terminal, you may appreciate the ``param==value`` syntax for appending
URL parameters. With that, you don't have to worry about escaping the ``&``
separators for your shell. Also, special characters in parameter values,
will also automatically escaped (HTTPie otherwise expects the URL to be
already escaped). To search for ``HTTPie logo`` on Google Images you could use
this command:

.. code-block:: bash

    $ http www.google.com search=='HTTPie logo' tbm==isch


.. code-block:: http

    GET /?search=HTTPie+logo&tbm=isch HTTP/1.1



URL shortcuts for ``localhost``
-------------------------------

Additionally, curl-like shorthand for localhost is supported.
This means that, for example ``:3000`` would expand to ``http://localhost:3000``
If the port is omitted, then port 80 is assumed.

.. code-block:: bash

    $ http :/foo


.. code-block:: http

    GET /foo HTTP/1.1
    Host: localhost


.. code-block:: bash

    $ http :3000/bar


.. code-block:: http

    GET /bar HTTP/1.1
    Host: localhost:3000


.. code-block:: bash

    $ http :


.. code-block:: http

    GET / HTTP/1.1
    Host: localhost


Custom default scheme
---------------------

You can use the ``--default-scheme <URL_SCHEME>`` option to create
shortcuts for other protocols than HTTP:

.. code-block:: bash

    $ alias https='http --default-scheme=https'


Request items
=============

There are a few different *request item* types that provide a
convenient mechanism for specifying HTTP headers, simple JSON and
form data, files, and URL parameters.

They are key/value pairs specified after the URL. All have in
common that they become part of the actual request that is sent and that
their type is distinguished only by the separator used:
``:``, ``=``, ``:=``, ``==``, ``@``, ``=@``, and ``:=@``. The ones with an
``@`` expect a file path as value.

+-----------------------+-----------------------------------------------------+
| Item Type             | Description                                         |
+=======================+=====================================================+
| HTTP Headers          | Arbitrary HTTP header, e.g. ``X-API-Token:123``.    |
| ``Name:Value``        |                                                     |
+-----------------------+-----------------------------------------------------+
| URL parameters        | Appends the given name/value pair as a query        |
| ``name==value``       | string parameter to the URL.                        |
|                       | The ``==`` separator is used.                       |
+-----------------------+-----------------------------------------------------+
| Data Fields           | Request data fields to be serialized as a JSON      |
| ``field=value``,      | object (default), or to be form-encoded             |
| ``field=@file.txt``   | (``--form, -f``).                                   |
+-----------------------+-----------------------------------------------------+
| Raw JSON fields       | Useful when sending JSON and one or                 |
| ``field:=json``,      | more fields need to be a ``Boolean``, ``Number``,   |
| ``field:=@file.json`` | nested ``Object``, or an ``Array``,  e.g.,          |
|                       | ``meals:='["ham","spam"]'`` or ``pies:=[1,2,3]``    |
|                       | (note the quotes).                                  |
+-----------------------+-----------------------------------------------------+
| Form File Fields      | Only available with ``--form, -f``.                 |
| ``field@/dir/file``   | For example ``screenshot@~/Pictures/img.png``.      |
|                       | The presence of a file field results                |
|                       | in a ``multipart/form-data`` request.               |
+-----------------------+-----------------------------------------------------+


Note that data fields aren't the only way to specify request data:
`Redirected input`_ is a mechanism for passing arbitrary request data.


Escaping rules
--------------

You can use ``\`` to escape characters that shouldn't be used as separators
(or parts thereof). For instance, ``foo\==bar`` will become a data key/value
pair (``foo=`` and ``bar``) instead of a URL parameter.

Often it is necessary to quote the values, e.g. ``foo='bar baz'``.

If any of the field names or headers starts with a minus
(e.g., ``-fieldname``), you need to place all such items after the special
token ``--`` to prevent confusion with ``--arguments``:

.. code-block:: bash

    $ http httpbin.org/post  --  -name-starting-with-dash=foo -Unusual-Header:bar

.. code-block:: http

    POST /post HTTP/1.1
    -Unusual-Header: bar
    Content-Type: application/json

    {
        "-name-starting-with-dash": "foo"
    }



JSON
====

JSON is the *lingua franca* of modern web services and it is also the
**implicit content type** HTTPie uses by default.


Simple example:

.. code-block:: bash

    $ http PUT example.org name=John email=john@example.org

.. code-block:: http

    PUT / HTTP/1.1
    Accept: application/json, */*
    Accept-Encoding: gzip, deflate
    Content-Type: application/json
    Host: example.org

    {
        "name": "John",
        "email": "john@example.org"
    }


Default behaviour
-----------------


If your command includes some data `request items`_, they are serialized as a JSON
object by default. HTTPie also automatically sets the following headers,
both of which can be overwritten:

================    =======================================
``Content-Type``    ``application/json``
``Accept``          ``application/json, */*``
================    =======================================


Explicit JSON
-------------

You can use ``--json, -j`` to explicitly set ``Accept``
to ``application/json`` regardless of whether you are sending data
(it's a shortcut for setting the header via the usual header notation:
``http url Accept:'application/json, */*'``). Additionally,
HTTPie will try to detect JSON responses even when the
``Content-Type`` is incorrectly ``text/plain`` or unknown.



Non-string JSON fields
----------------------

Non-string fields use the ``:=`` separator, which allows you to embed raw JSON
into the resulting object. Text and raw JSON files can also be embedded into
fields using ``=@`` and ``:=@``:

.. code-block:: bash

    $ http PUT api.example.com/person/1 \
        name=John \
        age:=29 married:=false hobbies:='["http", "pies"]' \  # Raw JSON
        description=@about-john.txt \   # Embed text file
        bookmarks:=@bookmarks.json      # Embed JSON file


.. code-block:: http

    PUT /person/1 HTTP/1.1
    Accept: application/json, */*
    Content-Type: application/json
    Host: api.example.com

    {
        "age": 29,
        "hobbies": [
            "http",
            "pies"
        ],
        "description": "John is a nice guy who likes pies.",
        "married": false,
        "name": "John",
        "bookmarks": {
            "HTTPie": "http://httpie.org",
        }
    }


Please note that with this syntax the command gets unwieldy when sending
complex data. In that case it's always better to use `redirected input`_:

.. code-block:: bash

    $ http POST api.example.com/person/1 < person.json


Forms
=====

Submitting forms is very similar to sending `JSON`_ requests. Often the only
difference is in adding the ``--form, -f`` option, which ensures that
data fields are serialized as, and ``Content-Type`` is set to,
``application/x-www-form-urlencoded; charset=utf-8``. It is possible to make
form data the implicit content type instead of JSON
via the `config`_ file.


Regular forms
-------------

.. code-block:: bash

    $ http --form POST api.example.org/person/1 name='John Smith'


.. code-block:: http

    POST /person/1 HTTP/1.1
    Content-Type: application/x-www-form-urlencoded; charset=utf-8

    name=John+Smith


File upload forms
-----------------

If one or more file fields is present, the serialization and content type is
``multipart/form-data``:

.. code-block:: bash

    $ http -f POST example.com/jobs name='John Smith' cv@~/Documents/cv.pdf


The request above is the same as if the following HTML form were
submitted:

.. code-block:: html

    <form enctype="multipart/form-data" method="post" action="http://example.com/jobs">
        <input type="text" name="name" />
        <input type="file" name="cv" />
    </form>

Note that ``@`` is used to simulate a file upload form field, whereas
``=@`` just embeds the file content as a regular text field value.


HTTP headers
============

To set custom headers you can use the ``Header:Value`` notation:

.. code-block:: bash

    $ http example.org  User-Agent:Bacon/1.0  'Cookie:valued-visitor=yes;foo=bar'  \
        X-Foo:Bar  Referer:http://httpie.org/


.. code-block:: http

    GET / HTTP/1.1
    Accept: */*
    Accept-Encoding: gzip, deflate
    Cookie: valued-visitor=yes;foo=bar
    Host: example.org
    Referer: http://httpie.org/
    User-Agent: Bacon/1.0
    X-Foo: Bar


Default request headers
-----------------------

There are a couple of default headers that HTTPie sets:

.. code-block:: http

    GET / HTTP/1.1
    Accept: */*
    Accept-Encoding: gzip, deflate
    User-Agent: HTTPie/<version>
    Host: <taken-from-URL>



Any of these except ``Host`` can be overwritten and some of them unset.



Empty headers and header un-setting
-----------------------------------

To unset a previously specified header
(such a one of the default headers), use ``Header:``:


.. code-block:: bash

    $ http httpbin.org/headers Accept: User-Agent:


To send a header with an empty value, use ``Header;``:


.. code-block:: bash

    $ http httpbin.org/headers 'Header;'


Cookies
=======

HTTP clients send cookies to the server as regular `HTTP headers`_. That means,
HTTPie does not offer any special syntax for specifying cookies — the usual
``Header:Value`` notation is used:


Send a single cookie:

.. code-block:: bash

    $ http example.org Cookie:sessionid=foo

.. code-block:: http

    GET / HTTP/1.1
    Accept: */*
    Accept-Encoding: gzip, deflate
    Connection: keep-alive
    Cookie: sessionid=foo
    Host: example.org
    User-Agent: HTTPie/0.9.9


Send multiple cookies
(note the header is quoted to prevent the shell from interpreting the ``;``):

.. code-block:: bash

    $ http example.org 'Cookie:sessionid=foo;another-cookie=bar'

.. code-block:: http

    GET / HTTP/1.1
    Accept: */*
    Accept-Encoding: gzip, deflate
    Connection: keep-alive
    Cookie: sessionid=foo;another-cookie=bar
    Host: example.org
    User-Agent: HTTPie/0.9.9


If you often deal with cookies in your requests, then chances are you'd appreciate
the `sessions`_ feature.


Authentication
==============

The currently supported authentication schemes are Basic and Digest
(see `auth plugins`_ for more). There are two flags that control authentication:

===================     ======================================================
``--auth, -a``          Pass a ``username:password`` pair as
                        the argument. Or, if you only specify a username
                        (``-a username``), you'll be prompted for
                        the password before the request is sent.
                        To send an empty password, pass ``username:``.
                        The ``username:password@hostname`` URL syntax is
                        supported as well (but credentials passed via ``-a``
                        have higher priority).

``--auth-type, -A``     Specify the auth mechanism. Possible values are
                        ``basic`` and ``digest``. The default value is
                        ``basic`` so it can often be omitted.
===================     ======================================================



Basic auth
----------


.. code-block:: bash

    $ http -a username:password example.org


Digest auth
-----------


.. code-block:: bash

    $ http -A digest -a username:password example.org


Password prompt
---------------

.. code-block:: bash

    $ http -a username example.org


``.netrc``
----------

Authentication information from your ``~/.netrc`` file is honored as well:

.. code-block:: bash

    $ cat ~/.netrc
    machine httpbin.org
    login httpie
    password test

    $ http httpbin.org/basic-auth/httpie/test
    HTTP/1.1 200 OK
    [...]


Auth plugins
------------

Additional authentication mechanism can be installed as plugins.
They can be found on the `Python Package Index <https://pypi.python.org/pypi?%3Aaction=search&term=httpie&submit=search>`_.
Here's a few picks:

* `httpie-api-auth <https://github.com/pd/httpie-api-auth>`_: ApiAuth
* `httpie-aws-auth <https://github.com/httpie/httpie-aws-auth>`_: AWS / Amazon S3
* `httpie-edgegrid <https://github.com/akamai-open/httpie-edgegrid>`_: EdgeGrid
* `httpie-hmac-auth <https://github.com/guardian/httpie-hmac-auth>`_: HMAC
* `httpie-jwt-auth <https://github.com/teracyhq/httpie-jwt-auth>`_: JWTAuth (JSON Web Tokens)
* `httpie-negotiate <https://github.com/ndzou/httpie-negotiate>`_: SPNEGO (GSS Negotiate)
* `httpie-ntlm <https://github.com/httpie/httpie-ntlm>`_: NTLM (NT LAN Manager)
* `httpie-oauth <https://github.com/httpie/httpie-oauth>`_: OAuth
* `requests-hawk <https://github.com/mozilla-services/requests-hawk>`_: Hawk




HTTP redirects
==============

By default, HTTP redirects are not followed and only the first
response is shown:


.. code-block:: bash

    $ http httpbin.org/redirect/3


Follow ``Location``
-------------------

To instruct HTTPie to follow the ``Location`` header of ``30x`` responses
and show the final response instead, use the ``--follow, -F`` option:


.. code-block:: bash

    $ http --follow httpbin.org/redirect/3


Showing intermediary redirect responses
---------------------------------------

If you additionally wish to see the intermediary requests/responses,
then use the ``--all`` option as well:


.. code-block:: bash

    $ http --follow --all httpbin.org/redirect/3



Limiting maximum redirects followed
-----------------------------------

To change the default limit of maximum ``30`` redirects, use the
``--max-redirects=<limit>`` option:


.. code-block:: bash

    $ http --follow --all --max-redirects=5 httpbin.org/redirect/3


Proxies
=======

You can specify proxies to be used through the ``--proxy`` argument for each
protocol (which is included in the value in case of redirects across protocols):

.. code-block:: bash

    $ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org


With Basic authentication:

.. code-block:: bash

    $ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org


Environment variables
---------------------

You can also configure proxies by environment variables ``HTTP_PROXY`` and
``HTTPS_PROXY``, and the underlying Requests library will pick them up as well.
If you want to disable proxies configured through the environment variables for
certain hosts, you can specify them in ``NO_PROXY``.

In your ``~/.bash_profile``:

.. code-block:: bash

 export HTTP_PROXY=http://10.10.1.10:3128
 export HTTPS_PROXY=https://10.10.1.10:1080
 export NO_PROXY=localhost,example.com


SOCKS
-----

Homebrew-installed HTTPie comes with SOCKS proxy support out of the box. To enable SOCKS proxy support for non-Homebrew  installations, you'll need to install ``requests[socks]`` manually using ``pip``:


.. code-block:: bash

    $ pip install -U requests[socks]

Usage is the same as for other types of `proxies`_:

.. code-block:: bash

    $ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org


HTTPS
=====


Server SSL certificate verification
-----------------------------------

To skip the host's SSL certificate verification, you can pass ``--verify=no``
(default is ``yes``):

.. code-block:: bash

    $ http --verify=no https://example.org


Custom CA bundle
----------------

You can also use ``--verify=<CA_BUNDLE_PATH>`` to set a custom CA bundle path:

.. code-block:: bash

    $ http --verify=/ssl/custom_ca_bundle https://example.org



Client side SSL certificate
---------------------------
To use a client side certificate for the SSL communication, you can pass
the path of the cert file with ``--cert``:

.. code-block:: bash

    $ http --cert=client.pem https://example.org


If the private key is not contained in the cert file you may pass the
path of the key file with ``--cert-key``:

.. code-block:: bash

    $ http --cert=client.crt --cert-key=client.key https://example.org


SSL version
-----------

Use the ``--ssl=<PROTOCOL>`` to specify the desired protocol version to use.
This will default to SSL v2.3 which will negotiate the highest protocol that both
the server and your installation of OpenSSL support. The available protocols
are ``ssl2.3``, ``ssl3``, ``tls1``, ``tls1.1``, ``tls1.2``, ``tls1.3``. (The actually
available set of protocols may vary depending on your OpenSSL installation.)

.. code-block:: bash

    # Specify the vulnerable SSL v3 protocol to talk to an outdated server:
    $ http --ssl=ssl3 https://vulnerable.example.org


SNI (Server Name Indication)
----------------------------

If you use HTTPie with `Python version`_ lower than 2.7.9
(can be verified with ``http --debug``) and need to talk to servers that
use SNI (Server Name Indication) you need to install some additional
dependencies:

.. code-block:: bash

    $ pip install --upgrade requests[security]


You can use the following command to test SNI support:

.. code-block:: bash

    $ http https://sni.velox.ch


Output options
==============

By default, HTTPie only outputs the final response and the whole response
message is printed (headers as well as the body). You can control what should
be printed via several options:

=================   =====================================================
``--headers, -h``   Only the response headers are printed.
``--body, -b``      Only the response body is printed.
``--verbose, -v``   Print the whole HTTP exchange (request and response).
                    This option also enables ``--all`` (see below).
``--print, -p``     Selects parts of the HTTP exchange.
=================   =====================================================

``--verbose`` can often be useful for debugging the request and generating
documentation examples:

.. code-block:: bash

    $ http --verbose PUT httpbin.org/put hello=world
    PUT /put HTTP/1.1
    Accept: application/json, */*
    Accept-Encoding: gzip, deflate
    Content-Type: application/json
    Host: httpbin.org
    User-Agent: HTTPie/0.2.7dev

    {
        "hello": "world"
    }


    HTTP/1.1 200 OK
    Connection: keep-alive
    Content-Length: 477
    Content-Type: application/json
    Date: Sun, 05 Aug 2012 00:25:23 GMT
    Server: gunicorn/0.13.4

    {
        […]
    }


What parts of the HTTP exchange should be printed
-------------------------------------------------

All the other `output options`_ are under the hood just shortcuts for
the more powerful ``--print, -p``. It accepts a string of characters each
of which represents a specific part of the HTTP exchange:

==========  ==================
Character   Stands for
==========  ==================
``H``       request headers
``B``       request body
``h``       response headers
``b``       response body
==========  ==================

Print request and response headers:

.. code-block:: bash

    $ http --print=Hh PUT httpbin.org/put hello=world


Viewing intermediary requests/responses
---------------------------------------

To see all the HTTP communication, i.e. the final request/response as
well as any possible  intermediary requests/responses, use the ``--all``
option. The intermediary HTTP communication include followed redirects
(with ``--follow``), the first unauthorized request when HTTP digest
authentication is used (``--auth=digest``), etc.

.. code-block:: bash

    # Include all responses that lead to the final one:
    $ http --all --follow httpbin.org/redirect/3


The intermediary requests/response are by default formatted according to
``--print, -p`` (and its shortcuts described above). If you'd like to change
that, use the ``--history-print, -P`` option. It takes the same
arguments as ``--print, -p`` but applies to the intermediary requests only.


.. code-block:: bash

    # Print the intermediary requests/responses differently than the final one:
    $ http -A digest -a foo:bar --all -p Hh -P H httpbin.org/digest-auth/auth/foo/bar


Conditional body download
-------------------------

As an optimization, the response body is downloaded from the server
only if it's part of the output. This is similar to performing a ``HEAD``
request, except that it applies to any HTTP method you use.

Let's say that there is an API that returns the whole resource when it is
updated, but you are only interested in the response headers to see the
status code after an update:

.. code-block:: bash

    $ http --headers PATCH example.org/Really-Huge-Resource name='New Name'


Since we are only printing the HTTP headers here, the connection to the server
is closed as soon as all the response headers have been received.
Therefore, bandwidth and time isn't wasted downloading the body
which you don't care about. The response headers are downloaded always,
even if they are not part of the output


Redirected Input
================

The universal method for passing request data is through redirected ``stdin``
(standard input)—piping. Such data is buffered and then with no further
processing used as the request body. There are multiple useful ways to use
piping:

Redirect from a file:

.. code-block:: bash

    $ http PUT example.com/person/1 X-API-Token:123 < person.json


Or the output of another program:

.. code-block:: bash

    $ grep '401 Unauthorized' /var/log/httpd/error_log | http POST example.org/intruders


You can use ``echo`` for simple data:

.. code-block:: bash

    $ echo '{"name": "John"}' | http PATCH example.com/person/1 X-API-Token:123


You can also use a Bash *here string*:

.. code-block:: bash

    $ http example.com/ <<<'{"name": "John"}'


You can even pipe web services together using HTTPie:

.. code-block:: bash

    $ http GET https://api.github.com/repos/jakubroztocil/httpie | http POST httpbin.org/post


You can use ``cat`` to enter multiline data on the terminal:

.. code-block:: bash

    $ cat | http POST example.com
    <paste>
    ^D


.. code-block:: bash

    $ cat | http POST example.com/todos Content-Type:text/plain
    - buy milk
    - call parents
    ^D


On OS X, you can send the contents of the clipboard with ``pbpaste``:

.. code-block:: bash

    $ pbpaste | http PUT example.com


Passing data through ``stdin`` cannot be combined with data fields specified
on the command line:


.. code-block:: bash

    $ echo 'data' | http POST example.org more=data   # This is invalid


To prevent HTTPie from reading ``stdin`` data you can use the
``--ignore-stdin`` option.


Request data from a filename
----------------------------

An alternative to redirected ``stdin`` is specifying a filename (as
``@/path/to/file``) whose content is used as if it came from ``stdin``.

It has the advantage that the ``Content-Type``
header is automatically set to the appropriate value based on the
filename extension. For example, the following request sends the
verbatim contents of that XML file with ``Content-Type: application/xml``:

.. code-block:: bash

    $ http PUT httpbin.org/put @/data/file.xml


Terminal output
===============

HTTPie does several things by default in order to make its terminal output
easy to read.


Colors and formatting
---------------------

Syntax highlighting is applied to HTTP headers and bodies (where it makes
sense). You can choose your preferred color scheme via the ``--style`` option
if you don't like the default one (see ``$ http --help`` for the possible
values).

Also, the following formatting is applied:

* HTTP headers are sorted by name.
* JSON data is indented, sorted by keys, and unicode escapes are converted
  to the characters they represent.

One of these options can be used to control output processing:

====================   ========================================================
``--pretty=all``       Apply both colors and formatting.
                       Default for terminal output.
``--pretty=colors``    Apply colors.
``--pretty=format``    Apply formatting.
``--pretty=none``      Disables output processing.
                       Default for redirected output.
====================   ========================================================

Binary data
-----------

Binary data is suppressed for terminal output, which makes it safe to perform
requests to URLs that send back binary data. Binary data is suppressed also in
redirected, but prettified output. The connection is closed as soon as we know
that the response body is binary,

.. code-block:: bash

    $ http example.org/Movie.mov


You will nearly instantly see something like this:

.. code-block:: http

    HTTP/1.1 200 OK
    Accept-Ranges: bytes
    Content-Encoding: gzip
    Content-Type: video/quicktime
    Transfer-Encoding: chunked

    +-----------------------------------------+
    | NOTE: binary data not shown in terminal |
    +-----------------------------------------+


Redirected output
=================

HTTPie uses a different set of defaults for redirected output than for
`terminal output`_. The differences being:

* Formatting and colors aren't applied (unless ``--pretty`` is specified).
* Only the response body is printed (unless one of the `output options`_ is set).
* Also, binary data isn't suppressed.

The reason is to make piping HTTPie's output to another programs and
downloading files work with no extra flags. Most of the time, only the raw
response body is of an interest when the output is redirected.

Download a file:

.. code-block:: bash

    $ http example.org/Movie.mov > Movie.mov


Download an image of Octocat, resize it using ImageMagick, upload it elsewhere:

.. code-block:: bash

    $ http octodex.github.com/images/original.jpg | convert - -resize 25% -  | http example.org/Octocats


Force colorizing and formatting, and show both the request and the response in
``less`` pager:

.. code-block:: bash

    $ http --pretty=all --verbose example.org | less -R


The ``-R`` flag tells ``less`` to interpret color escape sequences included
HTTPie`s output.

You can create a shortcut for invoking HTTPie with colorized and paged output
by adding the following to your ``~/.bash_profile``:

.. code-block:: bash

    function httpless {
        # `httpless example.org'
        http --pretty=all --print=hb "$@" | less -R;
    }


Download mode
=============

HTTPie features a download mode in which it acts similarly to ``wget``.

When enabled using the ``--download, -d`` flag, response headers are printed to
the terminal (``stderr``), and a progress bar is shown while the response body
is being saved to a file.

.. code-block:: bash

    $ http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz

.. code-block:: http

    HTTP/1.1 200 OK
    Content-Disposition: attachment; filename=httpie-master.tar.gz
    Content-Length: 257336
    Content-Type: application/x-gzip

    Downloading 251.30 kB to "httpie-master.tar.gz"
    Done. 251.30 kB in 2.73862s (91.76 kB/s)


Downloaded filename
--------------------

There are three mutually exclusive ways through which HTTPie determines
the output filename (with decreasing priority):

1. You can explicitly provide it via ``--output, -o``.
   The file gets overwritten if it already exists
   (or appended to with ``--continue, -c``).
2. The server may specify the filename in the optional ``Content-Disposition``
   response header. Any leading dots are stripped from a server-provided filename.
3. The last resort HTTPie uses is to generate the filename from a combination
   of the request URL and the response ``Content-Type``.
   The initial URL is always used as the basis for
   the generated filename — even if there has been one or more redirects.


To prevent data loss by overwriting, HTTPie adds a unique numerical suffix to the
filename when necessary (unless specified with ``--output, -o``).


Piping while downloading
------------------------

You can also redirect the response body to another program while the response
headers and progress are still shown in the terminal:

.. code-block:: bash

    $ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz |  tar zxf -



Resuming downloads
------------------

If ``--output, -o`` is specified, you can resume a partial download using the
``--continue, -c`` option. This only works with servers that support
``Range`` requests and ``206 Partial Content`` responses. If the server doesn't
support that, the whole file will simply be downloaded:

.. code-block:: bash

    $ http -dco file.zip example.org/file

Other notes
-----------

* The ``--download`` option only changes how the response body is treated.
* You can still set custom headers, use sessions, ``--verbose, -v``, etc.
* ``--download`` always implies ``--follow`` (redirects are followed).
* HTTPie exits with status code ``1`` (error) if the body hasn't been fully
  downloaded.
* ``Accept-Encoding`` cannot be set with ``--download``.


Streamed responses
==================

Responses are downloaded and printed in chunks which allows for streaming
and large file downloads without using too much memory. However, when
`colors and formatting`_ is applied, the whole response is buffered and only
then processed at once.


Disabling buffering
-------------------

You can use the ``--stream, -S`` flag to make two things happen:

1. The output is flushed in much smaller chunks without any buffering,
   which makes HTTPie behave kind of like ``tail -f`` for URLs.

2. Streaming becomes enabled even when the output is prettified: It will be
   applied to each line of the response and flushed immediately. This makes
   it possible to have a nice output for long-lived requests, such as one
   to the Twitter streaming API.


Examples use cases
------------------

Prettified streamed response:

.. code-block:: bash

    $ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track='Justin Bieber'


Streamed output by small chunks alá ``tail -f``:

.. code-block:: bash

    # Send each new tweet (JSON object) mentioning "Apple" to another
    # server as soon as it arrives from the Twitter streaming API:
    $ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track=Apple \
    | while read tweet; do echo "$tweet" | http POST example.org/tweets ; done

Sessions
========

By default, every request HTTPie makes is completely independent of any
previous ones to the same host.


However, HTTPie also supports persistent
sessions via the ``--session=SESSION_NAME_OR_PATH`` option. In a session,
custom `HTTP headers`_ (except for the ones starting with ``Content-`` or ``If-``),
`authentication`_, and `cookies`_
(manually specified or sent by the server) persist between requests
to the same host.


.. code-block:: bash

    # Create a new session
    $ http --session=/tmp/session.json example.org API-Token:123

    # Re-use an existing session — API-Token will be set:
    $ http --session=/tmp/session.json example.org


All session data, including credentials, cookie data,
and custom headers are stored in plain text.
That means session files can also be created and edited manually in a text
editor—they are regular JSON. It also means that they can be read by anyone
who has access to the session file.


Named sessions
--------------


You can create one or more named session per host. For example, this is how
you can create a new session named ``user1`` for ``example.org``:

.. code-block:: bash

    $ http --session=user1 -a user1:password example.org X-Foo:Bar

From now on, you can refer to the session by its name. When you choose to
use the session again, any previously specified authentication or HTTP headers
will automatically be set:

.. code-block:: bash

    $ http --session=user1 example.org

To create or reuse a different session, simple specify a different name:

.. code-block:: bash

    $ http --session=user2 -a user2:password example.org X-Bar:Foo

Named sessions' data is stored in JSON files in the directory
``~/.httpie/sessions/<host>/<name>.json``
(``%APPDATA%\httpie\sessions\<host>\<name>.json`` on Windows).


Anonymous sessions
------------------

Instead of a name, you can also directly specify a path to a session file. This
allows for sessions to be re-used across multiple hosts:

.. code-block:: bash

    $ http --session=/tmp/session.json example.org
    $ http --session=/tmp/session.json admin.example.org
    $ http --session=~/.httpie/sessions/another.example.org/test.json example.org
    $ http --session-read-only=/tmp/session.json example.org


Readonly session
----------------

To use an existing session file without updating it from the request/response
exchange once it is created, specify the session name via
``--session-read-only=SESSION_NAME_OR_PATH`` instead.


Config
======

HTTPie uses a simple JSON config file.



Config file location
--------------------


The default location of the configuration file is ``~/.httpie/config.json``
(or ``%APPDATA%\httpie\config.json`` on Windows). The config directory
location can be changed by setting the ``HTTPIE_CONFIG_DIR``
environment variable. To view the exact location run ``http --debug``.

Configurable options
--------------------

The JSON file contains an object with the following keys:


``default_options``
~~~~~~~~~~~~~~~~~~~


An ``Array`` (by default empty) of default options that should be applied to
every invocation of HTTPie.

For instance, you can use this option to change the default style and output
options: ``"default_options": ["--style=fruity", "--body"]`` Another useful
default option could be ``"--session=default"`` to make HTTPie always
use `sessions`_ (one named ``default`` will automatically be used).
Or you could change the implicit request content type from JSON to form by
adding ``--form`` to the list.


``__meta__``
~~~~~~~~~~~~

HTTPie automatically stores some of its metadata here. Please do not change.



Un-setting previously specified options
---------------------------------------

Default options from the config file, or specified any other way,
can be unset for a particular invocation via ``--no-OPTION`` arguments passed
on the command line (e.g., ``--no-style`` or ``--no-session``).



Scripting
=========

When using HTTPie from shell scripts, it can be handy to set the
``--check-status`` flag. It instructs HTTPie to exit with an error if the
HTTP status is one of ``3xx``, ``4xx``, or ``5xx``. The exit status will
be ``3`` (unless ``--follow`` is set), ``4``, or ``5``,
respectively.

.. code-block:: bash

    #!/bin/bash

    if http --check-status --ignore-stdin --timeout=2.5 HEAD example.org/health &> /dev/null; then
        echo 'OK!'
    else
        case $? in
            2) echo 'Request timed out!' ;;
            3) echo 'Unexpected HTTP 3xx Redirection!' ;;
            4) echo 'HTTP 4xx Client Error!' ;;
            5) echo 'HTTP 5xx Server Error!' ;;
            6) echo 'Exceeded --max-redirects=<n> redirects!' ;;
            *) echo 'Other Error!' ;;
        esac
    fi


Best practices
--------------

The default behaviour of automatically reading ``stdin`` is typically not
desirable during non-interactive invocations. You most likely want to
use the ``--ignore-stdin`` option to disable it.

It is a common gotcha that without this option HTTPie seemingly hangs.
What happens is that when HTTPie is invoked for example from a cron job,
``stdin`` is not connected to a terminal.
Therefore, rules for `redirected input`_ apply, i.e., HTTPie starts to read it
expecting that the request body will be passed through.
And since there's no data nor ``EOF``, it will be stuck. So unless you're
piping some data to HTTPie, this flag should be used in scripts.

Also, it might be good to override the default ``30`` second ``--timeout`` to
something that suits you.



Meta
====

Interface design
----------------

The syntax of the command arguments closely corresponds to the actual HTTP
requests sent over the wire. It has the advantage  that it's easy to remember
and read. It is often possible to translate an HTTP request to an HTTPie
argument list just by inlining the request elements. For example, compare this
HTTP request:

.. code-block:: http

    POST /collection HTTP/1.1
    X-API-Key: 123
    User-Agent: Bacon/1.0
    Content-Type: application/x-www-form-urlencoded

    name=value&name2=value2


with the HTTPie command that sends it:

.. code-block:: bash

    $ http -f POST example.org/collection \
      X-API-Key:123 \
      User-Agent:Bacon/1.0 \
      name=value \
      name2=value2


Notice that both the order of elements and the syntax is very similar,
and that only a small portion of the command is used to control HTTPie and
doesn't directly correspond to any part of the request (here it's only ``-f``
asking HTTPie to send a form request).

The two modes, ``--pretty=all`` (default for terminal) and ``--pretty=none``
(default for redirected output), allow for both user-friendly interactive use
and usage from scripts, where HTTPie serves as a generic HTTP client.

As HTTPie is still under heavy development, the existing command line
syntax and some of the ``--OPTIONS`` may change slightly before
HTTPie reaches its final version ``1.0``. All changes are recorded in the
`change log`_.



User support
------------

Please use the following support channels:

* `GitHub issues <https://github.com/jkbr/httpie/issues>`_
  for bug reports and feature requests.
* `Our Gitter chat room <https://gitter.im/jkbrzt/httpie>`_
  to ask questions, discuss features, and for general discussion.
* `StackOverflow <https://stackoverflow.com>`_
  to ask questions (please make sure to use the
  `httpie <http://stackoverflow.com/questions/tagged/httpie>`_ tag).
* Tweet directly to `@clihttp <https://twitter.com/clihttp>`_.
* You can also tweet directly to `@jakubroztocil`_.


Related projects
----------------

Dependencies
~~~~~~~~~~~~

Under the hood, HTTPie uses these two amazing libraries:

* `Requests <http://python-requests.org>`_
  — Python HTTP library for humans
* `Pygments <http://pygments.org/>`_
  — Python syntax highlighter


HTTPie friends
~~~~~~~~~~~~~~

HTTPie plays exceptionally well with the following tools:

* `jq <https://stedolan.github.io/jq/>`_
  — CLI JSON processor that
  works great in conjunction with HTTPie
* `http-prompt <https://github.com/eliangcs/http-prompt>`_
  —  interactive shell for HTTPie featuring autocomplete
  and command syntax highlighting


Alternatives
~~~~~~~~~~~~

* `httpcat <https://github.com/jakubroztocil/httpcat>`_ — a lower-level sister utility
  of HTTPie for constructing raw HTTP requests on the command line.
* `curl <https://curl.haxx.se>`_ — a "Swiss knife" command line tool and
  an exceptional library for transferring data with URLs.


Contributing
------------

See `CONTRIBUTING.rst <https://github.com/jakubroztocil/httpie/blob/master/CONTRIBUTING.rst>`_.


Change log
----------

See `CHANGELOG <https://github.com/jakubroztocil/httpie/blob/master/CHANGELOG.rst>`_.


Artwork
-------

* `Logo <https://github.com/claudiatd/httpie-artwork>`_ by `Cláudia Delgado <https://github.com/claudiatd>`_.
* `Animation <https://raw.githubusercontent.com/jakubroztocil/httpie/master/httpie.gif>`_ by `Allen Smith <https://github.com/loranallensmith>`_ of GitHub.



Licence
-------

BSD-3-Clause: `LICENSE <https://github.com/jakubroztocil/httpie/blob/master/LICENSE>`_.



Authors
-------

`Jakub Roztocil`_  (`@jakubroztocil`_) created HTTPie and `these fine people`_
have contributed.


.. _pip: https://pip.pypa.io/en/stable/installing/
.. _Github API: http://developer.github.com/v3/issues/comments/#create-a-comment
.. _these fine people: https://github.com/jakubroztocil/httpie/contributors
.. _Jakub Roztocil: https://roztocil.co
.. _@jakubroztocil: https://twitter.com/jakubroztocil


.. |pypi| image:: https://img.shields.io/pypi/v/httpie.svg?style=flat-square&label=latest%20stable%20version
    :target: https://pypi.python.org/pypi/httpie
    :alt: Latest version released on PyPi

.. |coverage| image:: https://img.shields.io/coveralls/jakubroztocil/httpie/master.svg?style=flat-square&label=coverage
    :target: https://coveralls.io/r/jakubroztocil/httpie?branch=master
    :alt: Test coverage

.. |unix_build| image:: https://img.shields.io/travis/jakubroztocil/httpie/master.svg?style=flat-square&label=unix%20build
    :target: http://travis-ci.org/jakubroztocil/httpie
    :alt: Build status of the master branch on Mac/Linux

.. |gitter| image:: https://img.shields.io/gitter/room/jkbrzt/httpie.svg?style=flat-square
    :target: https://gitter.im/jkbrzt/httpie
    :alt: Chat on Gitter



  * [httplib2-0.13.1](https://github.com/httplib2/httplib2) 

A comprehensive HTTP client library, ``httplib2`` supports many features left out of other HTTP libraries.

**HTTP and HTTPS**
  HTTPS support is only available if the socket module was compiled with SSL support.


**Keep-Alive**
  Supports HTTP 1.1 Keep-Alive, keeping the socket open and performing multiple requests over the same connection if possible.


**Authentication**
  The following three types of HTTP Authentication are supported. These can be used over both HTTP and HTTPS.

  * Digest
  * Basic
  * WSSE

**Caching**
  The module can optionally operate with a private cache that understands the Cache-Control:
  header and uses both the ETag and Last-Modified cache validators. Both file system
  and memcached based caches are supported.


**All Methods**
  The module can handle any HTTP request method, not just GET and POST.


**Redirects**
  Automatically follows 3XX redirects on GETs.


**Compression**
  Handles both 'deflate' and 'gzip' types of compression.


**Lost update support**
  Automatically adds back ETags into PUT requests to resources we have already cached. This implements Section 3.2 of Detecting the Lost Update Problem Using Unreserved Checkout


**Unit Tested**
  A large and growing set of unit tests.



  * [humanfriendly-4.18](https://humanfriendly.readthedocs.io) humanfriendly: Human friendly input/output in Python
====================================================

.. image:: https://travis-ci.org/xolox/python-humanfriendly.svg?branch=master
   :target: https://travis-ci.org/xolox/python-humanfriendly

.. image:: https://coveralls.io/repos/xolox/python-humanfriendly/badge.png?branch=master
   :target: https://coveralls.io/r/xolox/python-humanfriendly?branch=master

The functions and classes in the `humanfriendly` package can be used to make
text interfaces more user friendly. Some example features:

- Parsing and formatting numbers, file sizes, pathnames and timespans in
  simple, human friendly formats.

- Easy to use timers for long running operations, with human friendly
  formatting of the resulting timespans.

- Prompting the user to select a choice from a list of options by typing the
  option's number or a unique substring of the option.

- Terminal interaction including text styling (ANSI escape sequences), user
  friendly rendering of usage messages and querying the terminal for its
  size.

The `humanfriendly` package is currently tested on Python 2.6, 2.7, 3.4, 3.5,
3.6, 3.7 and PyPy (2.7) on Linux and Mac OS X. While the intention is to
support Windows as well, you may encounter some rough edges.

.. contents::
   :local:

Getting started
---------------

It's very simple to start using the `humanfriendly` package::

   >>> import humanfriendly
   >>> user_input = raw_input("Enter a readable file size: ")
   Enter a readable file size: 16G
   >>> num_bytes = humanfriendly.parse_size(user_input)
   >>> print num_bytes
   16000000000
   >>> print "You entered:", humanfriendly.format_size(num_bytes)
   You entered: 16 GB
   >>> print "You entered:", humanfriendly.format_size(num_bytes, binary=True)
   You entered: 14.9 GiB

Command line
------------

.. A DRY solution to avoid duplication of the `humanfriendly --help' text:
..
.. [[[cog
.. from humanfriendly.usage import inject_usage
.. inject_usage('humanfriendly.cli')
.. ]]]

**Usage:** `humanfriendly [OPTIONS]`

Human friendly input/output (text formatting) on the command
line based on the Python package with the same name.

**Supported options:**

.. csv-table::
   :header: Option, Description
   :widths: 30, 70


   "``-c``, ``--run-command``","Execute an external command (given as the positional arguments) and render
   a spinner and timer while the command is running. The exit status of the
   command is propagated."
   ``--format-table``,"Read tabular data from standard input (each line is a row and each
   whitespace separated field is a column), format the data as a table and
   print the resulting table to standard output. See also the ``--delimiter``
   option."
   "``-d``, ``--delimiter=VALUE``","Change the delimiter used by ``--format-table`` to ``VALUE`` (a string). By default
   all whitespace is treated as a delimiter."
   "``-l``, ``--format-length=LENGTH``","Convert a length count (given as the integer or float ``LENGTH``) into a human
   readable string and print that string to standard output."
   "``-n``, ``--format-number=VALUE``","Format a number (given as the integer or floating point number ``VALUE``) with
   thousands separators and two decimal places (if needed) and print the
   formatted number to standard output."
   "``-s``, ``--format-size=BYTES``","Convert a byte count (given as the integer ``BYTES``) into a human readable
   string and print that string to standard output."
   "``-b``, ``--binary``","Change the output of ``-s``, ``--format-size`` to use binary multiples of bytes
   (base-2) instead of the default decimal multiples of bytes (base-10)."
   "``-t``, ``--format-timespan=SECONDS``","Convert a number of seconds (given as the floating point number ``SECONDS``)
   into a human readable timespan and print that string to standard output."
   ``--parse-length=VALUE``,"Parse a human readable length (given as the string ``VALUE``) and print the
   number of metres to standard output."
   ``--parse-size=VALUE``,"Parse a human readable data size (given as the string ``VALUE``) and print the
   number of bytes to standard output."
   ``--demo``,"Demonstrate changing the style and color of the terminal font using ANSI
   escape sequences."
   "``-h``, ``--help``",Show this message and exit.

.. [[[end]]]

A note about size units
-----------------------

When I originally published the `humanfriendly` package I went with binary
multiples of bytes (powers of two). It was pointed out several times that this
was a poor choice (see issue `#4`_ and pull requests `#8`_ and `#9`_) and thus
the new default became decimal multiples of bytes (powers of ten):

+------+---------------+---------------+
| Unit | Binary value  | Decimal value |
+------+---------------+---------------+
| KB   |          1024 |          1000 +
+------+---------------+---------------+
| MB   |       1048576 |       1000000 |
+------+---------------+---------------+
| GB   |    1073741824 |    1000000000 |
+------+---------------+---------------+
| TB   | 1099511627776 | 1000000000000 |
+------+---------------+---------------+
| etc  |               |               |
+------+---------------+---------------+

The option to use binary multiples of bytes remains by passing the keyword
argument `binary=True` to the `format_size()`_ and `parse_size()`_ functions.

Contact
-------

The latest version of `humanfriendly` is available on PyPI_ and GitHub_. The
documentation is hosted on `Read the Docs`_ and includes a changelog_. For bug
reports please create an issue on GitHub_. If you have questions, suggestions,
etc. feel free to send me an e-mail at `peter@peterodding.com`_.

License
-------

This software is licensed under the `MIT license`_.

© 2018 Peter Odding.

.. External references:
.. _#4: https://github.com/xolox/python-humanfriendly/issues/4
.. _#8: https://github.com/xolox/python-humanfriendly/pull/8
.. _#9: https://github.com/xolox/python-humanfriendly/pull/9
.. _changelog: https://humanfriendly.readthedocs.io/en/latest/changelog.html
.. _format_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.format_size
.. _GitHub: https://github.com/xolox/python-humanfriendly
.. _MIT license: http://en.wikipedia.org/wiki/MIT_License
.. _parse_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.parse_size
.. _peter@peterodding.com: peter@peterodding.com
.. _PyPI: https://pypi.python.org/pypi/humanfriendly
.. _Read the Docs: https://humanfriendly.readthedocs.io



  * [hunter-3.0.1](https://github.com/ionelmc/python-hunter) ========
Overview
========



Hunter is a flexible code tracing toolkit, not for measuring coverage, but for debugging, logging, inspection and other
nefarious purposes. It has a `simple Python API <https://python-hunter.readthedocs.io/en/latest/introduction.html>`_,
a `convenient terminal API <environment-variable-activation>`_ and
a `CLI tool to attach to processes <tracing-processes>`_.

* Free software: BSD 2-Clause License

Installation
============

::

    pip install hunter

Documentation
=============


https://python-hunter.readthedocs.io/

Overview
========

Basic use involves passing various filters to the ``trace`` option. An example:

.. sourcecode:: python

    import hunter
    hunter.trace(module='posixpath', action=hunter.CallPrinter)

    import os
    os.path.join('a', 'b')

That would result in:

.. sourcecode:: pycon

    >>> os.path.join('a', 'b')
             /usr/lib/python3.6/posixpath.py:75    call      => join(a='a')
             /usr/lib/python3.6/posixpath.py:80    line         a = os.fspath(a)
             /usr/lib/python3.6/posixpath.py:81    line         sep = _get_sep(a)
             /usr/lib/python3.6/posixpath.py:41    call         => _get_sep(path='a')
             /usr/lib/python3.6/posixpath.py:42    line            if isinstance(path, bytes):
             /usr/lib/python3.6/posixpath.py:45    line            return '/'
             /usr/lib/python3.6/posixpath.py:45    return       <= _get_sep: '/'
             /usr/lib/python3.6/posixpath.py:82    line         path = a
             /usr/lib/python3.6/posixpath.py:83    line         try:
             /usr/lib/python3.6/posixpath.py:84    line         if not p:
             /usr/lib/python3.6/posixpath.py:86    line         for b in map(os.fspath, p):
             /usr/lib/python3.6/posixpath.py:87    line         if b.startswith(sep):
             /usr/lib/python3.6/posixpath.py:89    line         elif not path or path.endswith(sep):
             /usr/lib/python3.6/posixpath.py:92    line         path += sep + b
             /usr/lib/python3.6/posixpath.py:86    line         for b in map(os.fspath, p):
             /usr/lib/python3.6/posixpath.py:96    line         return path
             /usr/lib/python3.6/posixpath.py:96    return    <= join: 'a/b'
    'a/b'

In a terminal it would look like:

.. image:: https://raw.githubusercontent.com/ionelmc/python-hunter/master/docs/code-trace.png


Actions
-------

Output format can be controlled with "actions". There's an alternative ``CodePrinter`` action that doesn't handle
nesting (it was the default action until Hunter 2.0).

If filters match then action will be run. Example:

.. sourcecode:: python

    import hunter
    hunter.trace(module='posixpath', action=hunter.CodePrinter)

    import os
    os.path.join('a', 'b')

That would result in:

.. sourcecode:: pycon

    >>> os.path.join('a', 'b')
             /usr/lib/python3.6/posixpath.py:75    call      def join(a, *p):
             /usr/lib/python3.6/posixpath.py:80    line          a = os.fspath(a)
             /usr/lib/python3.6/posixpath.py:81    line          sep = _get_sep(a)
             /usr/lib/python3.6/posixpath.py:41    call      def _get_sep(path):
             /usr/lib/python3.6/posixpath.py:42    line          if isinstance(path, bytes):
             /usr/lib/python3.6/posixpath.py:45    line              return '/'
             /usr/lib/python3.6/posixpath.py:45    return            return '/'
                                                   ...       return value: '/'
             /usr/lib/python3.6/posixpath.py:82    line          path = a
             /usr/lib/python3.6/posixpath.py:83    line          try:
             /usr/lib/python3.6/posixpath.py:84    line              if not p:
             /usr/lib/python3.6/posixpath.py:86    line              for b in map(os.fspath, p):
             /usr/lib/python3.6/posixpath.py:87    line                  if b.startswith(sep):
             /usr/lib/python3.6/posixpath.py:89    line                  elif not path or path.endswith(sep):
             /usr/lib/python3.6/posixpath.py:92    line                      path += sep + b
             /usr/lib/python3.6/posixpath.py:86    line              for b in map(os.fspath, p):
             /usr/lib/python3.6/posixpath.py:96    line          return path
             /usr/lib/python3.6/posixpath.py:96    return        return path
                                                   ...       return value: 'a/b'
    'a/b'

- or in a terminal:

.. image:: https://raw.githubusercontent.com/ionelmc/python-hunter/master/docs/simple-trace.png

------

Another useful action is the ``VarsPrinter``:

.. sourcecode:: python

    import hunter
    # note that this kind of invocation will also use the default `CallPrinter` action
    hunter.trace(hunter.Q(module='posixpath', action=hunter.VarsPrinter('path')))

    import os
    os.path.join('a', 'b')

That would result in:

.. sourcecode:: pycon

    >>> os.path.join('a', 'b')
         /usr/lib/python3.6/posixpath.py:75    call      => join(a='a')
         /usr/lib/python3.6/posixpath.py:80    line         a = os.fspath(a)
         /usr/lib/python3.6/posixpath.py:81    line         sep = _get_sep(a)
         /usr/lib/python3.6/posixpath.py:41    call      [path => 'a']
         /usr/lib/python3.6/posixpath.py:41    call         => _get_sep(path='a')
         /usr/lib/python3.6/posixpath.py:42    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:42    line            if isinstance(path, bytes):
         /usr/lib/python3.6/posixpath.py:45    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:45    line            return '/'
         /usr/lib/python3.6/posixpath.py:45    return    [path => 'a']
         /usr/lib/python3.6/posixpath.py:45    return       <= _get_sep: '/'
         /usr/lib/python3.6/posixpath.py:82    line         path = a
         /usr/lib/python3.6/posixpath.py:83    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:83    line         try:
         /usr/lib/python3.6/posixpath.py:84    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:84    line         if not p:
         /usr/lib/python3.6/posixpath.py:86    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:86    line         for b in map(os.fspath, p):
         /usr/lib/python3.6/posixpath.py:87    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:87    line         if b.startswith(sep):
         /usr/lib/python3.6/posixpath.py:89    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:89    line         elif not path or path.endswith(sep):
         /usr/lib/python3.6/posixpath.py:92    line      [path => 'a']
         /usr/lib/python3.6/posixpath.py:92    line         path += sep + b
         /usr/lib/python3.6/posixpath.py:86    line      [path => 'a/b']
         /usr/lib/python3.6/posixpath.py:86    line         for b in map(os.fspath, p):
         /usr/lib/python3.6/posixpath.py:96    line      [path => 'a/b']
         /usr/lib/python3.6/posixpath.py:96    line         return path
         /usr/lib/python3.6/posixpath.py:96    return    [path => 'a/b']
         /usr/lib/python3.6/posixpath.py:96    return    <= join: 'a/b'
    'a/b'

In a terminal it would look like:

.. image:: https://raw.githubusercontent.com/ionelmc/python-hunter/master/docs/vars-trace.png

-----

You can give it a tree-like configuration where you can optionally configure specific actions for parts of the
tree (like dumping variables or a pdb set_trace):

.. sourcecode:: python

    from hunter import trace, Q, Debugger
    from pdb import Pdb

    trace(
        # drop into a Pdb session if ``foo.bar()`` is called
        Q(module="foo", function="bar", kind="call", action=Debugger(klass=Pdb))
        |  # or
        Q(
            # show code that contains "mumbo.jumbo" on the current line
            lambda event: event.locals.get("mumbo") == "jumbo",
            # and it's not in Python's stdlib
            stdlib=False,
            # and it contains "mumbo" on the current line
            source__contains="mumbo"
        )
    )

    import foo
    foo.func()

With a ``foo.py`` like this:

.. sourcecode:: python

    def bar():
        execution_will_get_stopped  # cause we get a Pdb session here

    def func():
        mumbo = 1
        mumbo = "jumbo"
        print("not shown in trace")
        print(mumbo)
        mumbo = 2
        print(mumbo) # not shown in trace
        bar()


We get:

.. sourcecode:: pycon

    >>> foo.func()
    not shown in trace
        /home/ionel/osp/python-hunter/foo.py:8     line          print(mumbo)
    jumbo
        /home/ionel/osp/python-hunter/foo.py:9     line          mumbo = 2
    2
        /home/ionel/osp/python-hunter/foo.py:1     call      def bar():
    > /home/ionel/osp/python-hunter/foo.py(2)bar()
    -> execution_will_get_stopped  # cause we get a Pdb session here
    (Pdb)

In a terminal it would look like:

.. image:: https://raw.githubusercontent.com/ionelmc/python-hunter/master/docs/tree-trace.png

.. _tracing-processes:

Tracing processes
-----------------

In similar fashion to ``strace`` Hunter can trace other processes, eg::

    hunter-trace --gdb -p 123

If you wanna play it safe (no messy GDB) then add this in your code::

    from hunter import remote
    remote.install()

Then you can do::

    hunter-trace -p 123

See `docs on the remote feature <https://python-hunter.readthedocs.org/en/latest/remote.html>`_.

**Note:** Windows ain't supported.

.. _environment-variable-activation:

Environment variable activation
-------------------------------

For your convenience environment variable activation is available. Just run your app like this::


    PYTHONHUNTER="module='os.path'" python yourapp.py

On Windows you'd do something like::

    set PYTHONHUNTER=module='os.path'
    python yourapp.py

The activation works with a clever ``.pth`` file that checks for that env var presence and before your app runs does something
like this::

    from hunter import *
    trace(<whatever-you-had-in-the-PYTHONHUNTER-env-var>)

Note that Hunter is activated even if the env var is empty, eg: ``PYTHONHUNTER=""``.

Environment variable configuration
``````````````````````````````````

Sometimes you always use the same options (like ``stdlib=False`` or ``force_colors=True``). To save typing you can
set something like this in your environment::

    PYTHONHUNTERCONFIG="stdlib=False,force_colors=True"

This is the same as ``PYTHONHUNTER="stdlib=False,action=CallPrinter(force_colors=True)"``.

Notes:

* Setting ``PYTHONHUNTERCONFIG`` alone doesn't activate hunter.
* All the options for the builtin actions are supported.
* Although using predicates is supported it can be problematic. Example of setup that won't trace anything::

    PYTHONHUNTERCONFIG="Q(module_startswith='django')"
    PYTHONHUNTER="Q(module_startswith='celery')"

  which is the equivalent of::

    PYTHONHUNTER="Q(module_startswith='django'),Q(module_startswith='celery')"

  which is the equivalent of::

    PYTHONHUNTER="Q(module_startswith='django')&Q(module_startswith='celery')"



Filtering DSL
-------------

Hunter supports a flexible query DSL, see the `introduction
<https://python-hunter.readthedocs.org/en/latest/introduction.html>`_.

Development
===========

To run the all tests run::

    tox


FAQ
===

Why not Smiley?
---------------

There's some obvious overlap with `smiley <https://pypi.python.org/pypi/smiley>`_ but there are few fundamental differences:

* Complexity. Smiley is simply over-engineered:

  * It uses IPC and a SQL database.
  * It has a webserver. Lots of dependencies.
  * It uses threads. Side-effects and subtle bugs are introduced in your code.
  * It records everything. Tries to dump any variable. Often fails and stops working.

  Why do you need all that just to debug some stuff in a terminal? Simply put, it's a nice idea but the design choices work
  against you when you're already neck-deep into debugging your own code. In my experience Smiley has been very buggy and
  unreliable. Your mileage may vary of course.

* Tracing long running code. This will make Smiley record lots of data, making it unusable.

  Now because Smiley records everything, you'd think it's better suited for short programs. But alas, if your program runs
  quickly then it's pointless to record the execution. You can just run it again.

  It seems there's only one situation where it's reasonable to use Smiley: tracing io-bound apps remotely. Those apps don't
  execute lots of code, they just wait on network so Smiley's storage won't blow out of proportion and tracing overhead might
  be acceptable.
* Use-cases. It seems to me Smiley's purpose is not really debugging code, but more of a "non interactive monitoring" tool.

In contrast, Hunter is very simple:

* Few dependencies.
* Low overhead (tracing/filtering code has an optional Cython extension).
* No storage. This simplifies lots of things.

  The only cost is that you might need to run the code multiple times to get the filtering/actions right. This means Hunter is
  not really suited for "post-mortem" debugging. If you can't reproduce the problem anymore then Hunter won't be of much help.

Why not pytrace?
----------------

`Pytrace <https://pypi.python.org/pypi/pytrace>`_ is another tracer tool. It seems quite similar to Smiley - it uses a sqlite
database for the events, threads and IPC.

TODO: Expand this.

Why (not) coverage?
-------------------

For purposes of debugging `coverage <https://pypi.python.org/pypi/coverage>`_ is a great tool but only as far as "debugging
by looking at what code is (not) run". Checking branch coverage is good but it will only get you as far.

From the other perspective, you'd be wondering if you could use Hunter to measure coverage-like things. You could do it but
for that purpose Hunter is very "rough": it has no builtin storage. You'd have to implement your own storage. You can do it
but it wouldn't give you any advantage over making your own tracer if you don't need to "pre-filter" whatever you're
recording.

In other words, filtering events is the main selling point of Hunter - it's fast (cython implementation) and the query API is
flexible enough.


Changelog
=========

3.0.2 (2019-10-10)
------------------

* Fixed setting ``stream`` from ``PYTHONHUNTERCONFIG`` environment variable.
* Fixed a couple minor documentation issues.

3.0.1 (2019-06-17)
------------------

* Fixed issue with coloring missing source message (coloring leaked into next line).

3.0.0 (2019-06-17)
------------------

* The package now uses setuptools-scm for development builds (available at https://test.pypi.org/project/hunter/). As a
  consequence installing the sdist will download setuptools-scm.
* Recompiled cython modules with latest Cython. Hunter can be installed without any Cython, as before.
* Refactored some of the cython modules to have more typing information and not use deprecated property syntax.
* Replaced ``unsafe_repr`` option with ``repr_func``. Now you can use your custom repr function in the builtin actions.
  **BACKWARDS INCOMPATIBLE**
* Fixed buggy filename handling when using Hunter in ipython/jupyter. Source code should be properly displayed now.
* Removed ``globals`` option from ``VarsPrinter`` action. Globals are now always looked up. **BACKWARDS INCOMPATIBLE**
* Added support for locals in ``VarsPrinter`` action. Now you can do ``VarsPrinter('len(foobar)')``.
* Always pass module_globals dict to linecache methods. Source code from PEP-302 loaders is now printed properly.
  Contributed by Mikhail Borisov in `#65 <https://github.com/ionelmc/python-hunter/pull/65>`_.
* Various code cleanup, style and docstring fixing.
* Added ``hunter.From`` helper to allow passing in filters directly as keyword arguments.
* Added ``hunter.event.Event.detach`` for storing events without leaks or side-effects (due to prolonged references
  to Frame objects, local or global variables).
* Refactored the internals of actions for easier subclassing.

  Added the
  ``hunter.actions.ColorStreamAction.filename_prefix``,
  ``hunter.actions.ColorStreamAction.output``,
  ``hunter.actions.ColorStreamAction.pid_prefix``,
  ``hunter.actions.ColorStreamAction.thread_prefix``,
  ``hunter.actions.ColorStreamAction.try_repr`` and
  ``hunter.actions.ColorStreamAction.try_source`` methods
  to the ``hunter.actions.ColorStreamAction`` baseclass.
* Added ``hunter.actions.VarsSnooper`` - a PySnooper-inspired variant of ``hunter.actions.VarsPrinter``. It
  will record and show variable changes, with the risk of leaking or using too much memory of course :)
* Fixed tracers to log error and automatically stop if there's an internal failure. Previously error may have been
  silently dropped in some situations.

2.2.1 (2019-01-19)
------------------

* Fixed a link in changelog.
* Fixed some issues in the Travis configuration.

2.2.0 (2019-01-19)
------------------

* Added ``hunter.predicates.From`` predicate for tracing from a specific point. It stop after returning back to the
  same call depth with a configurable offset.
* Fixed ``PYTHONHUNTERCONFIG`` not working in some situations (config values were resolved at the wrong time).
* Made tests in CI test the wheel that will eventually be published to PyPI
  (`tox-wheel <https://pypi.org/project/tox-wheel/>`_).
* Made ``event.stdlib`` more reliable: ``pkg_resources`` is considered part of stdlib and few more paths will be
  considered as stdlib.
* Dumbed down the ``get_peercred`` check that is done when attaching with ``hunter-trace`` CLI (via
  ``hunter.remote.install()``). It will be slightly insecure but will work on OSX.
* Added OSX in the Travis test grid.

2.1.0 (2018-11-17)
------------------

* Made ``threading_support`` on by default but output automatic (also, now ``1`` or ``0`` allowed).
* Added ``pid_alignment`` and ``force_pid`` action options to show a pid prefix.
* Fixed some bugs around ``__eq__`` in various classes.
* Dropped Python 3.3 support.
* Dropped dependency on `fields <https://python-fields.readthedocs.io/en/stable/>`_.
* Actions now repr using a simplified implementation that tries to avoid calling ``__repr__`` on user classes in order
  to avoid creating side-effects while tracing.
* Added support for the ``PYTHONHUNTERCONFIG`` environment variable (stores defaults and doesn't activate hunter).

2.0.2 (2017-11-24)
------------------

* Fixed indentation in ``hunter.actions.CallPrinter`` action (shouldn't deindent on exception).
* Fixed option filtering in Cython Query implementation (filtering on ``tracer`` was allowed by mistake).
* Various fixes to docstrings and docs.

2.0.1 (2017-09-09)
------------------

* Now ``Py_AddPendingCall`` is used instead of acquiring the GIL (when using GDB).

2.0.0 (2017-09-02)
------------------

* Added the ``hunter.event.Event.count`` and ``hunter.event.Event.calls``` attributes.
* Added the ``lt``/``lte``/``gt``/``gte`` lookups.
* Added convenience aliases for ``startswith`` (``sw``), ``endswith`` (``ew``), ``contains`` (``has``)
  and ``regex`` (``rx``).
* Added a convenience ``hunter.wrap`` decorator to start tracing around a function.
* Added support for remote tracing (with two backends: `manhole <https://pypi.python.org/pypi/manhole>`__ and GDB) via
  the ``hunter-trace`` bin. Note: **Windows is NOT SUPPORTED**.
* Changed the default action to ``hunter.actions.CallPrinter``.
  You'll need to use ``action=CodePrinter`` if you want the old output.

1.4.1 (2016-09-24)
------------------

* Fix support for getting sources for Cython module (it was broken on Windows and Python3.5+).

1.4.0 (2016-09-24)
------------------

* Added support for tracing Cython modules (`#30 <https://github.com/ionelmc/python-hunter/issues/30>`_). A
  `# cython: linetrace=True` stanza or equivalent is required in Cython modules for this to work.

1.3.0 (2016-04-14)
------------------

* Added ``hunter.event.Event.thread``.
* Added ``hunter.event.Event.threadid`` and ``hunter.event.Event.threadname``
  (available for filtering with ``hunter.Q``).
* Added ``hunter.event.Event.threading_support`` argument to ``hunter.trace``.
  It makes new threads be traced and changes action output to include thread name.
* Added support for using `pdb++ <https://pypi.python.org/pypi/pdbpp>`_ in the ``hunter.actions.Debugger`` action.
* Added support for using `manhole <https://pypi.python.org/pypi/manhole>`_ via a new ``hunter.actions.Manhole``
  action.
* Made the ``hunter.event.Event.handler`` a public but readonly property.


1.2.2 (2016-01-28)
------------------

* Fix broken import. Require `fields>=4.0`.
* Simplify a string check in Cython code.

1.2.1 (2016-01-27)
------------------

* Fix "KeyError: 'normal'" bug in ``hunter.actions.CallPrinter``. Create the NO_COLORS dict from the COLOR dicts.
  Some keys were missing.

1.2.0 (2016-01-24)
------------------

* Fixed printouts of objects that return very large string in ``__repr__()``. Trimmed to 512. Configurable in actions
  with the ``repr_limit`` option.
* Improved validation of ``hunter.actions.VarsPrinter``'s initializer.
* Added a ``hunter.actions.CallPrinter`` action.

1.1.0 (2016-01-21)
------------------

* Implemented a destructor (``__dealloc__``) for the Cython tracer.
* Improved the restoring of the previous tracer in the Cython tracer (use ``PyEval_SetTrace``) directly.
* Removed ``tracer`` as an allowed filtering argument in ``hunter.Query``.
* Add basic validation (must be callable) for positional arguments and actions passed into ``hunter.Q``. Closes
  `#23 <https://github.com/ionelmc/python-hunter/issues/23>`_.
* Fixed ``stdlib`` checks (wasn't very reliable). Closes `#24 <https://github.com/ionelmc/python-hunter/issues/24>`_.

1.0.2 (2016-01-05)
------------------

* Fixed missing import in ``setup.py``.

1.0.1 (2015-12-24)
------------------

* Fix a compile issue with the MSVC compiler (seems it don't like the inline option on the ``fast_When_call``).

1.0.0 (2015-12-24)
------------------

* Implemented fast tracer and query objects in Cython. **MAY BE BACKWARDS INCOMPATIBLE**

  To force using the old pure-python implementation set the ``PUREPYTHONHUNTER`` environment variable to non-empty value.
* Added filtering operators: ``contains``, ``startswith``, ``endswith`` and ``in``. Examples:

  * ``Q(module_startswith='foo'`` will match events from ``foo``, ``foo.bar`` and ``foobar``.
  * ``Q(module_startswith=['foo', 'bar']`` will match events from ``foo``, ``foo.bar``, ``foobar``, ``bar``, ``bar.foo`` and ``baroo`` .
  * ``Q(module_endswith='bar'`` will match events from ``foo.bar`` and ``foobar``.
  * ``Q(module_contains='ip'`` will match events from ``lipsum``.
  * ``Q(module_in=['foo', 'bar']`` will match events from ``foo`` and ``bar``.
  * ``Q(module_regex=r"(re|sre.*)\b") will match events from ``re``, ``re.foobar``, ``srefoobar`` but not from ``repr``.

* Removed the ``merge`` option. Now when you call ``hunter.trace(...)`` multiple times only the last one is active.
  **BACKWARDS INCOMPATIBLE**
* Remove the `previous_tracer handling`. Now when you call ``hunter.trace(...)`` the previous tracer (whatever was in
  ``sys.gettrace()``) is disabled and restored when ``hunter.stop()`` is called. **BACKWARDS INCOMPATIBLE**
* Fixed ``CodePrinter`` to show module name if it fails to get any sources.

0.6.0 (2015-10-10)
------------------

* Added a ``clear_env_var`` option on the tracer (disables tracing in subprocess).
* Added ``force_colors`` option on ``hunter.actions.VarsPrinter`` and ``hunter.actions.CodePrinter``.
* Allowed setting the `stream` to a file name (option on ``hunter.actions.VarsPrinter`` and
  ``hunter.actions.CodePrinter``).
* Bumped up the filename alignment to 40 cols.
* If not merging then `self` is not kept as a previous tracer anymore.
  Closes `#16 <https://github.com/ionelmc/python-hunter/issues/16>`_.
* Fixed handling in VarsPrinter: properly print eval errors and don't try to show anything if there's an AttributeError.
  Closes `#18 <https://github.com/ionelmc/python-hunter/issues/18>`_.
* Added a ``stdlib`` boolean flag (for filtering purposes).
  Closes `#15 <https://github.com/ionelmc/python-hunter/issues/15>`_.
* Fixed broken frames that have "None" for filename or module (so they can still be treated as strings).
* Corrected output files in the ``install_lib`` command so that pip can uninstall the pth file.
  This only works when it's installed with pip (sadly, ``setup.py install/develop`` and ``pip install -e`` will still
  leave pth garbage on ``pip uninstall hunter``).

0.5.1 (2015-04-15)
------------------

* Fixed ``hunter.event.Event.globals`` to actually be the dict of global vars (it was just the locals).

0.5.0 (2015-04-06)
------------------

* Fixed ``hunter.And`` and ``hunter.Or`` "single argument unwrapping".
* Implemented predicate compression. Example: ``Or(Or(a, b), c)`` is converted to ``Or(a, b, c)``.
* Renamed ``hunter.event.Event.source`` to ``hunter.event.Event.fullsource``.
* Added ``hunter.event.Event.source`` that doesn't do any fancy sourcecode tokenization.
* Fixed ``hunter.event.Event.fullsource`` return value for situations where the tokenizer would fail.
* Made the print function available in the ``PYTHONHUNTER`` env var payload.
* Added a __repr__ for ``hunter.event.Event``.

0.4.0 (2015-03-29)
------------------

* Disabled colors for Jython.
  Contributed by Claudiu Popa in `#12 <https://github.com/ionelmc/python-hunter/pull/12>`_.
* Test suite fixes for Windows.
  Contributed by Claudiu Popa in `#11 <https://github.com/ionelmc/python-hunter/pull/11>`_.
* Added an introduction section in the docs.
* Implemented a prettier fallback for when no sources are available for that frame.
* Implemented fixups in cases where you use action classes as a predicates.

0.3.1 (2015-03-29)
------------------

* Forgot to merge some commits ...

0.3.0 (2015-03-29)
------------------

* Added handling for internal repr failures.
* Fixed issues with displaying code that has non-ascii characters.
* Implemented better display for ``call`` frames so that when a function has decorators the
  function definition is shown (instead of just the first decorator).
  See: `#8 <https://github.com/ionelmc/python-hunter/issues/8>`_.

0.2.1 (2015-03-28)
------------------

* Added missing color entry for exception events.
* Added ``hunter.event.Event.line`` property. It returns the source code for the line being run.

0.2.0 (2015-03-27)
------------------

* Added color support (and ``colorama`` as dependency).
* Added support for expressions in ``hunter.actions.VarsPrinter``.
* Breaking changes:

  * Renamed ``F`` to ``hunter.Q``. And ``hunter.Q`` is now just a convenience wrapper for
    ``hunter.predicates.Query``.
  * Renamed the ``PYTHON_HUNTER`` env variable to ``PYTHONHUNTER``.
  * Changed ``hunter.predicates.When`` to take positional arguments.
  * Changed output to show 2 path components (still not configurable).
  * Changed ``hunter.actions.VarsPrinter`` to take positional arguments for the names.
* Improved error reporting for env variable activation (``PYTHONHUNTER``).
* Fixed env var activator (the ``.pth`` file) installation with ``setup.py install`` (the "egg installs") and
  ``setup.py develop``/``pip install -e`` (the "egg links").

0.1.0 (2015-03-22)
------------------

* First release on PyPI.



  * [hyperlink-19.0.0](https://github.com/python-hyper/hyperlink) The humble, but powerful, URL runs everything around us. Chances
are you've used several just to read this text.

Hyperlink is a featureful, pure-Python implementation of the URL, with
an emphasis on correctness. BSD licensed.

See the docs at http://hyperlink.readthedocs.io.

  * [hyperopt-0.1.2](http://hyperopt.github.com/hyperopt/) 
  * [hypothesis-4.32.3](https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python) ==========
Hypothesis
==========

Hypothesis is an advanced testing library for Python. It lets you write tests which
are parametrized by a source of examples, and then generates simple and comprehensible
examples that make your tests fail. This lets you find more bugs in your code with less
work.

e.g.

.. code-block:: python

  @given(st.lists(
    st.floats(allow_nan=False, allow_infinity=False), min_size=1))
  def test_mean(xs):
      assert min(xs) <= mean(xs) <= max(xs)

.. code-block::

  Falsifying example: test_mean(
    xs=[1.7976321109618856e+308, 6.102390043022755e+303]
  )

Hypothesis is extremely practical and advances the state of the art of
unit testing by some way. It's easy to use, stable, and powerful. If
you're not using Hypothesis to test your project then you're missing out.

------------------------
Quick Start/Installation
------------------------
If you just want to get started:

.. code-block::

  pip install hypothesis


-----------------
Links of interest
-----------------

The main Hypothesis site is at `hypothesis.works <https://hypothesis.works/>`_, and contains a lot
of good introductory and explanatory material.

Extensive documentation and examples of usage are `available at readthedocs <https://hypothesis.readthedocs.io/en/latest/>`_.

If you want to talk to people about using Hypothesis, `we have both an IRC channel
and a mailing list <https://hypothesis.readthedocs.io/en/latest/community.html>`_.

If you want to receive occasional updates about Hypothesis, including useful tips and tricks, there's a
`TinyLetter mailing list to sign up for them <https://tinyletter.com/DRMacIver/>`_.

If you want to contribute to Hypothesis, `instructions are here <https://github.com/HypothesisWorks/hypothesis-python/blob/master/CONTRIBUTING.rst>`_.

If you want to hear from people who are already using Hypothesis, some of them `have written
about it <https://hypothesis.readthedocs.io/en/latest/endorsements.html>`_.

If you want to create a downstream package of Hypothesis, please read `these guidelines for packagers <https://hypothesis.readthedocs.io/en/latest/packaging.html>`_.



  * [idna-2.7](https://github.com/kjd/idna) Internationalized Domain Names in Applications (IDNA)
=====================================================

Support for the Internationalised Domain Names in Applications
(IDNA) protocol as specified in `RFC 5891 <http://tools.ietf.org/html/rfc5891>`_.
This is the latest version of the protocol and is sometimes referred to as
“IDNA 2008”.

This library also provides support for Unicode Technical Standard 46,
`Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_.

This acts as a suitable replacement for the “encodings.idna” module that
comes with the Python standard library, but only supports the
old, deprecated IDNA specification (`RFC 3490 <http://tools.ietf.org/html/rfc3490>`_).

Basic functions are simply executed:

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode('ドメイン.テスト')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ドメイン.テスト

    # Python 2
    >>> import idna
    >>> idna.encode(u'ドメイン.テスト')
    'xn--eckwd4c7c.xn--zckzah'
    >>> print idna.decode('xn--eckwd4c7c.xn--zckzah')
    ドメイン.テスト

Packages
--------

The latest tagged release version is published in the PyPI repository:

.. image:: https://badge.fury.io/py/idna.svg
   :target: http://badge.fury.io/py/idna


Installation
------------

To install this library, you can use pip:

.. code-block:: bash

    $ pip install idna

Alternatively, you can install the package using the bundled setup script:

.. code-block:: bash

    $ python setup.py install

This library works with Python 2.7 and Python 3.4 or later.


Usage
-----

For typical usage, the ``encode`` and ``decode`` functions will take a domain
name argument and perform a conversion to A-labels or U-labels respectively.

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode('ドメイン.テスト')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ドメイン.テスト

You may use the codec encoding and decoding methods using the
``idna.codec`` module:

.. code-block:: pycon

    # Python 2
    >>> import idna.codec
    >>> print u'домена.испытание'.encode('idna')
    xn--80ahd1agd.xn--80akhbyknj4f
    >>> print 'xn--80ahd1agd.xn--80akhbyknj4f'.decode('idna')
    домена.испытание

Conversions can be applied at a per-label basis using the ``ulabel`` or ``alabel``
functions if necessary:

.. code-block:: pycon

    # Python 2
    >>> idna.alabel(u'测试')
    'xn--0zwm56d'

Compatibility Mapping (UTS #46)
+++++++++++++++++++++++++++++++

As described in `RFC 5895 <http://tools.ietf.org/html/rfc5895>`_, the IDNA
specification no longer normalizes input from different potential ways a user
may input a domain name. This functionality, known as a “mapping”, is now
considered by the specification to be a local user-interface issue distinct
from IDNA conversion functionality.

This library provides one such mapping, that was developed by the Unicode
Consortium. Known as `Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_,
it provides for both a regular mapping for typical applications, as well as
a transitional mapping to help migrate from older IDNA 2003 applications.

For example, “Königsgäßchen” is not a permissible label as *LATIN CAPITAL
LETTER K* is not allowed (nor are capital letters in general). UTS 46 will
convert this into lower case prior to applying the IDNA conversion.

.. code-block:: pycon

    # Python 3
    >>> import idna
    >>> idna.encode(u'Königsgäßchen')
    ...
    idna.core.InvalidCodepoint: Codepoint U+004B at position 1 of 'Königsgäßchen' not allowed
    >>> idna.encode('Königsgäßchen', uts46=True)
    b'xn--knigsgchen-b4a3dun'
    >>> print(idna.decode('xn--knigsgchen-b4a3dun'))
    königsgäßchen

Transitional processing provides conversions to help transition from the older
2003 standard to the current standard. For example, in the original IDNA
specification, the *LATIN SMALL LETTER SHARP S* (ß) was converted into two
*LATIN SMALL LETTER S* (ss), whereas in the current IDNA specification this
conversion is not performed.

.. code-block:: pycon

    # Python 2
    >>> idna.encode(u'Königsgäßchen', uts46=True, transitional=True)
    'xn--knigsgsschen-lcb0w'

Implementors should use transitional processing with caution, only in rare
cases where conversion from legacy labels to current labels must be performed
(i.e. IDNA implementations that pre-date 2008). For typical applications
that just need to convert labels, transitional processing is unlikely to be
beneficial and could produce unexpected incompatible results.

``encodings.idna`` Compatibility
++++++++++++++++++++++++++++++++

Function calls from the Python built-in ``encodings.idna`` module are
mapped to their IDNA 2008 equivalents using the ``idna.compat`` module.
Simply substitute the ``import`` clause in your code to refer to the
new module name.

Exceptions
----------

All errors raised during the conversion following the specification should
raise an exception derived from the ``idna.IDNAError`` base class.

More specific exceptions that may be generated as ``idna.IDNABidiError``
when the error reflects an illegal combination of left-to-right and right-to-left
characters in a label; ``idna.InvalidCodepoint`` when a specific codepoint is
an illegal character in an IDN label (i.e. INVALID); and ``idna.InvalidCodepointContext``
when the codepoint is illegal based on its positional context (i.e. it is CONTEXTO
or CONTEXTJ but the contextual requirements are not satisfied.)

Building and Diagnostics
------------------------

The IDNA and UTS 46 functionality relies upon pre-calculated lookup tables for
performance. These tables are derived from computing against eligibility criteria
in the respective standards. These tables are computed using the command-line
script ``tools/idna-data``.

This tool will fetch relevant tables from the Unicode Consortium and perform the
required calculations to identify eligibility. It has three main modes:

* ``idna-data make-libdata``. Generates ``idnadata.py`` and ``uts46data.py``,
  the pre-calculated lookup tables using for IDNA and UTS 46 conversions. Implementors
  who wish to track this library against a different Unicode version may use this tool
  to manually generate a different version of the ``idnadata.py`` and ``uts46data.py``
  files.

* ``idna-data make-table``. Generate a table of the IDNA disposition
  (e.g. PVALID, CONTEXTJ, CONTEXTO) in the format found in Appendix B.1 of RFC
  5892 and the pre-computed tables published by `IANA <http://iana.org/>`_.

* ``idna-data U+0061``. Prints debugging output on the various properties
  associated with an individual Unicode codepoint (in this case, U+0061), that are
  used to assess the IDNA and UTS 46 status of a codepoint. This is helpful in debugging
  or analysis.

The tool accepts a number of arguments, described using ``idna-data -h``. Most notably,
the ``--version`` argument allows the specification of the version of Unicode to use
in computing the table data. For example, ``idna-data --version 9.0.0 make-libdata``
will generate library data against Unicode 9.0.0.

Note that this script requires Python 3, but all generated library data will work
in Python 2.7.


Testing
-------

The library has a test suite based on each rule of the IDNA specification, as
well as tests that are provided as part of the Unicode Technical Standard 46,
`Unicode IDNA Compatibility Processing <http://unicode.org/reports/tr46/>`_.

The tests are run automatically on each commit at Travis CI:

.. image:: https://travis-ci.org/kjd/idna.svg?branch=master
   :target: https://travis-ci.org/kjd/idna

  * [idna_ssl-1.1.0](https://github.com/aio-libs/idna-ssl) idna-ssl
========

:info: Patch ssl.match_hostname for Unicode(idna) domains support

.. image:: https://travis-ci.com/aio-libs/idna-ssl.svg?branch=master
    :target: https://travis-ci.com/aio-libs/idna-ssl

.. image:: https://img.shields.io/pypi/v/idna_ssl.svg
    :target: https://pypi.python.org/pypi/idna_ssl

.. image:: https://codecov.io/gh/aio-libs/idna-ssl/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/aio-libs/idna-ssl

Installation
------------

.. code-block:: shell

    pip install idna-ssl

Usage
-----

.. code-block:: python

    from idna_ssl import patch_match_hostname  # noqa isort:skip
    patch_match_hostname()  # noqa isort:skip

    import asyncio

    import aiohttp

    URL = 'https://цфоут.мвд.рф/news/item/8065038/'


    async def main():
        async with aiohttp.ClientSession() as session:
            async with session.get(URL) as response:
                print(response)


    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())

Motivation
----------

* Here is 100% backward capability
* Related aiohttp `issue <https://github.com/aio-libs/aiohttp/issues/949>`_
* Related Python `bug <https://bugs.python.org/issue31872>`_
* Related Python `pull request <https://github.com/python/cpython/pull/3462>`_
* It is fixed (by January 27 2018) in upcoming Python 3.7, but `IDNA2008 <https://tools.ietf.org/html/rfc5895>`_ is still broken

Thanks
------

The library was donated by `Ocean S.A. <https://ocean.io/>`_

Thanks to the company for contribution.
  * [imageio-2.5.0](http://imageio.github.io/) 
.. image:: https://travis-ci.org/imageio/imageio.svg?branch=master
    :target: https://travis-ci.org/imageio/imageio'

.. image:: https://coveralls.io/repos/imageio/imageio/badge.png?branch=master
  :target: https://coveralls.io/r/imageio/imageio?branch=master


Imageio is a Python library that provides an easy interface to read and
write a wide range of image data, including animated images, volumetric
data, and scientific formats. It is cross-platform, runs on Python 2.7
and 3.4+, and is easy to install.

Main website: http://imageio.github.io


Release notes: http://imageio.readthedocs.io/en/latest/releasenotes.html

Example:

.. code-block:: python
    
    >>> import imageio
    >>> im = imageio.imread('imageio:astronaut.png')
    >>> im.shape  # im is a numpy array
    (512, 512, 3)
    >>> imageio.imwrite('astronaut-gray.jpg', im[:, :, 0])

See the `user API <http://imageio.readthedocs.io/en/latest/userapi.html>`_
or `examples <http://imageio.readthedocs.io/en/latest/examples.html>`_
for more information.

  * [imagesize-1.1.0](https://github.com/shibukawa/imagesize_py) 
It parses image files' header and return image size.

* PNG
* JPEG
* JPEG2000
* GIF
* TIFF (experimental)

This is a pure Python library.



  * [importlib-metadata-0.19](http://importlib-metadata.readthedocs.io/) =========================
 ``importlib_metadata``
=========================

``importlib_metadata`` is a library to access the metadata for a Python
package.  It is intended to be ported to Python 3.8.


Usage
=====

See the `online documentation <https://importlib_metadata.readthedocs.io/>`_
for usage details.

`Finder authors
<https://docs.python.org/3/reference/import.html#finders-and-loaders>`_ can
also add support for custom package installers.  See the above documentation
for details.


Caveats
=======

This project primarily supports third-party packages installed by PyPA
tools (or other conforming packages). It does not support:

- Packages in the stdlib.
- Packages installed without metadata.

Project details
===============

 * Project home: https://gitlab.com/python-devs/importlib_metadata
 * Report bugs at: https://gitlab.com/python-devs/importlib_metadata/issues
 * Code hosting: https://gitlab.com/python-devs/importlib_metadata.git
 * Documentation: http://importlib_metadata.readthedocs.io/



  * [incremental-17.5.0](https://github.com/twisted/incremental) Incremental
===========

|travis|
|pypi|
|coverage|

Incremental is a small library that versions your Python projects.

API documentation can be found `here <https://hawkowl.github.io/incremental/docs/>`_.


Quick Start
-----------

Add this to your ``setup.py``\ 's ``setup()`` call, removing any other versioning arguments:

.. code::

   setup(
       use_incremental=True,
       setup_requires=['incremental'],
       install_requires=['incremental'], # along with any other install dependencies
       ...
   }


Then run ``python -m incremental.update <projectname> --create`` (you will need ``click`` installed from PyPI).
It will create a file in your package named ``_version.py`` and look like this:

.. code::

   from incremental import Version

   __version__ = Version("widgetbox", 17, 1, 0)
   __all__ = ["__version__"]


Then, so users of your project can find your version, in your root package's ``__init__.py`` add:

.. code::

   from ._version import __version__


Subsequent installations of your project will then use Incremental for versioning.


Incremental Versions
--------------------

``incremental.Version`` is a class that represents a version of a given project.
It is made up of the following elements (which are given during instantiation):

- ``package`` (required), the name of the package this ``Version`` represents.
- ``major``, ``minor``, ``micro`` (all required), the X.Y.Z of your project's ``Version``.
- ``release_candidate`` (optional), set to 0 or higher to mark this ``Version`` being of a release candidate (also sometimes called a "prerelease").
- ``dev`` (optional), set to 0 or higher to mark this ``Version`` as a development release.

You can extract a PEP-440 compatible version string by using the following methods:

- ``.local()``, which returns a ``str`` containing the full version plus any Git or SVN information, if available. An example output would be ``"17.1.1rc1+r123"`` or ``"3.7.0+rb2e812003b5d5fcf08efd1dffed6afa98d44ac8c"``.
- ``.public()``, which returns a ``str`` containing the full version, without any Git or SVN information. This is the version you should provide to users, or publicly use. An example output would be ``"13.2.0"``, ``"17.1.2dev1"``, or ``"18.8.0rc2"``.

Calling ``repr()`` with a ``Version`` will give a Python-source-code representation of it, and calling ``str()`` with a ``Version`` will provide a string similar to ``'[Incremental, version 16.10.1]'``.


Updating
--------

Incremental includes a tool to automate updating your Incremental-using project's version called ``incremental.update``.
It updates the ``_version.py`` file and automatically updates some uses of Incremental versions from an indeterminate version to the current one.
It requires ``click`` from PyPI.

``python -m incremental.update <projectname>`` will perform updates on that package.
The commands that can be given after that will determine what the next version is.

- ``--newversion=<version>``, to set the project version to a fully-specified version (like 1.2.3, or 17.1.0dev1).
- ``--rc``, to set the project version to ``<year-2000>.<month>.0rc1`` if the current version is not a release candidate, or bump the release candidate number by 1 if it is.
- ``--dev``, to set the project development release number to 0 if it is not a development release, or bump the development release number by 1 if it is.
- ``--patch``, to increment the patch number of the release. This will also reset the release candidate number, pass ``--rc`` at the same time to increment the patch number and make it a release candidate.

If you give no arguments, it will strip the release candidate number, making it a "full release".

Incremental supports "indeterminate" versions, as a stand-in for the next "full" version. This can be used when the version which will be displayed to the end-user is unknown (for example "introduced in" or "deprecated in"). Incremental supports the following indeterminate versions:

- ``Version("<projectname>", "NEXT", 0, 0)``
- ``<projectname> NEXT``

When you run ``python -m incremental.update <projectname> --rc``, these will be updated to real versions (assuming the target final version is 17.1.0):

- ``Version("<projectname>", 17, 1, 0, release_candidate=1)``
- ``<projectname> 17.1.0rc1``

Once the final version is made, it will become:

- ``Version("<projectname>", 17, 1, 0)``
- ``<projectname> 17.1.0``


.. |coverage| image:: https://codecov.io/github/hawkowl/incremental/coverage.svg?branch=master
.. _coverage: https://codecov.io/github/hawkowl/incremental

.. |travis| image:: https://travis-ci.org/hawkowl/incremental.svg?branch=master
.. _travis: http://travis-ci.org/hawkowl/incremental

.. |pypi| image:: http://img.shields.io/pypi/v/incremental.svg
.. _pypi: https://pypi.python.org/pypi/incremental



  * [inflection-0.3.1](http://github.com/jpvanhal/inflection) Inflection
==========

|build status|_

.. |build status| image:: https://secure.travis-ci.org/jpvanhal/inflection.png?branch=master
   :alt: Build Status
.. _build status: http://travis-ci.org/jpvanhal/inflection

Inflection is a string transformation library.  It singularizes and pluralizes
English words, and transforms strings from CamelCase to underscored string.
Inflection is a port of `Ruby on Rails`_' `inflector`_ to Python.

.. _Ruby on Rails: http://rubyonrails.org
.. _inflector: http://api.rubyonrails.org/classes/ActiveSupport/Inflector.html

Resources
---------

- `Documentation <http://inflection.readthedocs.org/>`_
- `Issue Tracker <http://github.com/jpvanhal/inflection/issues>`_
- `Code <http://github.com/jpvanhal/inflection>`_
- `Development Version
  <http://github.com/jpvanhal/inflection/zipball/master#egg=Inflection-dev>`_


Changelog
---------

Here you can see the full list of changes between each Inflection release.

0.3.1 (May 3, 2015)
^^^^^^^^^^^^^^^^^^^

- Fixed trove classifiers not showing up on PyPI.
- Fixed "human" pluralized as "humen" and not "humans".
- Fixed "potato" pluralized as "potatos" and not "potatoes".

0.3.0 (March 1, 2015)
+++++++++++++++++++++

- Added `tableize()` function.

0.2.1 (September 3, 2014)
+++++++++++++++++++++++++

- Added Python 2, Python 3 and Python 3.4 trove classifiers.

0.2.0 (June 15, 2013)
+++++++++++++++++++++

- Added initial support for Python 3.
- Dropped Python 2.5 support.

0.1.2 (March 13, 2012)
++++++++++++++++++++++

- Added Python 2.5 support.

0.1.1 (February 24, 2012)
+++++++++++++++++++++++++

- Fixed some files not included in the distribution package.

0.1.0 (February 24, 2012)
+++++++++++++++++++++++++

- Initial public release
  * [ip_associations_python_novaclient_ext-0.2](https://github.com/rackerlabs/ip_associations_python_novaclient_ext) =====================================
ip_associations_python_novaclient_ext
=====================================

Adds IP association extension support to python-novaclient.

Install
=======

::

   pip install ip_associations_python_novaclient_ext


Usage
=====

This extension is autodiscovered once installed.

::

    nova ip-association              Show an IP association
    nova ip-association-create       Create an IP association
    nova ip-association-delete       Delete an IP association
    nova ip-association-list         List IP associations
    nova ip-association-show         Show an IP association
  * [ipaddress-1.0.22](https://github.com/phihag/ipaddress) Port of the 3.3+ ipaddress module to 2.6, 2.7, 3.2
  * [ipdb-0.12.2](https://github.com/gotcha/ipdb) IPython `pdb`
=============

.. image:: https://travis-ci.org/gotcha/ipdb.png?branch=master
  :target: https://travis-ci.org/gotcha/ipdb
.. image:: https://codecov.io/gh/gotcha/ipdb/branch/master/graphs/badge.svg?style=flat
  :target: https://codecov.io/gh/gotcha/ipdb?branch=master

Use
---

ipdb exports functions to access the IPython_ debugger, which features
tab completion, syntax highlighting, better tracebacks, better introspection
with the same interface as the `pdb` module.

Example usage:

.. code-block:: python

        import ipdb
        ipdb.set_trace()
        ipdb.set_trace(context=5)  # will show five lines of code
                                   # instead of the default three lines
        ipdb.pm()
        ipdb.run('x[0] = 3')
        result = ipdb.runcall(function, arg0, arg1, kwarg='foo')
        result = ipdb.runeval('f(1,2) - 3')

The post-mortem function, ``ipdb.pm()``, is equivalent to the magic function
``%debug``.

.. _IPython: http://ipython.org

If you install ``ipdb`` with a tool which supports ``setuptools`` entry points,
an ``ipdb`` script is made for you. You can use it to debug your python 2 scripts like

::

        $ bin/ipdb mymodule.py

And for python 3

::

        $ bin/ipdb3 mymodule.py

Alternatively with Python 2.7 only, you can also use

::

        $ python -m ipdb mymodule.py

You can also enclose code with the ``with`` statement to launch ipdb if an exception is raised:

.. code-block:: python

        from ipdb import launch_ipdb_on_exception

        with launch_ipdb_on_exception():
            [...]

.. warning::
   Context managers were introduced in Python 2.5.
   Adding a context manager implies dropping Python 2.4 support.
   Use ``ipdb==0.6`` with 2.4.

.. warning::
   Using ``from future import print_function`` for Python 3 compat implies dropping Python 2.5 support.
   Use ``ipdb<=0.8`` with 2.5.

Issues with ``stdout``
----------------------

Some tools, like ``nose`` fiddle with ``stdout``.

Until ``ipdb==0.9.4``, we tried to guess when we should also
fiddle with ``stdout`` to support those tools.
However, all strategies tried until 0.9.4 have proven brittle.

If you use ``nose`` or another tool that fiddles with ``stdout``, you should
explicitely ask for ``stdout`` fiddling by using ``ipdb`` like this

.. code-block:: python

        import ipdb
        ipdb.sset_trace()
        ipdb.spm()

        from ipdb import slaunch_ipdb_on_exception
        with slaunch_ipdb_on_exception():
            [...]


Development
-----------

``ipdb`` source code and tracker are at https://github.com/gotcha/ipdb.

Pull requests should take care of updating the changelog ``HISTORY.txt``.

Third-party support
-------------------

Products.PDBDebugMode
+++++++++++++++++++++

Zope2 Products.PDBDebugMode_ uses ``ipdb``, if available, in place of ``pdb``.

.. _Products.PDBDebugMode: http://pypi.python.org/pypi/Products.PDBDebugMode

iw.debug
++++++++

iw.debug_ allows you to trigger an ``ipdb`` debugger on any published object
of a Zope2 application.

.. _iw.debug: http://pypi.python.org/pypi/iw.debug

ipdbplugin
++++++++++

ipdbplugin_ is a nose_ test runner plugin that also uses the IPython debugger
instead of ``pdb``. (It does not depend on ``ipdb`` anymore).

.. _ipdbplugin: http://pypi.python.org/pypi/ipdbplugin
.. _nose: http://readthedocs.org/docs/nose


Changelog
=========

0.12.2 (2019-07-30)
-------------------

- Avoid emitting term-title bytes
  [steinnes]


0.12.1 (2019-07-26)
-------------------

- Fix --help 
  [native-api]


0.12 (2019-03-20)
-----------------

- Drop support for Python 3.3.x
  [bmw]
- Stop deprecation warnings from being raised when IPython >= 5.1 is used.
  Support for IPython < 5.1 has been dropped.
  [bmw]


0.11 (2018-02-15)
-----------------

- Simplify loading IPython and getting information from it.
  Drop support for python 2.6
  Drop support for IPython < 5.0.0
  [takluyver]


0.10.3 (2017-04-22)
-------------------

- For users using python 2.6, do not install IPython >= 2.0.0.
  And for users using python 2.7, do not install IPython >= 6.0.0.
  [vphilippon]
- Drop support for python 3.2.
  [vphilippon]
- Command line usage consistent with pdb - Add argument commands
  [zvodd]


0.10.2 (2017-01-25)
-------------------

- Ask IPython which debugger class to use.
  Closes https://github.com/gotcha/ipdb/issues/105
  [gnebehay, JBKahn] 

- ipdb.set_trace() does not ignore context arg anymore.
  Closes https://github.com/gotcha/ipdb/issues/93.
  [gnebehay, Garrett-R]


0.10.1 (2016-06-14)
-------------------

- Support IPython 5.0.
  [ngoldbaum]


0.10.0 (2016-04-29)
-------------------

- Stop trying to magically guess when stdout needs to be captured.
  Like needed by `nose`.
  Rather, provide a set of function that can be called explicitely when needed.
  [gotcha]

- drop support of IPython before 0.10.2


0.9.4 (2016-04-29)
------------------

- Fix Restart error when using `python -m ipdb`
  Closes https://github.com/gotcha/ipdb/issues/93.
  [gotcha]


0.9.3 (2016-04-15)
------------------

- Don't require users to pass a traceback to post_mortem.
  [Wilfred]


0.9.2 (2016-04-15)
------------------

- Closes https://github.com/gotcha/ipdb/issues/93.
  [gotcha]


0.9.1 (2016-04-12)
------------------

- Reset ``sys.modules['__main__']`` to original value.
  Closes https://github.com/gotcha/ipdb/issues/85
  [gotcha]

- Fix support of IPython versions 0.x
  [asivokon]


0.9.0 (2016-02-22)
------------------

- Switch to revised BSD license (with approval of all contributors).
  Closes https://github.com/gotcha/ipdb/issues/68
  [lebedov, gotcha]

0.8.3 (2016-02-17)
------------------

- Don't pass sys.argv to IPython for configuration.
  [emulbreh]


0.8.2 (2016-02-15)
------------------

- Fix lexical comparison for version numbers.
  [sas23]

- Allow configuring how many lines of code context are displayed
  by `set_trace`
  [JamshedVesuna]

- If an instance of IPython is already running its configuration will be
  loaded.
  [IxDay]


0.8.1 (2015-06-03)
------------------

- Make Nose support less invasive.
  Closes https://github.com/gotcha/ipdb/issues/52
  Closes https://github.com/gotcha/ipdb/issues/31
  [blink1073, gotcha]

- Fix for post_mortem in context manager.
  Closes https://github.com/gotcha/ipdb/issues/20
  [omergertel]


0.8 (2013-09-19)
----------------

- More Python 3 compatibility; implies dropping Python 2.5 support.
  Closes https://github.com/gotcha/ipdb/issues/37
  [gotcha]


0.7.1 (2013-09-19)
------------------

- IPython 1.0 compatibility.
  Closes https://github.com/gotcha/ipdb/issues/44
  [pgularski]

- Index into version_info in setup.py for Python 2.6 compatibility.
  [kynan]

- Add Travis CI configuration.
  [kynan]

0.7 (2012-07-06)
----------------

- Add ``launch_ipdb_on_exception`` context manager. Implies dropping Python 2.4 support.
  [Psycojoker]

- Wrap sys.excepthook only once.
  [marciomazza]

- Add GPL file and refer in headers.
  [stan3]

- Python 3 support.
  [Piet Delport]

- Basic tests.
  [msabramo]

- Added the functions ``runcall``, ``runeval`` and ``run``.
  [dimasad]


0.6.1 (2011-10-17)
------------------

- State dependency on IPython later or equal to 0.10.
  [gotcha]


0.6 (2011-09-01)
----------------

- Add setuptools ``console_scripts`` entry point.
  [akrito, gotcha] 

- Nose support.
  Closes https://github.com/gotcha/ipdb/issues/8
  [akaihola, gotcha]


0.5 (2011-08-05)
----------------

- IPython 0.11 support.
  [lebedov]


0.4 (2011-06-13)
----------------

- When used from IPython, use its colors.
  Closes https://github.com/gotcha/ipdb/issues/1
  [gotcha]

- Fixed errors when exiting with "q". 
  [gotcha]

- Allow use of ``python -m ipdb mymodule.py``.
  Python 2.7 only. 
  Closes https://github.com/gotcha/ipdb/issues/3 
  [gotcha]

- Fixed post_mortem when the traceback is None.
  [maurits]


0.3 (2011-01-16)
----------------

- Add ``post_mortem()`` for ``Products.PDBDebugMode`` support.
  [Jean Jordaan]

- Moved to github.com.


0.2 (2010-10-20)
----------------

- Added ``pm()``.
  [Paulo Benedict Ang]


0.1 (2010-04-26)
----------------

- First "non dev" release.
  * [ipykernel-5.1.2](https://ipython.org) The IPython kernel for Jupyter



  * [ipython-7.7.0](https://ipython.org) 
IPython provides a rich toolkit to help you make the most out of using Python
interactively.  Its main components are:

* A powerful interactive Python shell
* A `Jupyter <https://jupyter.org/>`_ kernel to work with Python code in Jupyter
  notebooks and other interactive frontends.

The enhanced interactive Python shells have the following main features:

* Comprehensive object introspection.

* Input history, persistent across sessions.

* Caching of output results during a session with automatically generated
  references.

* Extensible tab completion, with support by default for completion of python
  variables and keywords, filenames and function keywords.

* Extensible system of 'magic' commands for controlling the environment and
  performing many tasks related either to IPython or the operating system.

* A rich configuration system with easy switching between different setups
  (simpler than changing $PYTHONSTARTUP environment variables every time).

* Session logging and reloading.

* Extensible syntax processing for special purpose situations.

* Access to the system shell with user-extensible alias system.

* Easily embeddable in other Python programs and GUIs.

* Integrated access to the pdb debugger and the Python profiler.

The latest development version is always available from IPython's `GitHub
site <http://github.com/ipython>`_.



  * [ipython_genutils-0.2.0](http://ipython.org) Pretend this doesn't exist. Nobody should use it.



  * [ipywidgets-7.5.1](http://ipython.org) 
.. image:: https://img.shields.io/pypi/v/ipywidgets.svg
   :target: https://pypi.python.org/pypi/ipywidgets/
   :alt: Version Number

.. image:: https://img.shields.io/pypi/dm/ipywidgets.svg
   :target: https://pypi.python.org/pypi/ipywidgets/
   :alt: Number of PyPI downloads

Interactive HTML Widgets
========================

Interactive HTML widgets for Jupyter notebooks and the IPython kernel.

Usage
=====

.. code-block:: python

    from ipywidgets import IntSlider
    IntSlider()



  * [iso8601-0.1.12](https://bitbucket.org/micktwomey/pyiso8601) Simple module to parse ISO 8601 dates

This module parses the most common forms of ISO 8601 date strings (e.g.
2007-01-14T20:34:22+00:00) into datetime objects.

>>> import iso8601
>>> iso8601.parse_date("2007-01-25T12:00:00Z")
datetime.datetime(2007, 1, 25, 12, 0, tzinfo=<iso8601.Utc>)
>>>

See the LICENSE file for the license this package is released under.

If you want more full featured parsing look at:

- http://labix.org/python-dateutil - python-dateutil

Parsed Formats
==============

You can parse full date + times, or just the date. In both cases a datetime instance is returned but with missing times defaulting to 0, and missing days / months defaulting to 1.

Dates
-----

- YYYY-MM-DD
- YYYYMMDD
- YYYY-MM (defaults to 1 for the day)
- YYYY (defaults to 1 for month and day)

Times
-----

- hh:mm:ss.nn
- hhmmss.nn
- hh:mm (defaults to 0 for seconds)
- hhmm (defaults to 0 for seconds)
- hh (defaults to 0 for minutes and seconds)

Time Zones
----------

- Nothing, will use the default timezone given (which in turn defaults to UTC).
- Z (UTC)
- +/-hh:mm
- +/-hhmm
- +/-hh

Where it Differs From ISO 8601
==============================

Known differences from the ISO 8601 spec:

- You can use a " " (space) instead of T for separating date from time.
- Days and months without a leading 0 (2 vs 02) will be parsed.
- If time zone information is omitted the default time zone given is used (which in turn defaults to UTC). Use a default of None to yield naive datetime instances.

Homepage
========

- Documentation: http://pyiso8601.readthedocs.org/
- Source: https://bitbucket.org/micktwomey/pyiso8601/

This was originally hosted at https://code.google.com/p/pyiso8601/

References
==========

- http://en.wikipedia.org/wiki/ISO_8601

- http://www.cl.cam.ac.uk/~mgk25/iso-time.html - simple overview

- http://hydracen.com/dx/iso8601.htm - more detailed enumeration of valid formats.

Testing
=======

1. pip install -r dev-requirements.txt
2. tox

Note that you need all the pythons installed to perform a tox run (see below). Homebrew helps a lot on the mac, however you wind up having to add cellars to your PATH or symlinking the pythonX.Y executables.

Alternatively, to test only with your current python:

1. pip install -r dev-requirements.txt
2. py.test --verbose iso8601

Supported Python Versions
=========================

Tested against:

- Python 2.6
- Python 2.7
- Python 3.2
- Python 3.3
- Python 3.4
- Python 3.5
- Python 3.6
- PyPy
- PyPy 3

Python 3.0 and 3.1 are untested but should work (tests didn't run under them when last tried).

Jython is untested but should work (tests failed to run).

Python 2.5 is not supported (too old for the tests for the most part). It could work with some small changes but I'm not supporting it.

Changes
=======

0.1.12
------

* Fix class reference for iso8601.Utc in module docstring (thanks to felixschwarz in https://bitbucket.org/micktwomey/pyiso8601/pull-requests/7/fix-class-reference-for-iso8601utc-in/diff)

0.1.11
------

* Remove logging (thanks to Quentin Pradet in https://bitbucket.org/micktwomey/pyiso8601/pull-requests/6/remove-debug-logging/diff)
* Add support for , as separator for fractional part (thanks to ecksun in https://bitbucket.org/micktwomey/pyiso8601/pull-requests/5/add-support-for-as-separator-for/diff)
* Add Python 3.4 and 3.5 to tox test config.
* Add PyPy 3 to tox test config.
* Link to documentation at http://pyiso8601.readthedocs.org/


0.1.10
------

* Fixes https://bitbucket.org/micktwomey/pyiso8601/issue/14/regression-yyyy-mm-no-longer-parses (thanks to Kevin Gill for reporting)
* Adds YYYY as a valid date (uses 1 for both month and day)
* Woo, semantic versioning, .10 at last.

0.1.9
-----

* Lots of fixes tightening up parsing from jdanjou. In particular more invalid cases are treated as errors. Also includes fixes for tests (which is how these invalid cases got in in the first place).
* Release addresses https://bitbucket.org/micktwomey/pyiso8601/issue/13/new-release-based-on-critical-bug-fix

0.1.8
-----

* Remove +/- chars from README.rst and ensure tox tests run using LC_ALL=C. The setup.py egg_info command was failing in python 3.* on some setups (basically any where the system encoding wasn't UTF-8). (https://bitbucket.org/micktwomey/pyiso8601/issue/10/setuppy-broken-for-python-33) (thanks to klmitch)

0.1.7
-----

* Fix parsing of microseconds (https://bitbucket.org/micktwomey/pyiso8601/issue/9/regression-parsing-microseconds) (Thanks to dims and bnemec)

0.1.6
-----

* Correct negative timezone offsets (https://bitbucket.org/micktwomey/pyiso8601/issue/8/015-parses-negative-timezones-incorrectly) (thanks to Jonathan Lange)

0.1.5
-----

* Wow, it's alive! First update since 2007
* Moved over to https://bitbucket.org/micktwomey/pyiso8601
* Add support for python 3. https://code.google.com/p/pyiso8601/issues/detail?id=23 (thanks to zefciu)
* Switched to py.test and tox for testing
* Make seconds optional in date format ("1997-07-16T19:20+01:00" now valid). https://bitbucket.org/micktwomey/pyiso8601/pull-request/1/make-the-inclusion-of-seconds-optional-in/diff (thanks to Chris Down)
* Correctly raise ParseError for more invalid inputs (https://bitbucket.org/micktwomey/pyiso8601/issue/1/raise-parseerror-for-invalid-input) (thanks to manish.tomar)
* Support more variations of ISO 8601 dates, times and time zone specs.
* Fix microsecond rounding issues (https://bitbucket.org/micktwomey/pyiso8601/issue/2/roundoff-issues-when-parsing-decimal) (thanks to nielsenb@jetfuse.net)
* Fix pickling and deepcopy of returned datetime objects (https://bitbucket.org/micktwomey/pyiso8601/issue/3/dates-returned-by-parse_date-do-not) (thanks to fogathmann and john@openlearning.com)
* Fix timezone offsets without a separator (https://bitbucket.org/micktwomey/pyiso8601/issue/4/support-offsets-without-a-separator) (thanks to joe.walton.gglcd)
* "Z" produces default timezone if one is specified (https://bitbucket.org/micktwomey/pyiso8601/issue/5/z-produces-default-timezone-if-one-is) (thanks to vfaronov). This one may cause problems if you've been relying on default_timezone to use that timezone instead of UTC. Strictly speaking that was wrong but this is potentially backwards incompatible.
* Handle compact date format (https://bitbucket.org/micktwomey/pyiso8601/issue/6/handle-compact-date-format) (thanks to rvandolson@esri.com)

0.1.4
-----

* The default_timezone argument wasn't being passed through correctly, UTC was being used in every case. Fixes issue 10.

0.1.3
-----

* Fixed the microsecond handling, the generated microsecond values were way too small. Fixes issue 9.

0.1.2
-----

* Adding ParseError to __all__ in iso8601 module, allows people to import it. Addresses issue 7.
* Be a little more flexible when dealing with dates without leading zeroes. This violates the spec a little, but handles more dates as seen in the field. Addresses issue 6.
* Allow date/time separators other than T.

0.1.1
-----

* When parsing dates without a timezone the specified default is used. If no default is specified then UTC is used. Addresses issue 4.



  * [isodate-0.6.0](https://github.com/gweis/isodate/) 
ISO 8601 date/time parser
=========================

.. image:: https://travis-ci.org/gweis/isodate.svg?branch=master
    :target: https://travis-ci.org/gweis/isodate
    :alt: Travis-CI
.. image:: https://coveralls.io/repos/gweis/isodate/badge.svg?branch=master
    :target: https://coveralls.io/r/gweis/isodate?branch=master
    :alt: Coveralls
.. image:: https://img.shields.io/pypi/v/isodate.svg
    :target: https://pypi.python.org/pypi/isodate/          
    :alt: Latest Version
.. image:: https://img.shields.io/pypi/l/isodate.svg
    :target: https://pypi.python.org/pypi/isodate/          
    :alt: License


This module implements ISO 8601 date, time and duration parsing.
The implementation follows ISO8601:2004 standard, and implements only
date/time representations mentioned in the standard. If something is not
mentioned there, then it is treated as non existent, and not as an allowed
option.

For instance, ISO8601:2004 never mentions 2 digit years. So, it is not
intended by this module to support 2 digit years. (while it may still
be valid as ISO date, because it is not explicitly forbidden.)
Another example is, when no time zone information is given for a time,
then it should be interpreted as local time, and not UTC.

As this module maps ISO 8601 dates/times to standard Python data types, like
*date*, *time*, *datetime* and *timedelta*, it is not possible to convert
all possible ISO 8601 dates/times. For instance, dates before 0001-01-01 are
not allowed by the Python *date* and *datetime* classes. Additionally
fractional seconds are limited to microseconds. That means if the parser finds
for instance nanoseconds it will round it to microseconds.

Documentation
-------------

Currently there are four parsing methods available.
   * parse_time:
        parses an ISO 8601 time string into a *time* object
   * parse_date:
        parses an ISO 8601 date string into a *date* object
   * parse_datetime:
        parses an ISO 8601 date-time string into a *datetime* object
   * parse_duration:
        parses an ISO 8601 duration string into a *timedelta* or *Duration*
        object.
   * parse_tzinfo:
        parses the time zone info part of an ISO 8601 string into a
        *tzinfo* object.

As ISO 8601 allows to define durations in years and months, and *timedelta*
does not handle years and months, this module provides a *Duration* class,
which can be used almost like a *timedelta* object (with some limitations).
However, a *Duration* object can be converted into a *timedelta* object.

There are also ISO formatting methods for all supported data types. Each
*xxx_isoformat* method accepts a format parameter. The default format is
always the ISO 8601 expanded format. This is the same format used by
*datetime.isoformat*:

    * time_isoformat:
        Intended to create ISO time strings with default format
        *hh:mm:ssZ*.
    * date_isoformat:
        Intended to create ISO date strings with default format
        *yyyy-mm-dd*.
    * datetime_isoformat:
        Intended to create ISO date-time strings with default format
        *yyyy-mm-ddThh:mm:ssZ*.
    * duration_isoformat:
        Intended to create ISO duration strings with default format
        *PnnYnnMnnDTnnHnnMnnS*.
    * tz_isoformat:
        Intended to create ISO time zone strings with default format
        *hh:mm*.
    * strftime:
        A re-implementation mostly compatible with Python's *strftime*, but
        supports only those format strings, which can also be used for dates
        prior 1900. This method also understands how to format *datetime* and
        *Duration* instances.

Installation:
-------------

This module can easily be installed with Python standard installation methods.

Either use *python setup.py install* or in case you have *setuptools* or
*distribute* available, you can also use *easy_install*.

Limitations:
------------

   * The parser accepts several date/time representation which should be invalid
     according to ISO 8601 standard.

     1. for date and time together, this parser accepts a mixture of basic and extended format.
        e.g. the date could be in basic format, while the time is accepted in extended format.
        It also allows short dates and times in date-time strings.
     2. For incomplete dates, the first day is chosen. e.g. 19th century results in a date of
        1901-01-01.
     3. negative *Duration* and *timedelta* value are not fully supported yet.

Further information:
--------------------

The doc strings and unit tests should provide rather detailed information about
the methods and their limitations.

The source release provides a *setup.py* script,
which can be used to run the unit tests included.

Source code is available at `<http://github.com/gweis/isodate>`_.

CHANGES
=======

0.6.0 (2017-10-13)
------------------

- support incomplete month date (Fabien Loffredo)
- rely on duck typing when doing duration maths
- support ':' as separator in fractional time zones (usrenmae)


0.5.4 (2015-08-06)
------------------

- Fix parsing of Periods (Fabien Bochu)
- Make Duration objects hashable (Geoffrey Fairchild)
- Add multiplication to duration (Reinoud Elhorst)


0.5.1 (2014-11-07)
------------------

- fixed pickling of Duration objects
- raise ISO8601Error when there is no 'T' separator in datetime strings (Adrian Coveney)


0.5.0 (2014-02-23)
------------------

- ISO8601Error are subclasses of ValueError now (Michael Hrivnak)
- improve compatibility across various python variants and versions
- raise exceptions when using fractional years and months in date
  maths with durations
- renamed method todatetime on Duraction objects to totimedelta


0.4.9 (2012-10-30)
------------------

- support pickling FixedOffset instances
- make sure parsed fractional seconds are in microseconds
- add leading zeros when formattig microseconds (Jarom Loveridge)


0.4.8 (2012-05-04)
------------------

- fixed incompatibility of unittests with python 2.5 and 2.6 (runs fine on 2.7
  and 3.2)


0.4.7 (2012-01-26)
------------------

- fixed tzinfo formatting (never pass None into tzinfo.utcoffset())


0.4.6 (2012-01-06)
------------------

- added Python 3 compatibility via 2to3

0.4.5 (2012-01-06)
------------------

- made setuptools dependency optional

0.4.4 (2011-04-16)
------------------

- Fixed formatting of microseconds for datetime objects

0.4.3 (2010-10-29)
------------------

- Fixed problem with %P formating and fractions (supplied by David Brooks)

0.4.2 (2010-10-28)
------------------

- Implemented unary - for Duration (supplied by David Brooks)
- Output fractional seconds with '%P' format. (partly supplied by David Brooks)

0.4.1 (2010-10-13)
------------------

- fixed bug in comparison between timedelta and Duration.
- fixed precision problem with microseconds (reported by Tommi Virtanen)

0.4.0 (2009-02-09)
------------------

- added method to parse ISO 8601 time zone strings
- added methods to create ISO 8601 conforming strings

0.3.0 (2009-1-05)
------------------

- Initial release

TODOs
=====

This to do list contains some thoughts and ideas about missing features, and
parts to think about, whether to implement them or not. This list is probably
not complete.

Missing features:
-----------------

    * time formating does not allow to create fractional representations.
    * parser for ISO intervals.
    * currently microseconds are always padded to a length of 6 characters.
      trailing 0s should be optional

Documentation:
--------------

    * parse_datetime:
       - complete documentation to show what this function allows, but ISO forbids.
         and vice verse.
       - support other separators between date and time than 'T'

    * parse_date:
       - yeardigits should be always greater than 4
       - dates before 0001-01-01 are not supported

    * parse_duration:
       - alternative formats are not fully supported due to parse_date restrictions
       - standard duration format is fully supported but not very restrictive.

    * Duration:
       - support fractional years and month in calculations
       - implement w3c order relation? (`<http://www.w3.org/TR/xmlschema-2/#duration-order>`_)
       - refactor to have duration mathematics only at one place.
       - localize __str__ method (does timedelta do this?)
       - when is a Duration negative?
       - normalize Durations. months [00-12] and years ]-inf,+inf[

  * [isort-4.3.21](https://github.com/timothycrosley/isort) .. image:: https://raw.github.com/timothycrosley/isort/master/logo.png
    :alt: isort

########

.. image:: https://badge.fury.io/py/isort.svg
    :target: https://badge.fury.io/py/isort
    :alt: PyPI version

.. image:: https://travis-ci.org/timothycrosley/isort.svg?branch=master
    :target: https://travis-ci.org/timothycrosley/isort
    :alt: Build Status


.. image:: https://coveralls.io/repos/timothycrosley/isort/badge.svg?branch=release%2F2.6.0&service=github
  :target: https://coveralls.io/github/timothycrosley/isort?branch=release%2F2.6.0
  :alt: Coverage

.. image:: https://img.shields.io/github/license/mashape/apistatus.svg
    :target: https://pypi.org/project/hug/
    :alt: License

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/timothycrosley/isort
   :target: https://gitter.im/timothycrosley/isort?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. image:: https://pepy.tech/badge/isort
    :alt: Downloads
    :target: https://pepy.tech/project/isort

isort your python imports for you so you don't have to.

isort is a Python utility / library to sort imports alphabetically, and automatically separated into sections.
It provides a command line utility, Python library and `plugins for various editors <https://github.com/timothycrosley/isort/wiki/isort-Plugins>`_ to quickly sort all your imports.
It currently cleanly supports Python 2.7 and 3.4+ without any dependencies.

.. image:: https://raw.github.com/timothycrosley/isort/develop/example.gif
   :alt: Example Usage

Before isort:

.. code-block:: python

    from my_lib import Object

    print("Hey")

    import os

    from my_lib import Object3

    from my_lib import Object2

    import sys

    from third_party import lib15, lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11, lib12, lib13, lib14

    import sys

    from __future__ import absolute_import

    from third_party import lib3

    print("yo")

After isort:

.. code-block:: python

    from __future__ import absolute_import

    import os
    import sys

    from third_party import (lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8,
                             lib9, lib10, lib11, lib12, lib13, lib14, lib15)

    from my_lib import Object, Object2, Object3

    print("Hey")
    print("yo")

Installing isort
================

Installing isort is as simple as:

.. code-block:: bash

    pip install isort

Install isort with requirements.txt support:

.. code-block:: bash

    pip install isort[requirements]

Install isort with Pipfile support:

.. code-block:: bash

    pip install isort[pipfile]

Install isort with both formats support:

.. code-block:: bash

    pip install isort[requirements,pipfile]

Using isort
===========

**From the command line**:

.. code-block:: bash

    isort mypythonfile.py mypythonfile2.py

or recursively:

.. code-block:: bash

    isort -rc .

*which is equivalent to:*

.. code-block:: bash

    isort **/*.py

or to see the proposed changes without applying them:

.. code-block:: bash

    isort mypythonfile.py --diff

Finally, to atomically run isort against a project, only applying changes if they don't introduce syntax errors do:

.. code-block:: bash

    isort -rc --atomic .

(Note: this is disabled by default as it keeps isort from being able to run against code written using a different version of Python)

**From within Python**:

.. code-block:: bash

    from isort import SortImports

    SortImports("pythonfile.py")

or:

.. code-block:: bash

    from isort import SortImports

    new_contents = SortImports(file_contents=old_contents).output

**From within Kate:**

.. code-block:: bash

    ctrl+[

or:

.. code-block:: bash

    menu > Python > Sort Imports

Installing isort's Kate plugin
==============================

For KDE 4.13+ / Pate 2.0+:

.. code-block:: bash

    wget https://raw.github.com/timothycrosley/isort/master/kate_plugin/isort_plugin.py --output-document ~/.kde/share/apps/kate/pate/isort_plugin.py
    wget https://raw.github.com/timothycrosley/isort/master/kate_plugin/isort_plugin_ui.rc --output-document ~/.kde/share/apps/kate/pate/isort_plugin_ui.rc
    wget https://raw.github.com/timothycrosley/isort/master/kate_plugin/katepart_isort.desktop --output-document ~/.kde/share/kde4/services/katepart_isort.desktop

For all older versions:

.. code-block:: bash

    wget https://raw.github.com/timothycrosley/isort/master/kate_plugin/isort_plugin_old.py --output-document ~/.kde/share/apps/kate/pate/isort_plugin.py

You will then need to restart kate and enable Python Plugins as well as the isort plugin itself.

Installing isort's for your preferred text editor
=================================================

Several plugins have been written that enable to use isort from within a variety of text-editors.
You can find a full list of them `on the isort wiki <https://github.com/timothycrosley/isort/wiki/isort-Plugins>`_.
Additionally, I will enthusiastically accept pull requests that include plugins for other text editors
and add documentation for them as I am notified.

How does isort work?
====================

isort parses specified files for global level import lines (imports outside of try / except blocks, functions, etc..)
and puts them all at the top of the file grouped together by the type of import:

- Future
- Python Standard Library
- Third Party
- Current Python Project
- Explicitly Local (. before import, as in: ``from . import x``)
- Custom Separate Sections (Defined by forced_separate list in configuration file)
- Custom Sections (Defined by sections list in configuration file)

Inside of each section the imports are sorted alphabetically. isort automatically removes duplicate python imports,
and wraps long from imports to the specified line length (defaults to 79).

When will isort not work?
=========================

If you ever have the situation where you need to have a try / except block in the middle of top-level imports or if
your import order is directly linked to precedence.

For example: a common practice in Django settings files is importing * from various settings files to form
a new settings file. In this case if any of the imports change order you are changing the settings definition itself.

However, you can configure isort to skip over just these files - or even to force certain imports to the top.

Configuring isort
=================

If you find the default isort settings do not work well for your project, isort provides several ways to adjust
the behavior.

To configure isort for a single user create a ``~/.isort.cfg`` or ``$XDG_CONFIG_HOME/isort.cfg`` file:

.. code-block:: ini

    [settings]
    line_length=120
    force_to_top=file1.py,file2.py
    skip=file3.py,file4.py
    known_future_library=future,pies
    known_standard_library=std,std2
    known_third_party=randomthirdparty
    known_first_party=mylib1,mylib2
    indent='    '
    multi_line_output=3
    length_sort=1
    forced_separate=django.contrib,django.utils
    default_section=FIRSTPARTY
    no_lines_before=LOCALFOLDER

Additionally, you can specify project level configuration simply by placing a ``.isort.cfg`` file at the root of your
project. isort will look up to 25 directories up, from the file it is ran against, to find a project specific configuration.

Or, if you prefer, you can add an ``isort`` or ``tool:isort`` section to your project's ``setup.cfg`` or ``tox.ini`` file with any desired settings.

You can also add your desired settings under a ``[tool.isort]`` section in your ``pyproject.toml`` file.

You can then override any of these settings by using command line arguments, or by passing in override values to the
SortImports class.

Finally, as of version 3.0 isort supports editorconfig files using the standard syntax defined here:
https://editorconfig.org/

Meaning you place any standard isort configuration parameters within a .editorconfig file under the ``*.py`` section
and they will be honored.

For a full list of isort settings and their meanings `take a look at the isort wiki <https://github.com/timothycrosley/isort/wiki/isort-Settings>`_.

Multi line output modes
=======================

You will notice above the "multi_line_output" setting. This setting defines how from imports wrap when they extend
past the line_length limit and has 6 possible settings:

**0 - Grid**

.. code-block:: python

    from third_party import (lib1, lib2, lib3,
                             lib4, lib5, ...)

**1 - Vertical**

.. code-block:: python

    from third_party import (lib1,
                             lib2,
                             lib3
                             lib4,
                             lib5,
                             ...)

**2 - Hanging Indent**

.. code-block:: python

    from third_party import \
        lib1, lib2, lib3, \
        lib4, lib5, lib6

**3 - Vertical Hanging Indent**

.. code-block:: python

    from third_party import (
        lib1,
        lib2,
        lib3,
        lib4,
    )

**4 - Hanging Grid**

.. code-block:: python

    from third_party import (
        lib1, lib2, lib3, lib4,
        lib5, ...)

**5 - Hanging Grid Grouped**

.. code-block:: python

    from third_party import (
        lib1, lib2, lib3, lib4,
        lib5, ...
    )

**6 - Hanging Grid Grouped, No Trailing Comma**

In Mode 5 isort leaves a single extra space to maintain consistency of output when a comma is added at the end.
Mode 6 is the same - except that no extra space is maintained leading to the possibility of lines one character longer.
You can enforce a trailing comma by using this in conjunction with `-tc` or `trailing_comma: True`.

.. code-block:: python

    from third_party import (
        lib1, lib2, lib3, lib4,
        lib5
    )

**7 - NOQA**

.. code-block:: python

    from third_party import lib1, lib2, lib3, ...  # NOQA

Alternatively, you can set ``force_single_line`` to ``True`` (``-sl`` on the command line) and every import will appear on its
own line:

.. code-block:: python

    from third_party import lib1
    from third_party import lib2
    from third_party import lib3
    ...

Note: to change the how constant indents appear - simply change the indent property with the following accepted formats:

*   Number of spaces you would like. For example: 4 would cause standard 4 space indentation.
*   Tab
*   A verbatim string with quotes around it.

For example:

.. code-block:: python

    "    "

is equivalent to 4.

For the import styles that use parentheses, you can control whether or not to
include a trailing comma after the last import with the ``include_trailing_comma``
option (defaults to ``False``).

Intelligently Balanced Multi-line Imports
=========================================

As of isort 3.1.0 support for balanced multi-line imports has been added.
With this enabled isort will dynamically change the import length to the one that produces the most balanced grid,
while staying below the maximum import length defined.

Example:

.. code-block:: python

    from __future__ import (absolute_import, division,
                            print_function, unicode_literals)

Will be produced instead of:

.. code-block:: python

    from __future__ import (absolute_import, division, print_function,
                            unicode_literals)

To enable this set ``balanced_wrapping`` to ``True`` in your config or pass the ``-e`` option into the command line utility.

Custom Sections and Ordering
============================

You can change the section order with ``sections`` option from the default of:

.. code-block:: ini

    FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER

to your preference:

.. code-block:: ini

    sections=FUTURE,STDLIB,FIRSTPARTY,THIRDPARTY,LOCALFOLDER

You also can define your own sections and their order.

Example:

.. code-block:: ini

    known_django=django
    known_pandas=pandas,numpy
    sections=FUTURE,STDLIB,DJANGO,THIRDPARTY,PANDAS,FIRSTPARTY,LOCALFOLDER

would create two new sections with the specified known modules.

The ``no_lines_before`` option will prevent the listed sections from being split from the previous section by an empty line.

Example:

.. code-block:: ini

   sections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER
   no_lines_before=LOCALFOLDER

would produce a section with both FIRSTPARTY and LOCALFOLDER modules combined.

Auto-comment import sections
============================

Some projects prefer to have import sections uniquely titled to aid in identifying the sections quickly
when visually scanning. isort can automate this as well. To do this simply set the ``import_heading_{section_name}``
setting for each section you wish to have auto commented - to the desired comment.

For Example:

.. code-block:: ini

    import_heading_stdlib=Standard Library
    import_heading_firstparty=My Stuff

Would lead to output looking like the following:

.. code-block:: python

    # Standard Library
    import os
    import sys

    import django.settings

    # My Stuff
    import myproject.test

Ordering by import length
=========================

isort also makes it easy to sort your imports by length, simply by setting the ``length_sort`` option to ``True``.
This will result in the following output style:

.. code-block:: python

    from evn.util import (
        Pool,
        Dict,
        Options,
        Constant,
        DecayDict,
        UnexpectedCodePath,
    )

It is also possible to opt-in to sorting imports by length for only specific
sections by using ``length_sort_`` followed by the section name as a
configuration item, e.g.::

    length_sort_stdlib=1

Skip processing of imports (outside of configuration)
=====================================================

To make isort ignore a single import simply add a comment at the end of the import line containing the text ``isort:skip``:

.. code-block:: python

    import module  # isort:skip

or:

.. code-block:: python

    from xyz import (abc,  # isort:skip
                     yo,
                     hey)

To make isort skip an entire file simply add ``isort:skip_file`` to the module's doc string:

.. code-block:: python

    """ my_module.py
        Best module ever

       isort:skip_file
    """

    import b
    import a

Adding an import to multiple files
==================================

isort makes it easy to add an import statement across multiple files, while being assured it's correctly placed.

From the command line:

.. code-block:: bash

    isort -a "from __future__ import print_function" *.py

from within Kate:

.. code-block::

    ctrl+]

or:

.. code-block::

    menu > Python > Add Import

Removing an import from multiple files
======================================

isort also makes it easy to remove an import from multiple files, without having to be concerned with how it was originally
formatted.

From the command line:

.. code-block:: bash

    isort -rm "os.system" *.py

from within Kate:

.. code-block::

    ctrl+shift+]

or:

.. code-block::

    menu > Python > Remove Import

Using isort to verify code
==========================

The ``--check-only`` option
---------------------------

isort can also be used to used to verify that code is correctly formatted by running it with ``-c``.
Any files that contain incorrectly sorted and/or formatted imports will be outputted to ``stderr``.

.. code-block:: bash

    isort **/*.py -c -vb

    SUCCESS: /home/timothy/Projects/Open_Source/isort/isort_kate_plugin.py Everything Looks Good!
    ERROR: /home/timothy/Projects/Open_Source/isort/isort/isort.py Imports are incorrectly sorted.

One great place this can be used is with a pre-commit git hook, such as this one by @acdha:

https://gist.github.com/acdha/8717683

This can help to ensure a certain level of code quality throughout a project.


Git hook
--------

isort provides a hook function that can be integrated into your Git pre-commit script to check
Python code before committing.

To cause the commit to fail if there are isort errors (strict mode), include the following in
``.git/hooks/pre-commit``:

.. code-block:: python

    #!/usr/bin/env python
    import sys
    from isort.hooks import git_hook

    sys.exit(git_hook(strict=True, modify=True))

If you just want to display warnings, but allow the commit to happen anyway, call ``git_hook`` without
the `strict` parameter. If you want to display warnings, but not also fix the code, call ``git_hook`` without
the `modify` parameter.

Setuptools integration
----------------------

Upon installation, isort enables a ``setuptools`` command that checks Python files
declared by your project.

Running ``python setup.py isort`` on the command line will check the files
listed in your ``py_modules`` and ``packages``.  If any warning is found,
the command will exit with an error code:

.. code-block:: bash

    $ python setup.py isort

Also, to allow users to be able to use the command without having to install
isort themselves, add isort to the setup_requires of your ``setup()`` like so:

.. code-block:: python

    setup(
        name="project",
        packages=["project"],

        setup_requires=[
            "isort"
        ]
    )


Why isort?
==========

isort simply stands for import sort. It was originally called "sortImports" however I got tired of typing the extra
characters and came to the realization camelCase is not pythonic.

I wrote isort because in an organization I used to work in the manager came in one day and decided all code must
have alphabetically sorted imports. The code base was huge - and he meant for us to do it by hand. However, being a
programmer - I'm too lazy to spend 8 hours mindlessly performing a function, but not too lazy to spend 16
hours automating it. I was given permission to open source sortImports and here we are :)

--------------------------------------------

Thanks and I hope you find isort useful!

~Timothy Crosley



  * [itsdangerous-1.1.0](https://palletsprojects.com/p/itsdangerous/) itsdangerous
============

... so better sign this

Various helpers to pass data to untrusted environments and to get it
back safe and sound. Data is cryptographically signed to ensure that a
token has not been tampered with.

It's possible to customize how data is serialized. Data is compressed as
needed. A timestamp can be added and verified automatically while
loading a token.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    pip install -U itsdangerous

.. _pip: https://pip.pypa.io/en/stable/quickstart/


A Simple Example
----------------

Here's how you could generate a token for transmitting a user's id and
name between web requests.

.. code-block:: python

    from itsdangerous import URLSafeSerializer
    auth_s = URLSafeSerializer("secret key", "auth")
    token = auth_s.dumps({"id": 5, "name": "itsdangerous"})

    print(token)
    # eyJpZCI6NSwibmFtZSI6Iml0c2Rhbmdlcm91cyJ9.6YP6T0BaO67XP--9UzTrmurXSmg

    data = auth_s.loads(token)
    print(data["name"])
    # itsdangerous


Donate
------

The Pallets organization develops and supports itsdangerous and other
popular packages. In order to grow the community of contributors and
users, and allow the maintainers to devote more time to the projects,
`please donate today`_.

.. _please donate today: https://palletsprojects.com/donate


Links
-----

*   Website: https://palletsprojects.com/p/itsdangerous/
*   Documentation: https://itsdangerous.palletsprojects.com/
*   License: `BSD <https://github.com/pallets/itsdangerous/blob/master/LICENSE.rst>`_
*   Releases: https://pypi.org/project/itsdangerous/
*   Code: https://github.com/pallets/itsdangerous
*   Issue tracker: https://github.com/pallets/itsdangerous/issues
*   Test status: https://travis-ci.org/pallets/itsdangerous
*   Test coverage: https://codecov.io/gh/pallets/itsdangerous



  * [jdcal-1.4.1](https://github.com/phn/jdcal) jdcal
=====

.. _TPM: http://www.sal.wisc.edu/~jwp/astro/tpm/tpm.html
.. _Jeffrey W. Percival: http://www.sal.wisc.edu/~jwp/
.. _IAU SOFA: http://www.iausofa.org/
.. _pip: https://pypi.org/project/pip/
.. _easy_install: https://setuptools.readthedocs.io/en/latest/easy_install.html

.. image:: https://travis-ci.org/phn/jdcal.svg?branch=master
    :target: https://travis-ci.org/phn/jdcal


This module contains functions for converting between Julian dates and
calendar dates.

A function for converting Gregorian calendar dates to Julian dates, and
another function for converting Julian calendar dates to Julian dates
are defined. Two functions for the reverse calculations are also
defined.

Different regions of the world switched to Gregorian calendar from
Julian calendar on different dates. Having separate functions for Julian
and Gregorian calendars allow maximum flexibility in choosing the
relevant calendar.

Julian dates are stored in two floating point numbers (double).  Julian
dates, and Modified Julian dates, are large numbers. If only one number
is used, then the precision of the time stored is limited. Using two
numbers, time can be split in a manner that will allow maximum
precision. For example, the first number could be the Julian date for
the beginning of a day and the second number could be the fractional
day. Calculations that need the latter part can now work with maximum
precision.

All the above functions are "proleptic". This means that they work for
dates on which the concerned calendar is not valid. For example,
Gregorian calendar was not used prior to around October 1582.

A function to test if a given Gregorian calendar year is a leap year is
also defined.

Zero point of Modified Julian Date (MJD) and the MJD of 2000/1/1
12:00:00 are also given as module level constants.

Examples
--------

Some examples are given below. For more information see
https://oneau.wordpress.com/2011/08/30/jdcal/.

Gregorian calendar:

.. code-block:: python

    >>> from jdcal import gcal2jd, jd2gcal
    >>> gcal2jd(2000,1,1)
    (2400000.5, 51544.0)
    >>> 2400000.5 + 51544.0 + 0.5
    2451545.0

    >>> gcal2jd(2000,2,30)
    (2400000.5, 51604.0)
    >>> gcal2jd(2000,3,1)
    (2400000.5, 51604.0)
    >>> gcal2jd(2001,2,30)
    (2400000.5, 51970.0)
    >>> gcal2jd(2001,3,2)
    (2400000.5, 51970.0)

    >>> jd2gcal(*gcal2jd(2000,1,1))
    (2000, 1, 1, 0.0)
    >>> jd2gcal(*gcal2jd(1950,1,1))
    (1950, 1, 1, 0.0)

    >>> gcal2jd(2000,1,1)
    (2400000.5, 51544.0)
    >>> jd2gcal(2400000.5, 51544.0)
    (2000, 1, 1, 0.0)
    >>> jd2gcal(2400000.5, 51544.5)
    (2000, 1, 1, 0.5)
    >>> jd2gcal(2400000.5, 51544.245)
    (2000, 1, 1, 0.24500000000261934)
    >>> jd2gcal(2400000.5, 51544.1)
    (2000, 1, 1, 0.099999999998544808)
    >>> jd2gcal(2400000.5, 51544.75)
    (2000, 1, 1, 0.75)

Julian calendar:

.. code-block:: python

    >>> jd2jcal(*jcal2jd(2000, 1, 1))
    (2000, 1, 1, 0.0)
    >>> jd2jcal(*jcal2jd(-4000, 10, 11))
    (-4000, 10, 11, 0.0)

Gregorian leap year:

.. code-block:: python

    >>> from jdcal import is_leap
    >>> is_leap(2000)
    True
    >>> is_leap(2100)
    False

JD for zero point of MJD, and MJD for JD2000.0:

.. code-block:: python

    >>> from jdcal import MJD_0, MJD_JD2000
    >>> print MJD_0
    2400000.5
    >>> print MJD_JD2000
    51544.5


Installation
------------

The module can be installed using `pip`_ or `easy_install`_::

  $ pip install jdcal

or,

::

  $ easy_install jdcal


Tests are in ``test_jdcal.py``.

Credits
--------

1. A good amount of the code is based on the excellent `TPM`_ C library
   by `Jeffrey W. Percival`_.
2. The inspiration to split Julian dates into two numbers came from the
   `IAU SOFA`_ C library. No code or algorithm from the SOFA library is
   used in `jdcal`.

License
-------

Released under BSD; see LICENSE.txt.

For comments and suggestions, email to user `prasanthhn` in the `gmail.com`
domain.



  * [jedi-0.14.1](https://github.com/davidhalter/jedi) ###################################################################
Jedi - an awesome autocompletion/static analysis library for Python
###################################################################

.. image:: https://img.shields.io/pypi/v/jedi.svg?style=flat
    :target: https://pypi.python.org/pypi/jedi
    :alt: PyPI version

.. image:: https://img.shields.io/pypi/pyversions/jedi.svg
    :target: https://pypi.python.org/pypi/jedi
    :alt: Supported Python versions

.. image:: https://travis-ci.org/davidhalter/jedi.svg?branch=master
    :target: https://travis-ci.org/davidhalter/jedi
    :alt: Linux Tests

.. image:: https://ci.appveyor.com/api/projects/status/mgva3bbawyma1new/branch/master?svg=true
    :target: https://ci.appveyor.com/project/davidhalter/jedi/branch/master
    :alt: Windows Tests

.. image:: https://coveralls.io/repos/davidhalter/jedi/badge.svg?branch=master
    :target: https://coveralls.io/r/davidhalter/jedi
    :alt: Coverage status


*If you have specific questions, please add an issue or ask on* `Stack Overflow
<https://stackoverflow.com/questions/tagged/python-jedi>`_ *with the label* ``python-jedi``.


Jedi is a static analysis tool for Python that can be used in IDEs/editors.
Jedi has a focus on autocompletion and goto functionality. Jedi is fast and is
very well tested. It understands Python and stubs on a deep level.

Jedi has support for different goto functions. It's possible to search for
usages and list names in a Python file to get information about them.

Jedi uses a very simple API to connect with IDE's. There's a reference
implementation as a `VIM-Plugin <https://github.com/davidhalter/jedi-vim>`_,
which uses Jedi's autocompletion.  We encourage you to use Jedi in your IDEs.
Autocompletion in your REPL is also possible, IPython uses it natively and for
the CPython REPL you have to install it.

Jedi can currently be used with the following editors/projects:

- Vim (jedi-vim_, YouCompleteMe_, deoplete-jedi_, completor.vim_)
- Emacs (Jedi.el_, company-mode_, elpy_, anaconda-mode_, ycmd_)
- Sublime Text (SublimeJEDI_ [ST2 + ST3], anaconda_ [only ST3])
- TextMate_ (Not sure if it's actually working)
- Kate_ version 4.13+ supports it natively, you have to enable it, though. [`proof
  <https://projects.kde.org/projects/kde/applications/kate/repository/show?rev=KDE%2F4.13>`_]
- Atom_ (autocomplete-python-jedi_)
- `GNOME Builder`_ (with support for GObject Introspection)
- `Visual Studio Code`_ (via `Python Extension <https://marketplace.visualstudio.com/items?itemName=ms-python.python>`_)
- Gedit (gedi_)
- wdb_ - Web Debugger
- `Eric IDE`_ (Available as a plugin)
- `IPython 6.0.0+ <https://ipython.readthedocs.io/en/stable/whatsnew/version6.html>`_

and many more!


Here are some pictures taken from jedi-vim_:

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_complete.png

Completion for almost anything (Ctrl+Space).

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_function.png

Display of function/class bodies, docstrings.

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_pydoc.png

Pydoc support (Shift+k).

There is also support for goto and renaming.

Get the latest version from `github <https://github.com/davidhalter/jedi>`_
(master branch should always be kind of stable/working).

Docs are available at `https://jedi.readthedocs.org/en/latest/
<https://jedi.readthedocs.org/en/latest/>`_. Pull requests with documentation
enhancements and/or fixes are awesome and most welcome. Jedi uses `semantic
versioning <https://semver.org/>`_.

If you want to stay up-to-date (News / RFCs), please subscribe to this `github
thread <https://github.com/davidhalter/jedi/issues/1063>`_.:



Installation
============

    pip install jedi

Note: This just installs the Jedi library, not the editor plugins. For
information about how to make it work with your editor, refer to the
corresponding documentation.

You don't want to use ``pip``? Please refer to the `manual
<https://jedi.readthedocs.org/en/latest/docs/installation.html>`_.


Feature Support and Caveats
===========================

Jedi really understands your Python code. For a comprehensive list what Jedi
understands, see: `Features
<https://jedi.readthedocs.org/en/latest/docs/features.html>`_. A list of
caveats can be found on the same page.

You can run Jedi on CPython 2.7 or 3.4+ but it should also
understand/parse code older than those versions. Additionally you should be able
to use `Virtualenvs <https://jedi.readthedocs.org/en/latest/docs/api.html#environments>`_
very well.

Tips on how to use Jedi efficiently can be found `here
<https://jedi.readthedocs.org/en/latest/docs/features.html#recipes>`_.

API
---

You can find the documentation for the `API here <https://jedi.readthedocs.org/en/latest/docs/api.html>`_.


Autocompletion / Goto / Pydoc
-----------------------------

Please check the API for a good explanation. There are the following commands:

- ``jedi.Script.goto_assignments``
- ``jedi.Script.completions``
- ``jedi.Script.usages``

The returned objects are very powerful and really all you might need.


Autocompletion in your REPL (IPython, etc.)
-------------------------------------------

Starting with IPython `6.0.0` Jedi is a dependency of IPython. Autocompletion
in IPython is therefore possible without additional configuration.

It's possible to have Jedi autocompletion in REPL modes - `example video <https://vimeo.com/122332037>`_.
This means that in Python you can enable tab completion in a `REPL
<https://jedi.readthedocs.org/en/latest/docs/usage.html#tab-completion-in-the-python-shell>`_.


Static Analysis
------------------------

To do all forms of static analysis, please try to use ``jedi.names``. It will
return a list of names that you can use to infer types and so on.


Refactoring
-----------

Jedi's parser would support refactoring, but there's no API to use it right
now.  If you're interested in helping out here, let me know. With the latest
parser changes, it should be very easy to actually make it work.


Development
===========

There's a pretty good and extensive `development documentation
<https://jedi.readthedocs.org/en/latest/docs/development.html>`_.


Testing
=======

The test suite depends on ``tox`` and ``pytest``::

    pip install tox pytest

To run the tests for all supported Python versions::

    tox

If you want to test only a specific Python version (e.g. Python 2.7), it's as
easy as ::

    tox -e py27

Tests are also run automatically on `Travis CI
<https://travis-ci.org/davidhalter/jedi/>`_.

For more detailed information visit the `testing documentation
<https://jedi.readthedocs.org/en/latest/docs/testing.html>`_.


Acknowledgements
================

- Takafumi Arakaki (@tkf) for creating a solid test environment and a lot of
  other things.
- Danilo Bargen (@dbrgn) for general housekeeping and being a good friend :).
- Guido van Rossum (@gvanrossum) for creating the parser generator pgen2
  (originally used in lib2to3).



.. _jedi-vim: https://github.com/davidhalter/jedi-vim
.. _youcompleteme: https://github.com/ycm-core/YouCompleteMe
.. _deoplete-jedi: https://github.com/zchee/deoplete-jedi
.. _completor.vim: https://github.com/maralla/completor.vim
.. _Jedi.el: https://github.com/tkf/emacs-jedi
.. _company-mode: https://github.com/syohex/emacs-company-jedi
.. _elpy: https://github.com/jorgenschaefer/elpy
.. _anaconda-mode: https://github.com/proofit404/anaconda-mode
.. _ycmd: https://github.com/abingham/emacs-ycmd
.. _sublimejedi: https://github.com/srusskih/SublimeJEDI
.. _anaconda: https://github.com/DamnWidget/anaconda
.. _wdb: https://github.com/Kozea/wdb
.. _TextMate: https://github.com/lawrenceakka/python-jedi.tmbundle
.. _Kate: https://kate-editor.org
.. _Atom: https://atom.io/
.. _autocomplete-python-jedi: https://atom.io/packages/autocomplete-python-jedi
.. _GNOME Builder: https://wiki.gnome.org/Apps/Builder
.. _Visual Studio Code: https://code.visualstudio.com/
.. _gedi: https://github.com/isamert/gedi
.. _Eric IDE: https://eric-ide.python-projects.org


.. :changelog:

Changelog
---------

0.15.1 (2019-08-13)
+++++++++++++++++++

- Small bugfix and removal of a print statement

0.15.0 (2019-08-11)
+++++++++++++++++++

- Added file path completions, there's a **new ``Completion.type``** ``path``,
  now. Example: ``'/ho`` -> ``'/home/``
- ``*args``/``**kwargs`` resolving. If possible Jedi replaces the parameters
  with the actual alternatives.
- Better support for enums/dataclasses
- When using Interpreter, properties are now executed, since a lot of people
  have complained about this. Discussion in #1299, #1347.

New APIs:

- ``Definition.get_signatures() -> List[Signature]``. Signatures are similar to
  ``CallSignature``. ``Definition.params`` is therefore deprecated.
- ``Signature.to_string()`` to format call signatures.
- ``Signature.params -> List[ParamDefinition]``, ParamDefinition has the
  following additional attributes ``infer_default()``, ``infer_annotation()``,
  ``to_string()``, and ``kind``.
- ``Definition.execute() -> List[Definition]``, makes it possible to infer
    return values of functions.


0.14.1 (2019-07-13)
+++++++++++++++++++

- CallSignature.index should now be working a lot better
- A couple of smaller bugfixes

0.14.0 (2019-06-20)
+++++++++++++++++++

- Added ``goto_*(prefer_stubs=True)`` as well as ``goto_*(prefer_stubs=True)``
- Stubs are used now for type inference
- Typeshed is used for better type inference
- Reworked Definition.full_name, should have more correct return values

0.13.3 (2019-02-24)
+++++++++++++++++++

- Fixed an issue with embedded Python, see https://github.com/davidhalter/jedi-vim/issues/870

0.13.2 (2018-12-15)
+++++++++++++++++++

- Fixed a bug that led to Jedi spawning a lot of subprocesses.

0.13.1 (2018-10-02)
+++++++++++++++++++

- Bugfixes, because tensorflow completions were still slow.

0.13.0 (2018-10-02)
+++++++++++++++++++

- A small release. Some bug fixes.
- Remove Python 3.3 support. Python 3.3 support has been dropped by the Python
  foundation.
- Default environments are now using the same Python version as the Python
  process. In 0.12.x, we used to load the latest Python version on the system.
- Added ``include_builtins`` as a parameter to usages.
- ``goto_assignments`` has a new ``follow_builtin_imports`` parameter that
  changes the previous behavior slightly.

0.12.1 (2018-06-30)
+++++++++++++++++++

- This release forces you to upgrade parso. If you don't, nothing will work
  anymore. Otherwise changes should be limited to bug fixes. Unfortunately Jedi
  still uses a few internals of parso that make it hard to keep compatibility
  over multiple releases. Parso >=0.3.0 is going to be needed.

0.12.0 (2018-04-15)
+++++++++++++++++++

- Virtualenv/Environment support
- F-String Completion/Goto Support
- Cannot crash with segfaults anymore
- Cleaned up import logic
- Understand async/await and autocomplete it (including async generators)
- Better namespace completions
- Passing tests for Windows (including CI for Windows)
- Remove Python 2.6 support

0.11.1 (2017-12-14)
+++++++++++++++++++

- Parso update - the caching layer was broken
- Better usages - a lot of internal code was ripped out and improved.

0.11.0 (2017-09-20)
+++++++++++++++++++

- Split Jedi's parser into a separate project called ``parso``.
- Avoiding side effects in REPL completion.
- Numpy docstring support should be much better.
- Moved the `settings.*recursion*` away, they are no longer usable.

0.10.2 (2017-04-05)
+++++++++++++++++++

- Python Packaging sucks. Some files were not included in 0.10.1.

0.10.1 (2017-04-05)
+++++++++++++++++++

- Fixed a few very annoying bugs.
- Prepared the parser to be factored out of Jedi.

0.10.0 (2017-02-03)
+++++++++++++++++++

- Actual semantic completions for the complete Python syntax.
- Basic type inference for ``yield from`` PEP 380.
- PEP 484 support (most of the important features of it). Thanks Claude! (@reinhrst)
- Added ``get_line_code`` to ``Definition`` and ``Completion`` objects.
- Completely rewritten the type inference engine.
- A new and better parser for (fast) parsing diffs of Python code.

0.9.0 (2015-04-10)
++++++++++++++++++

- The import logic has been rewritten to look more like Python's. There is now
  an ``Evaluator.modules`` import cache, which resembles ``sys.modules``.
- Integrated the parser of 2to3. This will make refactoring possible. It will
  also be possible to check for error messages (like compiling an AST would give)
  in the future.
- With the new parser, the evaluation also completely changed. It's now simpler
  and more readable.
- Completely rewritten REPL completion.
- Added ``jedi.names``, a command to do static analysis. Thanks to that
  sourcegraph guys for sponsoring this!
- Alpha version of the linter.


0.8.1 (2014-07-23)
+++++++++++++++++++

- Bugfix release, the last release forgot to include files that improve
  autocompletion for builtin libraries. Fixed.

0.8.0 (2014-05-05)
+++++++++++++++++++

- Memory Consumption for compiled modules (e.g. builtins, sys) has been reduced
  drastically. Loading times are down as well (it takes basically as long as an
  import).
- REPL completion is starting to become usable.
- Various small API changes. Generally this release focuses on stability and
  refactoring of internal APIs.
- Introducing operator precedence, which makes calculating correct Array
  indices and ``__getattr__`` strings possible.

0.7.0 (2013-08-09)
++++++++++++++++++

- Switched from LGPL to MIT license.
- Added an Interpreter class to the API to make autocompletion in REPL
  possible.
- Added autocompletion support for namespace packages.
- Add sith.py, a new random testing method.

0.6.0 (2013-05-14)
++++++++++++++++++

- Much faster parser with builtin part caching.
- A test suite, thanks @tkf.

0.5 versions (2012)
+++++++++++++++++++

- Initial development.



  * [jeepney-0.4](https://gitlab.com/takluyver/jeepney) This is a low-level, pure Python DBus protocol client. It has an `I/O-free
<https://sans-io.readthedocs.io/>`__ core, and integration modules for different
event loops.

DBus is an inter-process communication system, mainly used in Linux.

`Jeepney docs on Readthedocs <https://jeepney.readthedocs.io/en/latest/>`__

This project is experimental, and there are a
number of `more mature Python DBus bindings <https://www.freedesktop.org/wiki/Software/DBusBindings/#python>`__.

  * [jellyfish-0.7.2](http://github.com/jamesturk/jellyfish) =========
jellyfish
=========

.. image:: https://travis-ci.com/jamesturk/jellyfish.svg?branch=master
    :target: https://travis-ci.com/jamesturk/jellyfish

.. image:: https://coveralls.io/repos/jamesturk/jellyfish/badge.png?branch=master
    :target: https://coveralls.io/r/jamesturk/jellyfish

.. image:: https://img.shields.io/pypi/v/jellyfish.svg
    :target: https://pypi.python.org/pypi/jellyfish

.. image:: https://readthedocs.org/projects/jellyfish/badge/?version=latest
    :target: https://readthedocs.org/projects/jellyfish/?badge=latest
    :alt: Documentation Status

.. image:: https://ci.appveyor.com/api/projects/status/9xeyl1f5sd5pl40h?svg=true
    :target: https://ci.appveyor.com/project/jamesturk/jellyfish/

Jellyfish is a python library for doing approximate and phonetic matching of strings.

Written by James Turk <dev@jamesturk.net> and Michael Stephens.

See https://github.com/jamesturk/jellyfish/graphs/contributors for contributors.

See http://jellyfish.readthedocs.io for documentation.

Source is available at http://github.com/jamesturk/jellyfish.

**Jellyfish >= 0.7 only supports Python 3, if you need Python 2 please use 0.6.x.**

Included Algorithms
===================

String comparison:

* Levenshtein Distance
* Damerau-Levenshtein Distance
* Jaro Distance
* Jaro-Winkler Distance
* Match Rating Approach Comparison
* Hamming Distance

Phonetic encoding:

* American Soundex
* Metaphone
* NYSIIS (New York State Identification and Intelligence System)
* Match Rating Codex

Example Usage
=============

>>> import jellyfish
>>> jellyfish.levenshtein_distance(u'jellyfish', u'smellyfish')
2
>>> jellyfish.jaro_distance(u'jellyfish', u'smellyfish')
0.89629629629629637
>>> jellyfish.damerau_levenshtein_distance(u'jellyfish', u'jellyfihs')
1

>>> jellyfish.metaphone(u'Jellyfish')
'JLFX'
>>> jellyfish.soundex(u'Jellyfish')
'J412'
>>> jellyfish.nysiis(u'Jellyfish')
'JALYF'
>>> jellyfish.match_rating_codex(u'Jellyfish')
'JLLFSH'

Running Tests
=============

If you are interested in contributing to Jellyfish, you may want to
run tests locally. Jellyfish uses tox_ to run tests, which you can
setup and run as follows::

  pip install tox
  # cd jellyfish/
  tox

.. _tox: https://tox.readthedocs.io/en/latest/
  * [jmespath-0.9.4](https://github.com/jmespath/jmespath.py) JMESPath
========


.. image:: https://badges.gitter.im/Join Chat.svg
   :target: https://gitter.im/jmespath/chat


.. image:: https://travis-ci.org/jmespath/jmespath.py.svg?branch=develop
    :target: https://travis-ci.org/jmespath/jmespath.py


.. image:: https://codecov.io/github/jmespath/jmespath.py/coverage.svg?branch=develop
    :target: https://codecov.io/github/jmespath/jmespath.py?branch=develop


JMESPath (pronounced "james path") allows you to declaratively specify how to
extract elements from a JSON document.

For example, given this document::

    {"foo": {"bar": "baz"}}

The jmespath expression ``foo.bar`` will return "baz".

JMESPath also supports:

Referencing elements in a list.  Given the data::

    {"foo": {"bar": ["one", "two"]}}

The expression: ``foo.bar[0]`` will return "one".
You can also reference all the items in a list using the ``*``
syntax::

   {"foo": {"bar": [{"name": "one"}, {"name": "two"}]}}

The expression: ``foo.bar[*].name`` will return ["one", "two"].
Negative indexing is also supported (-1 refers to the last element
in the list).  Given the data above, the expression
``foo.bar[-1].name`` will return "two".

The ``*`` can also be used for hash types::

   {"foo": {"bar": {"name": "one"}, "baz": {"name": "two"}}}

The expression: ``foo.*.name`` will return ["one", "two"].


API
===

The ``jmespath.py`` library has two functions
that operate on python data structures.  You can use ``search``
and give it the jmespath expression and the data:

.. code:: python

    >>> import jmespath
    >>> path = jmespath.search('foo.bar', {'foo': {'bar': 'baz'}})
    'baz'

Similar to the ``re`` module, you can use the ``compile`` function
to compile the JMESPath expression and use this parsed expression
to perform repeated searches:

.. code:: python

    >>> import jmespath
    >>> expression = jmespath.compile('foo.bar')
    >>> expression.search({'foo': {'bar': 'baz'}})
    'baz'
    >>> expression.search({'foo': {'bar': 'other'}})
    'other'

This is useful if you're going to use the same jmespath expression to
search multiple documents.  This avoids having to reparse the
JMESPath expression each time you search a new document.

Options
-------

You can provide an instance of ``jmespath.Options`` to control how
a JMESPath expression is evaluated.  The most common scenario for
using an ``Options`` instance is if you want to have ordered output
of your dict keys.  To do this you can use either of these options:

.. code:: python

    >>> import jmespath
    >>> jmespath.search('{a: a, b: b}',
    ...                 mydata,
    ...                 jmespath.Options(dict_cls=collections.OrderedDict))


    >>> import jmespath
    >>> parsed = jmespath.compile('{a: a, b: b}')
    >>> parsed.search(mydata,
    ...               jmespath.Options(dict_cls=collections.OrderedDict))


Custom Functions
~~~~~~~~~~~~~~~~

The JMESPath language has numerous
`built-in functions
<http://jmespath.org/specification.html#built-in-functions>`__, but it is
also possible to add your own custom functions.  Keep in mind that
custom function support in jmespath.py is experimental and the API may
change based on feedback.

**If you have a custom function that you've found useful, consider submitting
it to jmespath.site and propose that it be added to the JMESPath language.**
You can submit proposals
`here <https://github.com/jmespath/jmespath.site/issues>`__.

To create custom functions:

* Create a subclass of ``jmespath.functions.Functions``.
* Create a method with the name ``_func_<your function name>``.
* Apply the ``jmespath.functions.signature`` decorator that indicates
  the expected types of the function arguments.
* Provide an instance of your subclass in a ``jmespath.Options`` object.

Below are a few examples:

.. code:: python

    import jmespath
    from jmespath import functions

    # 1. Create a subclass of functions.Functions.
    #    The function.Functions base class has logic
    #    that introspects all of its methods and automatically
    #    registers your custom functions in its function table.
    class CustomFunctions(functions.Functions):

        # 2 and 3.  Create a function that starts with _func_
        # and decorate it with @signature which indicates its
        # expected types.
        # In this example, we're creating a jmespath function
        # called "unique_letters" that accepts a single argument
        # with an expected type "string".
        @functions.signature({'types': ['string']})
        def _func_unique_letters(self, s):
            # Given a string s, return a sorted
            # string of unique letters: 'ccbbadd' ->  'abcd'
            return ''.join(sorted(set(s)))

        # Here's another example.  This is creating
        # a jmespath function called "my_add" that expects
        # two arguments, both of which should be of type number.
        @functions.signature({'types': ['number']}, {'types': ['number']})
        def _func_my_add(self, x, y):
            return x + y

    # 4. Provide an instance of your subclass in a Options object.
    options = jmespath.Options(custom_functions=CustomFunctions())

    # Provide this value to jmespath.search:
    # This will print 3
    print(
        jmespath.search(
            'my_add(`1`, `2`)', {}, options=options)
    )

    # This will print "abcd"
    print(
        jmespath.search(
            'foo.bar | unique_letters(@)',
            {'foo': {'bar': 'ccbbadd'}},
            options=options)
    )

Again, if you come up with useful functions that you think make
sense in the JMESPath language (and make sense to implement in all
JMESPath libraries, not just python), please let us know at
`jmespath.site <https://github.com/jmespath/jmespath.site/issues>`__.


Specification
=============

If you'd like to learn more about the JMESPath language, you can check out
the `JMESPath tutorial <http://jmespath.org/tutorial.html>`__.  Also check
out the `JMESPath examples page <http://jmespath.org/examples.html>`__ for
examples of more complex jmespath queries.

The grammar is specified using ABNF, as described in
`RFC4234 <http://www.ietf.org/rfc/rfc4234.txt>`_.
You can find the most up to date
`grammar for JMESPath here <http://jmespath.org/specification.html#grammar>`__.

You can read the full
`JMESPath specification here <http://jmespath.org/specification.html>`__.


Testing
=======

In addition to the unit tests for the jmespath modules,
there is a ``tests/compliance`` directory that contains
.json files with test cases.  This allows other implementations
to verify they are producing the correct output.  Each json
file is grouped by feature.


Discuss
=======

Join us on our `Gitter channel <https://gitter.im/jmespath/chat>`__
if you want to chat or if you have any questions.



  * [joblib-0.12.2](https://joblib.readthedocs.io) Joblib is a set of tools to provide **lightweight pipelining in
Python**. In particular:

1. transparent disk-caching of functions and lazy re-evaluation
   (memoize pattern)

2. easy simple parallel computing

Joblib is optimized to be **fast** and **robust** on large
data in particular and has specific optimizations for `numpy` arrays. It is
**BSD-licensed**.


    ==================== ===============================================
    **Documentation:**       https://joblib.readthedocs.io

    **Download:**            https://pypi.python.org/pypi/joblib#downloads

    **Source code:**         https://github.com/joblib/joblib

    **Report issues:**       https://github.com/joblib/joblib/issues
    ==================== ===============================================


Vision
--------

The vision is to provide tools to easily achieve better performance and
reproducibility when working with long running jobs.

 *  **Avoid computing the same thing twice**: code is often rerun again and
    again, for instance when prototyping computational-heavy jobs (as in
    scientific development), but hand-crafted solutions to alleviate this
    issue are error-prone and often lead to unreproducible results.

 *  **Persist to disk transparently**: efficiently persisting
    arbitrary objects containing large data is hard. Using
    joblib's caching mechanism avoids hand-written persistence and
    implicitly links the file on disk to the execution context of
    the original Python object. As a result, joblib's persistence is
    good for resuming an application status or computational job, eg
    after a crash.

Joblib addresses these problems while **leaving your code and your flow
control as unmodified as possible** (no framework, no new paradigms).

Main features
------------------

1) **Transparent and fast disk-caching of output value:** a memoize or
   make-like functionality for Python functions that works well for
   arbitrary Python objects, including very large numpy arrays. Separate
   persistence and flow-execution logic from domain logic or algorithmic
   code by writing the operations as a set of steps with well-defined
   inputs and  outputs: Python functions. Joblib can save their
   computation to disk and rerun it only if necessary::

      >>> from joblib import Memory
      >>> cachedir = 'your_cache_dir_goes_here'
      >>> mem = Memory(cachedir)
      >>> import numpy as np
      >>> a = np.vander(np.arange(3)).astype(np.float)
      >>> square = mem.cache(np.square)
      >>> b = square(a)                                   # doctest: +ELLIPSIS
      ________________________________________________________________________________
      [Memory] Calling square...
      square(array([[0., 0., 1.],
             [1., 1., 1.],
             [4., 2., 1.]]))
      ___________________________________________________________square - 0...s, 0.0min

      >>> c = square(a)
      >>> # The above call did not trigger an evaluation

2) **Embarrassingly parallel helper:** to make it easy to write readable
   parallel code and debug it quickly::

      >>> from joblib import Parallel, delayed
      >>> from math import sqrt
      >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))
      [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]


3) **Fast compressed Persistence**: a replacement for pickle to work
   efficiently on Python objects containing large data (
   *joblib.dump* & *joblib.load* ).

..
    >>> import shutil ; shutil.rmtree(cachedir)




  * [json2html-1.3.0](https://github.com/softvar/json2html) json2html
=========

Python wrapper to convert ``JSON`` into a human readable ``HTML Table`` representation.

|Latest Version| |Downloads| |Build|

.. |Build| image:: https://api.travis-ci.org/softvar/json2html.png

.. |Latest Version| image:: https://img.shields.io/pypi/v/json2html.svg
    :target: https://pypi.python.org/pypi/json2html

.. |Downloads| image:: https://img.shields.io/pypi/dm/json2html.svg
        :target: https://pypi.python.org/pypi/json2html

Features
--------

* User friendly tablular fomat, easy to read and share.
* If value of the key is array of objects and all the keys are same(value of the key is a dict of list), the module will club by default. Eg.

.. code-block:: bash

	input = {
		"sampleData": [{
			"a":1, "b":2, "c":3
		}, {
			"a":5, "b":6, "c":7
		}]
	}

	will create only one row combining the results. This feature can be turned off by explicitly passing an argument ``clubbing = False``.

* Generated table can be provided some ``attributes`` explicitly. Eg. giving an ``id``, ``class`` or any ``data-*`` attribute.
* Python 3 compatible

Live Demo
----------

`Click here <http://json2html.varunmalhotra.xyz/>`_ for the online demo.

List of valid arguments
-----------------------

``json2html.convert`` - The module's ``convert`` method accepts the following arguments:

===================== ================
Argument              Description
--------------------- ----------------
`json`                a valid JSON; This can either be a string in valid JSON format or a python object that is either dict-like or list-like at the top level.
--------------------- ----------------
`table_attributes`    e.g. pass `id="info-table"` or `class="bootstrap-class"`/`data-*` to apply these attributes to the generated table
--------------------- ----------------
`clubbing`            turn on[default]/off clubbing of list with same keys of a dict / Array of objects with same key
--------------------- ----------------
`encode`              turn on/off[default] encoding of result to escaped html, compatible with any browser
--------------------- ----------------
`escape`              turn on[default]/off escaping of html tags in text nodes (prevents XSS attacks in case you pass untrusted data to json2html)
===================== ================

Installation
------------

.. code-block:: bash

	$ pip install json2html

Or, Download [here](https://github.com/softvar/json2html/releases) and run `python setup.py install` after changing directory to `/json2html`

Example Usage
-------------

**Example 1:** Basic usage

.. code-block:: python

	from json2html import *
	input = {
		"name": "json2html",
		"description": "Converts JSON to HTML tabular representation"
	}
	json2html.convert(json = input)

Output:

.. code-block:: bash

	<table border="1"><tr><th>name</th><td>json2html</td></tr><tr><th>description</th><td>converts JSON to HTML tabular representation</td></tr></table>

============ ========================================================
name         json2html
------------ --------------------------------------------------------
description  Converts JSON to HTML tabular representation
============ ========================================================

**Example 2:** Setting custom attributes to table

.. code-block:: python

	from json2html import *
	input = {
		"name": "json2html",
		"description": "Converts JSON to HTML tabular representation"
	}
	json2html.convert(json = input, table_attributes="id=\"info-table\" class=\"table table-bordered table-hover\"")

Output:

.. code-block:: bash

	<table id="info-table" class="table table-bordered table-hover"><tr><th>name</th><td>json2html</td></tr><tr><th>description</th><td>Converts JSON to HTML tabular representation</td></tr></table>

**Example 3:** Clubbing same keys of: Array of Objects

.. code-block:: python

	from json2html import *
	input = {
		"sample": [{
			"a":1, "b":2, "c":3
		}, {
			"a":5, "b":6, "c":7
		}]
	}
	json2html.convert(json = input)

Output:

.. code-block:: bash

	<table border="1"><tr><th>sample</th><td><table border="1"><thead><tr><th>b</th><th>c</th><th>a</th></tr></thead><tbody><tr><td>2</td><td>3</td><td>1</td></tr><tr><td>6</td><td>7</td><td>5</td></tr></tbody></table></td></tr></table>

======== ======= =======
  a         c      b
-------- ------- -------
   1        3       2
-------- ------- -------
   5        7       6
======== ======= =======

**Example 4:** Each row for different key(s) of: Array of Objects

.. code-block:: python

	from json2html import *
	input = {
		"sample": [{
			"a":1, "b":2, "c":3
		}, {
			"1a1":5, "1b1":6, "c":7
		}]
	}
	json2html.convert(json = input)

Output:

.. code-block:: bash

	<table border="1"><tr><th>sample</th><td><ul><li><table border="1"><tr><th>a</th><td>1</td></tr><tr><th>c</th><td>3</td></tr><tr><th>b</th><td>2</td></tr></table></li><li><table border="1"><tr><th>1b1</th><td>6</td></tr><tr><th>c</th><td>7</td></tr><tr><th>1a1</th><td>5</td></tr></table></li></ul></td></tr></table>

**Example 5:** [Source: `json.org/example <http://json.org/example>`_]

.. code-block:: python

	from json2html import *

	input = {
		"glossary": {
			"title": "example glossary",
			"GlossDiv": {
				"title": "S",
				"GlossList": {
					"GlossEntry": {
						"ID": "SGML",
						"SortAs": "SGML",
						"GlossTerm": "Standard Generalized Markup Language",
						"Acronym": "SGML",
						"Abbrev": "ISO 8879:1986",
						"GlossDef": {
							"para": "A meta-markup language, used to create markup languages such as DocBook.",
							"GlossSeeAlso": ["GML", "XML"]
						},
						"GlossSee": "markup"
					}
				}
			}
		}
	}

	json2html.convert(json = input)

Output:

.. code-block:: bash

	<table border="1"><tr><th>glossary</th><td><table border="1"><tr><th>GlossDiv</th><td><table border="1"><tr><th>GlossList</th><td><table border="1"><tr><th>GlossEntry</th><td><table border="1"><tr><th>GlossDef</th><td><table border="1"><tr><th>GlossSeeAlso</th><td><ul><li>GML</li><li>XML</li></ul></td></tr><tr><th>para</th><td>A meta-markup language, used to create markup languages such as DocBook.</td></tr></table></td></tr><tr><th>GlossSee</th><td>markup</td></tr><tr><th>Acronym</th><td>SGML</td></tr><tr><th>GlossTerm</th><td>Standard Generalized Markup Language</td></tr><tr><th>Abbrev</th><td>ISO 8879:1986</td></tr><tr><th>SortAs</th><td>SGML</td></tr><tr><th>ID</th><td>SGML</td></tr></table></td></tr></table></td></tr><tr><th>title</th><td>S</td></tr></table></td></tr><tr><th>title</th><td>example glossary</td></tr></table></td></tr></table>

Tests
------

.. code-block:: bash

	cd test/
	python run_tests.py

Tested with Python 2.6, 2.7 3.4, and 3.5.

Contributors
------------

1. Michel Mueller: [@muellermichel](https://github.com/muellermichel)
	* Added support for clubbing Array of Objects with same keys, more readable format.
	* Added support for adding custom `table_attributes`.
	* Convert now accepts unicode and bytestrings for the keyword argument "json".
	* Output now should always appear in the same order as input.
	* Now supports JSON Lists (at top level), including clubbing.
	* Now supports empty inputs and positional arguments for convert.
	* Python 3 support ; Added integration tests for Python 2.6, 3.4 and 3.5 such that support doesn't break.
	* Can now also do the proper encoding for you (disabled by default to not break backwards compatibility).
	* Can now handle non-JSON objects on a best-effort principle.
	* Now by default escapes html in text nodes to prevent XSS attacks.

2. Daniel Lekic: [@lekic](https://github.com/lekic)
	* Fixed issue with one-item lists not rendering correctly.
	* General code cleanup, fixed all naming conventions and coding standards to adhere to PEP8 conventions.

3. Kyle Smith: [@smithk86](https://github.com/smithk86)
    * Added thead and tbody tags to group header and content rows when creating a table from an array of objects.

Copyright and License
---------------------

	The `MIT license <https://opensource.org/licenses/MIT>`_

	Copyright (c) 2014-2017 Varun Malhotra

	Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

	The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

	THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  * [json5-0.8.5](https://github.com/dpranke/pyjson5) # pyjson5

A Python implementation of the JSON5 data format.

[JSON5](https://json5.org) extends the
[JSON](http://www.json.org) data interchange format to make it
slightly more usable as a configuration language:

* JavaScript-style comments (both single and multi-line) are legal.

* Object keys may be unquoted if they are legal ECMAScript identifiers

* Objects and arrays may end with trailing commas.

* Strings can be single-quoted, and multi-line string literals are allowed.

There are a few other more minor extensions to JSON; see the above page for
the full details.

This project implements a reader and writer implementation for Python;
where possible, it mirrors the
[standard Python JSON API](https://docs.python.org/library/json.html)
package for ease of use.

There is one notable difference from the JSON api: the `load()` and
`loads()` methods support optionally checking for (and rejecting) duplicate
object keys; pass `allow_duplicate_keys=False` to do so (duplicates are
allowed by default).

This is an early release. It has been reasonably well-tested, but it is
**SLOW**. It can be 1000-6000x slower than the C-optimized JSON module,
and is 200x slower (or more) than the pure Python JSON module.

## Known issues

* Did I mention that it is **SLOW**?

* The implementation follows Python3's `json` implementation where
  possible. This means that the `encoding` method to `dump()` is
  ignored, and unicode strings are always returned.

* The `cls` keyword argument that `json.load()`/`json.loads()` accepts
  to specify a custom subclass of ``JSONDecoder`` is not and will not be
  supported, because this implementation uses a completely different
  approach to parsing strings and doesn't have anything like the
  `JSONDecoder` class.

* The `cls` keyword argument that `json.dump()`/`json.dumps()` accepts
  is also not supported, for consistency with `json5.load()`. The `default`
  keyword *is* supported, though, and might be able to serve as a
  workaround.

## Version History / Release Notes

* v0.8.5 (2019-07-04)
     * [GitHub issue #25](https://github.com/dpranke/pyjson5/issues/25):
       Add LICENSE and README.md to the dist.
     * [GitHub issue #26](https://github.com/dpranke/pyjson5/issues/26):
       Fix printing of empty arrays and objects with indentation, fix
       misreporting of the position on parse failures in some cases.
* v0.8.4 (2019-06-11)
    * Updated the version history, too.
* v0.8.3 (2019-06-11)
    * Tweaked the README, bumped the version, forgot to update the version
      history :).
* v0.8.2 (2019-06-11)
    * Actually bump the version properly, to 0.8.2.

* v0.8.1 (2019-06-11)
    * Fix bug in setup.py that messed up the description. Unfortunately,
      I forgot to bump the version for this, so this also identifies as 0.8.0.

* v0.8.0 (2019-06-11)
    * Add `allow_duplicate_keys=True` as a default argument to
      `json5.load()`/`json5.loads()`. If you set the key to `False`, duplicate
      keys in a single dict will be rejected. The default is set to `True`
      for compatibility with `json.load()`, earlier versions of json5, and
      because it's simply not clear if people would want duplicate checking
      enabled by default.

* v0.7 (2019-03-31)
    * Changes dump()/dumps() to not quote object keys by default if they are
      legal identifiers. Passing `quote_keys=True` will turn that off
      and always quote object keys.
    * Changes dump()/dumps() to insert trailing commas after the last item
      in an array or an object if the object is printed across multiple lines
      (i.e., if `indent` is not None). Passing `trailing_commas=False` will
      turn that off.
    * The `json5.tool` command line tool now supports the `--indent`,
      `--[no-]quote-keys`, and `--[no-]trailing-commas` flags to allow
      for more control over the output, in addition to the existing
      `--as-json` flag.
    * The `json5.tool` command line tool no longer supports reading from
      multiple files, you can now only read from a single file or
      from standard input.
    * The implementation no longer relies on the standard `json` module
      for anything. The output should still match the json module (except
      as noted above) and discrepancies should be reported as bugs.

* v0.6.2 (2019-03-08)
    * Fix [GitHub issue #23](https://github.com/dpranke/pyjson5/issues/23) and
      pass through unrecognized escape sequences.

* v0.6.1 (2018-05-22)
    * Cleaned up a couple minor nits in the package.

* v0.6.0 (2017-11-28)
    * First implementation that attempted to implement 100% of the spec.

* v0.5.0 (2017-09-04)
    * First implementation that supported the full set of kwargs that
      the `json` module supports.



  * [jsonpatch-1.24](https://github.com/stefankoegl/python-json-patch) python-json-patch
=================

|PyPI version| |Supported Python versions| |Build Status| |Coverage
Status|

Applying JSON Patches in Python
-------------------------------

Library to apply JSON Patches according to `RFC
6902 <http://tools.ietf.org/html/rfc6902>`__

See source code for examples

-  Website: https://github.com/stefankoegl/python-json-patch
-  Repository: https://github.com/stefankoegl/python-json-patch.git
-  Documentation: https://python-json-patch.readthedocs.org/
-  PyPI: https://pypi.python.org/pypi/jsonpatch
-  Travis CI: https://travis-ci.org/stefankoegl/python-json-patch
-  Coveralls: https://coveralls.io/r/stefankoegl/python-json-patch

Running external tests
----------------------

To run external tests (such as those from
https://github.com/json-patch/json-patch-tests) use ext\_test.py

::

    ./ext_tests.py ../json-patch-tests/tests.json

.. |PyPI version| image:: https://img.shields.io/pypi/v/jsonpatch.svg
   :target: https://pypi.python.org/pypi/jsonpatch/
.. |Supported Python versions| image:: https://img.shields.io/pypi/pyversions/jsonpatch.svg
   :target: https://pypi.python.org/pypi/jsonpatch/
.. |Build Status| image:: https://travis-ci.org/stefankoegl/python-json-patch.png?branch=master
   :target: https://travis-ci.org/stefankoegl/python-json-patch
.. |Coverage Status| image:: https://coveralls.io/repos/stefankoegl/python-json-patch/badge.png?branch=master
   :target: https://coveralls.io/r/stefankoegl/python-json-patch?branch=master



  * [jsonpointer-2.0](https://github.com/stefankoegl/python-json-pointer) python-json-pointer
===================

|PyPI version| |Supported Python versions| |Build Status| |Coverage
Status|

Resolve JSON Pointers in Python
-------------------------------

Library to resolve JSON Pointers according to `RFC
6901 <http://tools.ietf.org/html/rfc6901>`__

See source code for examples \* Website:
https://github.com/stefankoegl/python-json-pointer \* Repository:
https://github.com/stefankoegl/python-json-pointer.git \* Documentation:
https://python-json-pointer.readthedocs.org/ \* PyPI:
https://pypi.python.org/pypi/jsonpointer \* Travis CI:
https://travis-ci.org/stefankoegl/python-json-pointer \* Coveralls:
https://coveralls.io/r/stefankoegl/python-json-pointer

.. |PyPI version| image:: https://img.shields.io/pypi/v/jsonpointer.svg
   :target: https://pypi.python.org/pypi/jsonpointer/
.. |Supported Python versions| image:: https://img.shields.io/pypi/pyversions/jsonpointer.svg
   :target: https://pypi.python.org/pypi/jsonpointer/
.. |Build Status| image:: https://travis-ci.org/stefankoegl/python-json-pointer.png?branch=master
   :target: https://travis-ci.org/stefankoegl/python-json-pointer
.. |Coverage Status| image:: https://coveralls.io/repos/stefankoegl/python-json-pointer/badge.png?branch=master
   :target: https://coveralls.io/r/stefankoegl/python-json-pointer?branch=master



  * [jsonschema-3.0.2](https://github.com/Julian/jsonschema) ==========
jsonschema
==========

|PyPI| |Pythons| |Travis| |AppVeyor| |Codecov| |ReadTheDocs|

.. |PyPI| image:: https://img.shields.io/pypi/v/jsonschema.svg
   :alt: PyPI version
   :target: https://pypi.org/project/jsonschema/

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/jsonschema.svg
   :alt: Supported Python versions
   :target: https://pypi.org/project/jsonschema/

.. |Travis| image:: https://travis-ci.org/Julian/jsonschema.svg?branch=master
   :alt: Travis build status
   :target: https://travis-ci.org/Julian/jsonschema

.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/adtt0aiaihy6muyn/branch/master?svg=true
   :alt: AppVeyor build status
   :target: https://ci.appveyor.com/project/Julian/jsonschema

.. |Codecov| image:: https://codecov.io/gh/Julian/jsonschema/branch/master/graph/badge.svg
   :alt: Codecov Code coverage
   :target: https://codecov.io/gh/Julian/jsonschema

.. |ReadTheDocs| image:: https://readthedocs.org/projects/python-jsonschema/badge/?version=stable&style=flat
   :alt: ReadTheDocs status
   :target: https://python-jsonschema.readthedocs.io/en/stable/


``jsonschema`` is an implementation of `JSON Schema <https://json-schema.org>`_
for Python (supporting 2.7+ including Python 3).

.. code-block:: python

    >>> from jsonschema import validate

    >>> # A sample schema, like what we'd get from json.load()
    >>> schema = {
    ...     "type" : "object",
    ...     "properties" : {
    ...         "price" : {"type" : "number"},
    ...         "name" : {"type" : "string"},
    ...     },
    ... }

    >>> # If no exception is raised by validate(), the instance is valid.
    >>> validate(instance={"name" : "Eggs", "price" : 34.99}, schema=schema)

    >>> validate(
    ...     instance={"name" : "Eggs", "price" : "Invalid"}, schema=schema,
    ... )                                   # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValidationError: 'Invalid' is not of type 'number'

It can also be used from console:

.. code-block:: bash

    $ jsonschema -i sample.json sample.schema

Features
--------

* Full support for
  `Draft 7 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft7Validator>`_,
  `Draft 6 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft6Validator>`_,
  `Draft 4 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft4Validator>`_
  and
  `Draft 3 <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.Draft3Validator>`_

* `Lazy validation <https://python-jsonschema.readthedocs.io/en/latest/validate/#jsonschema.IValidator.iter_errors>`_
  that can iteratively report *all* validation errors.

* `Programmatic querying <https://python-jsonschema.readthedocs.io/en/latest/errors/>`_
  of which properties or items failed validation.


Installation
------------

``jsonschema`` is available on `PyPI <https://pypi.org/project/jsonschema/>`_. You can install using `pip <https://pip.pypa.io/en/stable/>`_:

.. code-block:: bash

    $ pip install jsonschema


Demo
----

Try ``jsonschema`` interactively in this online demo:

.. image:: https://user-images.githubusercontent.com/1155573/56745335-8b158a00-6750-11e9-8776-83fa675939c4.png
    :target: https://notebooks.ai/demo/gh/Julian/jsonschema
    :alt: Open Live Demo


Online demo Notebook will look similar to this:


.. image:: https://user-images.githubusercontent.com/1155573/56820861-5c1c1880-6823-11e9-802a-ce01c5ec574f.gif
    :alt: Open Live Demo
    :width: 480 px


Release Notes
-------------

v3.1 brings support for ECMA 262 dialect regular expressions
throughout schemas, as recommended by the specification. Big
thanks to @Zac-HD for authoring support in a new `js-regex
<https://pypi.org/project/js-regex/>`_ library.


Running the Test Suite
----------------------

If you have ``tox`` installed (perhaps via ``pip install tox`` or your
package manager), running ``tox`` in the directory of your source
checkout will run ``jsonschema``'s test suite on all of the versions
of Python ``jsonschema`` supports. If you don't have all of the
versions that ``jsonschema`` is tested under, you'll likely want to run
using ``tox``'s ``--skip-missing-interpreters`` option.

Of course you're also free to just run the tests on a single version with your
favorite test runner. The tests live in the ``jsonschema.tests`` package.


Benchmarks
----------

``jsonschema``'s benchmarks make use of `pyperf
<https://pyperf.readthedocs.io>`_.

Running them can be done via ``tox -e perf``, or by invoking the ``pyperf``
commands externally (after ensuring that both it and ``jsonschema`` itself are
installed)::

    $ python -m pyperf jsonschema/benchmarks/test_suite.py --hist --output results.json

To compare to a previous run, use::

    $ python -m pyperf compare_to --table reference.json results.json

See the ``pyperf`` documentation for more details.


Community
---------

There's a `mailing list <https://groups.google.com/forum/#!forum/jsonschema>`_
for this implementation on Google Groups.

Please join, and feel free to send questions there.


Contributing
------------

I'm Julian Berman.

``jsonschema`` is on `GitHub <https://github.com/Julian/jsonschema>`_.

Get in touch, via GitHub or otherwise, if you've got something to contribute,
it'd be most welcome!

You can also generally find me on Freenode (nick: ``tos9``) in various
channels, including ``#python``.

If you feel overwhelmingly grateful, you can also woo me with beer money
via Google Pay with the email in my GitHub profile.

And for companies who appreciate ``jsonschema`` and its continued support
and growth, ``jsonschema`` is also now supportable via `TideLift
<https://tidelift.com/subscription/pkg/pypi-jsonschema?utm_source=pypi-j
sonschema&utm_medium=referral&utm_campaign=readme>`_.



  * [jsontableschema-0.10.1](https://github.com/frictionlessdata/jsontableschema-py) # JSON Table Schema

[![Travis](https://travis-ci.org/frictionlessdata/jsontableschema-py.svg?branch=master)](https://travis-ci.org/frictionlessdata/jsontableschema-py)
[![Coveralls](http://img.shields.io/coveralls/frictionlessdata/jsontableschema-py.svg?branch=master)](https://coveralls.io/r/frictionlessdata/jsontableschema-py?branch=master)
[![PyPi](https://img.shields.io/pypi/v/jsontableschema.svg)](https://pypi.python.org/pypi/jsontableschema)
[![SemVer](https://img.shields.io/badge/versions-SemVer-brightgreen.svg)](http://semver.org/)
[![Gitter](https://img.shields.io/gitter/room/frictionlessdata/chat.svg)](https://gitter.im/frictionlessdata/chat)

A utility library for working with [JSON Table Schema](http://dataprotocols.org/json-table-schema/) in Python.

> With v0.7 renewed API has been introduced in backward-compatibility manner. Documentation for deprecated API could be found [here](https://github.com/frictionlessdata/jsontableschema-py/tree/0.6.5#json-table-schema). Deprecated API will be removed with v1 release.

## Features

- `Table` to work with data tables described by JSON Table Schema
- `Schema` representing JSON Table Schema
- `Field` representing JSON Table Schema field
- `validate` to validate JSON Table Schema
- `infer` to infer JSON Table Schema from data
- built-in command-line interface to validate and infer schemas
- storage/plugins system to connect tables to different storage backends like SQL Database

## Gettings Started

### Installation

```bash
pip install jsontableschema
```
### Example

```python
from jsontableschema import Table

# Create table
table = Table('path.csv', schema='schema.json')

# Print schema descriptor
print(table.schema.descriptor)

# Print cast rows in a dict form
for keyed_row in table.iter(keyed=True):
    print(keyed_row)
```

### Table

Table represents data described by JSON Table Schema:

```python
# pip install sqlalchemy jsontableschema-sql
import sqlalchemy as sa
from pprint import pprint
from jsontableschema import Table

# Data source
SOURCE = 'https://raw.githubusercontent.com/okfn/jsontableschema-py/master/data/data_infer.csv'

# Create SQL database
db = sa.create_engine('sqlite://')

# Data processor
def skip_under_30(erows):
    for number, headers, row in erows:
        krow = dict(zip(headers, row))
        if krow['age'] >= 30:
            yield (number, headers, row)

# Work with table
table = Table(SOURCE, post_cast=[skip_under_30])
table.schema.save('tmp/persons.json') # Save INFERRED schema
table.save('persons', backend='sql', engine=db) # Save data to SQL
table.save('tmp/persons.csv')  # Save data to DRIVE

# Check the result
pprint(Table('persons', backend='sql', engine=db).read(keyed=True))
pprint(Table('tmp/persons.csv').read(keyed=True))
# Will print (twice)
# [{'age': 39, 'id': 1, 'name': 'Paul'},
#  {'age': 36, 'id': 3, 'name': 'Jane'}]
```

### Schema

A model of a schema with helpful methods for working with the schema and supported data. Schema instances can be initialized with a schema source as a filepath or url to a JSON file, or a Python dict. The schema is initially validated (see [validate](#validate) below), and will raise an exception if not a valid JSON Table Schema.

```python
from jsontableschema import Schema

# Init schema
schema = Schema('path.json')

# Cast a row
schema.cast_row(['12345', 'a string', 'another field'])
```

Methods available to `Schema` instances:

- `descriptor` - return schema descriptor
- `fields` - an array of the schema's Field instances
- `headers` - an array of the schema headers
- `primary_key` - the primary key field for the schema as an array
- `foreignKey` - the foreign key property for the schema as an array
- `get_field(name)` - return the field object for given name
- `has_field(name)` - return a bool if the field exists in the schema
- `cast_row(row, no_fail_fast=False)` - return row cast against schema
- `save(target)` - save schema to filesystem

Where the option `no_fail_fast` is given, it will collect all errors it encouters and an exceptions.MultipleInvalid will be raised (if there are errors).

### Field

```python
from jsontableschemal import Field

# Init field
field = Field({'type': 'number'})

# Cast a value
field.cast_value('12345') # -> 12345
```

Data values can be cast to native Python objects with a Field instance. Type instances can be initialized with [field descriptors](http://dataprotocols.org/json-table-schema/#field-descriptors). This allows formats and constraints to be defined.

Casting a value will check the value is of the expected type, is in the correct format, and complies with any constraints imposed by a schema. E.g. a date value (in ISO 8601 format) can be cast with a DateType instance. Values that can't be cast will raise an `InvalidCastError` exception.

Casting a value that doesn't meet the constraints will raise a `ConstraintError` exception.

### validate

Given a schema as JSON file, url to JSON file, or a Python dict, `validate` returns `True` for a valid JSON Table Schema, or raises an exception, `SchemaValidationError`. It validates only **schema**, not data against schema!

```python
import io
import json

from jsontableschema import validate

with io.open('schema_to_validate.json') as stream:
    descriptor = json.load(stream)

try:
    jsontableschema.validate(descriptor)
except jsontableschema.exceptions.SchemaValidationError as exception:
   # handle error

```

It may be useful to report multiple errors when validating a schema. This can be done with `no_fail_fast` flag set to True.

```python
try:
    jsontableschema.validate(descriptor, no_fail_fast=True)
except jsontableschema.exceptions.MultipleInvalid as exception:
    for error in exception.errors:
        # handle error
```

### infer

Given headers and data, `infer` will return a JSON Table Schema as a Python dict based on the data values. Given the data file, data_to_infer.csv:

```
id,age,name
1,39,Paul
2,23,Jimmy
3,36,Jane
4,28,Judy
```

Call `infer` with headers and values from the datafile:

```python
import io
import csv

from jsontableschema import infer

filepath = 'data_to_infer.csv'
with io.open(filepath) as stream:
    headers = stream.readline().rstrip('\n').split(',')
    values = csv.reader(stream)

schema = infer(headers, values)
```

`schema` is now a schema dict:

```python
{u'fields': [
    {
        u'description': u'',
        u'format': u'default',
        u'name': u'id',
        u'title': u'',
        u'type': u'integer'
    },
    {
        u'description': u'',
        u'format': u'default',
        u'name': u'age',
        u'title': u'',
        u'type': u'integer'
    },
    {
        u'description': u'',
        u'format': u'default',
        u'name': u'name',
        u'title': u'',
        u'type': u'string'
    }]
}
```

The number of rows used by `infer` can be limited with the `row_limit` argument.

### CLI

> It's a provisional API excluded from SemVer. If you use it as a part of other program please pin concrete `goodtables` version to your requirements file.

JSON Table Schema features a CLI called `jsontableschema`. This CLI exposes the `infer` and `validate` functions for command line use.

Example of `validate` usage:

```
$ jsontableschema validate path/to-schema.json
```

Example of `infer` usage:

```
$ jsontableschema infer path/to/data.csv
```

The response is a schema as JSON. The optional argument `--encoding` allows a character encoding to be specified for the data file. The default is utf-8.

### Storage

The library includes interface declaration to implement tabular `Storage`:

![Storage](data/storage.png)

An implementor should follow `jsontableschema.Storage` interface to write his
own storage backend. This backend could be used with `Table` class. See `plugins`
system below to know how to integrate custom storage plugin.

### plugins

JSON Table Schema has a plugin system.  Any package with the name like `jsontableschema_<name>` could be imported as:

```python
from jsontableschema.plugins import <name>
```

If a plugin is not installed `ImportError` will be raised with a message describing how to install the plugin.

A list of officially supported plugins:
- BigQuery Storage - https://github.com/frictionlessdata/jsontableschema-bigquery-py
- Pandas Storage - https://github.com/frictionlessdata/jsontableschema-pandas-py
- SQL Storage - https://github.com/frictionlessdata/jsontableschema-sql-py

## API Reference

### Snapshot

```
Table(source, schema=None, post_cast=None, backend=None, **options)
    stream -> tabulator.Stream
    schema -> Schema
    name -> str
    iter(keyed/extended=False) -> (generator) (keyed/extended)row[]
    read(keyed/extended=False, limit=None) -> (keyed/extended)row[]
    save(target, backend=None, **options)
Schema(descriptor)
    descriptor -> dict
    fields -> Field[]
    headers -> str[]
    primary_key -> str[]
    foreign_keys -> str[]
    get_field(name) -> Field
    has_field(name) -> bool
    cast_row(row, no_fail_fast=False) -> row
    save(target)
Field(descriptor)
    descriptor -> dict
    name -> str
    type -> str
    format -> str
    constraints -> dict
    cast_value(value, skip_constraints=False) -> value
    test_value(value, skip_constraints=False, constraint=None) -> bool
validate(descriptor, no_fail_fast=False) -> bool
infer(headers, values) -> descriptor
exceptions
~cli
---
Storage(**options)
    buckets -> str[]
    create(bucket, descriptor, force=False)
    delete(bucket=None, ignore=False)
    describe(bucket, descriptor=None) -> descriptor
    iter(bucket) -> (generator) row[]
    read(bucket) -> row[]
    write(bucket, rows)
plugins
```

### Detailed

- [Docstrings](https://github.com/frictionlessdata/jsontableschema-py/tree/master/jsontableschema)
- [Changelog](https://github.com/frictionlessdata/jsontableschema-py/commits/master)

## Contributing

Please read the contribution guideline:

[How to Contribute](CONTRIBUTING.md)

Thanks!
  * [jupyter-1.0.0](http://jupyter.org) Install the Jupyter system, including the notebook, qtconsole, and the IPython kernel.
  * [jupyter-client-5.3.1](https://jupyter.org) # Jupyter Client

[![Code Health](https://landscape.io/github/jupyter/jupyter_client/master/landscape.svg?style=flat)](https://landscape.io/github/jupyter/jupyter_client/master)


`jupyter_client` contains the reference implementation of the [Jupyter protocol][].
It also provides client and kernel management APIs for working with kernels.

It also provides the `jupyter kernelspec` entrypoint
for installing kernelspecs for use with Jupyter frontends.

[Jupyter protocol]: https://jupyter-client.readthedocs.io/en/latest/messaging.html


# Development Setup

The [Jupyter Contributor Guides](http://jupyter.readthedocs.io/en/latest/contributor/content-contributor.html) provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.

## Coding

You'll need Python and `pip` on the search path. Clone the Jupyter Client git repository to your computer, for example in `/my/project/jupyter_client`.
Now create an [editable install](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs)
and download the dependencies of code and test suite by executing:

    cd /my/projects/jupyter_client/
    pip install -e .[test]
    py.test

The last command runs the test suite to verify the setup. During development, you can pass filenames to `py.test`, and it will execute only those tests.

## Documentation

The documentation of Jupyter Client is generated from the files in `docs/` using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the [Documentation Guide](http://jupyter.readthedocs.io/en/latest/contrib_docs/index.html). You'll also need the `make` command.
For a minimal Sphinx installation to process the Jupyter Client docs, execute:

    pip install ipykernel sphinx sphinx_rtd_theme

The following commands build the documentation in HTML format and check for broken links:

    cd /my/projects/jupyter_client/docs/
    make html linkcheck

Point your browser to the following URL to access the generated documentation:

_file:///my/projects/jupyter\_client/docs/\_build/html/index.html_




  * [jupyter-core-4.5.0](https://jupyter.org) There is no reason to install this package on its own.



  * [jupyter_console-6.0.0](https://jupyter.org) An IPython-like terminal frontend for Jupyter kernels in any language.



  * [jupyterlab-1.0.6](http://jupyter.org) **[Installation](#installation)** |
**[Documentation](http://jupyterlab.readthedocs.io)** |
**[Contributing](#contributing)** |
**[License](#license)** |
**[Team](#team)** |
**[Getting help](#getting-help)** |

# [JupyterLab](http://jupyterlab.github.io/jupyterlab/)

[![PyPI version](https://badge.fury.io/py/jupyterlab.svg)](https://badge.fury.io/py/jupyterlab)
[![Downloads](https://pepy.tech/badge/jupyterlab/month)](https://pepy.tech/project/jupyterlab/month)
[![Build Status](https://dev.azure.com/jupyterlab/jupyterlab/_apis/build/status/jupyterlab.jupyterlab?branchName=master)](https://dev.azure.com/jupyterlab/jupyterlab/_build/latest?definitionId=1&branchName=master)
[![Documentation Status](https://readthedocs.org/projects/jupyterlab/badge/?version=stable)](http://jupyterlab.readthedocs.io/en/stable/)
[![GitHub](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/jupyterlab/jupyterlab/issues)
[![Discourse](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.jupyter.org/c/jupyterlab)
[![Gitter](https://img.shields.io/badge/social_chat-gitter-blue.svg)](https://gitter.im/jupyterlab/jupyterlab)

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/master?urlpath=lab/tree/demo)

An extensible environment for interactive and reproducible computing, based on the
Jupyter Notebook and Architecture. [Currently ready for users.](https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906)

[JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) is the next-generation user interface for [Project Jupyter](https://jupyter.org) offering
all the familiar building blocks of the classic Jupyter Notebook (notebook,
terminal, text editor, file browser, rich outputs, etc.) in a flexible and
powerful user interface.
JupyterLab will eventually replace the classic Jupyter Notebook.

JupyterLab can be extended using [npm](https://www.npmjs.com/) packages
that use our public APIs. To find JupyterLab extensions, search for the npm keyword [jupyterlab-extension](https://www.npmjs.com/search?q=keywords:jupyterlab-extension) or the GitHub topic [jupyterlab-extension](https://github.com/topics/jupyterlab-extension). To learn more about extensions, see the [user documentation](https://jupyterlab.readthedocs.io/en/latest/user/extensions.html).

The current JupyterLab releases are suitable for general
usage, and the extension APIs will continue to
evolve for JupyterLab extension developers.

Read the latest version's documentation on [ReadTheDocs](http://jupyterlab.readthedocs.io/en/latest/).

---

## Getting started

### Installation

JupyterLab can be installed using `conda` or `pip`. For more detailed instructions, consult the [installation guide](http://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html).

Project installation instructions from the git sources are available in the [contributor documentation](CONTRIBUTING.md).

### conda

If you use `conda`, you can install it with:

```shell
conda install -c conda-forge jupyterlab
```

### pip

If you use `pip`, you can install it with:

```shell
pip install jupyterlab
```

If installing using `pip install --user`, you must add the user-level `bin` directory to your `PATH` environment variable in order to launch `jupyter lab`.

#### Installing with Previous Versions of Jupyter Notebook

When using a version of Jupyter Notebook earlier than 5.3, the following command must be run
after installation to enable the JupyterLab server extension:

```bash
jupyter serverextension enable --py jupyterlab --sys-prefix
```

### Running

Start up JupyterLab using:

```bash
jupyter lab
```

JupyterLab will open automatically in the browser. See the [documentation](http://jupyterlab.readthedocs.io/en/stable/getting_started/starting.html) for additional details.

### Prerequisites and Supported Browsers

Jupyter notebook version 4.3 or later is required. To check the notebook version, run the command:

```bash
jupyter notebook --version
```

The latest versions of the following browsers are currently _known to work_:

- Firefox
- Chrome
- Safari

See our [documentation](http://jupyterlab.readthedocs.io/en/latest/getting_started/installation.html) for additional details.

---

## Development

### Contributing

To contribute to the project, please read the [contributor documentation](CONTRIBUTING.md).

JupyterLab follows the Jupyter [Community Guides](https://jupyter.readthedocs.io/en/latest/community/content-community.html).

### Extending JupyterLab

To start developing an extension, see the [developer documentation](https://jupyterlab.readthedocs.io/en/latest/developer/extension_dev.html) and the [API docs](http://jupyterlab.github.io/jupyterlab/index.html).

### License

JupyterLab uses a shared copyright model that enables all contributors to maintain the
copyright on their contributions. All code is licensed under the terms of the revised [BSD license](https://github.com/jupyterlab/jupyterlab/blob/master/LICENSE).

### Team

JupyterLab is part of [Project Jupyter](http://jupyter.org/) and is developed by an open community. The maintenance team is assisted by a much larger group of contributors to JupyterLab and Project Jupyter as a whole.

JupyterLab's current maintainers are listed in alphabetical order, with affiliation, and main areas of contribution:

- Chris Colbert, Project Jupyter (co-creator, application/low-level architecture,
  technical leadership, vision, PhosphorJS)
- Afshin Darian, Two Sigma (co-creator, application/high-level architecture,
  prolific contributions throughout the code base).
- Jessica Forde, Project Jupyter (demo, documentation)
- Tim George, Cal Poly (UI/UX design, strategy, management, user needs analysis)
- Brian Granger, Cal Poly (co-creator, strategy, vision, management, UI/UX design,
  architecture).
- Jason Grout, Bloomberg (co-creator, vision, general development).
- Fernando Perez, UC Berkeley (co-creator, vision).
- Ian Rose, Quansight/City of LA (general core development, extensions).
- Saul Shanabrook, Quansight (general development, extensions)
- Steven Silvester, JPMorgan Chase (co-creator, release management, packaging,
  prolific contributions throughout the code base).
- Vidar T. Fauske, JPMorgan Chase (general development, extensions).

Maintainer emeritus:

- Cameron Oelsen, Cal Poly (UI/UX design).

This list is provided to give the reader context on who we are and how our team functions.
To be listed, please submit a pull request with your information.

---

## Getting help

We encourage you to ask questions on the [Discourse forum](https://discourse.jupyter.org/c/jupyterlab). A question answered there can become a useful resource for others.

Please use the [GitHub issues page](https://github.com/jupyterlab/jupyterlab/issues) to provide feedback or submit a bug report. To keep resolved issues self-contained, the [lock bot](https://github.com/apps/lock) will lock closed issues as resolved after a period of inactivity. If related discussion is still needed after an issue is locked, please open a new issue and reference the old issue.

### Weekly Dev Meeting

We have videoconference meetings every week where we discuss what we have been working on and get feedback from one another.

Anyone is welcome to attend, if they would like to discuss a topic or just to listen in.

- When: Wednesdays [9AM Pacific Time](https://www.thetimezoneconverter.com/?t=9%3A00%20am&tz=San%20Francisco&)
- Where: [`calpoly/jupyter` Zoom](https://calpoly.zoom.us/my/jupyter)
- What: [Meeting notes on Dropbox Paper](https://paper.dropbox.com/doc/JLab-Dev-Meeting-Minutes-2019--AZlv6L3jnv8ntl6kJK88y5M5Ag-Lj0P4kI2JrbA0eXHZSdY5)



  * [jupyterlab-server-1.0.4](https://jupyter.org) # jupyterlab server

https://github.com/jupyterlab/jupyterlab_server

## Install

`pip install jupyterlab_server`

## Usage
The application author creates a JupyterLab build on their machine
using the core JupyterLab application.  They can then serve their
files by subclassing the `LabServerApp` with the appropriate
configuration and creating a Python entry point that launches the app.


## Development Install

```
git clone https://github.com/jupyterlab/jupyterlab_server.git
cd jupyterlab_server
pip install -e .
```



  * [jupyterlab_launcher-0.13.1](http://jupyter.org) 
This package is used to launch an application built using JupyterLab



  * [kazoo-2.6.1](https://kazoo.readthedocs.io) Kazoo
=====

[![Build Status](https://travis-ci.org/python-zk/kazoo.svg?branch=master)](https://travis-ci.org/python-zk/kazoo)
[![Latest Version](https://img.shields.io/pypi/v/kazoo.svg)](https://pypi.org/project/kazoo/)
[![Reviewed by Hound](https://img.shields.io/badge/Reviewed_by-Hound-8E64B0.svg)](https://houndci.com)

`kazoo` implements a higher level API to [Apache
Zookeeper](http://zookeeper.apache.org/) for Python clients.

See [the full docs](http://kazoo.rtfd.org/) for more information.

License
-------

`kazoo` is offered under the Apache License 2.0.

Authors
-------

`kazoo` started under the [Nimbus
Project](http://www.nimbusproject.org/) and through collaboration with
the open-source community has been merged with code from
[Mozilla](http://www.mozilla.org/) and the [Zope
Corporation](http://zope.com/). It has since gathered an active
community of over fifty contributors.


<a name="2.6.1"></a>
### 2.6.1 (2019-01-22)


#### Bug Fixes

* **client:**  add missing paren (#550) ([1452a48f](https://github.com/python-zk/kazoo/commit/1452a48f3070fe9034314476a6fdb94ca206dede))
* **core:**
  *  support deprecated KazooRetry argument (#545) ([4242da80](https://github.com/python-zk/kazoo/commit/4242da801e8da7b76d7e88e37c3948f97a2b5aae))
  *  reduce timeout for the first Connect() request (#540) ([2ae392e6](https://github.com/python-zk/kazoo/commit/2ae392e69c4b2daca5d8e7f0e79b7ce90423e65c))
* **handlers:**  make AsyncResult call all registered callbacks instantly if the handler has stopped running (#549) ([d9e0e720](https://github.com/python-zk/kazoo/commit/d9e0e7208e56c31f0abec60a3701f8d6ec1e7d32))
* **recipe:**
  *  No more memory leak when ChildrenWatch was stopped (#543) ([37bcda35](https://github.com/python-zk/kazoo/commit/37bcda357463155aba5f2383bc70528413a10f1b))
  *  No more memory leak once TreeCache was closed (#524) ([c48f2733](https://github.com/python-zk/kazoo/commit/c48f2733f2a6b2c2941738e4208e8cfede676730))



<a name="2.6.0"></a>
## 2.6.0 (2018-11-14)


#### Features

* **core:**
  *  add SASL DIGEST-MD5 support ([aa2664b8](https://github.com/python-zk/kazoo/commit/aa2664b880d1456c3ccf6515c6ca42653047e272))
  *  Added SSL support (#513) ([35ce1066](https://github.com/python-zk/kazoo/commit/35ce10669ace9d0d7e787793f0d4937d5d389f69))
* **tests:**
  *  update Zookeeper 3.5.2-alpha to 3.5.4-beta ([30330915](https://github.com/python-zk/kazoo/commit/3033091530b8f0aba13a1b8d031a7297e54006c2), closes [#477](https://github.com/python-zk/kazoo/issues/477))
  *  update Zookeeper version from 3.4.10 to 3.4.13 ([287749b4](https://github.com/python-zk/kazoo/commit/287749b422c886f69e46d108d2ddbb5ad064773e))

#### Bug Fixes

* **core:**
  *  ensure timeout argument is positive (#534) ([8c5ce118](https://github.com/python-zk/kazoo/commit/8c5ce11883a86b15bc6497706cf36abf1b36145f))
  *  get_children with include_data=True uses GetChildren2 types (#514) ([901cba7a](https://github.com/python-zk/kazoo/commit/901cba7a40d67ec96c06abe109e3cf51a992b24d))
  *  allow authentification in RO and RW modes ([2320ab39](https://github.com/python-zk/kazoo/commit/2320ab391f3804a9ddea9c5c86eb86467bb1dbf8))
  *  Use a copy of auth data when reconnecting (#509) ([de20be91](https://github.com/python-zk/kazoo/commit/de20be917855713169863b65a7aa0634fb78b698))
  *  change KazooRetry to uniformly jitter over the whole backoff interval (#521) ([60366d2c](https://github.com/python-zk/kazoo/commit/60366d2c7910fc833991fad8e04bbe33817c0544))
* **recipe:**  Delete lock node in queue recipe if entry already consumed ([7a8167de](https://github.com/python-zk/kazoo/commit/7a8167dea381b3a2015c869a443c96b9d5179411))

#### Doc

*   make badges link to travis and pypi (#528) ([367a1df3](https://github.com/python-zk/kazoo/commit/367a1df35b3b168580327e9f52a4cb9c000ea750))
*   specify description type for pypi to render correctly ([cc4006e6](https://github.com/python-zk/kazoo/commit/cc4006e6a8ea9441b01a631f3350357a8a2e4088))
*   remove broken downloads badge ([47e07a96](https://github.com/python-zk/kazoo/commit/47e07a9674713b0756bce0811545c4442d40b60b))
* **core:**  fix broken zookeeper programmers guide link ([8ecf8a50](https://github.com/python-zk/kazoo/commit/8ecf8a50c746ab7cc588ce9ac10a91ad1d1e5f34))



<a name="2.5.0"></a>
## 2.5.0 (2018-06-01)


#### Performance

* **recipe:**  Give TreeCache standalone queue ([4456f180](https://github.com/python-zk/kazoo/commit/4456f180735a0f8520bfc42474de9d27fa01bb2c))

#### Doc

*   Correctly document supported versions ([f860de52](https://github.com/python-zk/kazoo/commit/f860de5284e8ba7d0ed4171d9e5c5bef41f6b64d))
*   Add license to Wheel ([43d156de](https://github.com/python-zk/kazoo/commit/43d156de3d41173d7baf6e8edb4efa377b3732fc))
*   minor tweaks to contributing.md (#464) ([5837d11b](https://github.com/python-zk/kazoo/commit/5837d11bb541be1296e1b3c1842b11ceb7e743d4))

#### Features

* **recipe:**  allow non ephemeral locking ([6f7a603d](https://github.com/python-zk/kazoo/commit/6f7a603de9f04ec46d9946c6f5b1cb6e2b913b63))

#### Chore

*   drop python 2.6 compatible gevent ([2e8dcd38](https://github.com/python-zk/kazoo/commit/2e8dcd3836d01640f07e8de911cdfb3639f97d20))
*   remove debian packaging (#468) ([a28423aa](https://github.com/python-zk/kazoo/commit/a28423aac7f33111388372dec50653a3091b9045))
*   stop bundling virtualenv ([af9b4cab](https://github.com/python-zk/kazoo/commit/af9b4cab39982d961647afca8119816ffe7bf5e6))
*   remove python 3.3 from tox ([cf66474b](https://github.com/python-zk/kazoo/commit/cf66474b27b4efc9f65e292434c2fcffb697d34d))
*   remove `easy_install` instructions ([6e6627d5](https://github.com/python-zk/kazoo/commit/6e6627d58628dec11d127fc5508f9a1d16e9c317))

#### Bug Fixes

*   Pass watch as keyword arg instead of positional arg (#495) ([23850792](https://github.com/python-zk/kazoo/commit/2385079267db0bea6793c4f20588644381803a98))
*   need gevent >= 1.2, not > 1.1 ([18f3531b](https://github.com/python-zk/kazoo/commit/18f3531b605119a547cfc4d5a4223b4b3f185405))
*   Remove use of "async" as a variable ([225d3369](https://github.com/python-zk/kazoo/commit/225d3369c7a0736125a9375951a079f70fbe9e79), closes [#455](https://github.com/python-zk/kazoo/issues/455))
* **core:**
  *  Fix gevent 1.3b1+ timeout import ([257b5896](https://github.com/python-zk/kazoo/commit/257b58961f7ddd9db04d6efa070739a1b0404487))
  *  Correctly fire multiple callbacks ([0905c47b](https://github.com/python-zk/kazoo/commit/0905c47bff3cfc42382daff0d5ac81189c8ba46d))
  *  resolve race in IAsyncResult.wait() (#487) ([4d268adf](https://github.com/python-zk/kazoo/commit/4d268adf9837836f05dde5ec81be0d7bbd759e78))
* **recipe:**
  *  conn hangs when TreeCache refreshing ([11194137](https://github.com/python-zk/kazoo/commit/111941371daec00a2ecb5d8c29b9b1d35d6aa4ff))
  *  Unexpected exceptions break TreeCache ([db0c2d4f](https://github.com/python-zk/kazoo/commit/db0c2d4f8ab5ecfb367b7b2accfd9c52c1c91fcd))

#### Refactor

*   Unify queue factory in various handlers ([cbd02f5c](https://github.com/python-zk/kazoo/commit/cbd02f5ccc6ea9e6dd6b5da01a0397ea649143a0))



<a name="2.4.0"></a>
## 2.4.0 (2017-06-14)

**PYTHON SUPPORTED VERSIONS CHANGE**: Please note that Kazoo no longer tests
on Python 2.6 or 3.3. Tested versions are 2.7, 3.4, 3.5, 3.6, and PyPy. The
multiple endpoint support for KazooClient may not work correctly on 2.6.

#### Bug Fixes

*   add missed parens to LockingQueue function call. ([88cf4aa7](https://github.com/python-zk/kazoo/commit/88cf4aa7bcd1437a44f6d3aeec34989c467fcf9d))
* **core:**  revert PR #305 SetWatches which caused RuntimeError ([a7b45390](https://github.com/python-zk/kazoo/commit/a7b45390f3720a33c9ad3896a8a185bfb2628839))

#### Features

*   pep8 all the things ([92880342](https://github.com/python-zk/kazoo/commit/928803420721b81962e50d425610e62b0c12e438), closes [#445](https://github.com/python-zk/kazoo/issues/445))
*   drop Python 2.6/3.3 official support, add 3.5/3.6 testing ([2faba9ff](https://github.com/python-zk/kazoo/commit/2faba9ff3bdeff151ac6c922bf65b2dcf9c7bd7b), closes [#441](https://github.com/python-zk/kazoo/issues/441))
* **core:**
  *  allow multiple endpoints in KazooClient hosts arg ([72a8d96c](https://github.com/python-zk/kazoo/commit/72a8d96ca188c3c2e93bd0243283539ebd6c16f1), closes [#411](https://github.com/python-zk/kazoo/issues/411))
  *  use epoll when available to support fds > 1023 ([267e61b4](https://github.com/python-zk/kazoo/commit/267e61b4323bc13505e8933fa9b89d0591af3a69), closes [#171](https://github.com/python-zk/kazoo/issues/171))
* **recipe:**  Add TreeCache recipe ([ec8b337e](https://github.com/python-zk/kazoo/commit/ec8b337e6f1a4ff12e669f4b96ca98fb37ee5d8a))



<a name="2.3.1"></a>
## 2.3.1 (2017-06-01)


#### Chore

*   update MANIFEST.in to reflect changes to filenames ([c9a38c5d](https://github.com/python-zk/kazoo/commit/c9a38c5d650d6d92ff30fd3c1c792fc71db9ce02))
*   add travis deployment and update ZK versions ([7d5d59cb](https://github.com/python-zk/kazoo/commit/7d5d59cb049244b89625d621c9d91d9a44c4b051), closes [#436](https://github.com/python-zk/kazoo/issues/436))

2.3.0 (2017-05-31)
------------------

Please note, there have been a few dozen merges that failed to update the
changelog here. As such, the log here should not be considered conclusive as
to the changes that are arriving in 2.3.0.

Changes being made now to ensure more accuracy in the changelog will appear
in all future versions going forward. Read the commit history for a better
understanding of changes merged between 2.2.1 and 2.3.0.

All future commits must abide by the new CONTRIBUTING.md document describing
how to label commits so they can be automatically used to automatically
generate an accurate changelog.

*WARNING:* THIS IS THE LAST KAZOO RELEASE THAT SUPPORTS PYTHON 2.6. ALL FUTURE
VERSIONS WILL REQUIRE PYTHON 2.7 AND ABOVE.

### Features

-   allow having observers and different sized clusters

### Bug Handling

-   \#372: fully resolve multiple records for hosts in the zookeeper
    connection string

### Documentation

-   Fix the recipe examples, so they actually work by connecting to
    ZooKeeper. Without start() they just hang and can't be killed.

2.2.1 (2015-06-17)
------------------

### Bug Handling

-   handle NameError with basestring on py3k.

### Documentation

2.2 (2015-06-15)
----------------

### Documentation

### Features

-   Issue \#234: Add support for reconfig cluster membership operation

### Bug Handling

-   \#315: multiple acquires of a kazoo lock using the lock recipe would
    block when using acquire even when non-blocking is specified (only
    when the lock was/has been already acquired).
-   \#318: At exit register takes `*args` and `**kwargs` not args and
    kargs

### Documentation

2.1 (2015-05-11)
----------------

### Features

-   Start running tests against Zookeeper 3.5.0 alpha and explicitly
    configure the admin.serverPort in tests to avoid port conflicts. The
    Zookeeper alpha version is not yet officially supported.
-   Integrate eventlet *handler* support into kazoo so that along with
    [gevent, threading] handlers there can now be a dedicated eventlet
    handler for projects that need to (or want to) use eventlet (such as
    those working in the openstack community). The
    `requirements_eventlet.txt` file lists the optional eventlet
    requirement(s) that needs to be satisfied when this new handler is
    used.
-   Use `six` to nicely handle the cross compatibility of kazoo with
    python 2.x and 3.x (reducing/removing the need to have custom
    compatibility code that replicates what six already provides).
-   Add `state_change_event` to
    `kazoo.recipe.partitioner.SetPartitioner` which is set on every
    state change.
-   Add a NonBlockingLease recipe. The recipe allows e.g. cron jobs
    scheduled on multiple machines to ensure that at most N instances
    will run a particular job, with lease timeout for graceful handover
    in case of node failures.

### Bug Handling

-   \#291: Kazoo lock recipe was only partially re-entrant in that
    multiple calls to acquire would obtain the the lock but the first
    call to release would remove the underlying lock. This would leave
    the X - 1 other acquire statements unprotected (and no longer
    holding there expected lock). To fix this the comment about that
    lock recipe being re-entrant has been removed and multiple acquires
    will now block when attempted.
-   \#78: Kazoo now uses socketpairs instead of pipes making it
    compatible with Windows.
-   \#144, \#221: Let client.command work with IPv6 addresses.
-   \#216: Fixed timeout for ConnectionHandler.\_invoke.
-   \#261: Creating a sequential znode under / doesn't work.
-   \#274: Add server\_version() retries (by default 4 attempts will be
    made) to better handle flakey responses.
-   \#271: Fixed handling of KazooState.SUSPENDED in SetPartitioner.
-   \#283: Fixed a race condition in SetPartitioner when party changes
    during handling of lock acquisition.
-   \#303: don't crash on random input as the hosts string.

### Documentation

-   \#222: Document that committed on the transaction is used to ensure
    only one commit and is not an indicator of whether operations in the
    transaction returned desired results.

2.0 (2014-06-19)
----------------

### Documentation

-   Extend support to Python 3.4, deprecating Python 3.2.
-   Issue \#198: Mention Zake as a sophisticated kazoo mock testing
    library.
-   Issue \#181: Add documentation on basic logging setup.

2.0b1 (2014-04-24)
------------------

### API Changes

-   Null or None data is no longer treated as "". Pull req \#165, patch
    by Raul Gutierrez S. This will affect how you should treat null data
    in a znode vs. an empty string.
-   Passing acl=[] to create() now works properly instead of an
    InvalidACLError as it returned before. Patch by Raul Gutierrez S in
    PR \#164.
-   Removed the dependency on zope.interface. The classes in the
    interfaces module are left for documentation purposes only (issue
    \#131).

### Features

-   Logging levels have been reduced.
    -   Logging previously at the `logging.DEBUG` level is now logged at
    the `kazoo.loggingsupport.BLATHER` level (5).
    -   Some low-level logging previously at the `logging.INFO` level is
    now logged at the `logging.DEBUG` level.
-   Issue \#133: Introduce a new environment variable
    ZOOKEEPER\_PORT\_OFFSET for the testing support, to run the testing
    cluster on a different range.

### Bug Handling

-   When authenticating via add\_auth() the auth data will be saved to
    ensure that the authentication happens on reconnect (as is the case
    when feeding auth data via KazooClient's constructor). PR \#172,
    patch by Raul Gutierrez S.
-   Change gevent import to remove deprecation warning when newer gevent
    is used. PR \#191, patch by Hiroaki Kawai.
-   Lock recipe was failing to use the client's sleep\_func causing
    issues with gevent. Issue \#150.
-   Calling a DataWatch or ChildrenWatch instance twice (decorator) now
    throws an exception as only a single function can be associated with
    a single watcher. Issue \#154.
-   Another fix for atexit handling so that when disposing of
    connections the atexit handler is removed. PR \#190, patch by Devaev
    Maxim.
-   Fix atexit handling for kazoo threading handler, PR \#183. Patch by
    Brian Wickman.
-   Partitioner should handle a suspended connection properly and
    restore an allocated state if it was allocated previously. Patch by
    Manish Tomar.
-   Issue \#167: Closing a client that was never started throws a type
    error. Patch by Joshua Harlow.
-   Passing dictionaries to KazooClient.\_\_init\_\_() wasn't actually
    working properly. Patch by Ryan Uber.
-   Issue \#119: Handler timeout takes the max of the random interval or
    the read timeout to ensure a negative number isn't used for the read
    timeout.
-   Fix ordering of exception catches in lock.acquire as it was
    capturing a parent exception before the child. Patch by ReneSac.
-   Fix issue with client.stop() not always setting the client state to
    KeeperState.CLOSED. Patch by Jyrki Pulliainen in PR \#174.
-   Issue \#169: Fixed pipes leaking into child processes.

### Documentation

-   Add section on contributing recipes, add maintainer/status
    information for existing recipes.
-   Add note about alternate use of DataWatch.

1.3.1 (2013-09-25)
------------------

### Bug Handling

-   \#118, \#125, \#128: Fix unknown variable in KazooClient
    command\_retry argument handling.
-   \#126: Fix KazooRetry.copy to correctly copy sleep function.
-   \#118: Correct session/socket timeout conversion (int vs. float).

### Documentation

-   \#121: Add a note about kazoo.recipe.queue.LockingQueue requiring a
    Zookeeper 3.4+ server.

1.3 (2013-09-05)
----------------

### Features

-   \#115: Limit the backends we use for SLF4J during tests.
-   \#112: Add IPv6 support. Patch by Dan Kruchinin.

1.2.1 (2013-08-01)
------------------

### Bug Handling

-   Issue \#108: Circular import fail when importing
    kazoo.recipe.watchers directly has now been resolved. Watchers and
    partitioner properly import the KazooState from
    kazoo.protocol.states rather than kazoo.client.
-   Issue \#109: Partials not usable properly as a datawatch call can
    now be used. All funcs will be called with 3 args and fall back to 2
    args if there's an argument error.
-   Issue \#106, \#107: client.create\_async didn't strip change root
    from the returned path.

1.2 (2013-07-24)
----------------

### Features

-   KazooClient can now be stopped more reliably even if its in the
    middle of a long retry sleep. This utilizes the new interrupt
    feature of KazooRetry which lets the sleep be broken down into
    chunks and an interrupt function called to determine if the retry
    should fail early.
-   Issue \#62, \#92, \#89, \#101, \#102: Allow KazooRetry to have a max
    deadline, transition properly when connection fails to LOST, and
    setup separate connection retry behavior from client command retry
    behavior. Patches by Mike Lundy.
-   Issue \#100: Make it easier to see exception context in threading
    and connection modules.
-   Issue \#85: Increase information density of logs and don't prevent
    dynamic reconfiguration of log levels at runtime.
-   Data-watchers for the same node are no longer 'stacked'. That is, if
    a get and an exists call occur for the same node with the same watch
    function, then it will be registered only once. This change results
    in Kazoo behaving per Zookeeper client spec regarding repeat watch
    use.

### Bug Handling

-   Issue \#53: Throw a warning upon starting if the chroot path doesn't
    exist so that it's more obvious when the chroot should be created
    before performing more operations.
-   Kazoo previously would let the same function be registered as a
    data-watch or child-watch multiple times, and then call it multiple
    times upon being triggered. This was non-compliant Zookeeper client
    behavior, the same watch can now only be registered once for the
    same znode path per Zookeeper client documentation.
-   Issue \#105: Avoid rare import lock problems by moving module
    imports in client.py to the module scope.
-   Issue \#103: Allow prefix-less sequential znodes.
-   Issue \#98: Extend testing ZK harness to work with different file
    locations on some versions of Debian/Ubuntu.
-   Issue \#97: Update some docstrings to reflect current state of
    handlers.
-   Issue \#62, \#92, \#89, \#101, \#102: Allow KazooRetry to have a max
    deadline, transition properly when connection fails to LOST, and
    setup separate connection retry behavior from client command retry
    behavior. Patches by Mike Lundy.

### API Changes

-   The kazoo.testing.harness.KazooTestHarness class directly inherits
    from unittest.TestCase and you need to ensure to call its
    \_\_init\_\_ method.
-   DataWatch no longer takes any parameters besides for the optional
    function during instantiation. The additional options are now
    implicitly True, with the user being left to ignore events as they
    choose. See the DataWatch API docs for more information.
-   Issue \#99: Better exception raised when the writer fails to close.
    A WriterNotClosedException that inherits from KazooException is now
    raised when the writer fails to close in time.

1.1 (2013-06-08)
----------------

### Features

-   Issue \#93: Add timeout option to lock/semaphore acquire methods.
-   Issue \#79 / \#90: Add ability to pass the WatchedEvent to DataWatch
    and ChildWatch functions.
-   Respect large client timeout values when closing the connection.
-   Add a max\_leases consistency check to the semaphore recipe.
-   Issue \#76: Extend testing helpers to allow customization of the
    Java classpath by specifying the new ZOOKEEPER\_CLASSPATH
    environment variable.
-   Issue \#65: Allow non-blocking semaphore acquisition.

### Bug Handling

-   Issue \#96: Provide Windows compatibility in testing harness.
-   Issue \#95: Handle errors deserializing connection response.
-   Issue \#94: Clean up stray bytes in connection pipe.
-   Issue \#87 / \#88: Allow re-acquiring lock after cancel.
-   Issue \#77: Use timeout in initial socket connection.
-   Issue \#69: Only ensure path once in lock and semaphore recipes.
-   Issue \#68: Closing the connection causes exceptions to be raised by
    watchers which assume the connection won't be closed when running
    commands.
-   Issue \#66: Require ping reply before sending another ping,
    otherwise the connection will be considered dead and a
    ConnectionDropped will be raised to trigger a reconnect.
-   Issue \#63: Watchers weren't reset on lost connection.
-   Issue \#58: DataWatcher failed to re-register for changes after
    non-existent node was created then deleted.

### API Changes

-   KazooClient.create\_async now supports the makepath argument.
-   KazooClient.ensure\_path now has an async version,
    ensure\_path\_async.

1.0 (2013-03-26)
----------------

### Features

-   Added a LockingQueue recipe. The queue first locks an item and
    removes it from the queue only after the consume() method is called.
    This enables other nodes to retake the item if an error occurs on
    the first node.

### Bug Handling

-   Issue \#50: Avoid problems with sleep function in mixed
    gevent/threading setup.
-   Issue \#56: Avoid issues with watch callbacks evaluating to false.

1.0b1 (2013-02-24)
------------------

### Features

-   Refactored the internal connection handler to use a single thread.
    It now uses a deque and pipe to signal the ZK thread that there's a
    new command to send, so that the ZK thread can send it, or retrieve
    a response. Processing ZK requests and responses serially in a
    single thread eliminates the need for a bunch of the locking, the
    peekable queue and two threads working on the same underlying
    socket.
-   Issue \#48: Added documentation for the retry helper module.
-   Issue \#55: Fix os.pipe file descriptor leak and introduce a
    KazooClient.close method. The method is particular useful in tests,
    where multiple KazooClients are created and closed in the same
    process.

### Bug Handling

-   Issue \#46: Avoid TypeError in GeneratorContextManager on process
    shutdown.
-   Issue \#43: Let DataWatch return node data if allow\_missing\_node
    is used.

0.9 (2013-01-07)
----------------

### API Changes

-   When a retry operation ultimately fails, it now raises a
    kazoo.retry.RetryFailedError exception, instead of a general
    Exception instance. RetryFailedError also inherits from the base
    KazooException.

### Features

-   Improvements to Debian packaging rules.

### Bug Handling

-   Issue \#39 / \#41: Handle connection dropped errors during session
    writes. Ensure client connection is re-established to a new ZK node
    if available.
-   Issue \#38: Set CLOEXEC flag on all sockets when available.
-   Issue \#37 / \#40: Handle timeout errors during select calls on
    sockets.
-   Issue \#36: Correctly set ConnectionHandler.writer\_stopped even if
    an exception is raised inside the writer, like a retry operation
    failing.

0.8 (2012-10-26)
----------------

### API Changes

-   The KazooClient.\_\_init\_\_ took as watcher argument as its second
    keyword argument. The argument had no effect anymore since version
    0.5 and was removed.

### Bug Handling

-   Issue \#35: KazooClient.\_\_init\_\_ didn't pass on
    retry\_max\_delay to the retry helper.
-   Issue \#34: Be more careful while handling socket connection errors.

0.7 (2012-10-15)
----------------

### Features

-   DataWatch now has a allow\_missing\_node setting that allows a watch
    to be set on a node that doesn't exist when the DataWatch is
    created.
-   Add new Queue recipe, with optional priority support.
-   Add new Counter recipe.
-   Added debian packaging rules.

### Bug Handling

-   Issue \#31 fixed: Only catch KazooExceptions in catch-all calls.
-   Issue \#15 fixed again: Force sleep delay to be a float to appease
    gevent.
-   Issue \#29 fixed: DataWatch and ChildrenWatch properly re-register
    their watches on server disconnect.

0.6 (2012-09-27)
----------------

### API Changes

-   Node paths are assumed to be Unicode objects. Under Python 2
    pure-ascii strings will also be accepted. Node values are considered
    bytes. The byte type is an alias for str under Python 2.
-   New KeeperState.CONNECTED\_RO state for Zookeeper servers connected
    in read-only mode.
-   New NotReadOnlyCallError exception when issuing a write change
    against a server thats currently read-only.

### Features

-   Add support for Python 3.2, 3.3 and PyPy (only for the threading
    handler).
-   Handles connecting to Zookeeper 3.4+ read-only servers.
-   Automatic background scanning for a Read/Write server when connected
    to a server in read-only mode.
-   Add new Semaphore recipe.
-   Add a new retry\_max\_delay argument to the client and by default
    limit the retry delay to at most an hour regardless of exponential
    backoff settings.
-   Add new randomize\_hosts argument to KazooClient, allowing one to
    disable host randomization.

### Bug Handling

-   Fix bug with locks not handling intermediary lock contenders
    disappearing.
-   Fix bug with set\_data type check failing to catch unicode values.
-   Fix bug with gevent 0.13.x backport of peekable queue.
-   Fix PatientChildrenWatch to use handler specific sleep function.

0.5 (2012-09-06)
----------------

Skipping a version to reflect the magnitude of the change. Kazoo is now
a pure Python client with no C bindings. This release should run without
a problem on alternate Python implementations such as PyPy and Jython.
Porting to Python 3 in the future should also be much easier.

### Documentation

-   Docs have been restructured to handle the new classes and locations
    of the methods from the pure Python refactor.

### Bug Handling

This change may introduce new bugs, however there is no longer the
possibility of a complete Python segfault due to errors in the C library
and/or the C binding.

-   Possible segfaults from the C lib are gone.
-   Password mangling due to the C lib is gone.
-   The party recipes didn't set their participating flag to False after
    leaving.

### Features

-   New client.command and client.server\_version API, exposing
    Zookeeper's four letter commands and giving access to structured
    version information.
-   Added 'include\_data' option for get\_children to include the node's
    Stat object.
-   Substantial increase in logging data with debug mode. All
    correspondence with the Zookeeper server can now be seen to help in
    debugging.

### API Changes

-   The testing helpers have been moved from testing.\_\_init\_\_ into a
    testing.harness module. The official API's of KazooTestCase and
    KazooTestHarness can still be directly imported from testing.
-   The kazoo.handlers.util module was removed.
-   Backwards compatible exception class aliases are provided for now in
    kazoo exceptions for the prior C exception names.
-   Unicode strings now work fine for node names and are properly
    converted to and from unicode objects.
-   The data value argument for the create and create\_async methods of
    the client was made optional and defaults to an empty byte string.
    The data value must be a byte string. Unicode values are no longer
    allowed and will raise a TypeError.

0.3 (2012-08-23)
----------------

### API Changes

-   Handler interface now has an rlock\_object for use by recipes.

### Bug Handling

-   Fixed password bug with updated zc-zookeeper-static release, which
    retains null bytes in the password properly.
-   Fixed reconnect hammering, so that the reconnection follows retry
    jitter and retry backoff's.
-   Fixed possible bug with using a threading.Condition in the set
    partitioner. Set partitioner uses new rlock\_object handler API to
    get an appropriate RLock for gevent.
-   Issue \#17 fixed: Wrap timeout exceptions with staticmethod so they
    can be used directly as intended. Patch by Bob Van Zant.
-   Fixed bug with client reconnection looping indefinitely using an
    expired session id.

0.2 (2012-08-12)
----------------

### Documentation

-   Fixed doc references to start\_async using an AsyncResult object, it
    uses an Event object.

### Bug Handling

-   Issue \#16 fixed: gevent zookeeper logging failed to handle a monkey
    patched logging setup. Logging is now setup such that a greenlet is
    used for logging messages under gevent, and the thread one is used
    otherwise.
-   Fixed bug similar to \#14 for ChildrenWatch on the session listener.
-   Issue \#14 fixed: DataWatch had inconsistent handling of the node it
    was watching not existing. DataWatch also properly spawns its
    \_get\_data function to avoid blocking session events.
-   Issue \#15 fixed: sleep\_func for SequentialGeventHandler was not
    set on the class appropriately leading to additional arguments being
    passed to gevent.sleep.
-   Issue \#9 fixed: Threads/greenlets didn't gracefully shut down.
    Handler now has a start/stop that is used by the client when calling
    start and stop that shuts down the handler workers. This addresses
    errors and warnings that could be emitted upon process shutdown
    regarding a clean exit of the workers.
-   Issue \#12 fixed: gevent 0.13 doesn't use the same
    start\_new\_thread as gevent 1.0 which resulted in a fully
    monkey-patched environment halting due to the wrong thread. Updated
    to use the older kazoo method of getting the real thread module
    object.

### API Changes

-   The KazooClient handler is now officially exposed as
    KazooClient.handler so that the appropriate sync objects can be used
    by end-users.
-   Refactored ChildrenWatcher used by SetPartitioner into a publicly
    exposed PatientChildrenWatch under recipe.watchers.

### Deprecations

-   connect/connect\_async has been renamed to start/start\_async to
    better match the stop to indicate connection handling. The prior
    names are aliased for the time being.

### Recipes

-   Added Barrier and DoubleBarrier implementation.

0.2b1 (2012-07-27)
------------------

### Bug Handling

-   ZOOKEEPER-1318: SystemError is caught and rethrown as the proper
    invalid state exception in older zookeeper python bindings where
    this issue is still valid.
-   ZOOKEEPER-1431: Install the latest zc-zookeeper-static library or
    use the packaged ubuntu one for ubuntu 12.04 or later.
-   ZOOKEEPER-553: State handling isn't checked via this method, we
    track it in a simpler manner with the watcher to ensure we know the
    right state.

### Features

-   Exponential backoff with jitter for retrying commands.
-   Gevent 0.13 and 1.0b support.
-   Lock, Party, SetPartitioner, and Election recipe implementations.
-   Data and Children watching API's.
-   State transition handling with listener registering to handle
    session state changes (choose to fatal the app on session
    expiration, etc.)
-   Zookeeper logging stream redirected into Python logging channel
    under the name 'Zookeeper'.
-   Base client library with handler support for threading and gevent
    async environments.




  * [keepalive-0.5](https://github.com/wikier/keepalive) An HTTP handler for `urllib2` that supports HTTP 1.1 and keepalive.
  * [keyring-19.0.2](https://github.com/jaraco/keyring) .. image:: https://img.shields.io/pypi/v/keyring.svg
   :target: https://pypi.org/project/keyring

.. image:: https://img.shields.io/pypi/pyversions/keyring.svg

.. image:: https://img.shields.io/travis/jaraco/keyring/master.svg
   :target: https://travis-ci.org/jaraco/keyring

.. image:: https://img.shields.io/appveyor/ci/jaraco/keyring/master.svg
   :target: https://ci.appveyor.com/project/jaraco/keyring/branch/master

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
   :alt: Code style: Black

.. image:: https://readthedocs.org/projects/keyring/badge/?version=latest
   :target: https://keyring.readthedocs.io/en/latest/?badge=latest

.. image:: https://tidelift.com/badges/github/jaraco/keyring
   :target: https://tidelift.com/subscription/pkg/pypi-keyring?utm_source=pypi-keyring&utm_medium=readme

=======================================
Installing and Using Python Keyring Lib
=======================================

.. contents:: **Table of Contents**

---------------------------
What is Python keyring lib?
---------------------------

The Python keyring lib provides an easy way to access the system keyring service
from python. It can be used in any application that needs safe password storage.

The keyring library is licensed under both the `MIT license
<http://opensource.org/licenses/MIT>`_ and the PSF license.

These recommended keyring backends are supported by the Python keyring lib:

* macOS `Keychain
  <https://en.wikipedia.org/wiki/Keychain_%28software%29>`_
* Freedesktop `Secret Service
  <http://standards.freedesktop.org/secret-service/>`_ supports many DE including
  GNOME (requires `secretstorage <https://pypi.python.org/pypi/secretstorage>`_)
* KDE4 & KDE5 `KWallet <https://en.wikipedia.org/wiki/KWallet>`_
  (requires `dbus <https://pypi.python.org/pypi/dbus-python>`_)
* `Windows Credential Locker
  <https://docs.microsoft.com/en-us/windows/uwp/security/credential-locker>`_

Other keyring implementations are available through `Third-Party Backends`_.

-------------------------
Installation Instructions
-------------------------

Install from Index
==================

Install using your favorite installer. For example:

    $ pip install keyring

Linux
-----

On Linux, the KWallet backend relies on dbus-python_, which does not always
install correctly when using pip (compilation is needed). So we recommend
that dbus-python is installed as a system package. The same also applies to
the Secret Storage backend under Python 2 (under Python 3 a different D-Bus
implementation is used).

.. _dbus-python: https://gitlab.freedesktop.org/dbus/dbus-python

-------------
Using Keyring
-------------

The basic usage of keyring is pretty simple: just call `keyring.set_password`
and `keyring.get_password`:

    >>> import keyring
    >>> keyring.set_password("system", "username", "password")
    >>> keyring.get_password("system", "username")
    'password'

Command-line Utility
====================

Keyring supplies a ``keyring`` command which is installed with the
package. After installing keyring in most environments, the
command should be available for setting, getting, and deleting
passwords. For more information on usage, invoke with no arguments
or with ``--help`` as so::

    $ keyring --help
    $ keyring set system username
    Password for 'username' in 'system':
    $ keyring get system username
    password

The command-line functionality is also exposed as an executable
package, suitable for invoking from Python like so::

    $ python -m keyring --help
    $ python -m keyring set system username
    Password for 'username' in 'system':
    $ python -m keyring get system username
    password

--------------------------
Configure your keyring lib
--------------------------

The python keyring lib contains implementations for several backends. The
library will
automatically choose the keyring that is most suitable for your current
environment. You can also specify the keyring you like to be used in the
config file or by calling the ``set_keyring()`` function.

Customize your keyring by config file
=====================================

This section describes how to change your option in the config file.

Config file path
----------------

The configuration of the lib is stored in a file named "keyringrc.cfg". This
file must be found in a platform-specific location. To determine
where the config file is stored, run the following::

    python -c "import keyring.util.platform_; print(keyring.util.platform_.config_root())"

Some keyrings also store the keyring data in the file system. To determine
where the data files are stored, run this command::

    python -c "import keyring.util.platform_; print(keyring.util.platform_.data_root())"


Config file content
-------------------

To specify a keyring backend, set the **default-keyring** option to the
full path of the class for that backend, such as
``keyring.backends.OS_X.Keyring``.

If **keyring-path** is indicated, keyring will add that path to the Python
module search path before loading the backend.

For example, this config might be used to load the
``SimpleKeyring`` from the ``simplekeyring`` module in
the ``./demo`` directory (not implemented)::

    [backend]
    default-keyring=simplekeyring.SimpleKeyring
    keyring-path=demo

Third-Party Backends
====================

In addition to the backends provided by the core keyring package for
the most common and secure use cases, there
are additional keyring backend implementations available for other
use-cases. Simply install them to make them available:

- `keyrings.cryptfile <https://pypi.org/project/keyrings.cryptfile>`_
  - Encrypted text file storage.
- `keyring_jeepney <https://pypi.org/project/keyring_jeepney>`__ - a
  pure Python backend using the secret service DBus API for desktop
  Linux.
- `keyrings.alt <https://pypi.org/project/keyrings.alt>`_ - "alternate",
  possibly-insecure backends, originally part of the core package, but
  available for opt-in.
- `gsheet-keyring <https://pypi.org/project/gsheet-keyring>`_
  - a backend that stores secrets in a Google Sheet. For use with
  `ipython-secrets <https://pypi.org/project/ipython-secrets>`_.
- `bitwarden-keyring <https://pypi.org/project/bitwarden-keyring/0.1.0/>`_
  - a backend that stores secrets in the `BitWarden <https://bitwarden.com/>`_
  password manager.


Write your own keyring backend
==============================

The interface for the backend is defined by ``keyring.backend.KeyringBackend``.
Every backend should derive from that base class and define a ``priority``
attribute and three functions: ``get_password()``, ``set_password()``, and
``delete_password()``. The ``get_credential()`` function may be defined if
desired.

See the ``backend`` module for more detail on the interface of this class.

Keyring employs entry points to allow any third-party package to implement
backends without any modification to the keyring itself. Those interested in
creating new backends are encouraged to create new, third-party packages
in the ``keyrings`` namespace, in a manner modeled by the `keyrings.alt
package <https://github.com/jaraco/keyrings.alt>`_. See the ``setup.py`` file
in that project for a hint on how to create the requisite entry points.
Backends that prove essential may be considered for inclusion in the core
library, although the ease of installing these third-party packages should
mean that extensions may be readily available.

If you've created an extension for Keyring, please submit a pull request to
have your extension mentioned as an available extension.

Set the keyring in runtime
==========================

Keyring additionally allows programmatic configuration of the
backend calling the api ``set_keyring()``. The indicated backend
will subsequently be used to store and retrieve passwords.

Here's an example demonstrating how to invoke ``set_keyring``::

    # define a new keyring class which extends the KeyringBackend
    import keyring.backend

    class TestKeyring(keyring.backend.KeyringBackend):
        """A test keyring which always outputs same password
        """
        priority = 1

        def set_password(self, servicename, username, password):
            pass

        def get_password(self, servicename, username):
            return "password from TestKeyring"

        def delete_password(self, servicename, username, password):
            pass

    # set the keyring for keyring lib
    keyring.set_keyring(TestKeyring())

    # invoke the keyring lib
    try:
        keyring.set_password("demo-service", "tarek", "passexample")
        print("password stored successfully")
    except keyring.errors.PasswordSetError:
        print("failed to store password")
    print("password", keyring.get_password("demo-service", "tarek"))


Using Keyring on Ubuntu 16.04
=============================

The following is a complete transcript for installing keyring in a
virtual environment on Ubuntu 16.04.  No config file was used.::

  $ sudo apt install python3-venv libdbus-glib-1-dev
  $ cd /tmp
  $ pyvenv py3
  $ source py3/bin/activate
  $ pip install -U pip
  $ pip install secretstorage dbus-python
  $ pip install keyring
  $ python
  >>> import keyring
  >>> keyring.get_keyring()
  <keyring.backends.SecretService.Keyring object at 0x7f9b9c971ba8>
  >>> keyring.set_password("system", "username", "password")
  >>> keyring.get_password("system", "username")
  'password'


Using Keyring on headless Linux systems
=======================================

It is possible to use the SecretService backend on Linux systems without
X11 server available (only D-Bus is required). To do that, you need the
following:

* Install the `GNOME Keyring`_ daemon.
* Start a D-Bus session, e.g. run ``dbus-run-session -- sh`` and run
  the following commands inside that shell.
* Run ``gnome-keyring-daemon`` with ``--unlock`` option. The description of
  that option says:

      Read a password from stdin, and use it to unlock the login keyring
      or create it if the login keyring does not exist.

  When that command is started, enter your password into stdin and
  press Ctrl+D (end of data). After that the daemon will fork into
  background (use ``--foreground`` option to prevent that).
* Now you can use the SecretService backend of Keyring. Remember to
  run your application in the same D-Bus session as the daemon.

.. _GNOME Keyring: https://wiki.gnome.org/Projects/GnomeKeyring

-----------------------------------------------
Integrate the keyring lib with your application
-----------------------------------------------

API interface
=============

The keyring lib has a few functions:

* ``get_keyring()``: Return the currently-loaded keyring implementation.
* ``get_password(service, username)``: Returns the password stored in the
  active keyring. If the password does not exist, it will return None.
* ``get_credential(service, username)``: Return a credential object stored
  in the active keyring. This object contains at least ``username`` and
  ``password`` attributes for the specified service, where the returned
  ``username`` may be different from the argument.
* ``set_password(service, username, password)``: Store the password in the
  keyring.
* ``delete_password(service, username)``: Delete the password stored in
  keyring. If the password does not exist, it will raise an exception.

In all cases, the parameters (``service``, ``username``, ``password``)
should be Unicode text. On Python 2, these parameters are accepted as
simple ``str`` in the default encoding as they will be implicitly
decoded to text. Some backends may accept ``bytes`` for these parameters,
but such usage is discouraged.


Exceptions
==========

The keyring lib raises following exceptions:

* ``keyring.errors.KeyringError``: Base Error class for all exceptions in keyring lib.
* ``keyring.errors.InitError``: Raised when the keyring can't be initialized.
* ``keyring.errors.PasswordSetError``: Raise when password can't be set in the keyring.
* ``keyring.errors.PasswordDeleteError``: Raised when the password can't be deleted in the keyring.

------------
Get involved
------------

Python keyring lib is an open community project and highly welcomes new
contributors.

* Repository: https://github.com/jaraco/keyring/
* Bug Tracker: https://github.com/jaraco/keyring/issues/
* Mailing list: http://groups.google.com/group/python-keyring

Security Contact
================

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.

Making Releases
===============

This project makes use of automated releases via Travis-CI. The
simple workflow is to tag a commit and push it to Github. If it
passes tests on a late Python version, it will be automatically
deployed to PyPI.

Other things to consider when making a release:

 - first ensure that tests pass (preferably on Windows and Linux)
 - check that the changelog is current for the intended release

Running Tests
=============

Tests are `continuously run <https://travis-ci.org/#!/jaraco/keyring>`_ using
Travis-CI.

To run the tests yourself, you'll want keyring installed to some environment
in which it can be tested. Recommended technique is described below.

Using tox
---------

Keyring prefers use of `tox <https://pypi.org/project/tox>`_ to run tests.
Simply install and invoke ``tox``.

This technique is the one used by the Travis-CI script.

----------
Background
----------

The project was based on Tarek Ziade's idea in `this post`_. Kang Zhang
initially carried it out as a `Google Summer of Code`_ project, and Tarek
mentored Kang on this project.

.. _this post: http://tarekziade.wordpress.com/2009/03/27/pycon-hallway-session-1-a-keyring-library-for-python/
.. _Google Summer of Code: http://socghop.appspot.com/


.. image:: https://badges.gitter.im/jaraco/keyring.svg
   :alt: Join the chat at https://gitter.im/jaraco/keyring
   :target: https://gitter.im/jaraco/keyring?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge



  * [keystoneauth1-3.15.0](https://docs.openstack.org/keystoneauth/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/keystoneauth.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

============
keystoneauth
============

.. image:: https://img.shields.io/pypi/v/keystoneauth1.svg
    :target:https://pypi.org/project/keystoneauth1
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/keystoneauth1.svg
    :target: https://pypi.org/project/keystoneauth1/
    :alt: Downloads

This package contains tools for authenticating to an OpenStack-based cloud.
These tools include:

* Authentication plugins (password, token, and federation based)
* Discovery mechanisms to determine API version support
* A session that is used to maintain client settings across requests (based on
  the requests Python library)

Further information:

* Free software: Apache license
* Documentation: https://docs.openstack.org/keystoneauth/latest/
* Source: https://opendev.org/openstack/keystoneauth
* Bugs: https://bugs.launchpad.net/keystoneauth
* Release notes: https://docs.openstack.org/releasenotes/keystoneauth/




  * [kivy-garden-0.1.4](https://github.com/kivy-garden/garden) garden
======

The kivy garden installation script, split into its own package for convenient use in buildozer.

Licenses
========
Garden is released under the terms of the MIT License. Please refer to the LICENSE file.
  * [kiwisolver-1.1.0](https://github.com/nucleic/kiwi) Welcome to Kiwi
===============

.. image:: https://travis-ci.org/nucleic/kiwi.svg?branch=master
    :target: https://travis-ci.org/nucleic/kiwi
.. image:: https://codecov.io/gh/nucleic/kiwi/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/nucleic/kiwi
.. image:: https://readthedocs.org/projects/kiwisolver/badge/?version=latest
    :target: https://kiwisolver.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

Kiwi is an efficient C++ implementation of the Cassowary constraint solving
algorithm. Kiwi is an implementation of the algorithm based on the seminal
Cassowary paper. It is *not* a refactoring of the original C++ solver. Kiwi
has been designed from the ground up to be lightweight and fast. Kiwi ranges
from 10x to 500x faster than the original Cassowary solver with typical use
cases gaining a 40x improvement. Memory savings are consistently > 5x.

In addition to the C++ solver, Kiwi ships with hand-rolled Python bindings.
  * [kombu-4.6.3](https://kombu.readthedocs.io) 
  * [lark-parser-0.7.2](https://github.com/erezsh/lark) 
Lark is a modern general-purpose parsing library for Python.

With Lark, you can parse any context-free grammar, efficiently, with very little code.

Main Features:
 - Builds a parse-tree (AST) automagically, based on the structure of the grammar
 - Earley parser
    - Can parse all context-free grammars
    - Full support for ambiguous grammars
 - LALR(1) parser
    - Fast and light, competitive with PLY
    - Can generate a stand-alone parser
 - CYK parser, for highly ambiguous grammars
 - EBNF grammar
 - Unicode fully supported
 - Python 2 & 3 compatible
 - Automatic line & column tracking
 - Standard library of terminals (strings, numbers, names, etc.)
 - Import grammars from Nearley.js
 - Extensive test suite
 - And much more!

  * [lazr.uri-1.0.3](https://launchpad.net/lazr.uri) ..
    This file is part of lazr.uri.

    lazr.uri is free software: you can redistribute it and/or modify it
    under the terms of the GNU Lesser General Public License as published by
    the Free Software Foundation, version 3 of the License.

    lazr.uri is distributed in the hope that it will be useful, but
    WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
    or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
    License for more details.

    You should have received a copy of the GNU Lesser General Public License
    along with lazr.uri.  If not, see <http://www.gnu.org/licenses/>.

lazr.uri
********

The lazr.uri package includes code for parsing and dealing with URIs.

    >>> import lazr.uri
    >>> print('VERSION:', lazr.uri.__version__)
    VERSION: ...

=============
The URI class
=============

    >>> from lazr.uri import URI
    >>> uri1 = URI('http://localhost/foo/bar?123')
    >>> uri2 = URI('http://localhost/foo/bar/baz')
    >>> uri1.contains(uri2)
    True

These next two are equivalent, so the answer should be True, even through
the "outside" one is shorter than the "inside" one.

    >>> uri1 = URI('http://localhost/foo/bar/')
    >>> uri2 = URI('http://localhost/foo/bar')
    >>> uri1.contains(uri2)
    True

The next two are exactly the same.  We consider a url to be inside itself.

    >>> uri1 = URI('http://localhost/foo/bar/')
    >>> uri2 = URI('http://localhost/foo/bar/')
    >>> uri1.contains(uri2)
    True

In the next case, the string of url2 starts with the string of url1.  But,
because url2 continues within the same path step, url2 is not inside url1.

    >>> uri1 = URI('http://localhost/foo/ba')
    >>> uri2 = URI('http://localhost/foo/bar')
    >>> uri1.contains(uri2)
    False

Here, url2 is url1 plus an extra path step.  So, url2 is inside url1.

    >>> uri1 = URI('http://localhost/foo/bar/')
    >>> uri2 = URI('http://localhost/foo/bar/baz')
    >>> uri1.contains(uri2)
    True

Once the URI is parsed, its parts are accessible.

    >>> uri = URI('https://fish.tree:8666/blee/blah')
    >>> uri.scheme
    'https'
    >>> uri.host
    'fish.tree'
    >>> uri.port
    '8666'
    >>> uri.authority
    'fish.tree:8666'
    >>> uri.path
    '/blee/blah'

    >>> uri = URI('https://localhost/blee/blah')
    >>> uri.scheme
    'https'
    >>> uri.host
    'localhost'
    >>> uri.port is None
    True
    >>> uri.authority
    'localhost'
    >>> uri.path
    '/blee/blah'

The grammar from RFC 3986 does not allow for square brackets in the
query component, but Section 3.4 does say how such delimeter
characters should be handled if found in the component.

    >>> uri = URI('http://www.apple.com/store?delivery=[slow]#horse+cart')
    >>> uri.scheme
    'http'
    >>> uri.host
    'www.apple.com'
    >>> uri.port is None
    True
    >>> uri.path
    '/store'
    >>> uri.query
    'delivery=[slow]'
    >>> uri.fragment
    'horse+cart'

====================
Finding URIs in Text
====================

lazr.uri also knows how to retrieve a list of URIs from a block of
text.  This is intended for uses like finding bug tracker URIs or
similar.

The find_uris_in_text() function returns an iterator that yields URI
objects for each URI found in the text.  Note that the returned URIs
have been canonicalised by the URI class:

  >>> from lazr.uri import find_uris_in_text
  >>> text = '''
  ... A list of URIs:
  ...  * http://localhost/a/b
  ...  * http://launchpad.net
  ...  * MAILTO:joe@example.com
  ...  * xmpp:fred@example.org
  ...  * http://bazaar.launchpad.net/%7ename12/firefox/foo
  ...  * http://somewhere.in/time?track=[02]#wasted-years
  ... '''

  >>> for uri in find_uris_in_text(text):
  ...     print(uri)
  http://localhost/a/b
  http://launchpad.net/
  mailto:joe@example.com
  xmpp:fred@example.org
  http://bazaar.launchpad.net/~name12/firefox/foo
  http://somewhere.in/time?track=[02]#wasted-years

===============
Other Documents
===============

.. toctree::
   :glob:

   *
   docs/*

=================
NEWS for lazr.uri
=================

1.0.3 (2012-01-18)
==================

- Add compatibility with Python 3 (Thomas Kluyver).

1.0.1 (2009-06-01)
==================

- Eliminate dependency on setuptools_bzr so sdists do not bring bzr ini, among
  others.

1.0 (2009-03-23)
================

- Initial release on PyPI
  * [lazy-object-proxy-1.4.1](https://github.com/ionelmc/python-lazy-object-proxy) ========
Overview
========



A fast and thorough lazy object proxy.

* Free software: BSD license

Note that this is based on `wrapt`_'s ObjectProxy with one big change: it calls a function the first time the proxy object is
used, while `wrapt.ObjectProxy` just forwards the method calls to the target object.

In other words, you use `lazy-object-proxy` when you only have the object way later and you use `wrapt.ObjectProxy` when you
want to override few methods (by subclassing) and forward everything else to the target object.

Example::

    import lazy_object_proxy

    def expensive_func():
        from time import sleep
        print('starting calculation')
        # just as example for a very slow computation
        sleep(2)
        print('finished calculation')
        # return the result of the calculation
        return 10

    obj = lazy_object_proxy.Proxy(expensive_func)
    # function is called only when object is actually used
    print(obj)  # now expensive_func is called

    print(obj)  # the result without calling the expensive_func

Installation
============

::

    pip install lazy-object-proxy

Documentation
=============

https://python-lazy-object-proxy.readthedocs.io/

Development
===========

To run the all tests run::

    tox

Acknowledgements
================

This project is based on some code from `wrapt`_ as you can see in the git history.

.. _wrapt: https://github.com/GrahamDumpleton/wrapt


Changelog
=========

1.4.1 (2019-05-10)
------------------

* Fixed wheels being built with ``-coverage`` cflags. No more issues about bogus ``cext.gcda`` files.
* Removed useless C file from wheels.
* Changed ``setup.py`` to use setuptools-scm.

1.4.0 (2019-05-05)
------------------

* Fixed ``__mod__`` for the slots backend. Contributed by Ran Benita in
  `#28 <https://github.com/ionelmc/python-lazy-object-proxy/pull/28>`_.
* Dropped support for Python 2.6 and 3.3. Contributed by "hugovk" in
  `#24 <https://github.com/ionelmc/python-lazy-object-proxy/pull/24>`_.

1.3.1 (2017-05-05)
------------------

* Fix broken release (``sdist`` had a broken ``MANIFEST.in``).

1.3.0 (2017-05-02)
------------------

* Speed up arithmetic operations involving ``cext.Proxy`` subclasses.

1.2.2 (2016-04-14)
------------------

* Added `manylinux <https://www.python.org/dev/peps/pep-0513/>`_ wheels.
* Minor cleanup in readme.

1.2.1 (2015-08-18)
------------------

* Fix a memory leak (the wrapped object would get bogus references). Contributed by Astrum Kuo in
  `#10 <https://github.com/ionelmc/python-lazy-object-proxy/pull/10>`_.

1.2.0 (2015-07-06)
------------------

* Don't instantiate the object when __repr__ is called. This aids with debugging (allows one to see exactly in
  what state the proxy is).

1.1.0 (2015-07-05)
------------------

* Added support for pickling. The pickled value is going to be the wrapped object *without* any Proxy container.
* Fixed a memory management issue in the C extension (reference cycles weren't garbage collected due to improper
  handling in the C extension). Contributed by Alvin Chow in
  `#8 <https://github.com/ionelmc/python-lazy-object-proxy/pull/8>`_.

1.0.2 (2015-04-11)
-----------------------------------------

* First release on PyPI.
  * [ldap3-2.6](https://github.com/cannatag/ldap3) LDAP3
=====

.. image:: https://img.shields.io/pypi/v/ldap3.svg
    :target: https://pypi.python.org/pypi/ldap3/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/l/ldap3.svg
    :target: https://pypi.python.org/pypi/ldap3/
    :alt: License

.. image:: https://img.shields.io/travis/cannatag/ldap3/master.svg
    :target: https://travis-ci.org/cannatag/ldap3
    :alt: TRAVIS-CI build status for master branch


ldap3 is a strictly RFC 4510 conforming **LDAP V3 pure Python client** library. The same codebase runs in Python 2, Python 3, PyPy and PyPy3.


Version 2 warning
-----------------

In version 2 of ldap3 some default values have been changed and the ldap3 namespace has been decluttered, removing redundant
constants (look at the changelog for details). Also, the result code constants were moved to ldap3.core.results and the ldap3 custom exceptions
were stored in ldap3.core.exceptions. If you experience errors in your existing code you should rearrange the import statements or explicitly
set the defaults to their former values.


A more pythonic LDAP
--------------------

LDAP operations look clumsy and hard-to-use because they reflect the old-age idea that time-consuming operations should be performed client-side
to not hog the server with heavy elaborations. To alleviate this ldap3 includes a fully functional **Abstraction Layer** that lets you
interact with the LDAP server in a modern and *pythonic* way. With the Abstraction Layer you don't need to directly issue any LDAP operation at all.


Home Page
---------

Project home page is https://github.com/cannatag/ldap3


Documentation
-------------

Documentation is available at http://ldap3.readthedocs.io


License
-------

The ldap3 project is open source software released under the **LGPL v3 license**.
Copyright 2013 - 2018 Giovanni Cannata


PEP8 Compliance
---------------

ldap3 is PEP8 compliant, except for line length.


Download
--------

Package download is available at https://pypi.python.org/pypi/ldap3.


Install
-------

Install with **pip install ldap3**


Git repository
--------------

You can download the latest source at https://github.com/cannatag/ldap3


Continuous integration
----------------------

Continuous integration for testing is at https://travis-ci.org/cannatag/ldap3


Support
-------

You can submit support tickets on https://github.com/cannatag/ldap3/issues/new
You can submit pull request on the **dev** branch at https://github.com/cannatag/ldap3/tree/dev


Thanks to
---------

* **Ilya Etingof**, the author of the *pyasn1* package for his excellent work and support.

* **Mark Lutz** for his *Learning Python* and *Programming Python* excellent books series and **John Goerzen** and **Brandon Rhodes** for their book *Foundations of Python Network Programming*. These books are wonderful tools for learning Python and this project owes a lot to them.

* **JetBrains** for donating to this project the Open Source license of *PyCharm Professional*.

* **GitHub** for providing the *free source repository space and the tools* I use to develop this project.

* The **FreeIPA** team for letting me use their demo LDAP server in the ldap3 tutorial.


Contact me
----------

For information and suggestions you can contact me at cannatag@gmail.com. You can also open a support ticket on https://github.com/cannatag/ldap3/issues/new


Donate
------

If you want to keep this project up and running you can send me an Amazon gift card. I will use it to improve my skills in the Information and Communication technology.


Changelog
---------

Updated changelog at https://ldap3.readthedocs.io/changelog.html




  * [leidenalg-0.7.0](https://github.com/vtraag/leidenalg) Leiden is a general algorithm for methods of community detection in large networks.

 Please refer to the `documentation <http://leidenalg.readthedocs.io/en/latest>`_
 for more details.

 The source code of this package is hosted at `GitHub <https://github.com/vtraag/leidenalg>`_.
 Issues and bug reports are welcome at https://github.com/vtraag/leidenalg/issues.
  * [liac-arff-2.3](https://github.com/renatopp/liac-arff) =========
LIAC-ARFF
=========

.. image:: https://travis-ci.org/renatopp/liac-arff.svg
    :target: https://travis-ci.org/renatopp/liac-arff

The liac-arff module implements functions to read and write ARFF files in
Python. It was created in the Connectionist Artificial Intelligence Laboratory
(LIAC), which takes place at the Federal University of Rio Grande do Sul 
(UFRGS), in Brazil.

ARFF (Attribute-Relation File Format) is an file format specially created for
describe datasets which are used commonly for machine learning experiments and
softwares. This file format was created to be used in Weka, the best 
representative software for machine learning automated experiments.

You can clone the `arff-datasets <https://github.com/renatopp/arff-datasets>`_ 
repository for a large set of ARFF files.

--------
Features
--------

- Read and write ARFF files using python built-in structures, such dictionaries
  and lists;
- Supports `scipy.sparse.coo <http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix>`_
  and lists of dictionaries as used by SVMLight
- Supports the following attribute types: NUMERIC, REAL, INTEGER, STRING, and
  NOMINAL;
- Has an interface similar to other built-in modules such as ``json``, or 
  ``zipfile``;
- Supports read and write the descriptions of files;
- Supports missing values and names with spaces;
- Supports unicode values and names;
- Fully compatible with Python 2.7+ and Python 3.3+;
- Under `MIT License <http://opensource.org/licenses/MIT>`_

--------------
How To Install
--------------

Via pip::

    $ pip install liac-arff

Via easy_install::

    $ easy_install liac-arff

Manually::

    $ python setup.py install


-------------
Documentation
-------------

For a complete description of the module, consult the official documentation at
http://packages.python.org/liac-arff/ with mirror in
http://inf.ufrgs.br/~rppereira/docs/liac-arff/index.html


-----
Usage
-----

You can read an ARFF file as follows::

    >>> import arff
    >>> data = arff.load(open('wheater.arff', 'rb'))

Which results in::

    >>> data
    {
        u'attributes': [
            (u'outlook', [u'sunny', u'overcast', u'rainy']),
            (u'temperature', u'REAL'),
            (u'humidity', u'REAL'),
            (u'windy', [u'TRUE', u'FALSE']),
            (u'play', [u'yes', u'no'])],
        u'data': [
            [u'sunny', 85.0, 85.0, u'FALSE', u'no'],
            [u'sunny', 80.0, 90.0, u'TRUE', u'no'],
            [u'overcast', 83.0, 86.0, u'FALSE', u'yes'],
            [u'rainy', 70.0, 96.0, u'FALSE', u'yes'],
            [u'rainy', 68.0, 80.0, u'FALSE', u'yes'],
            [u'rainy', 65.0, 70.0, u'TRUE', u'no'],
            [u'overcast', 64.0, 65.0, u'TRUE', u'yes'],
            [u'sunny', 72.0, 95.0, u'FALSE', u'no'],
            [u'sunny', 69.0, 70.0, u'FALSE', u'yes'],
            [u'rainy', 75.0, 80.0, u'FALSE', u'yes'],
            [u'sunny', 75.0, 70.0, u'TRUE', u'yes'],
            [u'overcast', 72.0, 90.0, u'TRUE', u'yes'],
            [u'overcast', 81.0, 75.0, u'FALSE', u'yes'],
            [u'rainy', 71.0, 91.0, u'TRUE', u'no']
        ],
        u'description': u'',
        u'relation': u'weather'
    }

You can write an ARFF file with this structure::

    >>> print arff.dumps(data)
    @RELATION weather

    @ATTRIBUTE outlook {sunny, overcast, rainy}
    @ATTRIBUTE temperature REAL
    @ATTRIBUTE humidity REAL
    @ATTRIBUTE windy {TRUE, FALSE}
    @ATTRIBUTE play {yes, no}

    @DATA
    sunny,85.0,85.0,FALSE,no
    sunny,80.0,90.0,TRUE,no
    overcast,83.0,86.0,FALSE,yes
    rainy,70.0,96.0,FALSE,yes
    rainy,68.0,80.0,FALSE,yes
    rainy,65.0,70.0,TRUE,no
    overcast,64.0,65.0,TRUE,yes
    sunny,72.0,95.0,FALSE,no
    sunny,69.0,70.0,FALSE,yes
    rainy,75.0,80.0,FALSE,yes
    sunny,75.0,70.0,TRUE,yes
    overcast,72.0,90.0,TRUE,yes
    overcast,81.0,75.0,FALSE,yes
    rainy,71.0,91.0,TRUE,no
    %
    %
    %


Contributors
------------

- `Nate Moseley (FinalDoom) <https://github.com/FinalDoom>`_
- `Tarek Amr (gr33ndata) <https://github.com/gr33ndata>`_
- `Simon (M3t0r) <https://github.com/M3t0r>`_
- `Gonzalo Almeida (flecox) <https://github.com/flecox>`_
- `André Nordbø (AndyNor) <http://andynor.net>`_
- `Niedakh <https://github.com/niedakh>`_
- `Zichen Wang (wangz10) <https://github.com/wangz10>`_
- `Matthias Feurer (mfeurer) <https://github.com/mfeurer>`_
- `Hongjoo Lee (midnightradio) <https://github.com/midnightradio>`_
- `Calvin Jeong (calvin) <http://ty.pe.kr/>`_
- `Joel Nothman (jnothman) <https://github.com/jnothman>`_
- `Guillaume Lemaitre (glemaitre) <https://github.com/glemaitre>`_

Project Page
------------

https://github.com/renatopp/liac-arff

  * [librabbitmq-2.0.0](http://github.com/celery/librabbitmq) ================================================================
 librabbitmq - Python AMQP Client using the rabbitmq-c library.
================================================================

:Version: 2.0.0
:Download: http://pypi.python.org/pypi/librabbitmq/
:Code: http://github.com/celery/librabbitmq/
:Keywords: rabbitmq, amqp, messaging, librabbitmq, rabbitmq-c, python,
           kombu, celery

.. contents::
    :local:

Python bindings to the RabbitMQ C-library `rabbitmq-c`_.
Supported by Kombu and Celery.

.. _`rabbitmq-c`: https://github.com/alanxz/rabbitmq-c

Installation
============

Install via pip::

    $ pip install librabbitmq

or, install via easy_install::

    $ easy_install librabbitmq

Downloading and installing from source
--------------------------------------

Download the latest version from
    http://pypi.python.org/pypi/librabbitmq/

Then install it by doing the following,::

    $ tar xvfz librabbitmq-0.0.0.tar.gz
    $ cd librabbitmq-0.0.0
    $ python setup.py build
    # python setup.py install # as root

Using the development version
-----------------------------

You can clone the repository by doing the following::

    $ git clone git://github.com/celery/librabbitmq.git

Then install it by doing the following::

    $ cd librabbitmq
    $ make install        # or make develop

Examples
========

Using with Kombu::

    >>> from kombu import Connection
    >>> x = Connection("librabbitmq://")


Stand-alone::

    >>> from librabbitmq import Connection

    >>> conn = Connection(host="localhost", userid="guest",
    ...                   password="guest", virtual_host="/")

    >>> channel = conn.channel()
    >>> channel.exchange_declare(exchange, type, ...)
    >>> channel.queue_declare(queue, ...)
    >>> channel.queue_bind(queue, exchange, routing_key)

Producing
---------

::

    >>> channel.basic_publish(body, exchange, routing_key, ...)

Consuming
---------

::

    >>> def dump_message(message):
    ...     print("Body:'%s', Properties:'%s', DeliveryInfo:'%s'" % (
    ...         message.body, message.properties, message.delivery_info))
    ...     message.ack()

    >>> channel.basic_consume(queue, ..., callback=dump_message)

    >>> while True:
    ...    connection.drain_events()

Poll
----

::

    >>> message = channel.basic_get(queue, ...)
    >>> if message:
    ...     dump_message(message)
    ...     print("Body:'%s' Properties:'%s' DeliveryInfo:'%s'" % (
    ...         message.body, message.properties, message.delivery_info))


Other
-----

::

    >>> channel.queue_unbind(queue, ...)
    >>> channel.close()
    >>> connection.close()

License
=======

This software is licensed under the ``Mozilla Public License``.
See the ``LICENSE-MPL-RabbitMQ`` file in the top distribution directory
for the full license text.

.. # vim: syntax=rst expandtab tabstop=4 shiftwidth=4 shiftround



  * [llvmlite-0.29.0](http://llvmlite.pydata.org) ========
llvmlite
========
.. image:: https://travis-ci.org/numba/llvmlite.svg?branch=master
   :target: https://travis-ci.org/numba/llvmlite
   :alt: Travis CI
.. image:: https://codeclimate.com/github/numba/llvmlite/badges/gpa.svg
   :target: https://codeclimate.com/github/numba/llvmlite
   :alt: Code Climate
.. image:: https://coveralls.io/repos/github/numba/llvmlite/badge.svg
   :target: https://coveralls.io/github/numba/llvmlite
   :alt: Coveralls.io
.. image:: https://readthedocs.org/projects/llvmlite/badge/
   :target: https://llvmlite.readthedocs.io
   :alt: Readthedocs.io

A lightweight LLVM python binding for writing JIT compilers

The old llvmpy_  binding exposes a lot of LLVM APIs but the mapping of
C++-style memory management to Python is error prone. Numba_ and many JIT
compilers do not need a full LLVM API.  Only the IR builder, optimizer,
and JIT compiler APIs are necessary.

.. _llvmpy: https://github.com/llvmpy/llvmpy

llvmlite is a project originally tailored for Numba_'s needs, using the
following approach:

* A small C wrapper around the parts of the LLVM C++ API we need that are
  not already exposed by the LLVM C API.
* A ctypes Python wrapper around the C API.
* A pure Python implementation of the subset of the LLVM IR builder that we
  need for Numba.


Key Benefits
============

* The IR builder is pure Python code and decoupled from LLVM's
  frequently-changing C++ APIs.
* Materializing a LLVM module calls LLVM's IR parser which provides
  better error messages than step-by-step IR building through the C++
  API (no more segfaults or process aborts).
* Most of llvmlite uses the LLVM C API which is small but very stable
  (low maintenance when changing LLVM version).
* The binding is not a Python C-extension, but a plain DLL accessed using
  ctypes (no need to wrestle with Python's compiler requirements and C++ 11
  compatibility).
* The Python binding layer has sane memory management.
* llvmlite is quite faster than llvmpy's thanks to a much simpler architeture
  (the Numba_ test suite is twice faster than it was).

llvmpy Compatibility Layer
--------------------------

The ``llvmlite.llvmpy`` namespace provides a minimal llvmpy compatibility
layer.


Compatibility
=============

llvmlite works with Python 2.7 and Python 3.4 or greater.

As of version 0.29.0, llvmlite requires LLVM 7.0.x or later

Historical compatibility table:

=================  ========================
llvmlite versions  compatible LLVM versions
=================  ========================
0.29.0 - ...       7.0.x, 7.1.x, 8.0.x
0.27.0 - 0.28.0    7.0.x
0.23.0 - 0.26.0    6.0.x
0.21.0 - 0.22.0    5.0.x
0.17.0 - 0.20.0    4.0.x
0.16.0 - 0.17.0    3.9.x
0.13.0 - 0.15.0    3.8.x
0.9.0 - 0.12.1     3.7.x
0.6.0 - 0.8.0      3.6.x
0.1.0 - 0.5.1      3.5.x
=================  ========================

Documentation
=============

You'll find the documentation at http://llvmlite.pydata.org


Pre-built binaries
==================

We recommend you use the binaries provided by the Numba_ team for
the Conda_ package manager.  You can find them in Numba's `anaconda.org
channel <https://anaconda.org/numba>`_.  For example::

   $ conda install --channel=numba llvmlite

(or, simply, the official llvmlite package provided in the Anaconda_
distribution)

.. _Numba: http://numba.pydata.org/
.. _Conda: http://conda.pydata.org/
.. _Anaconda: http://docs.continuum.io/anaconda/index.html


Other build methods
===================

If you don't want to use our pre-built packages, you can compile
and install llvmlite yourself.  The documentation will teach you how:
http://llvmlite.pydata.org/en/latest/install/index.html



  * [locket-0.2.0](http://github.com/mwilliamson/locket.py) locket.py
=========

Locket implements a lock that can be used by multiple processes provided they use the same path.

.. code-block:: python

    import locket

    # Wait for lock
    with locket.lock_file("path/to/lock/file"):
        perform_action()

    # Raise error if lock cannot be acquired immediately
    with locket.lock_file("path/to/lock/file", timeout=0):
        perform_action()
        
    # Raise error if lock cannot be acquired after thirty seconds
    with locket.lock_file("path/to/lock/file", timeout=30):
        perform_action()
        
    # Without context managers:
    lock = locket.lock_file("path/to/lock/file")
    try:
        lock.acquire()
        perform_action()
    finally:
        lock.release()

Locks largely behave as (non-reentrant) `Lock` instances from the `threading`
module in the standard library. Specifically, their behaviour is:

* Locks are uniquely identified by the file being locked,
  both in the same process and across different processes.

* Locks are either in a locked or unlocked state.

* When the lock is unlocked, calling `acquire()` returns immediately and changes
  the lock state to locked.

* When the lock is locked, calling `acquire()` will block until the lock state
  changes to unlocked, or until the timeout expires.

* If a process holds a lock, any thread in that process can call `release()` to
  change the state to unlocked.

* Behaviour of locks after `fork` is undefined.
  * [lockfile-0.12.2](http://launchpad.net/pylockfile) Note: This package is **deprecated**. It is highly preferred that instead of
using this code base that instead `fasteners`_ or `oslo.concurrency`_ is
used instead. For any questions or comments or further help needed
please email `openstack-dev`_ and prefix your email subject
with ``[oslo][pylockfile]`` (for a faster response).

.. _fasteners: https://pypi.python.org/pypi/fasteners
.. _oslo.concurrency: http://docs.openstack.org/developer/oslo.concurrency/
.. _openstack-dev: http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev

The lockfile package exports a LockFile class which provides a simple API for
locking files.  Unlike the Windows msvcrt.locking function, the fcntl.lockf
and flock functions, and the deprecated posixfile module, the API is
identical across both Unix (including Linux and Mac) and Windows platforms.
The lock mechanism relies on the atomic nature of the link (on Unix) and
mkdir (on Windows) system calls.  An implementation based on SQLite is also
provided, more as a demonstration of the possibilities it provides than as
production-quality code.

Note: In version 0.9 the API changed in two significant ways:

 * It changed from a module defining several classes to a package containing
   several modules, each defining a single class.

 * Where classes had been named SomethingFileLock before the last two words
   have been reversed, so that class is now SomethingLockFile.

The previous module-level definitions of LinkFileLock, MkdirFileLock and
SQLiteFileLock will be retained until the 1.0 release.

To install:

    python setup.py install

* Documentation: http://docs.openstack.org/developer/pylockfile
* Source: http://git.openstack.org/cgit/openstack/pylockfile
* Bugs: http://bugs.launchpad.net/pylockfile
  * [logilab-common-1.4.3](http://www.logilab.org/project/logilab-common) Logilab's common library
========================

What's this ?
-------------

This package contains some modules used by different Logilab projects.

It is released under the GNU Lesser General Public License.

There is no documentation available yet but the source code should be clean and
well documented.

Designed to ease:

* handling command line options and configuration files
* writing interactive command line tools
* manipulation of files and character strings
* manipulation of common structures such as graph, tree, and pattern such as visitor
* generating text and HTML reports
* more...


Installation
------------

logilab-common is available on pypi so you can install it using pip ::

    pip install logilab-common

Or alternatively extract the tarball, jump into the created directory and run ::

    python setup.py install

For installation options, see ::

    python setup.py install --help


Building the documentation
--------------------------

Create a virtualenv and install dependencies ::

    virtualenv venv
    source venv/bin/activate

    # you need the krb5-config command to build all dependencies
    # on debian you can get it using "apt-get install libkrb5-dev"

    pip install doc/requirements-doc.txt

    # install logilab-common
    pip install -e .

Then build the doc ::

    cd doc
    make html

It's now available under `doc/_build/html/`

Comments, support, bug reports
------------------------------

Project page https://www.logilab.org/project/logilab-common

Use the cubicweb-devel at lists.cubicweb.org mailing list.

You can subscribe to this mailing list at
https://lists.cubicweb.org/mailman/listinfo/cubicweb-devel

Archives are available at
https://lists.cubicweb.org/pipermail/cubicweb-devel/
  * [loompy-2.0.17](https://github.com/linnarsson-lab/loompy) 
  * [louvain-0.6.1](https://github.com/vtraag/louvain-igraph) Louvain is a general algorithm for methods of community detection in large networks.

Please refer to the `documentation <http://louvain-igraph.readthedocs.io/en/latest>`_
for more details.

The source code of this package is hosted at `GitHub <https://github.com/vtraag/louvain-igraph>`_.
Issues and bug reports are welcome at https://github.com/vtraag/louvain-igraph/issues.



  * [lxml-4.4.0](http://lxml.de/) lxml is a Pythonic, mature binding for the libxml2 and libxslt libraries.  It
provides safe and convenient access to these libraries using the ElementTree
API.

It extends the ElementTree API significantly to offer support for XPath,
RelaxNG, XML Schema, XSLT, C14N and much more.

To contact the project, go to the `project home page
<http://lxml.de/>`_ or see our bug tracker at
https://launchpad.net/lxml

In case you want to use the current in-development version of lxml,
you can get it from the github repository at
https://github.com/lxml/lxml .  Note that this requires Cython to
build the sources, see the build instructions on the project home
page.  To the same end, running ``easy_install lxml==dev`` will
install lxml from
https://github.com/lxml/lxml/tarball/master#egg=lxml-dev if you have
an appropriate version of Cython installed.


After an official release of a new stable series, bug fixes may become
available at
https://github.com/lxml/lxml/tree/lxml-4.4 .
Running ``easy_install lxml==4.4bugfix`` will install
the unreleased branch state from
https://github.com/lxml/lxml/tarball/lxml-4.4#egg=lxml-4.4bugfix
as soon as a maintenance branch has been established.  Note that this
requires Cython to be installed at an appropriate version for the build.

4.4.1 (2019-08-11)
==================

Bugs fixed
----------

* LP#1838252: The order of an OrderedDict was lost in 4.4.0 when passing it as
  attrib mapping during element creation.

* LP#1838521: The package metadata now lists the supported Python versions.





  * [macholib-1.11](http://bitbucket.org/ronaldoussoren/macholib) macholib can be used to analyze and edit Mach-O headers, the executable
format used by Mac OS X.

It's typically used as a dependency analysis tool, and also to rewrite dylib
references in Mach-O headers to be @executable_path relative.

Though this tool targets a platform specific file format, it is pure python
code that is platform and endian independent.

Project links
-------------

* `Documentation <https://macholib.readthedocs.io/en/latest/>`_

* `Issue Tracker <https://bitbucket.org/ronaldoussoren/macholib/issues?status=new&status=open>`_

* `Repository <https://bitbucket.org/ronaldoussoren/macholib/>`_


Release history
===============

macholib 1.11
-------------

* Add very hacky limited support for @loader_path. This is just
  enough to deal with extensions and dylibs found in Python
  binary wheels.

macholib 1.10
-------------

* #25: Add support for LC_NOTE and LC_BUILD_VERSION

macholib 1.9
------------

Features:

* Add definition for ``macholib.mach_o.reloc_type_generic``, which
  was used in code but never defined.

* #22: Add LICENSE file

* #23: Added "--help" option for "python -m macholib"

* Added function ``macholib.MachO.lc_str_value`` which should
  help in decoding value of ``macholib.mach_o.lc_str``. Those
  values are offsets in the data of a load command, the function
  will return the actually value as a byte string.

  See also issue #21.

Bug fixes:

* Pull request #15: Fix typo in thread_command class

  Patch by user "phdphuc" on bitbucket.

macholib 1.8
------------

* Use the same dependency walk logic as otool

  Patch by Taras Tsugrii <ttsugrii@fb.com>

* Added support for new load commands

  Patch by David Dorsey <trogdorsey@gmail.com>,
  with enhancements by Ronald Oussoren.

* Fix procesing DSYM file from XCODE 6.x

  Patch by HolmsBlazhey <andrey.blazhey@gmail.com>

* MachOGraph.locate(): When calling dyld_find(), use kwarg 'loader_path', not 'loader'.

  Patch by Stuart Berg <bergs@janelia.hhmi.org>

* Add fields to thread_command

  Patch by Asger Hautop Drewsen <asgerdrewsen@gmail.com>

* Add missing ARM_V7S subtype.

  Patch by "NN"

* Fix for SymbolTable

  Patch by Christian Klein <chris@5711.org>

* Use first Mach-O header as the default header

  Patch by Christian Klein <chris@5711.org>

* Issue #17: add LC_LOAD_UPWARD_DYLIB to _RELOCATABLE set

* Issue #16: macholib "hangs" on invalid input

  Due to the use of the range function on untrusted input
  the python process could hang when reading invalid input, due
  to trying to construct an enormous list.

* Issue #18: Bad version parsing in macho_version_helper

  The order of subfields in ``mach_version_helper`` was reversed from
  reality.

* Issue #19: Fix aligment issue that prevented code signing

  Patch by Brendan Simon

* Fix issue #14: Can't pass endian argument to p_uint64.from_str


macholib 1.7
------------

* Added support for ARM64, LC_ENCRYPTION_INFO_64 and LC_LINKER_OPTION

  Patch by Matthias Ringwald.

* Load commands now have a "describe" method that returns more information
  about the command.

  Patch by David Dorsey.

* The MAGIC value in the header was always represented in the native
  byte order, instead of as the value read from the binary.

  Patch by David Dorsey.

* Added various new constants to "macholib.mach_o".

  Patch by David Dorsey.

macholib 1.6.1
--------------

* ?

macholib 1.6
------------

* Add support for '@loader_path' link command in
  macholib.dyld:

  - Added function ``macholib.dyld.dyld_loader_search``

  - This function is used by ``macholib.dyld.dyld_find``,
    and that function now has an new (optional) argument
    with the path to the loader.

* Also add support for '@loader_path' to macholib.MachoGraph,
  using the newly added '@loader_path' support in the
  dyld module.

  Due to this suppport the *macho_standalone* tool can
  now rewrite binaries that contain an '@loader_path' load
  command.


macholib 1.5.2
--------------

* Issue #93: Show the name of the affected file in the exception message
  for Mach-O headers that are too large to relocate.


macholib 1.5.1
--------------

* There were no 'classifiers' in the package metadata due to
  a bug in setup.py.

macholib 1.5
--------------

macholib 1.5 is a minor feature release

* No longer use 2to3 to provide Python 3 support

  As a side-effect of this macholib no longer supports
  Python 2.5 and earlier.

* Adds suppport for some new macho load commands

* Fix for py3k problem in macho_standalone.py

  Patch by Guanqun Lu.

* Fix for some issues in macho_dump.py

  Patch by Nam Nguyen

* Issue #10: Fix for LC_DATA_IN_CODE linker commands, without
  this fix py2app cannot build application bundles when
  the source binaries have been compiled with Xcode 4.5.

* Issue #6: Fix for LC_ENCRYPTION_INFO linker commands

* Use the mach header information to print the cpu type of a
  binary, instead of trying to deduce that from pointer width
  and endianness.

  Changed the code because of issue #6, in which a user tries to
  dump a iOS binary which results in bogus output in the previous
  releases.

* The mapping ``macholib.macho_dump.ARCH_MAP`` is undocumented
  and no longer used by macholib itself. It will be removed
  in the next release.


* The command-line tools ``macho_find``, ``macho_dump`` and
  ``macho_standalone`` are deprecated. Use "python -mmacholib"
  instead. That is::

   $ python -mmacholib dump /usr/bin/grep

   $ python -mmacholib find ~

   $ python -mmacholib standalone myapp.app

  This makes it clearer which version of the tools are used.

macholib 1.4.3
--------------

macholib 1.4.3 is a minor feature release

* Added strings for 'x86_64' and 'ppc64' to
  macholib.mach_o.CPU_TYPE_NAMES.

* macho_find and macho_dump were broken in the 1.4.2 release

* added 'macholib.util.NOT_SYSTEM_FILES', a list of
  files that aren't system path's even though they are
  located in system locations.

  Needed to work around a bug in PySide (see issue #32 in the
  py2app tracker)



macholib 1.4.2
--------------

macholib 1.4.2 is a minor bugfix release

* The support for new load commands that was added in 1.4.1
  contained a typo that caused problems on OSX 10.7 (Lion).

macholib 1.4.1
--------------

macholib 1.4.1 is a minor feature release

Features:

- Add support for a number of new MachO load commands that were added
  during the lifetime of OSX 10.6: ``LC_LOAD_UPWARD_DYLIB``,
  ``LC_VERSION_MIN_MACOSX``, ``LC_VERSION_MIN_IPHONEOS`` and
  ``LC_FUNCTION_STARTS``.

macholib 1.4
-------------

macholib 1.4 is a feature release

Features:

- Documentation is now generated using `sphinx <http://pypi.python.org/pypi/sphinx>`_
  and can be viewed at <http://packages.python.org/macholib>.

- The repository has moved to bitbucket

- There now is a testsuite

- Private functionality inside modules was renamed to
  a name starting with an underscore.

  .. note:: if this change affects your code you are relying on undefined
     implementation features, please stop using private functions.

- The basic packable types in ``macholib.ptypes`` were renamed to better
  represent the corresponding C type. The table below lists the old
  an new names (the old names are still available, but are deprecated and
  will be removed in a future release).

  +--------------+--------------+
  | **Old name** | **New name** |
  +==============+==============+
  | p_byte       | p_int8       |
  +--------------+--------------+
  | p_ubyte      | p_uint8      |
  +--------------+--------------+
  | p_short      | p_int16      |
  +--------------+--------------+
  | p_ushort     | p_uint16     |
  +--------------+--------------+
  | p_int        | p_int32      |
  +--------------+--------------+
  | p_uint       | p_uint32     |
  +--------------+--------------+
  | p_long       | p_int32      |
  +--------------+--------------+
  | p_ulong      | p_uint32     |
  +--------------+--------------+
  | p_longlong   | p_int64      |
  +--------------+--------------+
  | p_ulonglong  | p_uint64     |
  +--------------+--------------+

  ``Macholib.ptypes.p_ptr`` is no longer present as it had an unclear
  definition and isn't actually used in the codebase.


Bug fixes:

- The semantics of ``dyld.dyld_default_search`` were changed a bit,
  it now first searches the framework path (if appropriate) and then
  the linker path, irrespective of the value of the ``DYLD_FALLBACK*``
  environment variables.

  Previous versions would change the search order when those variables
  was set, which is odd and doesn't correspond with the documented
  behaviour of the system dyld.

- It is once again possible to install using python2.5

- The source distribution includes all files, this was broken
  due to the switch to mercurial (which confused setuptools)

macholib 1.3
------------

macholib 1.3 is a feature release.

Features:

- Experimental Python 3.x support

  This version contains lightly tested support for Python 3.

macholib 1.2.2
--------------

macholib 1.2.2 is a bugfix release.

Bug fixes:

- Macholib should work better with 64-bit code
  (patch by Marc-Antoine Parent)



  * [manhole-1.6.0](https://github.com/ionelmc/python-manhole) ========
Overview
========




Features
========

* Uses unix domain sockets, only root or same effective user can connect.
* Can run the connection in a thread or in a signal handler (see ``oneshot_on`` option).
* Can start the thread listening for connections from a signal handler (see ``activate_on`` option)
* Compatible with apps that fork, reinstalls the Manhole thread after fork - had to monkeypatch os.fork/os.forkpty for
  this.
* Compatible with gevent and eventlet with some limitations - you need to either:

  * Use ``oneshot_on``, *or*
  * Disable thread monkeypatching (eg: ``gevent.monkey.patch_all(thread=False)``, ``eventlet.monkey_patch(thread=False)``

  Note: on eventlet `you might <https://github.com/eventlet/eventlet/issues/401>`_ need to setup the hub first to prevent
  circular import problems:

  .. sourcecode:: python

    import eventlet
    eventlet.hubs.get_hub()  # do this first
    eventlet.monkey_patch(thread=False)

* The thread is compatible with apps that use signalfd (will mask all signals for the Manhole threads).

Options
-------

.. code-block:: python

    manhole.install(
        verbose=True,
        verbose_destination=2,
        patch_fork=True,
        activate_on=None,
        oneshot_on=None,
        sigmask=manhole.ALL_SIGNALS,
        socket_path=None,
        reinstall_delay=0.5,
        locals=None,
        strict=True,
    )

* ``verbose`` - Set it to ``False`` to squelch the logging.
* ``verbose_destination`` - Destination for verbose messages. Set it to a file descriptor or handle. Default is
  unbuffered stderr (stderr ``2`` file descriptor).
* ``patch_fork`` - Set it to ``False`` if you don't want your ``os.fork`` and ``os.forkpy`` monkeypatched
* ``activate_on`` - Set to ``"USR1"``, ``"USR2"`` or some other signal name, or a number if you want the Manhole thread
  to start when this signal is sent. This is desirable in case you don't want the thread active all the time.
* ``thread`` - Set to ``True`` to start the always-on ManholeThread. Default: ``True``.
  Automatically switched to ``False`` if ``oneshot_on`` or ``activate_on`` are used.
* ``oneshot_on`` - Set to ``"USR1"``, ``"USR2"`` or some other signal name, or a number if you want the Manhole to
  listen for connection in the signal handler. This is desireable in case you don't want threads at all.
* ``sigmask`` - Will set the signal mask to the given list (using ``signalfd.sigprocmask``). No action is done if
  ``signalfd`` is not importable. **NOTE**: This is done so that the Manhole thread doesn't *steal* any signals;
  Normally that is fine because Python will force all the signal handling to be run in the main thread but signalfd
  doesn't.
* ``socket_path`` - Use a specific path for the unix domain socket (instead of ``/tmp/manhole-<pid>``). This disables
  ``patch_fork`` as children cannot reuse the same path.
* ``reinstall_delay`` - Delay the unix domain socket creation *reinstall_delay* seconds. This alleviates
  cleanup failures when using fork+exec patterns.
* ``locals`` - Names to add to manhole interactive shell locals.
* ``daemon_connection`` - The connection thread is daemonic (dies on app exit). Default: ``False``.
* ``redirect_stderr`` - Redirect output from stderr to manhole console. Default: ``True``.
* ``strict`` - If ``True`` then ``AlreadyInstalled`` will be raised when attempting to install manhole twice.
  Default: ``True``.

Environment variable installation
---------------------------------

Manhole can be installed via the ``PYTHONMANHOLE`` environment varialbe.

This::

    PYTHONMANHOLE='' python yourapp.py

Is equivalent to having this in ``yourapp.py``::

    import manhole
    manhole.install()

Any extra text in the environment variable is passed to ``manhole.install()``. Example::

    PYTHONMANHOLE='onshot_on="USR2"' python yourapp.py

What happens when you actually connect to the socket
----------------------------------------------------

1. Credentials are checked (if it's same user or root)
2. ``sys.__std*__``/``sys.std*`` are redirected to the UDS
3. Stacktraces for each thread are written to the UDS
4. REPL is started so you can fiddle with the process

Known issues
============

* Using threads and file handle (not raw file descriptor) ``verbose_destination`` can cause deadlocks. See bug reports:
  `PyPy <https://bitbucket.org/pypy/pypy/issue/1895/writing-to-stderr-from-multiple-processes>`_ and `Python 3.4
  <http://bugs.python.org/issue22697>`_.

SIGTERM and socket cleanup
--------------------------

By default Python doesn't call the ``atexit`` callbacks with the default SIGTERM handling. This makes manhole leave
stray socket files around. If this is undesirable you should install a custom SIGTERM handler so ``atexit`` is
properly invoked.

Example:

.. code-block:: python

    import signal
    import sys

    def handle_sigterm(signo, frame):
        sys.exit(128 + signo)  # this will raise SystemExit and cause atexit to be called

    signal.signal(signal.SIGTERM, handle_sigterm)

Using Manhole with uWSGI
------------------------

Because uWSGI overrides signal handling Manhole is a bit more tricky to setup. One way is to use "uWSGI signals" (not
the POSIX signals) and have the workers check a file for the pid you want to open the Manhole in.

Stick something this in your WSGI application file:

.. sourcecode:: python

    from __future__ import print_function
    import sys
    import os
    import manhole

    stack_dump_file = '/tmp/manhole-pid'
    uwsgi_signal_number = 17

    try:
        import uwsgi

        if not os.path.exists(stack_dump_file):
            open(stack_dump_file, 'w')

        def open_manhole(dummy_signum):
            with open(stack_dump_file, 'r') as fh:
                pid = fh.read().strip()
                if pid == str(os.getpid()):
                    inst = manhole.install(strict=False, thread=False)
                    inst.handle_oneshot(dummy_signum, dummy_signum)

        uwsgi.register_signal(uwsgi_signal_number, 'workers', open_manhole)
        uwsgi.add_file_monitor(uwsgi_signal_number, stack_dump_file)

        print("Listening for stack mahole requests via %r" % (stack_dump_file,), file=sys.stderr)
    except ImportError:
        print("Not running under uwsgi; unable to configure manhole trigger", file=sys.stderr)
    except IOError:
        print("IOError creating manhole trigger %r" % (stack_dump_file,), file=sys.stderr)


    # somewhere bellow you'd have something like
    from django.core.wsgi import get_wsgi_application
    application = get_wsgi_application()
    # or
    def application(environ, start_response):
        start_response('200 OK', [('Content-Type', 'text/plain'), ('Content-Length', '2')])
        yield b'OK'

To open the Manhole just run `echo 1234 > /tmp/manhole-pid` and then `manhole-cli 1234`.

Requirements
============

:OS: Linux, OS X
:Runtime: Python 2.7, 3.4, 3.5, 3.6 or PyPy

Similar projects
================

* Twisted's `manhole <http://twistedmatrix.com/documents/current/api/twisted.conch.manhole.html>`__ - it has colors and
  server-side history.
* `wsgi-shell <https://github.com/GrahamDumpleton/wsgi-shell>`_ - spawns a thread.
* `pyrasite <https://github.com/lmacken/pyrasite>`_ - uses gdb to inject code.
* `pydbattach <https://github.com/albertz/pydbattach>`_ - uses gdb to inject code.
* `pystuck <https://github.com/alonho/pystuck>`_ - very similar, uses `rpyc <https://github.com/tomerfiliba/rpyc>`_ for
  communication.
* `pyringe <https://github.com/google/pyringe>`_ - uses gdb to inject code, more reliable, but relies on `dbg` python
  builds unfortunatelly.
* `pdb-clone <https://pypi.python.org/pypi/pdb-clone>`_ - uses gdb to inject code, with a `different strategy
  <https://code.google.com/p/pdb-clone/wiki/RemoteDebugging>`_.


Changelog
=========

1.6.0 (2019-01-19)
------------------

* Testing improvements (changed some skips to xfail, added osx in Travis).
* Fixed long standing Python 2.7 bug where ``sys.getfilesystemencoding()`` would be broken after installing a threaded
  manhole. See `#51 <https://github.com/ionelmc/python-manhole/issues/51>`_.
* Dropped support for Python 2.6, 3.3 and 3.4.
* Fixed handling when ``socket.setdefaulttimeout()`` is used.
  Contributed by "honnix" in `#53 <https://github.com/ionelmc/python-manhole/pull/53>`_.
* Fixed some typos. Contributed by Jesús Cea in `#43 <https://github.com/ionelmc/python-manhole/pull/43>`_.
* Fixed handling in ``manhole-cli`` so that timeout is actually seconds and not milliseconds.
  Contributed by Nir Soffer in `#45 <https://github.com/ionelmc/python-manhole/pull/45>`_.
* Cleaned up useless polling options in ``manhole-cli``.
  Contributed by Nir Soffer in `#46 <https://github.com/ionelmc/python-manhole/pull/46>`_.
* Documented and implemented a solution for using Manhole with Eventlet.
  See `#49 <https://github.com/ionelmc/python-manhole/issues/49>`_.

1.5.0 (2017-08-31)
------------------

* Added two string aliases for ``connection_handler`` option. Now you can conveniently use ``connection_handler="exec"``.
* Improved ``handle_connection_exec``. It now has a clean way to exit (``exit()``) and properly closes the socket.

1.4.0 (2017-08-29)
------------------

* Added the ``connection_handler`` install option. Default value is ``manhole.handle_connection_repl``, and alternate
  ``manhole.handle_connection_exec`` is provided (very simple: no output redirection, no stacktrace dumping).
* Dropped Python 3.2 from the test grid. It may work but it's a huge pain to support (pip/pytest don't support it anymore).
* Added Python 3.5 and 3.6 in the test grid.
* Fixed issues with piping to ``manhole-cli``. Now ``echo foobar | manhole-cli`` will wait 1 second for output from manhole
  (you can customize this with the ``--timeout`` option).
* Fixed issues with newer PyPy (caused by gevent/eventlet socket unwrapping).

1.3.0 (2015-09-03)
------------------

* Allowed Manhole to be configured without any thread or activation (in case you want to manually activate).
* Added an example and tests for using Manhole with uWSGi.
* Fixed error handling in ``manhole-cli`` on Python 3 (exc vars don't leak anymore).
* Fixed support for running in gevent/eventlet-using apps on Python 3 (now that they support Python 3).
* Allowed reinstalling the manhole (in non-``strict`` mode). Previous install is undone.

1.2.0 (2015-07-06)
------------------

* Changed ``manhole-cli``:

  * Won't spam the terminal with errors if socket file doesn't exist.
  * Allowed sending any signal (new ``--signal`` argument).
  * Fixed some validation issues for the ``PID`` argument.

1.1.0 (2015-06-06)
------------------

* Added support for installing the manhole via the ``PYTHONMANHOLE`` environment variable.
* Added a ``strict`` install option. Set it to false to avoid getting the ``AlreadyInstalled`` exception.
* Added a ``manhole-cli`` script that emulates ``socat readline unix-connect:/tmp/manhole-1234``.

1.0.0 (2014-10-13)
------------------

* Added ``socket_path`` install option (contributed by `Nir Soffer`_).
* Added ``reinstall_delay`` install option.
* Added ``locals`` install option (contributed by `Nir Soffer`_).
* Added ``redirect_stderr`` install option (contributed by `Nir Soffer`_).
* Lots of internals cleanup (contributed by `Nir Soffer`_).

0.6.2 (2014-04-28)
------------------

* Fix OS X regression.

0.6.1 (2014-04-28)
------------------

* Support for OS X (contributed by `Saulius Menkevičius`_).

.. _Saulius Menkevičius: https://github.com/razzmatazz
.. _Nir Soffer: https://github.com/nirs



  * [manuel-1.10.1](http://pypi.python.org/pypi/manuel) .. image:: https://travis-ci.org/benji-york/manuel.png?branch=master
   :target: https://travis-ci.org/benji-york/manuel

.. image:: https://coveralls.io/repos/github/benji-york/manuel/badge.svg?branch=master
   :target: https://coveralls.io/github/benji-york/manuel?branch=master

.. image:: https://img.shields.io/pypi/v/manuel.svg
    :target: https://pypi.python.org/pypi/manuel

.. image:: https://img.shields.io/pypi/pyversions/manuel.svg
    :target: https://pypi.python.org/pypi/manuel/

Documentation, a full list of included plug-ins, and examples are available at
`<http://packages.python.org/manuel/>`_.

Source code and issues are managed at https://github.com/benji-york/manuel.


CHANGES
=======

1.10.1 (2018-11-15)
-------------------

- Add support for PyPy3.

1.10.0 (2018-11-14)
-------------------

- Fix DeprecationWarning about 'U' mode under Python 3.
- Drop Python 2.7 and 3.3 support. Add testing and support for Python 3.6 and
  3.7.

1.9.0 (2017-11-20)
------------------

- You can now use Manuel with the `nose
  <http://nose.readthedocs.io/en/latest/>`_ and `pytest
  <https://docs.pytest.org/en/latest/>`_ test runners by defining
  Manuel tests inside `unittest.TestCase` classes.
- Added support for Python 3.5 and Python 3.6.
- Dropped support for Python 2.6

1.8.0 (2014-07-15)
------------------

- Fixed ResourceWarnings under Python 3.
- Added support for PyPy and Python 3.4.
- Drop official support for Python 3.1 and 3.2.
- Fix odd ImportError problems when used with tox and coverage.
- Fix parsing of reST codeblock options with hyphens.

1.7.2 (2013-03-16)
------------------

- Fixed release issues.
- Updated copyright and license to reflect recent Zope Foundation release of
  claim on the project.


1.7.1 (2013-02-13)
------------------

- Fix brown-bag release.


1.7.0 (2013-02-13)
------------------

- Added support for docutils-style code blocks and options there-of.


1.6.1 (2013-01-24)
------------------

- Fixed a bug that made doctests fail if sys.argv contained the string "-v".


1.6.0 (2012-04-16)
------------------

- Ported to Python 3, still works in 2.6 and up.


1.5.0 (2011-03-08)
------------------

- Removed the dependency on zope.testrunner
- Added the ability to run the tests using "setup.py test".


1.4.1 (2011-01-25)
------------------

- Fixed a bug that caused extra example evaluation if multiple doctest
  manuels were used at once (e.g. to execute Python and shell code in
  the same document).


1.4.0 (2011-01-11)
------------------

- Added a ``parser`` keyword argument to manuel.doctest.Manuel to
  allow a custom doctest parser to be passed in.  This allows easily
  adding support for other languages or other (but similar) example
  syntaxes.


1.3.0 (2010-09-02)
------------------

- Respect test runner reporting switches (e.g., zope.testrunner's --ndiff
  switch)
- Fixed a bug that caused post-mortem debugging to not work most of the
  time.
- Made manuel.testing.TestCase.id return a sensible textual value
  at all times.  This keeps Twisted's trial testrunner happy.


1.2.0 (2010-06-10)
------------------

- Conform to repository policy.
- Switch to using zope.testrunner instead of zope.testing due to API changes.
  zope.testing is now only required for testing.


1.1.1 (2010-05-20)
------------------

- fix the way globs are handled; fixes
  https://bugs.launchpad.net/manuel/+bug/582482


1.1.0 (2010-05-18)
------------------

- fix a SyntaxError when running the tests under Python 2.5
- improved error message for improperly indented capture directive
- Manuel no longer uses the now depricated zope.testing.doctest (requires
  zope.testing 3.9.1 or newer)


1.0.5 (2010-01-29)
------------------

- fix a bug that caused Manuel to choke on empty documents (patch submitted by
  Bjorn Tillenius)
- add a pointer to Manuel's Subversion repo on the PyPI page
- add an optional parameter that allows a custom TestCase class to be passed to
  TestSuite() (patch submitted by Bjorn Tillenius)


1.0.4 (2010-01-06)
------------------

- use newer setuptools (one compatible with Subversion 1.6) so built
  distributions include all files


1.0.3 (2010-01-06)
------------------

- fix a small doc thinko
- fix the code-block handler to allow :linenos:
- open files in universal newlines mode


1.0.2 (2009-12-07)
------------------

- fix a bug that caused instances of zope.testing.doctest.Example (and
  instances of subclasses of the same) to be silently ignored.


1.0.1 (2009-08-31)
------------------

- fix line number reporting for test failures


1.0.0 (2009-08-09)
------------------

- Python 2.4 compatability fix


1.0.0b2 (2009-07-10)
--------------------

- add the ability to identify and run subsets of documents (using the -t switch
  of zope.testing's testrunner for example)


1.0.0b1 (2009-06-24)
--------------------

- major docs improvements
- added several new plug-ins


1.0.0a8 (2009-05-01)
--------------------

- add a larger example of using Manuel (table-example.txt)
- make the test suite factory function try harder to find the calling
  module
- fix a bug in the order regions are evaluated
- add a Manuel object that can evaluate Python code in
  ".. code-block:: python" regions of a reST document

1.0.0a4 (2009-05-01)
--------------------

- make the global state ("globs") shared between all evaluators, not just
  doctest


1.0.0a3 (2009-05-01)
--------------------

- make zope.testing's testrunner recognized the enhanced, doctest-style
  errors generated by Manuel
- rework the evaluaters to work region-by-region instead of on the
  entire document
- switch to using regular Python classes for Manuel objects instead of
  previous prototype-y style


1.0.0a2 (2008-10-17)
--------------------

- first release
  * [matplotlib-3.1.1](https://matplotlib.org) Matplotlib strives to produce publication quality 2D graphics
        for interactive graphing, scientific publishing, user interface
        development and web application servers targeting multiple user
        interfaces and hardcopy output formats.
  * [mccabe-0.6.1](https://github.com/pycqa/mccabe) McCabe complexity checker
=========================

Ned's script to check McCabe complexity.

This module provides a plugin for ``flake8``, the Python code checker.


Installation
------------

You can install, upgrade, uninstall ``mccabe`` with these commands::

  $ pip install mccabe
  $ pip install --upgrade mccabe
  $ pip uninstall mccabe


Standalone script
-----------------

The complexity checker can be used directly::

  $ python -m mccabe --min 5 mccabe.py
  ("185:1: 'PathGraphingAstVisitor.visitIf'", 5)
  ("71:1: 'PathGraph.to_dot'", 5)
  ("245:1: 'McCabeChecker.run'", 5)
  ("283:1: 'main'", 7)
  ("203:1: 'PathGraphingAstVisitor.visitTryExcept'", 5)
  ("257:1: 'get_code_complexity'", 5)


Plugin for Flake8
-----------------

When both ``flake8 2.0`` and ``mccabe`` are installed, the plugin is
available in ``flake8``::

  $ flake8 --version
  2.0 (pep8: 1.4.2, pyflakes: 0.6.1, mccabe: 0.2)

By default the plugin is disabled.  Use the ``--max-complexity`` switch to
enable it.  It will emit a warning if the McCabe complexity of a function is
higher that the value::

    $ flake8 --max-complexity 10 coolproject
    ...
    coolproject/mod.py:1204:1: C901 'CoolFactory.prepare' is too complex (14)

This feature is quite useful to detect over-complex code.  According to McCabe,
anything that goes beyond 10 is too complex.


Links
-----

* Feedback and ideas: http://mail.python.org/mailman/listinfo/code-quality

* Cyclomatic complexity: http://en.wikipedia.org/wiki/Cyclomatic_complexity.

* Ned Batchelder's script:
  http://nedbatchelder.com/blog/200803/python_code_complexity_microtool.html


Changes
-------

0.6.1 - 2017-01-26
``````````````````

* Fix signature for ``PathGraphingAstVisitor.default`` to match the signature
  for ``ASTVisitor``

0.6.0 - 2017-01-23
``````````````````

* Add support for Python 3.6

* Fix handling for missing statement types

0.5.3 - 2016-12-14
``````````````````

* Report actual column number of violation instead of the start of the line

0.5.2 - 2016-07-31
``````````````````

* When opening files ourselves, make sure we always name the file variable

0.5.1 - 2016-07-28
``````````````````

* Set default maximum complexity to -1 on the class itself

0.5.0 - 2016-05-30
``````````````````

* PyCon 2016 PDX release

* Add support for Flake8 3.0

0.4.0 - 2016-01-27
``````````````````

* Stop testing on Python 3.2

* Add support for async/await keywords on Python 3.5 from PEP 0492

0.3.1 - 2015-06-14
``````````````````

* Include ``test_mccabe.py`` in releases.

* Always coerce the ``max_complexity`` value from Flake8's entry-point to an
  integer.

0.3 - 2014-12-17
````````````````

* Computation was wrong: the mccabe complexity starts at 1, not 2.

* The ``max-complexity`` value is now inclusive.  E.g.: if the
  value is 10 and the reported complexity is 10, then it passes.

* Add tests.


0.2.1 - 2013-04-03
``````````````````

* Do not require ``setuptools`` in setup.py.  It works around an issue
  with ``pip`` and Python 3.


0.2 - 2013-02-22
````````````````

* Rename project to ``mccabe``.

* Provide ``flake8.extension`` setuptools entry point.

* Read ``max-complexity`` from the configuration file.

* Rename argument ``min_complexity`` to ``threshold``.


0.1 - 2013-02-11
````````````````
* First release



  * [memoized-property-1.0.3](https://github.com/estebistec/python-memoized-property) =================
memoized_property
=================

.. image:: https://badge.fury.io/py/memoized-property.png
    :target: http://badge.fury.io/py/memoized-property
    
.. image:: https://travis-ci.org/estebistec/python-memoized-property.png?branch=master
        :target: https://travis-ci.org/estebistec/python-memoized-property

.. image:: https://pypip.in/d/memoized-property/badge.png
        :target: https://crate.io/packages/memoized-property?version=latest


A simple python decorator for defining properties that only run their fget function once.

* Free software: BSD license

What?
-----

A Python property that only calls its ``fget`` function one time. How many times have you written
this code (or similar)?

::

    def class C(object):

        @property
        def name(self):
            if not hasattr(self, '_name'):
                self._name = some_expensive_load()
            return self._name

I've written it just enough times to be annoyed enough to capture this module. The result is this::

    from memoized_property import memoized_property

    def class C(object):

        @memoized_property
        def name(self):
            # Boilerplate guard conditional avoided, but this is still only called once
            return some_expensive_load()

Why?
----

I couldn't find a pre-existing version of this on PyPI. I found one other on GitHub,
https://github.com/ytyng/python-memoized-property, but it was not published to PyPI.




History
-------

1.0.3 (2016-09-28)
++++++++++++++++++

* Build universal wheels
* Support Python 3.4, 3.5

1.0.2 (2014-05-02)
++++++++++++++++++

* Remove dependency on six

1.0.1 (2014-01-01)
++++++++++++++++++

* Added python 3.2 compatability

1.0.0 (2013-12-26)
++++++++++++++++++

* First release on PyPI.
  * [messytables-0.15.2](https://okfn.org) 
Tabular data as published on the web is often not well formatted
and structured. Messytables tries to detect and fix errors in the
data. Typical examples include:

* Finding the header of a table when there are explanations and
  text fragments in the first few rows of the table.
* Guessing the type of columns in CSV data.

This library provides data structures and some heuristics to
fix these problems and read a wide number of different tabular
abominations.

See the full documentation at: https://messytables.readthedocs.io

  * [misopy-0.5.4](http://genes.mit.edu/burgelab/miso/) MISO (Mixture of Isoforms) is a probabilistic framework that quantitates the expression level of alternatively spliced genes from RNA-Seq data, and identifies differentially regulated isoforms or exons across samples. By modeling the generative process by which reads are produced from isoforms in RNA-Seq, the MISO model uses Bayesian inference to compute the probability that a read originated from a particular isoform.

MISO uses the inferred assignment of reads to isoforms to quantitate the abundances of the underlying set of alternative mRNA isoforms. Confidence intervals over estimates can be obtained, which quantify the reliability of the estimates. 

Please see http://genes.mit.edu/burgelab/miso/docs/ for MISO manual and documentation.

  * [mistune-0.8.4](https://github.com/lepture/mistune) Mistune
=======

The fastest markdown parser in pure Python with renderer features,
inspired by marked_.

.. image:: https://img.shields.io/badge/donate-lepture-green.svg
   :target: https://lepture.com/donate
   :alt: Donate lepture
.. image:: https://img.shields.io/pypi/wheel/mistune.svg?style=flat
   :target: https://pypi.python.org/pypi/mistune/
   :alt: Wheel Status
.. image:: https://anaconda.org/conda-forge/mistune/badges/version.svg
   :target: https://anaconda.org/conda-forge/mistune
   :alt: Conda Version
.. image:: https://img.shields.io/pypi/v/mistune.svg
   :target: https://pypi.python.org/pypi/mistune/
   :alt: Latest Version
.. image:: https://travis-ci.org/lepture/mistune.svg?branch=master
   :target: https://travis-ci.org/lepture/mistune
   :alt: Travis CI Status
.. image:: https://coveralls.io/repos/lepture/mistune/badge.svg?branch=master
   :target: https://coveralls.io/r/lepture/mistune
   :alt: Coverage Status
.. image:: https://ci.appveyor.com/api/projects/status/8ai8tfwp75oela17?svg=true
   :target: https://ci.appveyor.com/project/lepture/mistune
   :alt: App Veyor CI Status

.. _marked: https://github.com/chjj/marked


Features
--------

* **Pure Python**. Tested in Python 2.7, Python 3.5+ and PyPy.
* **Very Fast**. It is the fastest in all **pure Python** markdown parsers.
* **More Features**. Table, footnotes, autolink, fenced code etc.

View the `benchmark results <https://github.com/lepture/mistune/issues/1>`_.

Installation
------------

Installing mistune with pip::

    $ pip install mistune


Mistune can be faster, if you compile with cython::

    $ pip install cython mistune


Basic Usage
-----------

A simple API that render a markdown formatted text:

.. code:: python

    import mistune

    mistune.markdown('I am using **mistune markdown parser**')
    # output: <p>I am using <strong>mistune markdown parser</strong></p>

If you care about performance, it is better to re-use the Markdown instance:

.. code:: python

    import mistune

    markdown = mistune.Markdown()
    markdown('I am using **mistune markdown parser**')

Mistune has enabled all features by default. You don't have to configure
anything. But there are options for you to change the parser behaviors.


Options
-------

Here is a list of all options that will affect the rendering results,
configure them with ``mistune.Renderer``:

.. code:: python

    renderer = mistune.Renderer(escape=True, hard_wrap=True)
    # use this renderer instance
    markdown = mistune.Markdown(renderer=renderer)
    markdown(text)

* **escape**: if set to *False*, all raw html tags will not be escaped.
* **hard_wrap**: if set to *True*, it will has GFM line breaks feature.
  All new lines will be replaced with ``<br>`` tag
* **use_xhtml**: if set to *True*, all tags will be in xhtml, for example: ``<hr />``.
* **parse_block_html**: parse text only in block level html.
* **parse_inline_html**: parse text only in inline level html.

When using the default renderer, you can use one of the following shortcuts::

    mistune.markdown(text, escape=True, hard_wrap=True)

    markdown = mistune.Markdown(escape=True, hard_wrap=True)
    markdown(text)


Renderer
--------

Like misaka/sundown, you can influence the rendering by custom renderers.
All you need to do is subclassing a `Renderer` class.

Here is an example of code highlighting:

.. code:: python

    import mistune
    from pygments import highlight
    from pygments.lexers import get_lexer_by_name
    from pygments.formatters import html

    class HighlightRenderer(mistune.Renderer):
        def block_code(self, code, lang):
            if not lang:
                return '\n<pre><code>%s</code></pre>\n' % \
                    mistune.escape(code)
            lexer = get_lexer_by_name(lang, stripall=True)
            formatter = html.HtmlFormatter()
            return highlight(code, lexer, formatter)

    renderer = HighlightRenderer()
    markdown = mistune.Markdown(renderer=renderer)
    print(markdown('```python\nassert 1 == 1\n```'))

Find more renderers in `mistune-contrib`_.

Block Level
~~~~~~~~~~~

Here is a list of block level renderer API::

    block_code(code, language=None)
    block_quote(text)
    block_html(html)
    header(text, level, raw=None)
    hrule()
    list(body, ordered=True)
    list_item(text)
    paragraph(text)
    table(header, body)
    table_row(content)
    table_cell(content, **flags)

The *flags* tells you whether it is header with ``flags['header']``. And it
also tells you the align with ``flags['align']``.


Span Level
~~~~~~~~~~

Here is a list of span level renderer API::

    autolink(link, is_email=False)
    codespan(text)
    double_emphasis(text)
    emphasis(text)
    image(src, title, alt_text)
    linebreak()
    newline()
    link(link, title, content)
    strikethrough(text)
    text(text)
    inline_html(text)

Footnotes
~~~~~~~~~

Here is a list of renderers related to footnotes::

    footnote_ref(key, index)
    footnote_item(key, text)
    footnotes(text)

Lexers
------

Sometimes you want to add your own rules to Markdown, such as GitHub Wiki
links. You can't achieve this goal with renderers. You will need to deal
with the lexers, it would be a little difficult for the first time.

We will take an example for GitHub Wiki links: ``[[Page 2|Page 2]]``.
It is an inline grammar, which requires custom ``InlineGrammar`` and
``InlineLexer``:

.. code:: python

    import copy,re
    from mistune import Renderer, InlineGrammar, InlineLexer

    class WikiLinkRenderer(Renderer):
        def wiki_link(self, alt, link):
            return '<a href="%s">%s</a>' % (link, alt)

    class WikiLinkInlineLexer(InlineLexer):
        def enable_wiki_link(self):
            # add wiki_link rules
            self.rules.wiki_link = re.compile(
                r'\[\['                   # [[
                r'([\s\S]+?\|[\s\S]+?)'   # Page 2|Page 2
                r'\]\](?!\])'             # ]]
            )

            # Add wiki_link parser to default rules
            # you can insert it some place you like
            # but place matters, maybe 3 is not good
            self.default_rules.insert(3, 'wiki_link')

        def output_wiki_link(self, m):
            text = m.group(1)
            alt, link = text.split('|')
            # you can create an custom render
            # you can also return the html if you like
            return self.renderer.wiki_link(alt, link)

You should pass the inline lexer to ``Markdown`` parser:

.. code:: python

    renderer = WikiLinkRenderer()
    inline = WikiLinkInlineLexer(renderer)
    # enable the feature
    inline.enable_wiki_link()
    markdown = Markdown(renderer, inline=inline)
    markdown('[[Link Text|Wiki Link]]')

It is the same with block level lexer. It would take a while to understand
the whole mechanism. But you won't do the trick a lot.


Contribution & Extensions
-------------------------

Mistune itself doesn't accept any extension. It will always be a simple one
file script.

If you want to add features, you can head over to `mistune-contrib`_.

Here are some extensions already in `mistune-contrib`_:

* Math/MathJax features
* Highlight Code Renderer
* TOC table of content features
* MultiMarkdown Metadata parser

Get inspired with the contrib repository.

.. _`mistune-contrib`: https://github.com/lepture/mistune-contrib



  * [mock-2.0.0](http://mock.readthedocs.org/en/latest/) mock is a library for testing in Python. It allows you to replace parts of
your system under test with mock objects and make assertions about how they
have been used.

mock is now part of the Python standard library, available as `unittest.mock
<https://docs.python.org/dev/library/unittest.mock.html>`_ in Python 3.3
onwards.

This package contains a rolling backport of the standard library mock code
compatible with Python 2.7 and 3.4 and up.

Please see the standard library documentation for more details.

:Homepage: `Mock Homepage`_
:Download: `Mock on PyPI`_
:Documentation: `Python Docs`_
:License: `BSD License`_
:Support: `Mailing list (testing-in-python@lists.idyll.org)
 <http://lists.idyll.org/listinfo/testing-in-python>`_
:Code: `GitHub
 <https://github.com/testing-cabal/mock>`_
:Issue tracker: `GitHub Issues
 <https://github.com/testing-cabal/mock/issues>`_
:Build status:
    |CircleCI|_ |Docs|_

    .. |CircleCI| image:: https://circleci.com/gh/testing-cabal/mock/tree/master.svg?style=shield
    .. _CircleCI: https://circleci.com/gh/testing-cabal/mock/tree/master

    .. |Docs| image:: https://readthedocs.org/projects/mock/badge/?version=latest
    .. _Docs: http://mock.readthedocs.org/en/latest/

.. _Mock Homepage: http://mock.readthedocs.org/en/latest/
.. _BSD License: https://github.com/testing-cabal/mock/blob/master/LICENSE.txt
.. _Python Docs: https://docs.python.org/dev/library/unittest.mock.html
.. _mock on PyPI: https://pypi.org/project/mock/



  * [monotonic-1.5](https://github.com/atdt/monotonic) 
monotonic
~~~~~~~~~

This module provides a ``monotonic()`` function which returns the
value (in fractional seconds) of a clock which never goes backwards.

On Python 3.3 or newer, ``monotonic`` will be an alias of
``time.monotonic`` from the standard library. On older versions,
it will fall back to an equivalent implementation:

+------------------+----------------------------------------+
| Linux, BSD, AIX  | ``clock_gettime(3)``                   |
+------------------+----------------------------------------+
| Windows          | ``GetTickCount`` or ``GetTickCount64`` |
+------------------+----------------------------------------+
| OS X             | ``mach_absolute_time``                 |
+------------------+----------------------------------------+

If no suitable implementation exists for the current platform,
attempting to import this module (or to import from it) will
cause a ``RuntimeError`` exception to be raised.




  * [more-itertools-7.2.0](https://github.com/erikrose/more-itertools) ==============
More Itertools
==============

.. image:: https://coveralls.io/repos/github/erikrose/more-itertools/badge.svg?branch=master
  :target: https://coveralls.io/github/erikrose/more-itertools?branch=master

Python's ``itertools`` library is a gem - you can compose elegant solutions
for a variety of problems with the functions it provides. In ``more-itertools``
we collect additional building blocks, recipes, and routines for working with
Python iterables.

----

+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Grouping               | `chunked <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.chunked>`_,                                                                                                                        |
|                        | `ichunked <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.ichunked>`_,                                                                                                                      |
|                        | `sliced <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.sliced>`_,                                                                                                                          |
|                        | `distribute <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.distribute>`_,                                                                                                                  |
|                        | `divide <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.divide>`_,                                                                                                                          |
|                        | `split_at <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.split_at>`_,                                                                                                                      |
|                        | `split_before <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.split_before>`_,                                                                                                              |
|                        | `split_after <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.split_after>`_,                                                                                                                |
|                        | `split_into <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.split_into>`_,                                                                                                                  |
|                        | `bucket <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.bucket>`_,                                                                                                                          |
|                        | `unzip <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.unzip>`_,                                                                                                                            |
|                        | `grouper <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.grouper>`_,                                                                                                                        |
|                        | `partition <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.partition>`_                                                                                                                     |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lookahead and lookback | `spy <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.spy>`_,                                                                                                                                |
|                        | `peekable <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.peekable>`_,                                                                                                                      |
|                        | `seekable <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.seekable>`_                                                                                                                       |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Windowing              | `windowed <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.windowed>`_,                                                                                                                      |
|                        | `substrings <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.substrings>`_,                                                                                                                  |
|                        | `substrings_indexes <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.substrings_indexes>`_,                                                                                                  |
|                        | `stagger <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.stagger>`_,                                                                                                                        |
|                        | `pairwise <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.pairwise>`_                                                                                                                       |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Augmenting             | `count_cycle <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.count_cycle>`_,                                                                                                                |
|                        | `intersperse <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.intersperse>`_,                                                                                                                |
|                        | `padded <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.padded>`_,                                                                                                                          |
|                        | `adjacent <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.adjacent>`_,                                                                                                                      |
|                        | `groupby_transform <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.groupby_transform>`_,                                                                                                    |
|                        | `padnone <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.padnone>`_,                                                                                                                        |
|                        | `ncycles <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.ncycles>`_                                                                                                                         |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Combining              | `collapse <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.collapse>`_,                                                                                                                      |
|                        | `sort_together <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.sort_together>`_,                                                                                                            |
|                        | `interleave <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.interleave>`_,                                                                                                                  |
|                        | `interleave_longest <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.interleave_longest>`_,                                                                                                  |
|                        | `collate <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.collate>`_,                                                                                                                        |
|                        | `zip_offset <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.zip_offset>`_,                                                                                                                  |
|                        | `dotproduct <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.dotproduct>`_,                                                                                                                  |
|                        | `flatten <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.flatten>`_,                                                                                                                        |
|                        | `roundrobin <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.roundrobin>`_,                                                                                                                  |
|                        | `prepend <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.prepend>`_                                                                                                                         |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Summarizing            | `ilen <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.ilen>`_,                                                                                                                              |
|                        | `first <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.first>`_,                                                                                                                            |
|                        | `last <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.last>`_,                                                                                                                              |
|                        | `one <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.one>`_,                                                                                                                                |
|                        | `only <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.only>`_,                                                                                                                              |
|                        | `unique_to_each <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.unique_to_each>`_,                                                                                                          |
|                        | `locate <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.locate>`_,                                                                                                                          |
|                        | `rlocate <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.rlocate>`_,                                                                                                                        |
|                        | `consecutive_groups <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.consecutive_groups>`_,                                                                                                  |
|                        | `exactly_n <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.exactly_n>`_,                                                                                                                    |
|                        | `run_length <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.run_length>`_,                                                                                                                  |
|                        | `map_reduce <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.map_reduce>`_,                                                                                                                  |
|                        | `all_equal <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.all_equal>`_,                                                                                                                    |
|                        | `first_true <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.first_true>`_,                                                                                                                  |
|                        | `nth <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.nth>`_,                                                                                                                                |
|                        | `quantify <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.quantify>`_                                                                                                                       |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Selecting              | `islice_extended <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.islice_extended>`_,                                                                                                        |
|                        | `strip <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.strip>`_,                                                                                                                            |
|                        | `lstrip <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.lstrip>`_,                                                                                                                          |
|                        | `rstrip <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.rstrip>`_,                                                                                                                          |
|                        | `take <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.take>`_,                                                                                                                              |
|                        | `tail <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.tail>`_,                                                                                                                              |
|                        | `unique_everseen <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertoo ls.unique_everseen>`_,                                                                                                       |
|                        | `unique_justseen <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.unique_justseen>`_                                                                                                         |
|                        | `filter_except <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.filter_except>`_                                                                                                             |
|                        | `map_except <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.map_except>`_                                                                                                                   |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Combinatorics          | `distinct_permutations <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.distinct_permutations>`_,                                                                                            |
|                        | `distinct_combinations <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.distinct_combinations>`_,                                                                                            |
|                        | `circular_shifts <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.circular_shifts>`_,                                                                                                        |
|                        | `partitions <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.partitions>`_,                                                                                                                  |
|                        | `set_partitions <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.set_partitions>`_,                                                                                                          |
|                        | `powerset <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.powerset>`_,                                                                                                                      |
|                        | `random_product <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.random_product>`_,                                                                                                          |
|                        | `random_permutation <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.random_permutation>`_,                                                                                                  |
|                        | `random_combination <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.random_combination>`_,                                                                                                  |
|                        | `random_combination_with_replacement <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.random_combination_with_replacement>`_,                                                                |
|                        | `nth_combination <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.nth_combination>`_                                                                                                         |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Wrapping               | `always_iterable <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.always_iterable>`_,                                                                                                        |
|                        | `consumer <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.consumer>`_,                                                                                                                      |
|                        | `with_iter <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.with_iter>`_,                                                                                                                    |
|                        | `iter_except <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.iter_except>`_                                                                                                                 |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Others                 | `replace <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.replace>`_,                                                                                                                        |
|                        | `numeric_range <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.numeric_range>`_,                                                                                                            |
|                        | `always_reversible <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.always_reversible>`_,                                                                                                    |
|                        | `side_effect <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.side_effect>`_,                                                                                                                |
|                        | `iterate <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.iterate>`_,                                                                                                                        |
|                        | `difference <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.difference>`_,                                                                                                                  |
|                        | `make_decorator <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.make_decorator>`_,                                                                                                          |
|                        | `SequenceView <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.SequenceView>`_,                                                                                                              |
|                        | `time_limited <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.time_limited>`_,                                                                                                              |
|                        | `consume <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.consume>`_,                                                                                                                        |
|                        | `tabulate <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.tabulate>`_,                                                                                                                      |
|                        | `repeatfunc <https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.repeatfunc>`_                                                                                                                   |
+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


Getting started
===============

To get started, install the library with `pip <https://pip.pypa.io/en/stable/>`_:

.. code-block:: shell

    pip install more-itertools

The recipes from the `itertools docs <https://docs.python.org/3/library/itertools.html#itertools-recipes>`_
are included in the top-level package:

.. code-block:: python

    >>> from more_itertools import flatten
    >>> iterable = [(0, 1), (2, 3)]
    >>> list(flatten(iterable))
    [0, 1, 2, 3]

Several new recipes are available as well:

.. code-block:: python

    >>> from more_itertools import chunked
    >>> iterable = [0, 1, 2, 3, 4, 5, 6, 7, 8]
    >>> list(chunked(iterable, 3))
    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]

    >>> from more_itertools import spy
    >>> iterable = (x * x for x in range(1, 6))
    >>> head, iterable = spy(iterable, n=3)
    >>> list(head)
    [1, 4, 9]
    >>> list(iterable)
    [1, 4, 9, 16, 25]



For the full listing of functions, see the `API documentation <https://more-itertools.readthedocs.io/en/latest/api.html>`_.

Development
===========

``more-itertools`` is maintained by `@erikrose <https://github.com/erikrose>`_
and `@bbayles <https://github.com/bbayles>`_, with help from `many others <https://github.com/erikrose/more-itertools/graphs/contributors>`_.
If you have a problem or suggestion, please file a bug or pull request in this
repository. Thanks for contributing!


Version History
===============


   :noindex:

7.2.0
-----

* New itertools
    * distinct_combinations
    * set_partitions (thanks to kbarrett)
    * filter_except
    * map_except

7.1.0
-----

* New itertools
    * ichunked (thanks davebelais and youtux)
    * only (thanks jaraco)

* Changes to existing itertools:
    * numeric_range now supports ranges specified by
      ``datetime.datetime`` and ``datetime.timedelta`` objects (thanks to MSeifert04 for tests).
    * difference now supports an *initial* keyword argument.


* Other changes
    * Various documentation fixes (thanks raimon49, pylang)

7.0.0
-----

* New itertools:
    * time_limited
    * partitions (thanks to rominf and Saluev)
    * substrings_indexes (thanks to rominf)

* Changes to existing itertools:
    * collapse now treats ``bytes`` objects the same as ``str`` objects. (thanks to Sweenpet)

The major version update is due to the change in the default behavior of
collapse. It now treats ``bytes`` objects the same as ``str`` objects.
This aligns its behavior with always_iterable.

.. code-block:: python

    >>> from more_itertools import collapse
    >>> iterable = [[1, 2], b'345', [6]]
    >>> print(list(collapse(iterable)))
    [1, 2, b'345', 6]

6.0.0
-----

* Major changes:
    * Python 2.7 is no longer supported. The 5.0.0 release will be the last
      version targeting Python 2.7.
    * All future releases will target the active versions of Python 3.
      As of 2019, those are Python 3.4 and above.
    * The ``six`` library is no longer a dependency.
    * The accumulate function is no longer part of this library. You
      may import a better version from the standard ``itertools`` module.

* Changes to existing itertools:
    * The order of the parameters in grouper have changed to match
      the latest recipe in the itertools documentation. Use of the old order
      will be supported in this release, but emit a  ``DeprecationWarning``.
      The legacy behavior will be dropped in a future release. (thanks to jaraco)
    * distinct_permutations was improved (thanks to jferard - see also `permutations with unique values <https://stackoverflow.com/questions/6284396/permutations-with-unique-values>`_ at StackOverflow.)
    * An unused parameter was removed from substrings. (thanks to pylang)

* Other changes:
    * The docs for unique_everseen were improved. (thanks to jferard and MSeifert04)
    * Several Python 2-isms were removed. (thanks to jaraco, MSeifert04, and hugovk)

5.0.0
-----

* New itertools:
    * split_into (thanks to rovyko)
    * unzip (thanks to bmintz)
    * substrings (thanks to pylang)

* Changes to existing itertools:
    * ilen was optimized a bit (thanks to MSeifert04, achampion, and  bmintz)
    * first_true now returns ``None`` by default. This is the reason for the major version bump - see below. (thanks to sk and OJFord)

* Other changes:
   * Some code for old Python versions was removed (thanks to hugovk)
   * Some documentation mistakes were corrected  (thanks to belm0 and hugovk)
   * Tests now run properly on 32-bit versions of Python (thanks to Millak)
   * Newer versions of CPython and PyPy are now tested against

The major version update is due to the change in the default return value of
first_true. It's now ``None``.

.. code-block:: python

    >>> from more_itertools import first_true
    >>> iterable = [0, '', False, [], ()]  # All these are False
    >>> answer = first_true(iterable)
    >>> print(answer)
    None

4.3.0
-----

* New itertools:
    * last (thanks to tmshn)
    * replace (thanks to pylang)
    * rlocate (thanks to jferard and pylang)

* Improvements to existing itertools:
    * locate can now search for multiple items

* Other changes:
   * The docs now include a nice table of tools (thanks MSeifert04)

4.2.0
-----

* New itertools:
    * map_reduce (thanks to pylang)
    * prepend (from the `Python 3.7 docs <https://docs.python.org/3.7/library/itertools.html#itertools-recipes>`_)

* Improvements to existing itertools:
    * bucket now complies with PEP 479 (thanks to irmen)

* Other changes:
   * Python 3.7 is now supported (thanks to irmen)
   * Python 3.3 is no longer supported
   * The test suite no longer requires third-party modules to run
   * The API docs now include links to source code

4.1.0
-----

* New itertools:
    * split_at (thanks to michael-celani)
    * circular_shifts (thanks to hiqua)
    * make_decorator - see the blog post `Yo, I heard you like decorators <https://sites.google.com/site/bbayles/index/decorator_factory>`_
      for a tour (thanks to pylang)
    * always_reversible (thanks to michael-celani)
    * nth_combination (from the `Python 3.7 docs <https://docs.python.org/3.7/library/itertools.html#itertools-recipes>`_)

* Improvements to existing itertools:
    * seekable now has an ``elements`` method to return cached items.
    * The performance tradeoffs between roundrobin and
      interleave_longest are now documented (thanks michael-celani,
      pylang, and MSeifert04)

4.0.1
-----

* No code changes - this release fixes how the docs display on PyPI.

4.0.0
-----

* New itertools:
    * consecutive_groups (Based on the example in the `Python 2.4 docs <https://docs.python.org/release/2.4.4/lib/itertools-example.html>`_)
    * seekable (If you're looking for how to "reset" an iterator,
      you're in luck!)
    * exactly_n (thanks to michael-celani)
    * run_length.encode and run_length.decode
    * difference

* Improvements to existing itertools:
    * The number of items between filler elements in intersperse can
      now be specified (thanks to pylang)
    * distinct_permutations and peekable got some minor
      adjustments (thanks to MSeifert04)
    * always_iterable now returns an iterator object. It also now
      allows different types to be considered iterable (thanks to jaraco)
    * bucket can now limit the keys it stores in memory
    * one now allows for custom exceptions (thanks to kalekundert)

* Other changes:
    * A few typos were fixed (thanks to EdwardBetts)
    * All tests can now be run with ``python setup.py test``

The major version update is due to the change in the return value of always_iterable.
It now always returns iterator objects:

.. code-block:: python

    >>> from more_itertools import always_iterable
    # Non-iterable objects are wrapped with iter(tuple(obj))
    >>> always_iterable(12345)
    <tuple_iterator object at 0x7fb24c9488d0>
    >>> list(always_iterable(12345))
    [12345]
    # Iterable objects are wrapped with iter()
    >>> always_iterable([1, 2, 3, 4, 5])
    <list_iterator object at 0x7fb24c948c50>

3.2.0
-----

* New itertools:
    * lstrip, rstrip, and strip
      (thanks to MSeifert04 and pylang)
    * islice_extended
* Improvements to existing itertools:
    * Some bugs with slicing peekable-wrapped iterables were fixed

3.1.0
-----

* New itertools:
    * numeric_range (Thanks to BebeSparkelSparkel and MSeifert04)
    * count_cycle (Thanks to BebeSparkelSparkel)
    * locate (Thanks to pylang and MSeifert04)
* Improvements to existing itertools:
    * A few itertools are now slightly faster due to some function
      optimizations. (Thanks to MSeifert04)
* The docs have been substantially revised with installation notes,
  categories for library functions, links, and more. (Thanks to pylang)


3.0.0
-----

* Removed itertools:
    * ``context`` has been removed due to a design flaw - see below for
      replacement options. (thanks to NeilGirdhar)
* Improvements to existing itertools:
    * ``side_effect`` now supports ``before`` and ``after`` keyword
      arguments. (Thanks to yardsale8)
* PyPy and PyPy3 are now supported.

The major version change is due to the removal of the ``context`` function.
Replace it with standard ``with`` statement context management:

.. code-block:: python

    # Don't use context() anymore
    file_obj = StringIO()
    consume(print(x, file=f) for f in context(file_obj) for x in u'123')

    # Use a with statement instead
    file_obj = StringIO()
    with file_obj as f:
        consume(print(x, file=f) for x in u'123')

2.6.0
-----

* New itertools:
    * ``adjacent`` and ``groupby_transform`` (Thanks to diazona)
    * ``always_iterable`` (Thanks to jaraco)
    * (Removed in 3.0.0) ``context`` (Thanks to yardsale8)
    * ``divide`` (Thanks to mozbhearsum)
* Improvements to existing itertools:
    * ``ilen`` is now slightly faster. (Thanks to wbolster)
    * ``peekable`` can now prepend items to an iterable. (Thanks to diazona)

2.5.0
-----

* New itertools:
    * ``distribute`` (Thanks to mozbhearsum and coady)
    * ``sort_together`` (Thanks to clintval)
    * ``stagger`` and ``zip_offset`` (Thanks to joshbode)
    * ``padded``
* Improvements to existing itertools:
    * ``peekable`` now handles negative indexes and slices with negative
      components properly.
    * ``intersperse`` is now slightly faster. (Thanks to pylang)
    * ``windowed`` now accepts a ``step`` keyword argument.
      (Thanks to pylang)
* Python 3.6 is now supported.

2.4.1
-----

* Move docs 100% to readthedocs.io.

2.4
-----

* New itertools:
    * ``accumulate``, ``all_equal``, ``first_true``, ``partition``, and
      ``tail`` from the itertools documentation.
    * ``bucket`` (Thanks to Rosuav and cvrebert)
    * ``collapse`` (Thanks to abarnet)
    * ``interleave`` and ``interleave_longest`` (Thanks to abarnet)
    * ``side_effect`` (Thanks to nvie)
    * ``sliced`` (Thanks to j4mie and coady)
    * ``split_before`` and ``split_after`` (Thanks to astronouth7303)
    * ``spy`` (Thanks to themiurgo and mathieulongtin)
* Improvements to existing itertools:
    * ``chunked`` is now simpler and more friendly to garbage collection.
      (Contributed by coady, with thanks to piskvorky)
    * ``collate`` now delegates to ``heapq.merge`` when possible.
      (Thanks to kmike and julianpistorius)
    * ``peekable``-wrapped iterables are now indexable and sliceable.
      Iterating through ``peekable``-wrapped iterables is also faster.
    * ``one`` and ``unique_to_each`` have been simplified.
      (Thanks to coady)


2.3
-----

* Added ``one`` from ``jaraco.util.itertools``. (Thanks, jaraco!)
* Added ``distinct_permutations`` and ``unique_to_each``. (Contributed by
  bbayles)
* Added ``windowed``. (Contributed by bbayles, with thanks to buchanae,
  jaraco, and abarnert)
* Simplified the implementation of ``chunked``. (Thanks, nvie!)
* Python 3.5 is now supported. Python 2.6 is no longer supported.
* Python 3 is now supported directly; there is no 2to3 step.

2.2
-----

* Added ``iterate`` and ``with_iter``. (Thanks, abarnert!)

2.1
-----

* Added (tested!) implementations of the recipes from the itertools
  documentation. (Thanks, Chris Lonnen!)
* Added ``ilen``. (Thanks for the inspiration, Matt Basta!)

2.0
-----

* ``chunked`` now returns lists rather than tuples. After all, they're
  homogeneous. This slightly backward-incompatible change is the reason for
  the major version bump.
* Added ``@consumer``.
* Improved test machinery.

1.1
-----

* Added ``first`` function.
* Added Python 3 support.
* Added a default arg to ``peekable.peek()``.
* Noted how to easily test whether a peekable iterator is exhausted.
* Rewrote documentation.

1.0
-----

* Initial release, with ``collate``, ``peekable``, and ``chunked``. Could
  really use better docs.


  * [mox3-0.27.0](https://docs.openstack.org/mox3/latest/) =========================================
Mox3 - Mock object framework for Python 3
=========================================

.. image:: https://governance.openstack.org/tc/badges/mox3.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

Mox3 is an unofficial port of the Google mox framework
(http://code.google.com/p/pymox/) to Python 3. It was meant to be as compatible
with mox as possible, but small enhancements have been made. The library was
tested on Python version 3.2, 2.7 and 2.6.

Use at your own risk ;)

To install:

  $ python setup.py install

Running Tests
-------------
The testing system is based on a combination of tox and testr. The canonical
approach to running tests is to simply run the command `tox`. This will
create virtual environments, populate them with depenedencies and run all of
the tests that OpenStack CI systems run. Behind the scenes, tox is running
`testr run --parallel`, but is set up such that you can supply any additional
testr arguments that are needed to tox. For example, you can run:
`tox -- --analyze-isolation` to cause tox to tell testr to add
--analyze-isolation to its argument list.

It is also possible to run the tests inside of a virtual environment
you have created, or it is possible that you have all of the dependencies
installed locally already. In this case, you can interact with the testr
command directly. Running `testr run` will run the entire test suite. `testr
run --parallel` will run it in parallel (this is the default incantation tox
uses.) More information about testr can be found at:
https://wiki.openstack.org/wiki/Testr

Basic Usage
-----------

The basic usage of mox3 is the same as with mox, but the initial import should
be made from the mox3 module:

  from mox3 import mox

To learn how to use mox3 you may check the documentation of the original mox
framework:

  http://code.google.com/p/pymox/wiki/MoxDocumentation

Original Copyright
------------------

Mox is Copyright 2008 Google Inc, and licensed under the Apache
License, Version 2.0; see the file COPYING.txt for details.  If you would
like to help us improve Mox, join the group.

OpenStack Fork
--------------

* Free software: Apache license
* Documentation: https://docs.openstack.org/mox3/latest/
* Source: https://opendev.org/openstack/mox3
* Bugs: https://bugs.launchpad.net/python-mox3




  * [mpi4py-3.0.0](https://bitbucket.org/mpi4py/mpi4py/) MPI for Python
==============

This package provides Python bindings for the **Message Passing
Interface** (MPI_) standard. It is implemented on top of the MPI-1/2/3
specification and exposes an API which grounds on the standard MPI-2
C++ bindings.

.. _MPI: http://www.mpi-forum.org/

Features
--------

This package supports:

* Convenient communication of any *picklable* Python object

  + point-to-point (send & receive)
  + collective (broadcast, scatter & gather, reductions)

* Fast communication of Python object exposing the *Python buffer
  interface* (NumPy arrays, builtin bytes/string/array objects)

  + point-to-point (blocking/nonbloking/persistent send & receive)
  + collective (broadcast, block/vector scatter & gather, reductions)

* Process groups and communication domains

  + Creation of new intra/inter communicators
  + Cartesian & graph topologies

* Parallel input/output:

  + read & write
  + blocking/nonbloking & collective/noncollective
  + individual/shared file pointers & explicit offset

* Dynamic process management

  + spawn & spawn multiple
  + accept/connect
  + name publishing & lookup

* One-sided operations

  + remote memory access (put, get, accumulate)
  + passive target syncronization (start/complete & post/wait)
  + active target syncronization (lock & unlock)


Install
-------

Once you have a working MPI implementation and the ``mpicc`` compiler
wrapper is on your search path, you can install this package

* using ``pip``::

  $ pip install mpi4py

* using ``easy_install`` (deprecated)::

  $ easy_install mpi4py

You can also install the in-development version of mpi4py

* using ``pip``::

    $ pip install git+https://bitbucket.org/mpi4py/mpi4py

  or::

    $ pip install https://bitbucket.org/mpi4py/mpi4py/get/master.tar.gz

* using ``easy_install`` (deprecated)::

    $ easy_install git+https://bitbucket.org/mpi4py/mpi4py

  or::

    $ easy_install https://bitbucket.org/mpi4py/mpi4py/get/master.tar.gz

You can also install it directly on Fedora (as well as RHEL and their
derivatives using the EPEL software repository)

* using ``dnf`` and the ``mpich`` package on ``x86_64``::

  $ dnf install mpi4py-mpich

* using ``dnf`` and the ``openmpi`` package on ``x86_64``::

  $ dnf install mpi4py-openmpi

Please remember to load the correct module for your choosen MPI environment

* for ``mpich`` package on ``x86_64`` do::

  $ module load mpi/mpich-x86_64
  $ python -c "import mpi4py"

* for ``openmpi`` package on ``x86_64`` do::

  $ module load mpi/openmpi-x86_64
  $ python -c "import mpi4py"


Citations
---------

If MPI for Python been significant to a project that leads to an
academic publication, please acknowledge that fact by citing the
project.

* L. Dalcin, P. Kler, R. Paz, and A. Cosimo,
  *Parallel Distributed Computing using Python*,
  Advances in Water Resources, 34(9):1124-1139, 2011.
  http://dx.doi.org/10.1016/j.advwatres.2011.04.013

* L. Dalcin, R. Paz, M. Storti, and J. D'Elia,
  *MPI for Python: performance improvements and MPI-2 extensions*,
  Journal of Parallel and Distributed Computing, 68(5):655-662, 2008.
  http://dx.doi.org/10.1016/j.jpdc.2007.09.005

* L. Dalcin, R. Paz, and M. Storti,
  *MPI for Python*,
  Journal of Parallel and Distributed Computing, 65(9):1108-1115, 2005.
  http://dx.doi.org/10.1016/j.jpdc.2005.03.010
  * [mpld3-0.3](http://mpld3.github.com) mpld3: A D3 Viewer for Matplotlib
=================================

- Author: Jake Vanderplas <jakevdp@cs.washington.edu>
- License: BSD 3-clause

This is an interactive D3js-based viewer which brings matplotlib graphics to the browser.
Please visit [http://mpld3.github.io](http://mpld3.github.io) for documentation and examples.

You may also see the [blog post](http://jakevdp.github.io/blog/2013/12/19/a-d3-viewer-for-matplotlib/), or the
[IPython notebook examples](http://nbviewer.ipython.org/github/jakevdp/mpld3/tree/master/notebooks/)
available in the ``notebooks`` directory of this repository.

[![version status](https://img.shields.io/pypi/v/mpld3.svg)](https://pypi.python.org/pypi/mpld3)
[![downloads](https://img.shields.io/pypi/dm/mpld3.svg)](https://pypi.python.org/pypi/mpld3)
[![build status](https://travis-ci.org/jakevdp/mpld3.svg?branch=master)](https://travis-ci.org/jakevdp/mpld3)


About
-----
mpld3 provides a custom stand-alone javascript library built on D3, which
parses JSON representations of plots.  The mpld3 python module provides a
set of routines which parses matplotlib plots (using the 
[mplexporter](http://github.com/mpld3/mplexporter) framework) and outputs
the JSON description readable by mpld3.js.


Installation
------------
mpld3 is compatible with python 2.6-2.7 and 3.3-3.4. It requires
[matplotlib](http://matplotlib.org) version 1.3 and
[jinja2](http://jinja.pocoo.org/) version 2.7+.

Optionally, mpld3 can be used with [IPython](http://ipython.org) notebook,
and requires IPython version 1.x or (preferably) version 2.0+.

This package is based on the [mplexporter](http://github.com/mpld3/mplexporter)
framework for crawling and exporting matplotlib images. mplexporter is bundled
with the source distribution via git submodule.

Within the git source directory, you can download the mplexporter dependency
and copy it into the mpld3 source directory using the following command:

    [~]$ python setup.py submodule

The submodule command is not necessary if you are installing from a distribution
rather than from the git source.

Once the submodule command has been run, you can build the package locally using

    [~]$ python setup.py build

or install the package to the standard Python path using:

    [~]$ python setup.py install

Or, to install to another location, use

    [~]$ python setup.py install --prefix=/path/to/location/

Then make sure your PYTHONPATH environment variable points to this location.

Trying it out
-------------
The package is pure python, and very light-weight.  You can take a look at
the notebooks in the examples directory, or run ``create_example.py``, which
will create a set of plots and launch a browser window showing interactive
views of these plots.

For a more comprehensive set of examples, see the
[IPython notebook examples](http://nbviewer.ipython.org/github/jakevdp/mpld3/tree/master/notebooks/) available in the ``notebooks`` directory.

Test Plots
----------
To explore the comparison between D3 renderings and matplotlib renderings for
various plot types, run the script ``visualize_tests.py``.  This will generate
an HTML page with the D3 renderings beside corresponding matplotlib renderings.

Features
--------
Many of the core features of matplotlib are already supported.  And additionally
there is some extra interactivity provided via the plugin framework.  The
following is a non-exhausive list of features that are yet to be supported:

- tick specification & formatting
- some legend features
- blended transforms, such as those required by ``axvlines`` and ``axhlines``
- twin axes (i.e. multiple scales on one plot) tied together

If any of these look like something you'd like to tackle, feel free to submit
a pull request!
  * [mpmath-1.0.0](http://mpmath.org) 
  * [msgpack-python-0.5.6](http://msgpack.org/) This package is deprecated.  Install msgpack instead.
  * [msrest-0.6.9](https://github.com/Azure/msrest-for-python) AutoRest: Python Client Runtime
===============================

.. image:: https://travis-ci.org/Azure/msrest-for-python.svg?branch=master
 :target: https://travis-ci.org/Azure/msrest-for-python

.. image:: https://codecov.io/gh/azure/msrest-for-python/branch/master/graph/badge.svg
 :target: https://codecov.io/gh/azure/msrest-for-python

Installation
------------

To install:

.. code-block:: bash

    $ pip install msrest


Release History
---------------

2019-09-04 Version 0.6.10
+++++++++++++++++++++++++

**Features**

- XML mode now supports OpenAPI additional properties  # 174

**Bugfixes**

- Accept "is_xml" kwargs to force XML serialization  #178
- Disable XML deserialization if received element is not an ElementTree  #178
- A "null" enum deserialize as None, and not "None" anymore  #173
- Fix some UTF8 encoding issue in Python 2.7 and XML mode  #172


2019-07-24 Version 0.6.9
++++++++++++++++++++++++

**Features**

- Accept extensions of JSON mimetype as valid JSON  #167

2019-06-24 Version 0.6.8
++++++++++++++++++++++++

**BugFixes**

- Impossible to serialize XML if model contains UTF8 characters on Python 2.7  #165
- Impossible to deserialize a HTTP response as XML if body contains UTF8 characters on Python 2.7  #165
- Loading a serialized configuration fails with NameError on NoOptionError  #162

Thanks to cclauss for the contribution

2019-06-12 Version 0.6.7
++++++++++++++++++++++++

**Features**

- Add DomainCredentials credentials for EventGrid

Thanks to kalyanaj for the contribution

2019-03-21 Version 0.6.6
++++++++++++++++++++++++

**Bugfixes**

- Make 0.6.x series compatible with pyinstaller again
- sdist now includes tests

Thanks to dotlambda for the contribution

2019-03-11 Version 0.6.5
++++++++++++++++++++++++

**Bugfixes**

- Fix list of integers serialization if div is provided #151
- Fix parsing of UTF8 with BOM #145

Thanks to eduardomourar for the contribution

2019-01-09 Version 0.6.4
++++++++++++++++++++++++

**Bugfixes**

- Fix regression on credentials configuration if used outside of Autorest scope #135

2019-01-08 Version 0.6.3
++++++++++++++++++++++++

**Features**

- Updated **experimental** async support. Requires Autorest.Python 4.0.64.

2018-11-19 Version 0.6.2
++++++++++++++++++++++++

**Bugfixes**

- Fix circular dependency in TYPE_CHECKING mode #128

2018-10-15 Version 0.6.1
++++++++++++++++++++++++

**Bugfixes**

- Remove unecessary verbose "warnings" log #126

2018-10-02 Version 0.6.0
++++++++++++++++++++++++

**Features**

- The environment variable AZURE_HTTP_USER_AGENT, if present, is now injected part of the UserAgent
- New **preview** msrest.universal_http module. Provide tools to generic HTTP management (sync/async, requests/aiohttp, etc.)
- New **preview** msrest.pipeline implementation:

  - A Pipeline is an ordered list of Policies than can process an HTTP request and response in a generic way.
  - More details in the wiki page about Pipeline: https://github.com/Azure/msrest-for-python/wiki/msrest-0.6.0---Pipeline

- Adding new attributes to Configuration instance:

  - http_logger_policy - Policy to handle HTTP logging
  - user_agent_policy - Policy to handle UserAgent
  - pipeline - The current pipeline used by the SDK client
  - async_pipeline - The current async pipeline used by the async SDK client

- Installing "msrest[async]" now installs the **experimental** async support. Works ONLY for Autorest.Python 4.0.63.

**Breaking changes**

- The HTTPDriver API introduced in 0.5.0 has been replaced by the Pipeline implementation.

- The following classes have been moved from "msrest.pipeline" to "msrest.universal_http":

  - ClientRedirectPolicy
  - ClientProxies
  - ClientConnection

- The following classes have been moved from "msrest.pipeline" to "msrest.universal_http.requests":

  - ClientRetryPolicy

**Bugfixes**

- Fix "long" on Python 2 if used with the "object" type  #121

Thanks to robgolding for the contribution

2018-09-04 Version 0.5.5
++++++++++++++++++++++++

**Bugfixes**

- Fix a serialization issue if additional_properties is declared, and "automatic model" syntax is used
  ("automatic model" being the ability to pass a dict to command and have the model auto-created)  # 120

2018-07-12 Version 0.5.4
++++++++++++++++++++++++

**Features**

- Support additionalProperties and XML

**BugFixes**

- Better parse empty node and not string types
- Improve "object" XML parsing

2018-07-10 Version 0.5.3
++++++++++++++++++++++++

**BugFixes**

- Fix some XML serialization subtle scenarios

2018-07-09 Version 0.5.2
++++++++++++++++++++++++

**Features**

- deserialize/from_dict now accepts a content-type parameter to parse XML strings

**Bugfixes**

- Fix some complex XML Swagger definitions.

This release likely breaks already generated XML SDKs, that needs to be regenerated with autorest.python 3.0.58

2018-06-21 Version 0.5.1
++++++++++++++++++++++++

**Bugfixes**

- Lower Accept header overwrite logging message #110
- Fix 'object' type and XML format

Thanks to dharmab for the contribution

2018-06-12 Version 0.5.0
++++++++++++++++++++++++

**Disclaimer**

This released is designed to be backward compatible with 0.4.x, but there is too many internal refactoring
and new features to continue with 0.4.x versionning

**Features**

- Add XML support
- Add many type hints, and MyPY testing on CI.
- HTTP calls are made through a HTTPDriver API. Only implementation is `requests` for now. This driver API is *not* considered stable
  and you should pin your msrest version if you want to provide a personal implementation.

**Bugfixes**

- Incorrect milliseconds serialization for some datetime object #94

**Deprecation**

That will trigger a DeprecationWarning if an old Autorest generated code is used.

- _client.add_header is deprecated, and config.headers should be used instead
- _client.send_formdata is deprecated, and _client.put/get/delete/post + _client.send should be used instead

2018-04-30 Version 0.4.29
+++++++++++++++++++++++++

**Bugfixes**

- Improve `SDKClient.__exit__` to take exc_details as optional parameters and not required #93
- refresh_session should also use the permanent HTTP session if available #91

2018-04-18 Version 0.4.28
+++++++++++++++++++++++++

**Features**

- msrest is now able to keep the "requests.Session" alive for performance. To activate this behavior:

  - Use the final Client as a context manager (requires generation with Autorest.Python 3.0.50 at least)
  - Use `client.config.keep_alive = True` and `client.close()` (requires generation with Autorest.Python 3.0.50 at least)
  - Use `client.config.keep_alive = True` and client._client.close() (not recommended, but available in old releases of SDK)

- All Authentication classes now define `signed_session` and `refresh_session` with an optional `session` parameter.
  To take benefits of the session improvement, a subclass of Authentication *MUST* add this optional parameter
  and use it if it's not `None`:

     def signed_session(self, session=None):
         session = session or requests.Session()

         # As usual from here.

2018-03-07 Version 0.4.27
+++++++++++++++++++++++++

**Features**

- Disable HTTP log by default (security), add `enable_http_log` to restore it #86

**BugFixes**

- Fix incorrect date parsing if ms precision is over 6 digits #82

2018-01-30 Version 0.4.26
+++++++++++++++++++++++++

**Features**

- Add TopicCredentials for EventGrid client

**Bugfixes**

- Fix minimal dependency of isodate
- Fix serialisation from dict if datetime provided

2018-01-08 Version 0.4.25
+++++++++++++++++++++++++

**Features**

- Add LROPoller class. This is a customizable LRO engine.
  This is the poller engine of Autorest.Python 3.0, and is not used by code generated by previous Autorest version.

2018-01-03 Version 0.4.24
+++++++++++++++++++++++++

**Bugfixes**

- Date parsing is now compliant with Autorest / Swagger 2.0 specification (less lenient)

**Internal optimisation**

- Call that does not return a streamable object are now executed in requests stream mode False (was True whatever the type of the call).
  This should reduce the number of leaked opened session and allow urllib3 to manage connection pooling more efficiently.
  Only clients generated with Autorest.Python >= 2.1.31 (not impacted otherwise, fully backward compatible)

2017-12-21 Version 0.4.23
+++++++++++++++++++++++++

**Bugfixes**

- Accept to deserialize enum of different type if content string match #75
- Stop failing on deserialization if enum string is unkwon. Return the string instead.

**Features**

- Model now accept kwargs in constructor for future kwargs models

2017-12-15 Version 0.4.22
+++++++++++++++++++++++++

**Bugfixes**

- Do not validate additional_properties #73
- Improve validation error if expected type is dict, but actual type is not #73

2017-12-14 Version 0.4.21
+++++++++++++++++++++++++

**Bugfixes**

- Fix additional_properties if Swagger was flatten #72

2017-12-13 Version 0.4.20
+++++++++++++++++++++++++

**Features**

- Add support for additional_properties

  - By default, all additional_properties are kept.
  - Additional properties are sent to the server only if it was specified in the Swagger,
    or if "enable_additional_properties_sending" is called on the model we want it.
    This is a class method that enables it for all instance of this model.

2017-11-20 Version 0.4.19
+++++++++++++++++++++++++

**Features**

- The interpretation of Swagger 2.0 "discriminator" is now lenient. This means for these two scenarios:

  - Discriminator value is missing from the received payload
  - Discriminator value is not defined in the Swagger

  Instead of failing with an exception, this now returns the base type for this "discriminator".

  Note that this is not a contradiction of the Swagger 2.0 spec, that specifies
  "validation SHOULD fail [...] there may exist valid reasons in particular circumstances to ignore a particular item,
  but the full implications must be understood and carefully weighed before choosing a different course."

  This cannot be configured for now and is the new default behvaior, but can be in the future if needed.

**Bugfixes**

- Optional formdata parameters were raising an exception (#65)
- "application/x-www-form-urlencoded" form was sent using "multipart/form-data".
  This causes problems if the server does not support "multipart/form-data" (#66)

2017-10-26 Version 0.4.18
+++++++++++++++++++++++++

**Features**

- Add ApiKeyCredentials class. This can be used to support OpenAPI ApiKey feature.
- Add CognitiveServicesAuthentication class. Pre-declared ApiKeyCredentials class for Cognitive Services.

2017-10-12 Version 0.4.17
+++++++++++++++++++++++++

**Features**

This make Authentication classes more consistent:

- OAuthTokenAuthentication is now a subclass of BasicTokenAuthentication (was Authentication)
- BasicTokenAuthentication has now a "set_token" methods that does nothing.

This allows test like "isintance(o, BasicTokenAuthentication)" to be guaranted that the following attributes exists:

- token
- set_token()
- signed_session()

This means for users of "msrestazure", that they are guaranted that all AD classes somehow inherits from "BasicTokenAuthentication"

2017-10-05 Version 0.4.16
+++++++++++++++++++++++++

**Bugfixes**

- Fix regression: accept "set<str>" as a valid "[str]" (#60)

2017-09-28 Version 0.4.15
+++++++++++++++++++++++++

**Bugfixes**

- Always log response body (#16)
- Improved exception message if error JSON is Odata v4 (#55)
- Refuse "str" as a valid "[str]" type (#41)
- Better exception handling if input from server is not JSON valid

**Features**

- Add Configuration.session_configuration_callback to customize the requests.Session if necessary (#52)
- Add a flag to Serializer to disable client-side-validation (#51)
- Remove "import requests" from "exceptions.py" for apps that require fast loading time (#23)

Thank you to jayden-at-arista for the contribution

2017-08-23 Version 0.4.14
+++++++++++++++++++++++++

**Bugfixes**

- Fix regression introduced in msrest 0.4.12 - dict syntax with enum modeled as string and enum used

2017-08-22 Version 0.4.13
+++++++++++++++++++++++++

**Bugfixes**

- Fix regression introduced in msrest 0.4.12 - dict syntax using isodate.Duration (#42)

2017-08-21 Version 0.4.12
+++++++++++++++++++++++++

**Features**

- Input is now more lenient
- Model have a "validate" method to check content constraints
- Model have now 4 new methods:

  - "serialize" that gives the RestAPI that will be sent
  - "as_dict" that returns a dict version of the Model. Callbacks are available.
  - "deserialize" the parses the RestAPI JSON into a Model
  - "from_dict" that parses several dict syntax into a Model. Callbacks are available.

More details and examples in the Wiki article on Github:
https://github.com/Azure/msrest-for-python/wiki/msrest-0.4.12---Serialization-change

**Bugfixes**

- Better Enum checking (#38)

2017-06-21 Version 0.4.11
+++++++++++++++++++++++++

**Bugfixes**

- Fix incorrect dependency to "requests" 2.14.x, instead of 2.x meant in 0.4.8

2017-06-15 Version 0.4.10
+++++++++++++++++++++++++

**Features**

- Add requests hooks to configuration

2017-06-08 Version 0.4.9
++++++++++++++++++++++++

**Bugfixes**

- Accept "null" value for paging array as an empty list and do not raise (#30)

2017-05-22 Version 0.4.8
++++++++++++++++++++++++

**Bugfixes**

- Fix random "pool is closed" error (#29)
- Fix requests dependency to version 2.x, since version 3.x is annunced to be breaking.

2017-04-04 Version 0.4.7
++++++++++++++++++++++++

**BugFixes**

- Refactor paging #22:

   - "next" is renamed "advance_page" and "next" returns only 1 element (Python 2 expected behavior)
   - paging objects are now real generator and support the "next()" built-in function without need for "iter()"

- Raise accurate DeserialisationError on incorrect RestAPI discriminator usage #27
- Fix discriminator usage of the base class name #27
- Remove default mutable arguments in Clients #20
- Fix object comparison in some scenarios #24

2017-03-06 Version 0.4.6
++++++++++++++++++++++++

**Bugfixes**

- Allow Model sub-classes to be serialized if type is "object"

2017-02-13 Version 0.4.5
++++++++++++++++++++++++

**Bugfixes**

- Fix polymorphic deserialization #11
- Fix regexp validation if '\\w' is used in Python 2.7 #13
- Fix dict deserialization if keys are unicode in Python 2.7

**Improvements**

- Add polymorphic serialisation from dict objects
- Remove chardet and use HTTP charset declaration (fallback to utf8)

2016-09-14 Version 0.4.4
++++++++++++++++++++++++

**Bugfixes**

- Remove paging URL validation, part of fix https://github.com/Azure/autorest/pull/1420

**Disclaimer**

In order to get paging fixes for impacted clients, you need this package and Autorest > 0.17.0 Nightly 20160913

2016-09-01 Version 0.4.3
++++++++++++++++++++++++

**Bugfixes**

- Better exception message (https://github.com/Azure/autorest/pull/1300)

2016-08-15 Version 0.4.2
++++++++++++++++++++++++

**Bugfixes**

- Fix serialization if "object" type contains None (https://github.com/Azure/autorest/issues/1353)

2016-08-08 Version 0.4.1
++++++++++++++++++++++++

**Bugfixes**

- Fix compatibility issues with requests 2.11.0 (https://github.com/Azure/autorest/issues/1337)
- Allow url of ClientRequest to have parameters (https://github.com/Azure/autorest/issues/1217)

2016-05-25 Version 0.4.0
++++++++++++++++++++++++

This version has no bug fixes, but implements new features of Autorest:
- Base64 url type
- unixtime type
- x-ms-enum modelAsString flag

**Behaviour changes**

- Add Platform information in UserAgent
- Needs Autorest > 0.17.0 Nightly 20160525

2016-04-26 Version 0.3.0
++++++++++++++++++++++++

**Bugfixes**

- Read only values are no longer in __init__ or sent to the server (https://github.com/Azure/autorest/pull/959)
- Useless kwarg removed

**Behaviour changes**

- Needs Autorest > 0.16.0 Nightly 20160426


2016-03-25 Version 0.2.0
++++++++++++++++++++++++

**Bugfixes**

- Manage integer enum values (https://github.com/Azure/autorest/pull/879)
- Add missing application/json Accept HTTP header (https://github.com/Azure/azure-sdk-for-python/issues/553)

**Behaviour changes**

- Needs Autorest > 0.16.0 Nightly 20160324


2016-03-21 Version 0.1.3
++++++++++++++++++++++++

**Bugfixes**

- Deserialisation of generic resource if null in JSON (https://github.com/Azure/azure-sdk-for-python/issues/544)


2016-03-14 Version 0.1.2
++++++++++++++++++++++++

**Bugfixes**

- urllib3 side effect (https://github.com/Azure/autorest/issues/824)


2016-03-04 Version 0.1.1
++++++++++++++++++++++++

**Bugfixes**

- Source package corrupted in Pypi (https://github.com/Azure/autorest/issues/799)

2016-03-04 Version 0.1.0
+++++++++++++++++++++++++

**Behavioural Changes**

- Removed custom logging set up and configuration. All loggers are now children of the root logger 'msrest' with no pre-defined configurations.
- Replaced _required attribute in Model class with more extensive _validation dict.

**Improvement**

- Removed hierarchy scanning for attribute maps from base Model class - relies on generator to populate attribute
  maps according to hierarchy.
- Base class Paged now inherits from collections.Iterable.
- Data validation during serialization using custom parameters (e.g. max, min etc).
- Added ValidationError to be raised if invalid data encountered during serialization.

2016-02-29 Version 0.0.3
++++++++++++++++++++++++

**Bugfixes**

- Source package corrupted in Pypi (https://github.com/Azure/autorest/issues/718)

2016-02-19 Version 0.0.2
++++++++++++++++++++++++

**Bugfixes**

- Fixed bug in exception logging before logger configured.

2016-02-19 Version 0.0.1
++++++++++++++++++++++++

- Initial release.



  * [msrestazure-0.6.1](https://github.com/Azure/msrestazure-for-python) AutoRest: Python Client Runtime - Azure Module
===============================================

.. image:: https://travis-ci.org/Azure/msrestazure-for-python.svg?branch=master
 :target: https://travis-ci.org/Azure/msrestazure-for-python

.. image:: https://codecov.io/gh/azure/msrestazure-for-python/branch/master/graph/badge.svg
 :target: https://codecov.io/gh/azure/msrestazure-for-python

Installation
------------

To install:

.. code-block:: bash

    $ pip install msrestazure


Release History
---------------

2019-09-16 Version 0.6.2
++++++++++++++++++++++++

**Bugfix**

- Fix ARM error parsing if Type info is used  #135

2019-06-10 Version 0.6.1
++++++++++++++++++++++++

**Features**

- Add User Assigned identity support for WebApp/Functions #124
- Add timeout parameter for MSI token, is used from a VM #131

Thanks to @noelbundick for his contribution

2018-12-17 Version 0.6.0
++++++++++++++++++++++++

**Features**

- Implementation of LRO async, based on msrest 0.6.x series (*experimental*)

**Disclaimer**

- This version contains no direct breaking changes, but is bumped to 0.6.x since it requires a breaking change version of msrest.

Thanks to @gison93 for his documentation contribution

2018-11-01 Version 0.5.1
++++++++++++++++++++++++

**Bugfixes**

- Fix CloudError if response and error message are provided at the same time #114
- Fix LRO polling if last call is an empty Location (Autorest.Python 3.x only) #120

**Features**

- Altered resource id parsing logic to allow for resource group IDs #117

2018-08-02 Version 0.5.0
++++++++++++++++++++++++

**Features**

- Implementation is now using ADAL and not request-oauthlib. This allows more AD scenarios (like federated)  #94
- Add additionalInfo parsing for CloudError #102

**Breaking changes**

These breaking changes applies to ServicePrincipalCredentials, UserPassCredentials, AADTokenCredentials

- Remove "auth_uri" attribute and parameter. This was unused.
- Remove "state" attribute. This was unused.
- Remove "client" attribute. This was exposed by mistake and should have been internal. No replacement is possible.
- Remove "token_uri" attribute and parameter. Use "cloud_environment" and "tenant" to impact the login url now.
- Remove token caching based on "keyring". Token caching should be implemented using ADAL now. This implies:

  - Remove the "keyring" parameter
  - Remove the "clear_cached_token" method
  - Remove the "retrieve_session" method

2018-07-03 Version 0.4.35
+++++++++++++++++++++++++

**Bugfixes**

- MSIAuthentication regression for KeyVault since IMDS support #109

2018-07-02 Version 0.4.34
+++++++++++++++++++++++++

**Bugfixes**

- MSIAuthentication should initialize the token attribute on creation #106

2018-06-21 Version 0.4.33
+++++++++++++++++++++++++

**Bugfixes**

- Fixes refreshToken in UserPassCredentials and AADTokenCredentials #103
- Fix US government cloud definition #104

Thanks to mjcaley for his contribution

2018-06-13 Version 0.4.32
+++++++++++++++++++++++++

**Features**

- Implement new LRO options of Autorest #101

**Bug fixes**

- Reduce max MSI polling time for VM #100


2018-05-17 Version 0.4.31
+++++++++++++++++++++++++

**Features**

- Improve MSI for VM token polling algorithm

2018-05-16 Version 0.4.30
+++++++++++++++++++++++++

**Features**

- Allow ADAL 0.5.0 to 2.0.0 excluded as valid ADAL dependency

2018-04-30 Version 0.4.29
+++++++++++++++++++++++++

**Bugfixes**

- Fix refresh Token on `AADTokenCredentials` (was broken in 0.4.27)
- Now `UserPasswordCredentials` correctly use the refreshToken, and not user/password to refresh the session (was broken in 0.4.27)
- Bring back `keyring`, with minimal dependency 12.0.2 that fixes the installation problem on old Python

2018-04-23 Version 0.4.28
+++++++++++++++++++++++++

**Disclaimer**

Do to some stability issues with "keyring" dependency that highly change from one system to another,
this package is no longer a dependency of "msrestazure".
If you were using the secured token cache of `ServicePrincipalCredentials` and `UserPassCredentials`,
the feature is still available, but you need to install manually "keyring". The functionnality will activate automatically.

2018-04-18 Version 0.4.27
+++++++++++++++++++++++++

**Features**

- Implements new features of msrest 0.4.28 on session improvement. See msrest ChangeLog for details.

Update msrest dependency to 0.4.28

2018-04-17 Version 0.4.26
+++++++++++++++++++++++++

**Bugfixes**

- IMDS/MSI: Retry on more error codes (#87)
- IMDS/MSI: fix a boundary case on timeout (#86)

2018-03-29 Version 0.4.25
+++++++++++++++++++++++++

**Features**

- MSIAuthentication now uses IMDS endpoint if available
- MSIAuthentication can be used in any environment that defines MSI_ENDPOINT env variable

2018-03-26 Version 0.4.24
+++++++++++++++++++++++++

**Bugfix**

- Fix parse_resource_id() tool to be case-insensitive to keywords when matching #81
- Add missing baseclass init call for AdalAuthentication #82

2018-03-19 Version 0.4.23
+++++++++++++++++++++++++

**Bugfix**

- Fix LRO result if POST uses AsyncOperation header (Autorest.Python 3.0 only) #79

2018-02-27 Version 0.4.22
+++++++++++++++++++++++++

**Bugfix**

- Remove a possible infinite loop with MSIAuthentication #77

**Disclaimer**

From this version, MSIAuthentication will fail instantly if you try to get MSI token
from a VM where the extension is not installed, or not yet ready.
You need to do your own retry mechanism if you think the extension is provisioning and
the call might succeed later.
This behavior is consistent with other Azure SDK implementation of MSI scenarios.

2018-01-26 Version 0.4.21
+++++++++++++++++++++++++

- Update allowed ADAL dependency to 0.5.x

2018-01-08 Version 0.4.20
+++++++++++++++++++++++++

**Features**

- CloudError now includes the "innererror" attribute to match OData v4 #73
- Introduces ARMPolling implementation of Azure Resource Management LRO. Requires msrest 0.4.25 (new dependency).
  This is used by code generated with Autorest.Python 3.0, and is not used by code generated by previous Autorest version.
- Change msrest dependency to ">=0.4.25,<2.0.0" to allow (future) msrest 1.0.0 as compatible dependency.

Thank you to demyanenko for his contribution.

2017-12-14 Version 0.4.19
+++++++++++++++++++++++++

**Feature**

* Improve MSIAuthentication to support User Assigned Identity #70

**Bugfixes**

* Fix session obj for cloudmetadata endpoint #67
* Fix authentication resource node for AzureSatck #65
* Better detection of AppService with MSIAuthentication #70

2017-12-01 Version 0.4.18
+++++++++++++++++++++++++

**Bugfixes**

- get_cloud_from_metadata_endpoint incorrect on AzureStack #62
- get_cloud_from_metadata_endpoint certificate issue #61

2017-11-22 Version 0.4.17
+++++++++++++++++++++++++

**Bugfixes**

- Fix AttributeError if error JSON from ARM does not follow ODatav4 (as it should)

2017-10-31 Version 0.4.16
+++++++++++++++++++++++++

**Bugfixes**

- Fix AttributeError if input JSON is not a dict (#54)

2017-10-13 Version 0.4.15
+++++++++++++++++++++++++

**Features**

- Add support for WebApp/Functions in MSIAuthentication classes
- Add parse_resource_id(), resource_id(), validate_resource_id() to parse ARM ids
- Retry strategy now n reach 24 seconds (instead of 12 seconds)

2017-09-11 Version 0.4.14
+++++++++++++++++++++++++

**Features**

- Add Managed Service Integrated (MSI) authentication

**Bug fix**

- Fix AdalError handling in some scenarios (#44)

Thank you to Hexadite-Omer for his contribution

2017-08-24 Version 0.4.13
+++++++++++++++++++++++++

**Features**

- "keyring" is now completely optional

2017-08-23 Version 0.4.12
+++++++++++++++++++++++++

**Features**

- add "timeout" to ServicePrincipalCredentials and UserPasswordCredentials
- Threads created by AzureOperationPoller have now a name prefixed by "AzureOperationPoller" to help identify them

**Bugfixes**

- Do not fail if keyring is badly installed
- Update Azure Gov login endpoint
- Update metadata ARM endpoint parser

**Breaking changes**

- Remove InteractiveCredentials. This class was deprecated and unusable. Use ADAL device code instead.

2017-06-29 Version 0.4.11
+++++++++++++++++++++++++

**Features**

- Add cloud definitions for public Azure, German Azure, China Azure and Azure Gov
- Add get_cloud_from_metadata_endpoint to automatically create a Cloud object from an ARM endpoint
- Add `cloud_environment` to all Credentials objects (except AdalAuthentication)

**Note**

- This deprecates "china=True", to be replaced by "cloud_environment=AZURE_CHINA_CLOUD"

Example:

.. code:: python

  from msrestazure.azure_cloud import AZURE_CHINA_CLOUD
  from msrestazure.azure_active_directory import UserPassCredentials

  credentials = UserPassCredentials(
      login,
      password,
      cloud_environment=AZURE_CHINA_CLOUD
  )

`base_url` of SDK client can be pointed to "cloud_environment.endpoints.resource_manager" for basic scenario:

Example:

.. code:: python

  from msrestazure.azure_cloud import AZURE_CHINA_CLOUD
  from msrestazure.azure_active_directory import UserPassCredentials
  from azure.mgmt.resource import ResourceManagementClient

  credentials = UserPassCredentials(
      login,
      password,
      cloud_environment=AZURE_CHINA_CLOUD
  )
  client = ResourceManagementClient(
      credentials,
      subscription_id,
      base_url=AZURE_CHINA_CLOUD.endpoints.resource_manager
  )

Azure Stack connection can be done:

.. code:: python

  from msrestazure.azure_cloud import get_cloud_from_metadata_endpoint
  from msrestazure.azure_active_directory import UserPassCredentials
  from azure.mgmt.resource import ResourceManagementClient

  mystack_cloud = get_cloud_from_metadata_endpoint("https://myazurestack-arm-endpoint.com")
  credentials = UserPassCredentials(
      login,
      password,
      cloud_environment=mystack_cloud
  )
  client = ResourceManagementClient(
      credentials,
      subscription_id,
      base_url=mystack_cloud.endpoints.resource_manager
  )


2017-06-27 Version 0.4.10
+++++++++++++++++++++++++

**Bugfixes**

- Accept PATCH/201 as LRO valid state
- Close token session on exit (ServicePrincipal and UserPassword credentials)

2017-06-19 Version 0.4.9
++++++++++++++++++++++++

**Features**

- Add proxies parameters to ServicePrincipal and UserPassword credentials class #29
- Add automatic Azure provider registration if needed (requires msrest 0.4.10) #28

Thank you to likel for his contribution

2017-05-31 Version 0.4.8
++++++++++++++++++++++++

**Bugfixes**

- Fix LRO if first call never returns 200, but ends on 201 (#26)
- FiX LRO AttributeError if timeout is short (#21)

**Features**

- Expose a "status()" method in AzureOperationPoller (#18)

2017-01-23 Version 0.4.7
++++++++++++++++++++++++

**Bugfixes**

- Adding `accept_language` and `generate_client_request_id` default values

2016-12-12 Version 0.4.6
++++++++++++++++++++++++

**Bugfixes**

Refactor Long Running Operation algorithm.

- There is no breaking changes, however you might need to record again your offline HTTP records
  if you use unittests with VCRpy.
- Fix a couple of latent bugs

2016-11-30 Version 0.4.5
++++++++++++++++++++++++

**New features**

- Add AdalAuthentification class to wrap ADAL library (https://github.com/Azure/msrestazure-for-python/pull/8)

2016-10-17 Version 0.4.4
++++++++++++++++++++++++

**Bugfixes**

- More informative and well-formed CloudError exceptions (https://github.com/Azure/autorest/issues/1460)
- Raise CustomException is defined in Swagger (https://github.com/Azure/autorest/issues/1404)

2016-09-14 Version 0.4.3
++++++++++++++++++++++++

**Bugfixes**

- Make AzureOperationPoller thread as daemon (do not block anymore a Ctrl+C) (https://github.com/Azure/autorest/pull/1379)

2016-09-01 Version 0.4.2
++++++++++++++++++++++++

**Bugfixes**

- Better exception message (https://github.com/Azure/autorest/pull/1300)

This version needs msrest >= 0.4.3

2016-06-08 Version 0.4.1
++++++++++++++++++++++++

**Bugfixes**

- Fix for LRO PUT operation https://github.com/Azure/autorest/issues/1133

2016-05-25 Version 0.4.0
++++++++++++++++++++++++

Update msrest dependency to 0.4.0

**Bugfixes**

- Fix for several AAD issues https://github.com/Azure/autorest/issues/1055
- Fix for LRO PATCH bug and refactor https://github.com/Azure/autorest/issues/993

**Behaviour changes**

- Needs Autorest > 0.17.0 Nightly 20160525


2016-04-26 Version 0.3.0
++++++++++++++++++++++++

Update msrest dependency to 0.3.0

**Bugfixes**

- Read only values are no longer in __init__ or sent to the server (https://github.com/Azure/autorest/pull/959)
- Useless kwarg removed

**Behaviour changes**

- Needs Autorest > 0.16.0 Nightly 20160426


2016-03-31 Version 0.2.1
++++++++++++++++++++++++

**Bugfixes**

- Fix AzurePollerOperation if Swagger defines provisioning status as enum type (https://github.com/Azure/autorest/pull/892)


2016-03-25 Version 0.2.0
++++++++++++++++++++++++

Update msrest dependency to 0.2.0

**Behaviour change**

- async methods called with raw=True don't return anymore AzureOperationPoller but ClientRawResponse
- Needs Autorest > 0.16.0 Nightly 20160324


2016-03-21 Version 0.1.2
++++++++++++++++++++++++

Update msrest dependency to 0.1.3

**Bugfixes**

- AzureOperationPoller.wait() failed to raise exception if query error (https://github.com/Azure/autorest/pull/856)


2016-03-04 Version 0.1.1
++++++++++++++++++++++++

**Bugfixes**

- Source package corrupted in Pypi (https://github.com/Azure/autorest/issues/799)

2016-03-04 Version 0.1.0
++++++++++++++++++++++++

**Behaviour change**

- Replaced _required attribute in CloudErrorData class with _validation dict.

2016-02-29 Version 0.0.2
++++++++++++++++++++++++

**Bugfixes**

- Fixed AAD bug to include connection verification in UserPassCredentials. (https://github.com/Azure/autorest/pull/725)
- Source package corrupted in Pypi (https://github.com/Azure/autorest/issues/718)

2016-02-19 Version 0.0.1
++++++++++++++++++++++++

- Initial release.



  * [multidict-4.5.2](https://github.com/aio-libs/multidict) =========
multidict
=========

.. image:: https://img.shields.io/pypi/v/multidict.svg
   :target: https://pypi.org/project/multidict

.. image:: https://readthedocs.org/projects/multidict/badge/?version=latest
   :target: http://multidict.readthedocs.org/en/latest/?badge=latest

.. image:: https://img.shields.io/travis-ci/com/aio-libs/multidict/master.svg
   :align: right
   :target: http://travis-ci.com/aio-libs/multidict

.. image:: https://img.shields.io/appveyor/ci/aio-libs/multidict/master.svg?label=Windows%20build%20%40%20Appveyor
   :align: right
   :target: https://ci.appveyor.com/project/aio-libs/multidict/branch/master

.. image:: https://img.shields.io/pypi/pyversions/multidict.svg
   :target: https://pypi.org/project/multidict

.. image:: https://codecov.io/gh/aio-libs/multidict/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/aio-libs/multidict
   :alt: Coverage metrics

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :target: https://gitter.im/aio-libs/Lobby
   :alt: Chat on Gitter

Multidict is dict-like collection of *key-value pairs* where key
might be occurred more than once in the container.

Introduction
------------

*HTTP Headers* and *URL query string* require specific data structure:
*multidict*. It behaves mostly like a regular ``dict`` but it may have
several *values* for the same *key* and *preserves insertion ordering*.

The *key* is ``str`` (or ``istr`` for case-insensitive dictionaries).

``multidict`` has four multidict classes:
``MultiDict``, ``MultiDictProxy``, ``CIMultiDict``
and ``CIMultiDictProxy``.

Immutable proxies (``MultiDictProxy`` and
``CIMultiDictProxy``) provide a dynamic view for the
proxied multidict, the view reflects underlying collection changes. They
implement the ``collections.abc.Mapping`` interface.

Regular mutable (``MultiDict`` and ``CIMultiDict``) classes
implement ``collections.abc.MutableMapping`` and allows to change
their own content.


*Case insensitive* (``CIMultiDict`` and
``CIMultiDictProxy``) ones assume the *keys* are case
insensitive, e.g.::

   >>> dct = CIMultiDict(key='val')
   >>> 'Key' in dct
   True
   >>> dct['Key']
   'val'

*Keys* should be ``str`` or ``istr`` instances.

The library has optional Cython_ optimization for sake of speed.


License
-------

Apache 2


.. _aiohttp: https://github.com/KeepSafe/aiohttp
.. _Cython: http://cython.org/


Changelog
---------
See `RTD page <http://multidict.readthedocs.org/en/latest/changes.html>`_.


  * [multipledispatch-0.6.0](http://github.com/mrocklin/multipledispatch/) Multiple Dispatch
=================

|Build Status| |Coverage Status| |Version Status|

A relatively sane approach to multiple dispatch in Python.

This implementation of multiple dispatch is efficient, mostly complete,
performs static analysis to avoid conflicts, and provides optional namespace
support.  It looks good too.

See the documentation at https://multiple-dispatch.readthedocs.io/


Example
-------

.. code-block:: python

   >>> from multipledispatch import dispatch

   >>> @dispatch(int, int)
   ... def add(x, y):
   ...     return x + y

   >>> @dispatch(object, object)
   ... def add(x, y):
   ...     return "%s + %s" % (x, y)

   >>> add(1, 2)
   3

   >>> add(1, 'hello')
   '1 + hello'

What this does
--------------

-  Dispatches on all non-keyword arguments

-  Supports inheritance

-  Supports instance methods

-  Supports union types, e.g. ``(int, float)``

-  Supports builtin abstract classes, e.g. ``Iterator, Number, ...``

-  Caches for fast repeated lookup

-  Identifies possible ambiguities at function definition time

-  Provides hints to resolve ambiguities when they occur

-  Supports namespaces with optional keyword arguments

-  Supports variadic dispatch

What this doesn't do
--------------------

-  Diagonal dispatch

.. code-block:: python

   a = arbitrary_type()
   @dispatch(a, a)
   def are_same_type(x, y):
       return True

-  Efficient update: The addition of a new signature requires a full resolve of
   the whole function.  This becomes troublesome after you get to a few hundred
   type signatures.


Installation and Dependencies
-----------------------------

``multipledispatch`` is on the Python Package Index (PyPI):

::

    pip install multipledispatch

or

::

    easy_install multipledispatch


``multipledispatch`` supports Python 2.6+ and Python 3.2+ with a common
codebase.  It is pure Python and requires only the small `six
<https://pypi.org/project/six/>`_ library as a dependency.

It is, in short, a light weight dependency.


License
-------

New BSD. See `License file`_.


Links
-----

-  `Five-minute Multimethods in Python by Guido`_
-  `multimethods package on PyPI`_
-  `singledispatch in Python 3.4's functools`_
-  `Clojure Protocols`_
-  `Julia methods docs`_
-  `Karpinksi notebook: *The Design Impact of Multiple Dispatch*`_
-  `Wikipedia article`_
-  `PEP 3124 - *Overloading, Generic Functions, Interfaces, and Adaptation*`_


.. _`Five-minute Multimethods in Python by Guido`:
  http://www.artima.com/weblogs/viewpost.jsp?thread=101605
.. _`multimethods package on PyPI`:
  https://pypi.python.org/pypi/multimethods
.. _`singledispatch in Python 3.4's functools`:
  http://docs.python.org/3.4/library/functools.html#functools.singledispatch
.. _`Clojure Protocols`:
  http://clojure.org/protocols
.. _`Julia methods docs`:
  https://julia.readthedocs.io/en/latest/manual/methods/
.. _`Karpinksi notebook: *The Design Impact of Multiple Dispatch*`:
  http://nbviewer.ipython.org/gist/StefanKarpinski/b8fe9dbb36c1427b9f22
.. _`Wikipedia article`:
  http://en.wikipedia.org/wiki/Multiple_dispatch
.. _`PEP 3124 - *Overloading, Generic Functions, Interfaces, and Adaptation*`:
  http://legacy.python.org/dev/peps/pep-3124/

.. |Build Status| image:: https://travis-ci.org/mrocklin/multipledispatch.png
   :target: https://travis-ci.org/mrocklin/multipledispatch
.. |Version Status| image:: https://pypip.in/v/multipledispatch/badge.png
   :target: https://img.shields.io/pypi/v/multipledispatch.svg
.. |Coverage Status| image:: https://coveralls.io/repos/mrocklin/multipledispatch/badge.png
   :target: https://coveralls.io/r/mrocklin/multipledispatch
.. _License file: https://github.com/mrocklin/multipledispatch/blob/master/LICENSE.txt



  * [munch-2.3.2](http://github.com/Infinidat/munch) 
  * [mypy-0.720](http://www.mypy-lang.org/) Mypy -- Optional Static Typing for Python
=========================================

Add type annotations to your Python programs, and use mypy to type
check them.  Mypy is essentially a Python linter on steroids, and it
can catch many programming errors by analyzing your program, without
actually having to run it.  Mypy has a powerful type system with
features such as type inference, gradual typing, generics and union
types.



  * [mypy_extensions-0.4.1](http://www.mypy-lang.org/) Mypy Extensions
===============

The "mypy_extensions" module defines experimental extensions to the
standard "typing" module that are supported by the mypy typechecker.
  * [mypytools-0.1.15](https://github.com/nylas/mypy-tools) 
  * [nanoget-1.8.0](https://github.com/wdecoster/nanoget) # nanoget
This module provides functions to extract useful metrics from Oxford Nanopore sequencing reads and alignments.  

[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/wouter_decoster.svg?style=social&label=Follow%20%40wouter_decoster)](https://twitter.com/wouter_decoster)
[![install with conda](https://anaconda.org/bioconda/nanoget/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanoget)
[![Build Status](https://travis-ci.org/wdecoster/nanoget.svg?branch=master)](https://travis-ci.org/wdecoster/nanoget)


## FUNCTIONS
Data can be presented in the following formats, using the following functions:  
- A sorted bam file `process_bam(bamfile, threads)`  
- A standard fastq file `process_fastq_plain(fastqfile, 'threads')`  
- A fastq file with metadata from MinKNOW or Albacore `process_fastq_rich(fastqfile)`  
- A sequencing_summary file generated by Albacore `process_summary(sequencing_summary.txt, 'readtype')`  

Fastq files can be compressed using gzip, bzip2 or bgzip. The data is returned as a pandas DataFrame with standardized headernames for convenient extraction. The functions perform logging while being called and extracting data.


## INSTALLATION
```bash
pip install nanoget
```
or  
[![install with conda](https://anaconda.org/bioconda/nanoget/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanoget)
```
conda install -c bioconda nanoget
```
  * [nanomath-0.23.1](https://github.com/wdecoster/nanomath) # nanomath
This module provides a few simple math and statistics functions for other scripts processing Oxford Nanopore sequencing data

[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/wouter_decoster.svg?style=social&label=Follow%20%40wouter_decoster)](https://twitter.com/wouter_decoster)
[![install with conda](https://anaconda.org/bioconda/nanomath/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanomath)
[![Build Status](https://travis-ci.org/wdecoster/nanomath.svg?branch=master)](https://travis-ci.org/wdecoster/nanomath)
[![Code Health](https://landscape.io/github/wdecoster/nanomath/master/landscape.svg?style=flat)](https://landscape.io/github/wdecoster/nanomath/master)


## FUNCTIONS
* Calculate read N50 from a set of lengths `get_N50(readlenghts)`  
* Remove extreme length outliers from a dataset `remove_length_outliers(dataframe, columname)`  
* Calculate the average Phred quality of a read `ave_qual(qualscores)`  
* Write out the statistics report after calling readstats function `write_stats(dataframe, outputname)`  
* Compute a number of statistics, return a dictionary `calc_read_stats(dataframe)`  


## INSTALLATION
```bash
pip install nanomath
```
or  
[![install with conda](https://anaconda.org/bioconda/nanomath/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanomath)
```
conda install -c bioconda nanomath
```

## STATUS
[![Build Status](https://travis-ci.org/wdecoster/nanomath.svg?branch=master)](https://travis-ci.org/wdecoster/nanomath)


## CONTRIBUTORS
[@alexomics](https://github.com/alexomics) for fixing the indentation of the printed stats


## CITATION
If you use this tool, please consider citing our [publication](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty149/4934939).
  * [nanoplotter-1.10.0](https://github.com/wdecoster/nanoplotter) # nanoplotter
This module provides functions for plotting data extracted from Oxford Nanopore sequencing reads and alignments, but some of it's functions can also be used for other applications.

[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/wouter_decoster.svg?style=social&label=Follow%20%40wouter_decoster)](https://twitter.com/wouter_decoster)
[![install with conda](https://anaconda.org/bioconda/nanoplotter/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanoplotter)
[![Build Status](https://travis-ci.org/wdecoster/nanoplotter.svg?branch=master)](https://travis-ci.org/wdecoster/nanoplotter)



## FUNCTIONS
* Check if a specified color is a valid matplotlib color `checkvalidColor(color)`  
* Check if a specified output format is valid `checkvalidFormat(format)`  
* Create a bivariate plot with dots, hexbins and/or kernel density estimates. Also arguments for specifying axis names, color and xlim/ylim. `scatter(x, y, names, path, color, format, plots, stat=None, log=False, minvalx=0, minvaly=0)`  
* Create cumulative yield plot and evaluate read length and quality over time `timePlots(df, path, color, format)`  
* Create length distribution histogram and density curve `lengthPlots(array, name, path, n50, color, format, log=False)`  
* Create flowcell physical layout in numpy array `makeLayout()`  
* Present the activity (number of reads) per channel on the flowcell as a heatmap `spatialHeatmap(array, title, path, color, format)`  


## INSTALLATION
```bash
pip install nanoplotter
```
or  
[![install with conda](https://anaconda.org/bioconda/nanoplotter/badges/installer/conda.svg)](https://anaconda.org/bioconda/nanoplotter)
```
conda install -c bioconda nanoplotter
```

## CITATION
If you use this tool, please consider citing our [publication](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty149/4934939).
  * [natsort-6.0.0](https://github.com/SethMMorton/natsort) natsort
=======

.. image:: https://img.shields.io/pypi/v/natsort.svg
    :target: https://pypi.org/project/natsort/

.. image:: https://img.shields.io/pypi/pyversions/natsort.svg
    :target: https://pypi.org/project/natsort/

.. image:: https://img.shields.io/pypi/l/natsort.svg
    :target: https://github.com/SethMMorton/natsort/blob/master/LICENSE

.. image:: https://img.shields.io/travis/SethMMorton/natsort/master.svg?label=travis-ci
    :target: https://travis-ci.org/SethMMorton/natsort

.. image:: https://codecov.io/gh/SethMMorton/natsort/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/SethMMorton/natsort

.. image:: https://api.codacy.com/project/badge/Grade/f2bf04b1fc5d4792bf546f6e497cf4b8
    :target: https://www.codacy.com/app/SethMMorton/natsort

Simple yet flexible natural sorting in Python.

    - Source Code: https://github.com/SethMMorton/natsort
    - Downloads: https://pypi.org/project/natsort/
    - Documentation: https://natsort.readthedocs.io/

      - `Examples and Recipes <https://natsort.readthedocs.io/en/master/examples.html>`_
      - `How Does Natsort Work? <https://natsort.readthedocs.io/en/master/howitworks.html>`_
      - `API <https://natsort.readthedocs.io/en/master/api.html>`_

    - `FAQ`_
    - `Optional Dependencies`_

      - `fastnumbers <https://pypi.org/project/fastnumbers>`_ >= 2.0.0
      - `PyICU <https://pypi.org/project/PyICU>`_ >= 1.0.0

**NOTE**: Please see the `Deprecation Schedule`_ section for changes in
``natsort`` version 6.0.0 and in the upcoming version 7.0.0.

Quick Description
-----------------

When you try to sort a list of strings that contain numbers, the normal python
sort algorithm sorts lexicographically, so you might not get the results that you
expect:

.. code-block:: pycon

    >>> a = ['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']
    >>> sorted(a)
    ['1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '2 ft 7 in', '7 ft 6 in']

Notice that it has the order ('1', '10', '2') - this is because the list is
being sorted in lexicographical order, which sorts numbers like you would
letters (i.e. 'b', 'ba', 'c').

``natsort`` provides a function ``natsorted`` that helps sort lists
"naturally" ("naturally" is rather ill-defined, but in general it means
sorting based on meaning and not computer code point).
Using ``natsorted`` is simple:

.. code-block:: pycon

    >>> from natsort import natsorted
    >>> a = ['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']
    >>> natsorted(a)
    ['1 ft 5 in', '2 ft 7 in', '2 ft 11 in', '7 ft 6 in', '10 ft 2 in']

``natsorted`` identifies numbers anywhere in a string and sorts them
naturally. Below are some other things you can do with ``natsort``
(also see the `examples <https://natsort.readthedocs.io/en/master/examples.html>`_
for a quick start guide, or the
`api <https://natsort.readthedocs.io/en/master/api.html>`_ for complete details).

**Note**: ``natsorted`` is designed to be a drop-in replacement for the built-in
``sorted`` function. Like ``sorted``, ``natsorted`` `does not sort in-place`.
To sort a list and assign the output to the same variable, you must
explicitly assign the output to a variable:

.. code-block:: pycon

    >>> a = ['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']
    >>> natsorted(a)
    ['1 ft 5 in', '2 ft 7 in', '2 ft 11 in', '7 ft 6 in', '10 ft 2 in']
    >>> print(a)  # 'a' was not sorted; "natsorted" simply returned a sorted list
    ['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']
    >>> a = natsorted(a)  # Now 'a' will be sorted because the sorted list was assigned to 'a'
    >>> print(a)
    ['1 ft 5 in', '2 ft 7 in', '2 ft 11 in', '7 ft 6 in', '10 ft 2 in']

Please see `Generating a Reusable Sorting Key and Sorting In-Place`_ for
an alternate way to sort in-place naturally.

Examples
--------

Sorting Versions
++++++++++++++++

``natsort`` does not actually *comprehend* version numbers.
It just so happens that the most common versioning schemes are designed to
work with standard natural sorting techniques; these schemes include
``MAJOR.MINOR``, ``MAJOR.MINOR.PATCH``, ``YEAR.MONTH.DAY``. If your data
conforms to a scheme like this, then it will work out-of-the-box with
``natsorted`` (as of ``natsort`` version >= 4.0.0):

.. code-block:: pycon

    >>> a = ['version-1.9', 'version-2.0', 'version-1.11', 'version-1.10']
    >>> natsorted(a)
    ['version-1.9', 'version-1.10', 'version-1.11', 'version-2.0']

If you need to versions that use a more complicated scheme, please see
`these examples <https://natsort.readthedocs.io/en/master/examples.html#rc-sorting>`_.

Sorting by Real Numbers (i.e. Signed Floats)
++++++++++++++++++++++++++++++++++++++++++++

This is useful in scientific data analysis (and was
the default behavior of ``natsorted`` for ``natsort``
version < 4.0.0). Use the ``realsorted`` function:

.. code-block:: pycon

    >>> from natsort import realsorted, ns
    >>> # Note that when interpreting as signed floats, the below numbers are
    >>> #            +5.10,                -3.00,            +5.30,              +2.00
    >>> a = ['position5.10.data', 'position-3.data', 'position5.3.data', 'position2.data']
    >>> natsorted(a)
    ['position2.data', 'position5.3.data', 'position5.10.data', 'position-3.data']
    >>> natsorted(a, alg=ns.REAL)
    ['position-3.data', 'position2.data', 'position5.10.data', 'position5.3.data']
    >>> realsorted(a)  # shortcut for natsorted with alg=ns.REAL
    ['position-3.data', 'position2.data', 'position5.10.data', 'position5.3.data']

Locale-Aware Sorting (or "Human Sorting")
+++++++++++++++++++++++++++++++++++++++++

This is where the non-numeric characters are also ordered based on their meaning,
not on their ordinal value, and a locale-dependent thousands separator and decimal
separator is accounted for in the number.
This can be achieved with the ``humansorted`` function:

.. code-block:: pycon

    >>> a = ['Apple', 'apple15', 'Banana', 'apple14,689', 'banana']
    >>> natsorted(a)
    ['Apple', 'Banana', 'apple14,689', 'apple15', 'banana']
    >>> import locale
    >>> locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
    'en_US.UTF-8'
    >>> natsorted(a, alg=ns.LOCALE)
    ['apple15', 'apple14,689', 'Apple', 'banana', 'Banana']
    >>> from natsort import humansorted
    >>> humansorted(a)  # shortcut for natsorted with alg=ns.LOCALE
    ['apple15', 'apple14,689', 'Apple', 'banana', 'Banana']

You may find you need to explicitly set the locale to get this to work
(as shown in the example).
Please see `locale issues <https://natsort.readthedocs.io/en/master/locale_issues.html>`_ and the
`Optional Dependencies`_ section below before using the ``humansorted`` function.

Further Customizing Natsort
+++++++++++++++++++++++++++

If you need to combine multiple algorithm modifiers (such as ``ns.REAL``,
``ns.LOCALE``, and ``ns.IGNORECASE``), you can combine the options using the
bitwise OR operator (``|``). For example,

.. code-block:: pycon

    >>> a = ['Apple', 'apple15', 'Banana', 'apple14,689', 'banana']
    >>> natsorted(a, alg=ns.REAL | ns.LOCALE | ns.IGNORECASE)
    ['Apple', 'apple15', 'apple14,689', 'Banana', 'banana']
    >>> # The ns enum provides long and short forms for each option.
    >>> ns.LOCALE == ns.L
    True
    >>> # You can also customize the convenience functions, too.
    >>> natsorted(a, alg=ns.REAL | ns.LOCALE | ns.IGNORECASE) == realsorted(a, alg=ns.L | ns.IC)
    True
    >>> natsorted(a, alg=ns.REAL | ns.LOCALE | ns.IGNORECASE) == humansorted(a, alg=ns.R | ns.IC)
    True

All of the available customizations can be found in the documentation for
`the ns enum <https://natsort.readthedocs.io/en/master/api.html#natsort.ns>`_.

You can also add your own custom transformation functions with the ``key`` argument.
These can be used with ``alg`` if you wish.

.. code-block:: pycon

    >>> a = ['apple2.50', '2.3apple']
    >>> natsorted(a, key=lambda x: x.replace('apple', ''), alg=ns.REAL)
    ['2.3apple', 'apple2.50']

Sorting Mixed Types
+++++++++++++++++++

You can mix and match ``int``, ``float``, and ``str`` (or ``unicode``) types
when you sort:

.. code-block:: pycon

    >>> a = ['4.5', 6, 2.0, '5', 'a']
    >>> natsorted(a)
    [2.0, '4.5', '5', 6, 'a']
    >>> # On Python 2, sorted(a) would return [2.0, 6, '4.5', '5', 'a']
    >>> # On Python 3, sorted(a) would raise an "unorderable types" TypeError

Handling Bytes on Python 3
++++++++++++++++++++++++++

``natsort`` does not officially support the `bytes` type on Python 3, but
convenience functions are provided that help you decode to `str` first:

.. code-block:: pycon

    >>> from natsort import as_utf8
    >>> a = [b'a', 14.0, 'b']
    >>> # On Python 2, natsorted(a) would would work as expected.
    >>> # On Python 3, natsorted(a) would raise a TypeError (bytes() < str())
    >>> natsorted(a, key=as_utf8) == [14.0, b'a', 'b']
    True
    >>> a = [b'a56', b'a5', b'a6', b'a40']
    >>> # On Python 2, natsorted(a) would would work as expected.
    >>> # On Python 3, natsorted(a) would return the same results as sorted(a)
    >>> natsorted(a, key=as_utf8) == [b'a5', b'a6', b'a40', b'a56']
    True

Generating a Reusable Sorting Key and Sorting In-Place
++++++++++++++++++++++++++++++++++++++++++++++++++++++

Under the hood, ``natsorted`` works by generating a custom sorting
key using ``natsort_keygen`` and then passes that to the built-in
``sorted``. You can use the ``natsort_keygen`` function yourself to
generate a custom sorting key to sort in-place using the ``list.sort``
method.

.. code-block:: pycon

    >>> from natsort import natsort_keygen
    >>> natsort_key = natsort_keygen()
    >>> a = ['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']
    >>> natsorted(a) == sorted(a, key=natsort_key)
    True
    >>> a.sort(key=natsort_key)
    >>> a
    ['1 ft 5 in', '2 ft 7 in', '2 ft 11 in', '7 ft 6 in', '10 ft 2 in']

All of the algorithm customizations mentioned in the `Further Customizing Natsort`_
section can also be applied to ``natsort_keygen`` through the *alg* keyword option.

Other Useful Things
+++++++++++++++++++

 - recursively descend into lists of lists
 - automatic unicode normalization of input data
 - `controlling the case-sensitivity <https://natsort.readthedocs.io/en/master/examples.html#case-sort>`_
 - `sorting file paths correctly <https://natsort.readthedocs.io/en/master/examples.html#path-sort>`_
 - `allow custom sorting keys <https://natsort.readthedocs.io/en/master/examples.html#custom-sort>`_

FAQ
---

How do I debug ``natsort.natsorted()``?
    The best way to debug ``natsorted()`` is to generate a key using ``natsort_keygen()``
    with the same options being passed to ``natsorted``. One can take a look at
    exactly what is being done with their input using this key - it is highly recommended
    to `look at this issue describing how to debug <https://github.com/SethMMorton/natsort/issues/13#issuecomment-50422375>`_
    for *how* to debug, and also to review the
    `How Does Natsort Work? <https://natsort.readthedocs.io/en/master/howitworks.html>`_
    page for *why* ``natsort`` is doing that to your data.

    If you are trying to sort custom classes and running into trouble, please take a look at
    https://github.com/SethMMorton/natsort/issues/60. In short,
    custom classes are not likely to be sorted correctly if one relies
    on the behavior of ``__lt__`` and the other rich comparison operators in their
    custom class - it is better to use a ``key`` function with ``natsort``, or
    use the ``natsort`` key as part of your rich comparison operator definition.

How *does* ``natsort`` work?
    If you don't want to read `How Does Natsort Work? <https://natsort.readthedocs.io/en/master/howitworks.html>`_,
    here is a quick primer.

    ``natsort`` provides a `key function <https://docs.python.org/3/howto/sorting.html#key-functions>`_
    that can be passed to `list.sort() <https://docs.python.org/3/library/stdtypes.html#list.sort>`_
    or `sorted() <https://docs.python.org/3/library/functions.html#sorted>`_ in order to
    modify the default sorting behavior. This key is generated on-demand with the
    key generator ``natsort.natsort_keygen()``.  ``natsort.natsorted()`` is essentially
    a wrapper for the following code:

    .. code-block:: pycon

        >>> from natsort import natsort_keygen
        >>> natsort_key = natsort_keygen()
        >>> sorted(['1', '10', '2'], key=natsort_key)
        ['1', '2', '10']

    Users can further customize ``natsort`` sorting behavior with the ``key``
    and/or ``alg`` options (see details in the `Further Customizing Natsort`_
    section).

    The key generated by ``natsort_keygen`` *always* returns a ``tuple``. It
    does so in the following way (*some details omitted for clarity*):

      1. Assume the input is a string, and attempt to split it into numbers and
         non-numbers using regular expressions. Numbers are then converted into
         either ``int`` or ``float``.
      2. If the above fails because the input is not a string, assume the input
         is some other sequence (e.g. ``list`` or ``tuple``), and recursively
         apply the key to each element of the sequence.
      3. If the above fails because the input is not iterable, assume the input
         is an ``int`` or ``float``, and just return the input in a ``tuple``.

    Because a ``tuple`` is always returned, a ``TypeError`` should not be common
    unless one tries to do something odd like sort an ``int`` against a ``list``.

``natsort`` gave me results I didn't expect, and it's a terrible library!
    Did you try to debug using the above advice? If so, and you still cannot figure out
    the error, then please `file an issue <https://github.com/SethMMorton/natsort/issues/new>`_.

Shell script
------------

``natsort`` comes with a shell script called ``natsort``, or can also be called
from the command line with ``python -m natsort``.

Requirements
------------

``natsort`` requires Python version 2.7 or Python 3.4 or greater.

Optional Dependencies
---------------------

fastnumbers
+++++++++++

The most efficient sorting can occur if you install the
`fastnumbers <https://pypi.org/project/fastnumbers>`_ package
(version >=2.0.0); it helps with the string to number conversions.
``natsort`` will still run (efficiently) without the package, but if you need
to squeeze out that extra juice it is recommended you include this as a dependency.
``natsort`` will not require (or check) that
`fastnumbers <https://pypi.org/project/fastnumbers>`_ is installed
at installation.

PyICU
+++++

It is recommended that you install `PyICU <https://pypi.org/project/PyICU>`_
if you wish to sort in a locale-dependent manner, see
https://natsort.readthedocs.io/en/master/locale_issues.html for an explanation why.

Installation
------------

Use ``pip``!

.. code-block:: console

    $ pip install natsort

If you want to install the `Optional Dependencies`_, you can use the
`"extras" notation <https://packaging.python.org/tutorials/installing-packages/#installing-setuptools-extras>`_
at installation time to install those dependencies as well - use ``fast`` for
`fastnumbers <https://pypi.org/project/fastnumbers>`_ and ``icu`` for
`PyICU <https://pypi.org/project/PyICU>`_.

.. code-block:: console

    # Install both optional dependencies.
    $ pip install natsort[fast,icu]
    # Install just fastnumbers
    $ pip install natsort[fast]

How to Run Tests
----------------

Please note that ``natsort`` is NOT set-up to support ``python setup.py test``.

The recommended way to run tests is with `tox <https://tox.readthedocs.io/en/latest/>`_.
After installing ``tox``, running tests is as simple as executing the following in the
``natsort`` directory:

.. code-block:: console

    $ tox

``tox`` will create virtual a virtual environment for your tests and install all the
needed testing requirements for you.  You can specify a particular python version
with the ``-e`` flag, e.g. ``tox -e py36``. Static analysis is done with ``tox -e flake8``.
You can see all available testing environments with ``tox --listenvs``.

If you do not wish to use ``tox``, you can install the testing dependencies with the
``dev-requirements.txt`` file and then run the tests manually using
`pytest <https://docs.pytest.org/en/latest/>`_.

.. code-block:: console

    $ pip install -r dev-requirements.txt
    $ python -m pytest

Note that above I invoked ``python -m pytest`` instead of just ``pytest`` - this is because
`the former puts the CWD on sys.path <https://docs.pytest.org/en/latest/usage.html#calling-pytest-through-python-m-pytest>`_.

How to Build Documentation
--------------------------

If you want to build the documentation for ``natsort``, it is recommended to use ``tox``:

.. code-block:: console

    $ tox -e docs

This will place the documentation in ``build/sphinx/html``.  If you do not
which to use ``tox``, you can do the following:

.. code-block:: console

    $ pip install sphinx sphinx_rtd_theme
    $ python setup.py build_sphinx

Deprecation Schedule
--------------------

Dropping Python 2.7 Support
+++++++++++++++++++++++++++

``natsort`` version 7.0.0 will drop support for Python 2.7.

The version 6.X branch will remain as a "long term support" branch where bug fixes
are applied so that users who cannot update from Python 2.7 will not be forced to
use a buggy ``natsort`` version. Once version 7.0.0 is released, new features
will not be added to version 6.X, only bug fixes.

Deprecated APIs
+++++++++++++++

In ``natsort`` version 6.0.0, the following APIs and functions were removed

 - ``number_type`` keyword argument (deprecated since 3.4.0)
 - ``signed`` keyword argument (deprecated since 3.4.0)
 - ``exp`` keyword argument (deprecated since 3.4.0)
 - ``as_path`` keyword argument (deprecated since 3.4.0)
 - ``py3_safe`` keyword argument (deprecated since 3.4.0)
 - ``ns.TYPESAFE`` (deprecated since version 5.0.0)
 - ``ns.DIGIT`` (deprecated since version 5.0.0)
 - ``ns.VERSION`` (deprecated since version 5.0.0)
 - ``versorted()`` (discouraged since version 4.0.0, officially deprecated since version 5.5.0)
 - ``index_versorted()`` (discouraged since version 4.0.0, officially deprecated since version 5.5.0)

In general, if you want to determine if you are using deprecated APIs you can run your
code with the following flag

.. code-block:: console

    $ python -Wdefault::DeprecationWarning my-code.py

By default ``DeprecationWarnings`` are not shown, but this will cause them to be shown.
Alternatively, you can just set the environment variable ``PYTHONWARNINGS`` to
"default::DeprecationWarning" and then run your code.

Dropped Pipenv for Development
++++++++++++++++++++++++++++++

``natsort`` version 6.0.0 no longer uses `Pipenv <https://pipenv.readthedocs.io/en/latest/>`_
to install development dependencies.

Dropped Python 2.6 and 3.3 Support
++++++++++++++++++++++++++++++++++

``natsort`` version 6.0.0 dropped support for Python 2.6 and Python 3.3.

Author
------

Seth M. Morton

History
-------

Please visit the changelog
`on GitHub <https://github.com/SethMMorton/natsort/blob/master/CHANGELOG.rst>`_ or
`in the documentation <https://natsort.readthedocs.io/en/master/changelog.html>`_.



  * [nbconvert-5.6.0](https://jupyter.org) # nbconvert
### Jupyter Notebook Conversion

[![Google Group](https://img.shields.io/badge/-Google%20Group-lightgrey.svg)](https://groups.google.com/forum/#!forum/jupyter)
[![Build Status](https://travis-ci.org/jupyter/nbconvert.svg?branch=master)](https://travis-ci.org/jupyter/nbconvert)
[![Documentation Status](https://readthedocs.org/projects/nbconvert/badge/?version=latest)](https://nbconvert.readthedocs.io/en/latest/?badge=latest)
[![Documentation Status](https://readthedocs.org/projects/nbconvert/badge/?version=stable)](http://nbconvert.readthedocs.io/en/stable/?badge=stable)
[![codecov.io](https://codecov.io/github/jupyter/nbconvert/coverage.svg?branch=master)](https://codecov.io/github/jupyter/nbconvert?branch=master)


The **nbconvert** tool, `jupyter nbconvert`, converts notebooks to various other
formats via [Jinja][] templates. The nbconvert tool allows you to convert an
`.ipynb` notebook file into various static formats including:

* HTML
* LaTeX
* PDF
* Reveal JS
* Markdown (md)
* ReStructured Text (rst)
* executable script


## Usage

From the command line, use nbconvert to convert a Jupyter notebook (*input*) to a
a different format (*output*). The basic command structure is:

    $ jupyter nbconvert --to <output format> <input notebook>

where `<output format>` is the desired output format and `<input notebook>` is the
filename of the Jupyter notebook.

### Example: Convert a notebook to HTML

Convert Jupyter notebook file, `mynotebook.ipynb`, to HTML using:

    $ jupyter nbconvert --to html mynotebook.ipynb

This command creates an HTML output file named `mynotebook.html`.

## Dev Install
Check if pandoc is installed (``pandoc --version``); if needed, install:

```
sudo apt-get install pandoc
```

Or

```
brew install pandoc
```

Install nbconvert for development using:

```
git clone https://github.com/jupyter/nbconvert.git
cd nbconvert
pip install -e .
```

Running the tests after a dev install above:

```
pip install nbconvert[test]
py.test --pyargs nbconvert
```


## Resources

- [Documentation for Jupyter nbconvert](https://nbconvert.readthedocs.io/en/latest/)
  [[PDF](https://media.readthedocs.org/pdf/nbconvert/latest/nbconvert.pdf)]
- [nbconvert examples on GitHub](https://github.com/jupyter/nbconvert-examples)
- [Issues](https://github.com/jupyter/nbconvert/issues)
- [Technical support - Jupyter Google Group](https://groups.google.com/forum/#!forum/jupyter)
- [Project Jupyter website](https://jupyter.org)
- [Documentation for Project Jupyter](https://jupyter.readthedocs.io/en/latest/index.html)
  [[PDF](https://media.readthedocs.org/pdf/jupyter/latest/jupyter.pdf)]


[Jinja]: http://jinja.pocoo.org/



  * [nbformat-4.4.0](http://jupyter.org) 
This package contains the base implementation of the Jupyter Notebook format,
and Python APIs for working with notebooks.



  * [nbval-0.9.2](https://github.com/computationalmodelling/nbval) # Py.test plugin for validating Jupyter notebooks

[![Build Status](https://travis-ci.org/computationalmodelling/nbval.svg)](https://travis-ci.org/computationalmodelling/nbval)
[![PyPI Version](https://badge.fury.io/py/nbval.svg)](https://pypi.python.org/pypi/nbval)
[![Documentation Status](https://readthedocs.org/projects/nbval/badge/)](https://nbval.readthedocs.io/)

The plugin adds functionality to py.test to recognise and collect Jupyter
notebooks. The intended purpose of the tests is to determine whether execution
of the stored inputs match the stored outputs of the `.ipynb` file. Whilst also
ensuring that the notebooks are running without errors.

The tests were designed to ensure that Jupyter notebooks (especially those for
reference and documentation), are executing consistently.

Each cell is taken as a test, a cell that doesn't reproduce the expected
output will fail.

See [`docs/source/index.ipynb`](http://nbviewer.jupyter.org/github/computationalmodelling/nbval/blob/master/docs/source/index.ipynb) for the full documentation.

## Installation
Available on PyPi:

    pip install nbval

or install the latest version from cloning the repository and running:

    pip install .

from the main directory. To uninstall:

    pip uninstall nbval


## How it works
The extension looks through every cell that contains code in an IPython notebook
and then the `py.test` system compares the outputs stored in the notebook
with the outputs of the cells when they are executed. Thus, the notebook itself is
used as a testing function.
The output lines when executing the notebook can be sanitized passing an
extra option and file, when calling the `py.test` command. This file
is a usual configuration file for the `ConfigParser` library.

Regarding the execution, roughly, the script initiates an
IPython Kernel with a `shell` and
an `iopub` sockets. The `shell` is needed to execute the cells in
the notebook (it sends requests to the Kernel) and the `iopub` provides
an interface to get the messages from the outputs. The contents
of the messages obtained from the Kernel are organised in dictionaries
with different information, such as time stamps of executions,
cell data types, cell types, the status of the Kernel, username, etc.

In general, the functionality of the IPython notebook system is
quite complex, but a detailed explanation of the messages
and how the system works, can be found here

https://jupyter-client.readthedocs.io/en/latest/messaging.html#messaging

## Execution
To execute this plugin, you need to execute `py.test` with the `nbval` flag
to differentiate the testing from the usual python files:

    py.test --nbval

You can also specify `--nbval-lax`, which runs notebooks and checks for
errors, but only compares the outputs of cells with a `#NBVAL_CHECK_OUTPUT`
marker comment.

    py.test --nbval-lax

The commands above will execute all the `.ipynb` files and 'pytest' tests in the current folder.
Specify `-p no:python` if you would like to execute notebooks only. Alternatively, you can execute a specific notebook:

    py.test --nbval my_notebook.ipynb

If the output lines are going to be sanitized, an extra flag, `--sanitize-with`
together with the path to a confguration file with regex expressions, must be passed,
i.e.

    py.test --nbval my_notebook.ipynb --sanitize-with path/to/my_sanitize_file

where `my_sanitize_file` has the following structure.

```
[Section1]
regex: [a-z]*
replace: abcd

regex: [1-9]*
replace: 0000

[Section2]
regex: foo
replace: bar
```

The `regex` option contains the expression that is going to be matched in the outputs, and
`replace` is the string that will replace the `regex` match. Currently, the section
names do not have any meaning or influence in the testing system, it will take
all the sections and replace the corresponding options.


### Coverage

To use notebooks to generate coverage for imported code, use the pytest-cov plugin.
nbval should automatically detect the relevant options and configure itself with it.


### Parallel execution

nbval is compatible with the pytest-xdist plugin for parallel running of tests. It does
however require the use of the `--dist loadscope` flag to ensure that all cells of one
notebook are run on the same kernel.


## Help
The `py.test` system help can be obtained with `py.test -h`, which will
show all the flags that can be passed to the command, such as the
verbose `-v` option. The IPython notebook plugin can be found under the
`general` section.


## Acknowledgements
This plugin was inspired by Andrea Zonca's py.test plugin for collecting unit
tests in the IPython notebooks (https://github.com/zonca/pytest-ipynb).

The original prototype was based on the template in
https://gist.github.com/timo/2621679 and the code of a testing system
for notebooks https://gist.github.com/minrk/2620735 which we
integrated and mixed with the `py.test` system.

We acknowledge financial support from

- OpenDreamKit Horizon 2020 European Research Infrastructures project (#676541), http://opendreamkit.org

- EPSRC’s Centre for Doctoral Training in Next Generation
  Computational Modelling, http://ngcm.soton.ac.uk (#EP/L015382/1) and
  EPSRC’s Doctoral Training Centre in Complex System Simulation
  ((EP/G03690X/1),

- The Gordon and Betty Moore Foundation through Grant GBMF #4856,by the
  Alfred P. Sloan Foundation and by the Helmsley Trust.


## Authors

2014 - 2017 David Cortes-Ortuno, Oliver Laslett, T. Kluyver, Vidar
Fauske, Maximilian Albert, MinRK, Ondrej Hovorka, Hans Fangohr



  * [ndg-httpsclient-0.5.1](https://github.com/cedadev/ndg_httpsclient/) A HTTPS client implementation for 
 * ``httplib`` (Python 2), ``http.client`` (Python 3) and 
 * ``urllib2`` (Python 2) and ``urllib`` (Python 3)

... based on PyOpenSSL.  PyOpenSSL provides a more fully featured SSL implementation 
over the default provided with Python and importantly enables full verification 
of the SSL peer using ``pyasn1``.

Releases
========
0.5.1
-----
 * Clean up handling for description file - pull in content from this file into setup()
 * Allows the nightly build to fail
 * Add Trove version classifiers to make it explicit what is supported
 * Add python_requires to help pip
 * Drop support for EOL Python 2.6 and 3.3  
  
Thanks to @hugovk for contributions

0.5.0
-----
 * Fix to Subject Alternative Name handling to allow for certificates with
   more than 64 names (max now 1024).  Thanks to Matt Pegler
 * Fix to subjectAltName string to use byte type for correct matching 
 * Updated SSL Context objects to default to TLS 1.2

0.4.4
-----
 * Updated test certificates

0.4.3
-----
 * Fix to ``ndg`` namespace package warning issue (https://github.com/cedadev/ndg_httpsclient/issues/3).  
   ``__init__.py`` file now included in ``ndg`` directory so that there are no longer warnings with imports
   when using Python 2.x.  Thanks to Max Mauntner for fix.
 * Minor fix for installation: set minimum release for ``pyasn1`` to avoid conflicts with Ubuntu
   install - see https://github.com/cedadev/ndg_httpsclient/issues/5 and
   https://github.com/cedadev/ndg_httpsclient/pull/10.  ``pyasn1`` also becomes mandatory rather
   than optional package for install.  - It required by ``cryptography`` anyway which is a 
   dependency for ``pyOpenSSL`` from version 0.14.

0.4.2
-----
 * Fix to bug in ``ndg.httpsclient.utils.open_url`` - duplicate open call.  
   Nb. This bug and the fix DO NOT affect the ``httplib``and ``urllib2`` 
   interfaces that this package provides.
 
0.4.1
-----
 * Added explicit ref to Python 3 in classifier text for Python 3 checking tools.
 * Moved LICENSE file into package

0.4.0
-----
 * Made dual compatible with Python 2 / 3.
 
0.3.3
-----
 * Fix to add in AnotherName for ``subjectAltNames`` field - added for support for CACert issued
   certs (thanks to Gu1).
 * Fix to HTTP Basic Auth option for ``ndg.httpsclient.utils.main``
 * Fix to ``ServerSSLCertVerification`` so that it can pass a function-based callback instead of using ``__call__``. In newer versions of OpenSSL (>= 0.14) the latter failed because of a request for ``__name__`` attribute.

0.3.2
-----
 * Fix to SubjectAltNames support check - should only be enabled if pyasn1 is 
   installed.
 * Fix to open_url: HTTP Request object was being created inside if headers is 
   None block - now corrected to create regardless.
 * Added http basic auth support to script. (Thanks to Willem van Engen)
 
0.3.1
-----
 * extended utils functions to support keyword for passing additional ``urllib2``
   handlers.

0.3.0
-----
 * Added ``ndg.httpsclient.utils.fetch_stream_from_url`` function and added
   parameter for data to post in ``open_url`` and ``fetch_*`` methods.
 * fix to ndg.httpsclient.utils module _should_use_proxy and open_url functions

0.2.0
-----
 * added support for SSL verification with subjectAltNames using pyasn1
 * fixed minor bug - SSL cert DN prefix matching

0.1.0
-----
Initial release

Prerequisites
=============
This has been developed and tested for Python 2.7 with pyOpenSSL 0.13 
and 0.14.  Version 0.4.0 tested with ``pyOpenSSL`` 0.15.1 and Python 2.7 and 
3.4. 
``pyasn1`` is required for correct SSL verification with ``subjectAltNames``.

Installation
============
Installation can be performed using ``easy_install`` or ``pip``.

Running ndg_httpclient
======================
A simple script for fetching data using HTTP or HTTPS GET from a specified URL.

Parameter:

``url``
  The URL of the resource to be fetched

Options:

``-h, --help``
  Show help message and exit.

``-c FILE, --certificate=FILE``
  Certificate file - defaults to ``$HOME/credentials.pem``

``-k FILE, --private-key=FILE``
  Private key file - defaults to the certificate file

``-t DIR, --ca-certificate-dir=DIR``
  Trusted CA certificate file directory.

``-d, --debug``
  Print debug information - this may be useful in solving problems with HTTP or 
  HTTPS access to a server.
    
``-p FILE, --post-data-file=FILE``
  POST data file
    
``-f FILE, --fetch=FILE``
  Output file
    
``-n, --no-verify-peer``
  Skip verification of peer certificate.
  

  * [neo4j-driver-1.7.4](https://github.com/neo4j/neo4j-python-driver) ****************************
Neo4j Bolt Driver for Python
****************************

The official Neo4j driver for Python supports Neo4j 3.0 and above and Python versions 2.7, 3.4, 3.5, 3.6, and 3.7.

.. note::

    Python 2 support is deprecated and will be discontinued in the 2.x series driver releases.


Quick Example
=============

.. code-block:: python

    from neo4j import GraphDatabase

    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))

    def add_friend(tx, name, friend_name):
        tx.run("MERGE (a:Person {name: $name}) "
               "MERGE (a)-[:KNOWS]->(friend:Person {name: $friend_name})",
               name=name, friend_name=friend_name)

    def print_friends(tx, name):
        for record in tx.run("MATCH (a:Person)-[:KNOWS]->(friend) WHERE a.name = $name "
                             "RETURN friend.name ORDER BY friend.name", name=name):
            print(record["friend.name"])

    with driver.session() as session:
        session.write_transaction(add_friend, "Arthur", "Guinevere")
        session.write_transaction(add_friend, "Arthur", "Lancelot")
        session.write_transaction(add_friend, "Arthur", "Merlin")
        session.read_transaction(print_friends, "Arthur")


Installation
============

To install the latest stable version, use:

.. code:: bash

    pip install neo4j

.. note::

    Installation from the ``neo4j-driver`` package on PyPI is now deprecated and will be discontinued in the 2.x series driver releases.
    Please install from the ``neo4j`` package instead.

For the most up-to-date version (generally unstable), use:

.. code:: bash

    pip install git+https://github.com/neo4j/neo4j-python-driver.git#egg=neo4j


Other Information
=================

* `Neo4j Manual`_
* `Neo4j Quick Reference Card`_
* `Example Project`_
* `Driver Wiki`_ (includes change logs)

.. _`Neo4j Manual`: https://neo4j.com/docs/developer-manual/current/drivers/
.. _`Neo4j Quick Reference Card`: https://neo4j.com/docs/cypher-refcard/current/
.. _`Example Project`: https://github.com/neo4j-examples/movies-python-bolt
.. _`Driver Wiki`: https://github.com/neo4j/neo4j-python-driver/wiki

  * [netCDF4-1.5.1.2](http://github.com/Unidata/netcdf4-python) netCDF version 4 has many features not found in earlier versions of the library, such as hierarchical groups, zlib compression, multiple unlimited dimensions, and new data types.  It is implemented on top of HDF5.  This module implements most of the new features, and can read and write netCDF files compatible with older versions of the library.  The API is modelled after Scientific.IO.NetCDF, and should be familiar to users of that module.

This project is hosted on a `GitHub repository <https://github.com/Unidata/netcdf4-python>`_ where you may access the most up-to-date source.


  * [netaddr-0.7.19](https://github.com/drkjam/netaddr/) Provides support for:

Layer 3 addresses
-----------------

- IPv4 and IPv6 addresses, subnets, masks, prefixes
- iterating, slicing, sorting, summarizing and classifying IP networks
- dealing with various ranges formats (CIDR, arbitrary ranges and globs, nmap)
- set based operations (unions, intersections etc) over IP addresses and subnets
- parsing a large variety of different formats and notations
- looking up IANA IP block information
- generating DNS reverse lookups
- supernetting and subnetting

Layer 2 addresses
-----------------

- representation and manipulation MAC addresses and EUI-64 identifiers
- looking up IEEE organisational information (OUI, IAB)
- generating derived IPv6 addresses

Documentation
-------------
  * [netifaces-0.10.7](https://github.com/al45tair/netifaces) netifaces 0.10.8
================

+-------------+------------------+
| Linux/macOS | |BuildStatus|    |
+-------------+------------------+
| Windows     | |WinBuildStatus| |
+-------------+------------------+

.. |BuildStatus| image:: https://travis-ci.org/al45tair/netifaces.svg?branch=master
   :target: https://travis-ci.org/al45tair/dmgbuild
   :alt: Build Status (Linux/Mac)

.. |WinBuildStatus| image:: https://ci.appveyor.com/api/projects/status/3ctn1bl0aigpfjoo/branch/master?svg=true
   :target: https://ci.appveyor.com/project/al45tair/netifaces/branch/master
   :alt: Build Status (Windows)

1. What is this?
----------------

It's been annoying me for some time that there's no easy way to get the
address(es) of the machine's network interfaces from Python.  There is
a good reason for this difficulty, which is that it is virtually impossible
to do so in a portable manner.  However, it seems to me that there should
be a package you can easy_install that will take care of working out the
details of doing so on the machine you're using, then you can get on with
writing Python code without concerning yourself with the nitty gritty of
system-dependent low-level networking APIs.

This package attempts to solve that problem.

2. How do I use it?
-------------------

First you need to install it, which you can do by typing::

  tar xvzf netifaces-0.10.8.tar.gz
  cd netifaces-0.10.8
  python setup.py install

**Note that you will need the relevant developer tools for your platform**,
as netifaces is written in C and installing this way will compile the extension.

Once that's done, you'll need to start Python and do something like the
following::

>>> import netifaces

Then if you enter

>>> netifaces.interfaces()
['lo0', 'gif0', 'stf0', 'en0', 'en1', 'fw0']

you'll see the list of interface identifiers for your machine.

You can ask for the addresses of a particular interface by doing

>>> netifaces.ifaddresses('lo0')
{18: [{'addr': ''}], 2: [{'peer': '127.0.0.1', 'netmask': '255.0.0.0', 'addr': '127.0.0.1'}], 30: [{'peer': '::1', 'netmask': 'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', 'addr': '::1'}, {'peer': '', 'netmask': 'ffff:ffff:ffff:ffff::', 'addr': 'fe80::1%lo0'}]}

Hmmmm.  That result looks a bit cryptic; let's break it apart and explain
what each piece means.  It returned a dictionary, so let's look there first::

  { 18: [...], 2: [...], 30: [...] }

Each of the numbers refers to a particular address family.  In this case, we
have three address families listed; on my system, 18 is ``AF_LINK`` (which means
the link layer interface, e.g. Ethernet), 2 is ``AF_INET`` (normal Internet
addresses), and 30 is ``AF_INET6`` (IPv6).

But wait!  Don't use these numbers in your code.  The numeric values here are
system dependent; fortunately, I thought of that when writing netifaces, so
the module declares a range of values that you might need.  e.g.

>>> netifaces.AF_LINK
18

Again, on your system, the number may be different.

So, what we've established is that the dictionary that's returned has one
entry for each address family for which this interface has an address.  Let's
take a look at the ``AF_INET`` addresses now:

>>> addrs = netifaces.ifaddresses('lo0')
>>> addrs[netifaces.AF_INET]
[{'peer': '127.0.0.1', 'netmask': '255.0.0.0', 'addr': '127.0.0.1'}]

You might be wondering why this value is a list.  The reason is that it's
possible for an interface to have more than one address, even within the
same family.  I'll say that again: *you can have more than one address of
the same type associated with each interface*.

*Asking for "the" address of a particular interface doesn't make sense.*

Right, so, we can see that this particular interface only has one address,
and, because it's a loopback interface, it's point-to-point and therefore
has a *peer* address rather than a broadcast address.

Let's look at a more interesting interface.

>>> addrs = netifaces.ifaddresses('en0')
>>> addrs[netifaces.AF_INET]
[{'broadcast': '10.15.255.255', 'netmask': '255.240.0.0', 'addr': '10.0.1.4'}, {'broadcast': '192.168.0.255', 'addr': '192.168.0.47'}]

This interface has two addresses (see, I told you...)  Both of them are
regular IPv4 addresses, although in one case the netmask has been changed
from its default.  The netmask *may not* appear on your system if it's set
to the default for the address range.

Because this interface isn't point-to-point, it also has broadcast addresses.

Now, say we want, instead of the IP addresses, to get the MAC address; that
is, the hardware address of the Ethernet adapter running this interface.  We
can do

>>> addrs[netifaces.AF_LINK]
[{'addr': '00:12:34:56:78:9a'}]

Note that this may not be available on platforms without getifaddrs(), unless
they happen to implement ``SIOCGIFHWADDR``.  Note also that you just get the
address; it's unlikely that you'll see anything else with an ``AF_LINK`` address.
Oh, and don't assume that all ``AF_LINK`` addresses are Ethernet; you might, for
instance, be on a Mac, in which case:

>>> addrs = netifaces.ifaddresses('fw0')
>>> addrs[netifaces.AF_LINK]
[{'addr': '00:12:34:56:78:9a:bc:de'}]

No, that isn't an exceptionally long Ethernet MAC address---it's a FireWire
address.

As of version 0.10.0, you can also obtain a list of gateways on your
machine:

>>> netifaces.gateways()
{2: [('10.0.1.1', 'en0', True), ('10.2.1.1', 'en1', False)], 30: [('fe80::1', 'en0', True)], 'default': { 2: ('10.0.1.1', 'en0'), 30: ('fe80::1', 'en0') }}

This dictionary is keyed on address family---in this case, ``AF_INET``---and
each entry is a list of gateways as ``(address, interface, is_default)`` tuples.
Notice that here we have two separate gateways for IPv4 (``AF_INET``); some
operating systems support configurations like this and can either route packets
based on their source, or based on administratively configured routing tables.

For convenience, we also allow you to index the dictionary with the special
value ``'default'``, which returns a dictionary mapping address families to the
default gateway in each case.  Thus you can get the default IPv4 gateway with

>>> gws = netifaces.gateways()
>>> gws['default'][netifaces.AF_INET]
('10.0.1.1', 'en0')

Do note that there may be no default gateway for any given address family;
this is currently very common for IPv6 and much less common for IPv4 but it
can happen even for ``AF_INET``.

BTW, if you're trying to configure your machine to have multiple gateways for
the same address family, it's a very good idea to check the documentation for
your operating system *very* carefully, as some systems become extremely
confused or route packets in a non-obvious manner.

I'm very interested in hearing from anyone (on any platform) for whom the
``gateways()`` method doesn't produce the expected results.  It's quite
complicated extracting this information from the operating system (whichever
operating system we're talking about), and so I expect there's at least one
system out there where this just won't work.

3. This is great!  What platforms does it work on?
--------------------------------------------------

It gets regular testing on OS X, Linux and Windows.  It has also been used
successfully on Solaris, and it's expected to work properly on other UNIX-like
systems as well.  If you are running something that is not supported, and
wish to contribute a patch, please use Github to send a pull request.

4. What license is this under?
------------------------------

It's an MIT-style license. See `LICENSE <./LICENSE>`_.

5. Why the jump to 0.10.0?
--------------------------

Because someone released a fork of netifaces with the version 0.9.0.
Hopefully skipping the version number should remove any confusion.  In
addition starting with 0.10.0 Python 3 is now supported and other
features/bugfixes have been included as well.  See the CHANGELOG for a
more complete list of changes.



  * [networkx-2.3](http://networkx.github.io/) NetworkX is a Python package for the creation, manipulation, and
study of the structure, dynamics, and functions of complex networks.
  * [nltk-3.4.4](http://nltk.org/) The Natural Language Toolkit (NLTK) is a Python package for
natural language processing.  NLTK requires Python 2.7, 3.5, 3.6, or 3.7.
  * [nmslib-1.8.1](https://github.com/searchivarius/nmslib) Non-Metric Space Library (NMSLIB) is an efficient cross-platform
 similarity search library and a toolkit for evaluation of similarity search methods. 
 The goal of the project is to create an effective and comprehensive toolkit for searching 
 in generic and non-metric spaces. Even though the library contains a variety of metric-space 
 access methods, our main focus is on generic and approximate search methods, in particular, 
 on methods for non-metric spaces. NMSLIB is possibly the first library with a principled 
 support for non-metric space searching.
  * [nose-1.3.7](http://readthedocs.org/docs/nose/) nose extends the test loading and running features of unittest, making
    it easier to write, find and run tests.

    By default, nose will run tests in files or directories under the current
    working directory whose names include "test" or "Test" at a word boundary
    (like "test_this" or "functional_test" or "TestClass" but not
    "libtest"). Test output is similar to that of unittest, but also includes
    captured stdout output from failing tests, for easy print-style debugging.

    These features, and many more, are customizable through the use of
    plugins. Plugins included with nose provide support for doctest, code
    coverage and profiling, flexible attribute-based test selection,
    output capture and more. More information about writing plugins may be
    found on in the nose API documentation, here:
    http://readthedocs.org/docs/nose/

    If you have recently reported a bug marked as fixed, or have a craving for
    the very latest, you may want the development version instead:
    https://github.com/nose-devs/nose/tarball/master#egg=nose-dev
  * [nose-exclude-0.5.0](https://github.com/kgrandis/nose-exclude) Overview
========

nose-exclude is a `Nose`_ plugin that allows you to easily specify
directories to be excluded from testing.

.. _Nose: http://somethingaboutorange.com/mrl/projects/nose


Exclude Directories
===================

The ``--exclude-dir=`` option is made available after installation of the
plugin. The option may be used multiple times to exclude multiple directories 
from testing. The directory paths provided may be absolute or relative.

Example::

    $ nosetests --exclude-dir=test_dirs/build \
        --exclude-dir=test_dirs/test_not_me test_dirs
    ....
    ----------------------------------------------------------------------
    Ran 4 tests in 0.006s

    OK

This example will exclude the directories test_dirs/build and
test_dirs/test_not_me from nosetests' test searching.

Using File-Based Exclusion List
-------------------------------

The ``--exclude-dir-file=`` option can be used to pass in a predefined
list of directories contained within a file. ``nose-exclude`` expects each
directory to be excluded to be on its own line.

Example::

    $ nosetests --exclude-dir-file=test_dirs/exclude_dirs.txt \
        test_dirs
    ....
    ----------------------------------------------------------------------
    Ran 4 tests in 0.006s

    OK

where ``exclude_dirs.txt`` might look like: ::

    test_dirs/build
    # Start a line with a '#' to include
    # Comments
    test_dirs/test_not_me


Excluding Specific Test Methods and Classes
-------------------------------------------

Tests can now be excluded by specifying their fully qualified test paths.
Tests can be excluded using either ``--exclude-test`` or ``--exclude-test-file``.

To exclude test methods:

``--exclude-test=module1.module2.TestClass.test_method``

To exclude test classes:

``--exclude-test=module1.module2.TestClass``

To exclude test functions:

``--exclude-test=module1.module2.test_function``


Using Environment Variables
---------------------------

``--exclude-dir=`` and ``--exclude-test=`` can be set by the environment
variables ``NOSE_EXCLUDE_DIRS`` and ``NOSE_EXCLUDE_TESTS`` respectively.
Multiple exclude paths may be entered by separating them using a ``;``. The
environment variable ``NOSE_EXCLUDE_DIRS_FILE`` when set to the path of a
file-based exclusion list functions as though it were passed in with
``--exclude-dir-file=``.

Nose Configuration Files
========================

``nose-exclude`` options can also be passed to ``nosetests`` using a ``.noserc`` or ``nose.cfg`` file. If you more than one directory are to be excluded 
separate their values with newlines using the same configuration key: ::

    [nosetests]
    exclude-dir=test_dirs/exclude_dirs
                test_dirs/more_excludes



Bugs
====
Please report all bugs (and patches) to https://github.com/kgrandis/nose-exclude/

NOTE: The previous bitbucket repository is no longer actively maintained.
  * [nose_warnings_filters-0.1.5]() Put the same arguments as ``warnings.filterwarnings`` in ``setup.cfg``
at the root of your project. Separated each argument by pipes ``|``, one
filter per line. Whitespace are stripped.

for example:

::

    [nosetests]
    warningfilters=default         |.*            |DeprecationWarning |notebook.*
                   ignore          |.*metadata.*  |DeprecationWarning |notebook.*
                   once            |.*schema.*    |UserWarning        |nbfor.*
                   error           |.*warn.*      |DeprecationWarning |notebook.services.contents.manager*

If you prefer another name for the configuration file, you can tell nose
to load the configuration using the ``-c`` flag: run the tests with
``nosetests -c nose.cfg``.

details configuration.
======================

Each line of warning filter is separated in maximum 4 sections, that
match the first 4 sections of ``filterwarnings``:

.. code:: python

    filterwarnings(action, message="", category=Warning, module="", lineno=0, append=False)

fields 2 to 4 can be omitted, ie to say 1 line can be of the following
form:

::

    action
    action| message
    action| message | category
    action| message | category | module

the value of each fields is treated the same as for ``filterwarnigns``
except: - whitespace are trimmed. - if the ``category`` has dots, the
corresponding class try to be imported. If it does not have dots, the
name is looked up in ``builtins`` or ``__builtins__``

test are failing
================

For some reasons in some systems tests are failing; it seem that this
package have difficulty to self-test. That's likely due to the fact that
the tested package need to be in different namespaces, and by
self-testing we break this assumption.

Home-page: https://github.com/Carreau/nose_warnings_filters
Author: Matthias Bussonnier
Author-email: bussonniermatthias@gmail.com
License: MIT
Description: UNKNOWN
Platform: UNKNOWN
Requires-Python: >=2.7

  * [notebook-6.0.0](http://jupyter.org) 
The Jupyter Notebook is a web application that allows you to create and
share documents that contain live code, equations, visualizations, and
explanatory text. The Notebook has support for multiple programming
languages, sharing, and interactive widgets.

Read `the documentation <https://jupyter-notebook.readthedocs.io>`_
for more information.



  * [novaclient-auth-secretkey-0.1](https://github.com/fr33jc/novaclient-auth-secretkey) novaclient-auth-secretkey
=========================


This is an authentication plugin for python-novaclient.  It allows
novaclient users to specify an API key+secret key as credentials to
the Keystone authentication service instead of the normal
username+password.

Plugin users can specify the credentials either programmatically as
attributes of the httpclient object, or using environment variables (e.g.
when using the command-line nova client).
  * [npyscreen-4.10.5](http://www.npcole.com/npyscreen/) This library provides a framework for developing console applications using Python and curses.

This framework should be powerful enough to create everything from quick, simple programs to complex, multi-screen applications. It is designed to make doing the simple tasks very quick and to take much of the pain out of writing larger applications.

There is a very wide variety of default widgets - everything from simple text fields to more complex tree and grid views.

I have used versions of this library for private scripts and small applications for around ten years. As a result, it is fairly mature.	

Documentation is online at http://npyscreen.readthedocs.org

Please report bugs or make feature requests using the bug-tracker at http://code.google.com/p/npyscreen.

There is a mailing list available at https://groups.google.com/forum/?fromgroups#!forum/npyscreen/


*Latest Changes*:

Version 4.10.5:  Merged in bug-fixes and enhancements suggested by Nathan Lewis. 

Version 4.10.0: All widgets have a safe_to_exit() method that will be called
(at least by the default handlers) before exiting a widget.  Users can
perform input verification functions here.  Return True if the widget should
allow exiting, or False if it should not.  Various minor bug-fixes.

Version 4.9.1: minor change to Multiline widgets to make custom versions easier (final widget value is never set to MORE_LABEL).

Version 4.9:  new function blank_terminal() added. (User request)  Improvements to facilities for writing unit tests. (user request)
Bugs related to hidden widgets fixed.

Version 4.8.7 New methods added to the Multiline class to assist widget authors. 


Version 4.8.6 MultiLineAction widgets no longer raise an exception if empty
and selected unless set to do so explicitly.

Version 4.8.5 improves the writing of tests.

Version 4.8.4 adds support for custom-defined colours (not recommended.
Added at user request for a custom application).

Version 4.8.3 updates the documentation.

Version 4.8.2 makes the standard grid widget easier to customize by adding
the custom_print_cell method.

Version 4.8.1 fixes a bug that causes npyscreen to crash.

Version 4.8.0 adds a mechanism for finer control of BoxTitle contained
widgets.

Version 4.7.2 includes documentation updates.

Version 4.7.1 is a bugfix release.

Version 4.7.0 adds scripting support for writing automated tests.

Version 4.6.4 adds keybindings relevant to Windows.

Version 4.6.3 is a minor bug-fix release.

Version 4.6.1 updates the documentation to note that there is a bug in
Python's curses library in 3.4.0.  This is fixed in 3.4.1.  I do not propose
to put a workaround into npyscreen, which would complicate the code a great
deal for a bug that very few people face.  For more details, see:
http://bugs.python.org/issue21088




Version 4.6.0 introduces a way to define a callback for when widget values change.  The help system has been improved by minor interface changes.  
Both of these were user suggestions.  Thank you to those who suggested them.

Version 4.5.0 introduces a greater control of colour for certain widgets.  

Version 4.4.0 introduces the new tree class TreeData.  This is a new version of NPSTreeData that follows PEP 8 conventions for method names.  NPSTreeData is now deprecated.
The form class ActionFormMinimal has been added at a user request.  This is a special version of ActionFrom that only features an OK button by default.

Version 4.3.5 introduces the new classes SliderNoLabel, TitleSliderNoLabel, SliderPercent, TitleSliderPercent.


Version 4.3.4 Minor bugfixes.  The notify functions and ActionPopups derived from them now use the ActionFormV2 widgets.  
This change should not affect existing code, but let me know if there are problems.

Version 4.3.0 allows you to specify a negative value for rely or relx when creating a widget.  This will cause the widget to be aligned
with the bottom or right of the screen.  The new method *set_relyx(y, x)* can be used to set the position of the widget on the Form if you never need to do that manually. 

The classes *ActionFormV2*, *ActionFormExpandedV2* and *ActionFormV2WithMenus* have been introduced.
These feature cleaner code that should be easier to subclass.  

The *ButtonPress* class can now be created with the argument
*when_pressed_function=None*, which can be used in place of overriding the *whenPressed* method.  Note that this might create a reference cycle
within your application, so use with care. 


Version 4.2.0 introduces the ability of Grid widgets to highlight the whole line that the cursor is on (user request).

Version 4.1.0 introduces support for hvc consoles (thanks to wu.fuheng@********* for the bug report).  Title widgets can now define a when_cursor_moved() method directly 
on themselves that will be called as expected by the contained entry_widget during its edit loop (user request). 


Version 4.0.0 introduces a new version scheme.  Due to a packaging error in
the 3.0 release series some users were having problems obtaining the latest
version. This is most easily fixed with a new major version release.

Version 3.10 MultiLineEditable, MultiLineEditableTitle, MultiLineEditableBoxed classes added, allowing the user to edit lists of items. 
See EXAMPLE-MultilineEditable for an example.   

Version 3.6 Title.. widgets should now resize properly.  Menu items can now
be specified with arguments and keywords.

Version 3.5 when_value_edited defined on Title.. widgets now work as users expect.

Version 3.4 Fixed bugs in Title.. widgets and in the App classes.

Version 3.3 and the subsequent minor releases fix some bugs, mainly related
to changes caused by allowing resized forms.

Version 3.2 adds CheckboxBare - a checkbox without a label.  Added at user request. 

Version 3.0 *IMPORTANT* The version number has changed to version 3.0.  
This is because newer versions of pip distinguish between pre-release and released versions, 
and this will allow more flexibility in future releases. A version '2.0' might have caused confusion at this stage.  

Version 3.0 fixes the specification of max_width values for titled widgets (Thanks to Phil Rich for the bug report).  
Please report any further problems.

Version 2.0pre90 introduces a new BufferPager and TitleBufferPager class. (User request, suggested by dennis@wsec.be)

Version 2.0pre88 *IMPORTANT* This version supports resizing the terminal.
Read the documentation for more detail about how to disable this feature if
you need to.  It has been implemented in a way that should be compatible
with existing code.  New code can make the resizing even more flexible.

Version 2.0pre87 Updates the documentation and contains various bug fixes.

Version 2.0pre85 and 2.0pre86 are both bugfix releases. 

Version 2.0pre84 introduces an experimental system for editing lists of
options.  See documentation for details.

Version 2.0pre83 multi-line checkbox widgets are now possible.  These can also be used as contained widgets within the multiselect class. See documentation for details.

Version 2.0pre82 changes the menu system and allows menu items to be given keyboard shortcuts.

Version 2.0pre81 introduces FilenameCombo, TitleFilenameCombo.

Version 2.0pre79 is a bugfix release.

Version 2.0pre76 further improves the handling of mouse events on compatible
terminals.


Version 2.0pre75 improves the handling of the mouse on compatible terminals.

Version 2.0pre74 corrects one minor bug and introduces makes box widgets
behave slightly more predictably (.editable attribute now linked to that of
the contained widget.

Version 2.0pre73 corrects two bugs - thanks to Lasse for his help in finding
them and offering patches.

Version 2.0pre71 new tree classes introduced. Bug fixes.

Version 2.0pre70 introduces the MLTreeMultiSelect class.

Version 2.0pre69 fixes and tidies up some of the new tree classes.  There is an API change assocatied with this, noted in the documentation, though backward compatibility should have been maintained. 

Version 2.0pre68 setting a form's .editing attribute to False now causes it to exit immediately,
even if a widget is still being edited.

Version 2.0pre67 fixes minor bugs.

Version 2.0pre65 fixes several bugs.  All textboxes now honour the .hidden
attribute.  The major side effect of this is that tree classes are now
easier to write.

Version 2.0pre64 extends multi-page support and includes revision to the
documentation.

Version 2.0pre63 adds initial support for multi-page forms.  See documentation on the 
FormMultiPage class for details.

Version 2.0pre57 fixes color support - it should now be possible to display
a terminal with a different color background. Text widgets have some
additional color options.

Version 2.0pre52 fixes compatibility with python2.6, 3.0 and 3.1.  All other versions should be unaffected.

Version 2.0pre50 enables basic mouse support.  Note that the Apple terminal does not handle mouse events correctly.
  * [numba-0.45.1](http://numba.github.com) *****
Numba
*****

.. image:: https://badges.gitter.im/numba/numba.svg
   :target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge
   :alt: Gitter

A Just-In-Time Compiler for Numerical Functions in Python
#########################################################

Numba is an open source, NumPy-aware optimizing compiler for Python sponsored
by Anaconda, Inc.  It uses the LLVM compiler project to generate machine code
from Python syntax.

Numba can compile a large subset of numerically-focused Python, including many
NumPy functions.  Additionally, Numba has support for automatic
parallelization of loops, generation of GPU-accelerated code, and creation of
ufuncs and C callbacks.

For more information about Numba, see the Numba homepage: 
http://numba.pydata.org

Dependencies
============

* llvmlite
* NumPy (version 1.9 or higher)
* funcsigs (for Python 2)

Supported Platforms
===================

* Operating systems and CPU:

  - Linux: x86 (32-bit), x86_64, ppc64le (POWER8 and 9), ARMv7 (32-bit),
    ARMv8 (64-bit)
  - Windows: x86, x86_64
  - macOS: x86_64

* Python versions: 2.7, 3.5-3.7
* NumPy: >= 1.11
* NVIDIA GPUs (Kepler architecture or later) via CUDA driver on Linux, Windows,
  macOS (< 10.14)
* AMD GPUs via ROCm driver on Linux


Installing
==========

The easiest way to install Numba and get updates is by using the Anaconda
Distribution: https://www.anaconda.com/download

::

   $ conda install numba

For more options, see the Installation Guide: http://numba.pydata.org/numba-doc/latest/user/installing.html

Documentation
=============

http://numba.pydata.org/numba-doc/latest/index.html


Mailing Lists
=============

Join the Numba mailing list numba-users@continuum.io:
https://groups.google.com/a/continuum.io/d/forum/numba-users

Some old archives are at: http://librelist.com/browser/numba/


Continuous Integration
======================

.. image:: https://travis-ci.org/numba/numba.svg?branch=master
    :target: https://travis-ci.org/numba/numba
    :alt: Travis CI

.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=master
    :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=master
    :alt: Azure Pipelines



  * [numcodecs-0.6.3](https://github.com/alimanfoo/numcodecs) Numcodecs
=========

Numcodecs is a Python package providing buffer compression and transformation 
codecs for use in data storage and communication applications.

.. image:: https://readthedocs.org/projects/numcodecs/badge/?version=latest
    :target: http://numcodecs.readthedocs.io/en/latest/?badge=latest

.. image:: https://travis-ci.org/zarr-developers/numcodecs.svg?branch=master
    :target: https://travis-ci.org/zarr-developers/numcodecs

.. image:: https://ci.appveyor.com/api/projects/status/jhaaaxotvel24n9g?svg=true
    :target: https://ci.appveyor.com/project/zarr-developers/numcodecs

.. image:: https://coveralls.io/repos/github/zarr-developers/numcodecs/badge.svg?branch=master
    :target: https://coveralls.io/github/zarr-developers/numcodecs?branch=master
  * [numexpr-2.6.9](https://github.com/pydata/numexpr) ======================================================
NumExpr: Fast numerical expression evaluator for NumPy
======================================================

:Author: David M. Cooke, Francesc Alted and others
:Contact: faltet@gmail.com
:URL: https://github.com/pydata/numexpr
:Documentation: http://numexpr.readthedocs.io/en/latest/
:Travis CI: |travis|
:Appveyor: |appveyor|
:PyPi: |version|
:DOI: |doi|
:readthedocs: |docs|

.. |travis| image:: https://travis-ci.org/pydata/numexpr.png?branch=master
        :target: https://travis-ci.org/pydata/numexpr
.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/we2ff01vqlmlb9ip
        :target: https://ci.appveyor.com/project/robbmcleod/numexpr
.. |docs| image:: https://readthedocs.org/projects/numexpr/badge/?version=latest
        :target: http://numexpr.readthedocs.io/en/latest
.. |doi| image:: https://zenodo.org/badge/doi/10.5281/zenodo.2483274.svg
        :target:  https://doi.org/10.5281/zenodo.2483274
.. |version| image:: https://img.shields.io/pypi/v/numexpr.png
        :target: https://pypi.python.org/pypi/numexpr


What is NumExpr?
----------------

NumExpr is a fast numerical expression evaluator for NumPy.  With it,
expressions that operate on arrays (like :code:`'3*a+4*b'`) are accelerated
and use less memory than doing the same calculation in Python.

In addition, its multi-threaded capabilities can make use of all your
cores -- which generally results in substantial performance scaling compared
to NumPy.

Last but not least, numexpr can make use of Intel's VML (Vector Math
Library, normally integrated in its Math Kernel Library, or MKL).
This allows further acceleration of transcendent expressions.


How NumExpr achieves high performance
-------------------------------------

The main reason why NumExpr achieves better performance than NumPy is
that it avoids allocating memory for intermediate results. This
results in better cache utilization and reduces memory access in
general. Due to this, NumExpr works best with large arrays.

NumExpr parses expressions into its own op-codes that are then used by
an integrated computing virtual machine. The array operands are split
into small chunks that easily fit in the cache of the CPU and passed
to the virtual machine. The virtual machine then applies the
operations on each chunk. It's worth noting that all temporaries and
constants in the expression are also chunked. Chunks are distributed among 
the available cores of the CPU, resulting in highly parallelized code 
execution.

The result is that NumExpr can get the most of your machine computing
capabilities for array-wise computations. Common speed-ups with regard
to NumPy are usually between 0.95x (for very simple expressions like
:code:`'a + 1'`) and 4x (for relatively complex ones like :code:`'a*b-4.1*a >
2.5*b'`), although much higher speed-ups can be achieved for some functions 
and complex math operations (up to 15x in some cases).

NumExpr performs best on matrices that are too large to fit in L1 CPU cache. 
In order to get a better idea on the different speed-ups that can be achieved 
on your platform, run the provided benchmarks.


Usage
-----

::

  >>> import numpy as np
  >>> import numexpr as ne

  >>> a = np.arange(1e6)   # Choose large arrays for better speedups
  >>> b = np.arange(1e6)

  >>> ne.evaluate("a + 1")   # a simple expression
  array([  1.00000000e+00,   2.00000000e+00,   3.00000000e+00, ...,
           9.99998000e+05,   9.99999000e+05,   1.00000000e+06])

  >>> ne.evaluate('a*b-4.1*a > 2.5*b')   # a more complex one
  array([False, False, False, ...,  True,  True,  True], dtype=bool)

  >>> ne.evaluate("sin(a) + arcsinh(a/b)")   # you can also use functions
  array([        NaN,  1.72284457,  1.79067101, ...,  1.09567006,
          0.17523598, -0.09597844])

  >>> s = np.array(['abba', 'abbb', 'abbcdef'])
  >>> ne.evaluate("'abba' == s")   # string arrays are supported too
  array([ True, False, False], dtype=bool)


Documentation
-------------

Please see the official documentation at `numexpr.readthedocs.io <https://numexpr.readthedocs.io>`_.
Included is a user guide, benchmark results, and the reference API.


Authors
-------

Please see `AUTHORS.txt <https://github.com/pydata/numexpr/blob/master/AUTHORS.txt>`_.


License
-------

NumExpr is distributed under the `MIT <http://www.opensource.org/licenses/mit-license.php>`_ license.


.. Local Variables:
.. mode: text
.. coding: utf-8
.. fill-column: 70
.. End:



  * [numpy-1.15.0](https://www.numpy.org) It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities
- and much more

Besides its obvious scientific uses, NumPy can also be used as an efficient
multi-dimensional container of generic data. Arbitrary data-types can be
defined. This allows NumPy to seamlessly and speedily integrate with a wide
variety of databases.

All NumPy wheels distributed on PyPI are BSD licensed.




  * [numpydoc-0.9.1](https://numpydoc.readthedocs.io) .. image:: https://travis-ci.org/numpy/numpydoc.png?branch=master
   :target: https://travis-ci.org/numpy/numpydoc/

.. |docs| image:: https://readthedocs.org/projects/numpydoc/badge/?version=latest
   :alt: Documentation Status
   :scale: 100%
   :target: https://numpydoc.readthedocs.io/en/latest/?badge=latest


=====================================
numpydoc -- Numpy's Sphinx extensions
=====================================

This package provides the ``numpydoc`` Sphinx extension for handling
docstrings formatted according to the NumPy documentation format.
The extension also adds the code description directives
``np:function``, ``np-c:function``, etc.

For usage information, please refer to the `documentation
<https://numpydoc.readthedocs.io/>`_.

The `numpydoc docstring guide
<https://numpydoc.readthedocs.io/en/latest/format.html>`_ explains how
to write docs formatted for this extension, and the `user guide
<https://numpydoc.readthedocs.io>`_ explains how to use it with Sphinx.
  * [oauth-1.0.1](http://code.google.com/p/oauth) UNKNOWN
  * [oauthlib-3.0.2](https://github.com/oauthlib/oauthlib) OAuthLib - Python Framework for OAuth1 & OAuth2
===============================================

*A generic, spec-compliant, thorough implementation of the OAuth request-signing
logic for Python 2.7 and 3.4+.*

.. image:: https://travis-ci.org/oauthlib/oauthlib.svg?branch=master
  :target: https://travis-ci.org/oauthlib/oauthlib
  :alt: Travis
.. image:: https://coveralls.io/repos/oauthlib/oauthlib/badge.svg?branch=master
  :target: https://coveralls.io/r/oauthlib/oauthlib
  :alt: Coveralls
.. image:: https://img.shields.io/pypi/pyversions/oauthlib.svg
  :target: https://pypi.org/project/oauthlib/
  :alt: Download from PyPI
.. image:: https://img.shields.io/pypi/l/oauthlib.svg
  :target: https://pypi.org/project/oauthlib/
  :alt: License
.. image:: https://app.fossa.io/api/projects/git%2Bgithub.com%2Foauthlib%2Foauthlib.svg?type=shield
   :target: https://app.fossa.io/projects/git%2Bgithub.com%2Foauthlib%2Foauthlib?ref=badge_shield
   :alt: FOSSA Status
.. image:: https://img.shields.io/readthedocs/oauthlib.svg
  :target: https://oauthlib.readthedocs.io/en/latest/index.html
  :alt: Read the Docs
.. image:: https://badges.gitter.im/oauthlib/oauthlib.svg
  :target: https://gitter.im/oauthlib/Lobby
  :alt: Chat on Gitter

OAuth often seems complicated and difficult-to-implement. There are several
prominent libraries for handling OAuth requests, but they all suffer from one or
both of the following:

1. They predate the `OAuth 1.0 spec`_, AKA RFC 5849.
2. They predate the `OAuth 2.0 spec`_, AKA RFC 6749.
3. They assume the usage of a specific HTTP request library.

.. _`OAuth 1.0 spec`: https://tools.ietf.org/html/rfc5849
.. _`OAuth 2.0 spec`: https://tools.ietf.org/html/rfc6749

OAuthLib is a framework which implements the logic of OAuth1 or OAuth2 without
assuming a specific HTTP request object or web framework. Use it to graft OAuth
client support onto your favorite HTTP library, or provide support onto your
favourite web framework. If you're a maintainer of such a library, write a thin
veneer on top of OAuthLib and get OAuth support for very little effort.


Documentation
--------------

Full documentation is available on `Read the Docs`_. All contributions are very
welcome! The documentation is still quite sparse, please open an issue for what
you'd like to know, or discuss it in our `Gitter community`_, or even better, send a
pull request!

.. _`Gitter community`: https://gitter.im/oauthlib/Lobby
.. _`Read the Docs`: https://oauthlib.readthedocs.io/en/latest/index.html

Interested in making OAuth requests?
------------------------------------

Then you might be more interested in using `requests`_ which has OAuthLib
powered OAuth support provided by the `requests-oauthlib`_ library.

.. _`requests`: https://github.com/requests/requests
.. _`requests-oauthlib`: https://github.com/requests/requests-oauthlib

Which web frameworks are supported?
-----------------------------------

The following packages provide OAuth support using OAuthLib.

- For Django there is `django-oauth-toolkit`_, which includes `Django REST framework`_ support.
- For Flask there is `flask-oauthlib`_ and `Flask-Dance`_.
- For Pyramid there is `pyramid-oauthlib`_.
- For Bottle there is `bottle-oauthlib`_.

If you have written an OAuthLib package that supports your favorite framework,
please open a Pull Request, updating the documentation.

.. _`django-oauth-toolkit`: https://github.com/evonove/django-oauth-toolkit
.. _`flask-oauthlib`: https://github.com/lepture/flask-oauthlib
.. _`Django REST framework`: http://django-rest-framework.org
.. _`Flask-Dance`: https://github.com/singingwolfboy/flask-dance
.. _`pyramid-oauthlib`: https://github.com/tilgovi/pyramid-oauthlib
.. _`bottle-oauthlib`: https://github.com/thomsonreuters/bottle-oauthlib

Using OAuthLib? Please get in touch!
------------------------------------
Patching OAuth support onto an http request framework? Creating an OAuth
provider extension for a web framework? Simply using OAuthLib to Get Things Done
or to learn?

No matter which we'd love to hear from you in our `Gitter community`_ or if you have
anything in particular you would like to have, change or comment on don't
hesitate for a second to send a pull request or open an issue. We might be quite
busy and therefore slow to reply but we love feedback!

Chances are you have run into something annoying that you wish there was
documentation for, if you wish to gain eternal fame and glory, and a drink if we
have the pleasure to run into eachother, please send a docs pull request =)

.. _`Gitter community`: https://gitter.im/oauthlib/Lobby

License
-------

OAuthLib is yours to use and abuse according to the terms of the BSD license.
Check the LICENSE file for full details.

Credits
-------

OAuthLib has been started and maintained several years by Idan Gazit and other
amazing `AUTHORS`_. Thanks to their wonderful work, the open-source `community`_
creation has been possible and the project can stay active and reactive to users
requests.


.. _`AUTHORS`: https://github.com/oauthlib/oauthlib/blob/master/AUTHORS
.. _`community`: https://github.com/oauthlib/

Changelog
---------

*OAuthLib is in active development, with the core of both OAuth1 and OAuth2
completed, for providers as well as clients.* See `supported features`_ for
details.

.. _`supported features`: https://oauthlib.readthedocs.io/en/latest/feature_matrix.html

For a full changelog see ``CHANGELOG.rst``.



  * [objgraph-3.4.1](https://mg.pov.lt/objgraph/) Python Object Graphs
====================

.. image:: https://travis-ci.org/mgedmin/objgraph.svg?branch=master
   :target: https://travis-ci.org/mgedmin/objgraph
   :alt: Build Status

.. image:: https://ci.appveyor.com/api/projects/status/github/mgedmin/objgraph?branch=master&svg=true
   :target: https://ci.appveyor.com/project/mgedmin/objgraph
   :alt: Build Status (Windows)

.. image:: https://coveralls.io/repos/mgedmin/objgraph/badge.svg?branch=master
   :target: https://coveralls.io/r/mgedmin/objgraph?branch=master
   :alt: Test Coverage

.. image:: https://readthedocs.org/projects/objgraph/badge/?version=latest
   :target: https://readthedocs.org/projects/objgraph/?badge=latest
   :alt: Documentation Status


``objgraph`` is a module that lets you visually explore Python object graphs.

You'll need `graphviz <https://www.graphviz.org/>`_ if you want to draw
the pretty graphs.

I recommend `xdot <https://pypi.python.org/pypi/xdot>`_ for interactive use.
``pip install xdot`` should suffice; objgraph will automatically look for it
in your ``PATH``.


Installation and Documentation
------------------------------

``pip install objgraph`` or `download it from PyPI
<https://pypi.python.org/pypi/objgraph>`_.

Documentation lives at https://mg.pov.lt/objgraph.


.. _history:

History
-------

I've developed a set of functions that eventually became objgraph when I
was hunting for memory leaks in a Python program.  The whole story -- with
illustrated examples -- is in this series of blog posts:

* `Hunting memory leaks in Python
  <https://mg.pov.lt/blog/hunting-python-memleaks.html>`_
* `Python object graphs
  <https://mg.pov.lt/blog/python-object-graphs.html>`_
* `Object graphs with graphviz
  <https://mg.pov.lt/blog/object-graphs-with-graphviz.html>`_


.. _devel:

Support and Development
-----------------------

The source code can be found in this Git repository:
https://github.com/mgedmin/objgraph.

To check it out, use ``git clone https://github.com/mgedmin/objgraph``.

Report bugs at https://github.com/mgedmin/objgraph/issues.



Changes
=======



3.4.1 (2019-04-23)
------------------

- Add support for Python 3.7.

- Drop support for Python 3.3 and 3.4.


3.4.0 (2018-02-13)
------------------

- New functions: `get_new_ids`, `at_addrs`.

  Contributed by Justin Black in `PR 36
  <https://github.com/mgedmin/objgraph/pull/36>`_.


3.3.0 (2017-12-28)
------------------

- New function: `growth`.


3.2.0 (2017-12-20)
------------------

- New ``filter`` argument for `typestats`, `most_common_types`,
  `show_most_common_types`, `show_growth`.

- Show lambda functions in a more human-friendly way.


3.1.2 (2017-11-27)
------------------

- Correct UTF-8 mojibake in the changelog and switch all links to HTTPS.


3.1.1 (2017-10-30)
------------------

- Add support for Python 3.6.

- Replace bare ``except:`` in ``safe_repr()`` with ``except Exception:``.


3.1.0 (2016-12-07)
------------------

- Support displaying graphs inline in IPython/Jupyter notebooks (`issue 28
  <https://github.com/mgedmin/objgraph/pull/28>`).


3.0.1 (2016-09-17)
------------------

- The ``file`` argument of `show_most_common_types` and
  `show_growth` now defaults to ``None`` instead of ``sys.stdout``.
  ``None`` is interpreted to be the same as ``sys.stdout``, which means
  the right stdout will be used if you change it at runtime (which happens,
  in doctests).


3.0.0 (2016-04-13)
------------------

- `show_most_common_types` and `show_growth` now accept a ``file``
  argument if you want to redirect the output elsewhere.

  Fixes `issue 24 <https://github.com/mgedmin/objgraph/pull/24>`_.  Contributed
  by "d-sun-d".

- Don't trust ``__class__`` to be accurate and ``__name__`` to be a string.
  Fixes errors in some convoluted corner cases when mocks are involved.

  Contributed by Andrew Shannon Brown in `PR 26
  <https://github.com/mgedmin/objgraph/pull/26>`_.

- Drop support for Python 2.4, 2.5, and 2.6.

- Drop support for Python 3.1 and 3.2.

- Add support for Python 3.5.


2.0.1 (2015-07-28)
------------------

- Avoid creating reference cycles between the stack frame and the local
  ``objects`` variable in `by_type`, `count`, and
  `typestats`.

  Fixes `issue 22 <https://github.com/mgedmin/objgraph/pull/22>`_.  Contributed
  by Erik Bray.


2.0.0 (2015-04-18)
------------------

- `show_refs` and `show_backrefs` now accept a file-like object
  (via the new ``output`` argument) as an alternative to a filename.

- Made internal helper methods private. This includes `find_chain`,
  `show_graph`, `obj_node_id`, `obj_label`, `quote`,
  `long_typename`, `safe_repr`, `short_repr`, 
  `gradient`, `edge_label`, and `_program_in_path`.

- Correctly determine the name of old-style classes in `count`,
  `by_type`, and graph drawing functions.

  Fixes `issue 16 <https://github.com/mgedmin/objgraph/pull/16>`_.  Contributed
  by Mike Lambert.


1.8.1 (2014-05-15)
------------------

- Do not expect file objects to have an ``encoding`` attribute.  Makes objgraph
  compatible with Eventlet's monkey-patching.

  Fixes `issue 6 <https://github.com/mgedmin/objgraph/pull/6>`_.  Contributed
  by Jakub Stasiak.


1.8.0 (2014-02-13)
------------------

- Moved to GitHub.

- Python 3.4 support (`LP#1270872 <https://launchpad.net/bugs/1270872>`_).

- New function: `is_proper_module`.

- New ``shortnames`` argument for `typestats`, `most_common_types`,
  `show_most_common_types`, `show_growth`, `show_refs`,
  and `show_backrefs`.

  `count` and `by_type` accept fully-qualified type names now.

  Fixes `issue 4 <https://github.com/mgedmin/objgraph/issues/4>`_.


1.7.2 (2012-10-23)
------------------

- Bugfix: setup.py sdist was broken on Python 2.7 (UnicodeDecodeError in
  tarfile).

- The ``filename`` argument for `show_refs` and `show_backrefs` now
  allows arbitrary image formats, not just PNG.  Patch by `Riccardo
  Murri <https://launchpad.net/~rmurri>`_.

- Temporary dot files are now named `objgraph-*.dot` instead of `tmp*.dot`.

- Python 3.3 support: no code changes, but some tests started failing because
  the new and improved dictionary implementation no longer holds references to
  str objects used as dict keys.

- Added a tox.ini for convenient multi-Python testing.


1.7.1 (2011-12-11)
------------------

- Bugfix: non-ASCII characters in object representations would break graph
  generation on Python 3.x, in some locales (e.g. with LC_ALL=C).  Reported and
  fixed by `Stefano Rivera <https://launchpad.net/~stefanor>`_.

- Bugfix: setup.py was broken on Python 3.x

- Bugfix: dot.exe/xdot.exe were not found on Windows (`LP#767239
  <https://launchpad.net/bugs/767239>`_).

- Documentation updates: document the forgotten `find_ref_chain`,
  update `show_chain` prototype.


1.7.0 (2011-03-11)
------------------

- New function: `find_ref_chain`.

- New ``backrefs`` argument for `show_chain`.

- New function: `get_leaking_objects`, based on `a blog post by
  Kristján Valur
  <https://cosmicpercolator.com/2010/12/08/finding-c-reference-leaks-using-the-gc-module/>`_.

- New ``objects`` argument for `count`, `typestats`,
  `most_common_types`, `show_most_common_types`, and
  `by_type`.

- Edges pointing to function attributes such as __defaults__ or __globals__
  are now labeled.

- Edge labels that are not simple strings now show the type.

- Bugfix: '\0' and other unsafe characters used in a dictionary key could
  break graph generation.

- Bugfix: show_refs(..., filename='graph.dot') would then go to complain
  about unrecognized file types and then produce a png.


1.6.0 (2010-12-18)
------------------

- Python 3 support, thanks to Stefano Rivera (fixes `LP#687601
  <https://launchpad.net/bugs/687601>`_).

- Removed weird weakref special-casing.


1.5.1 (2010-12-09)
------------------

- Avoid test failures in uncollectable-garbage.txt (fixes `LP#686731
  <https://launchpad.net/bugs/686731>`_).

- Added HACKING.txt (later renamed to HACKING.rst).


1.5.0 (2010-12-05)
------------------

- Show frame objects as well (fixes `LP#361704
  <https://launchpad.net/bugs/361704>`_).

- New functions: `show_growth`, `show_chain`.

- `find_backref_chain` returns ``[obj]`` instead of ``None`` when a chain
  could not be found.  This makes ``show_chain(find_backref_chain(...), ...)``
  not break.

- Show how many references were skipped from the output of
  `show_refs`/`show_backrefs` by specifying ``too_many``.

- Make `show_refs` descend into modules.

- Do not highlight classes that define a ``__del__``, highlight only instances of
  those classes.

- Option to show reference counts in `show_refs`/`show_backrefs`.

- Add `Sphinx <https://pypi.python.org/pypi/Sphinx>`_ documentation and a PyPI
  long description.


1.4.0 (2010-11-03)
------------------

- Compatibility with Python 2.4 and 2.5 (``tempfile.NamedTemporaryFile`` has no
  ``delete`` argument).

- New function: `most_common_types`.


1.3.1 (2010-07-17)
------------------

- Rebuild an sdist with no missing files (fixes `LP#606604
  <https://launchpad.net/bugs/606604>`_).

- Added MANIFEST.in and a Makefile to check that setup.py sdist generates
  source distributions with no files missing.


1.3 (2010-07-13)
----------------

- Highlight objects with a ``__del__`` method.

- Fixes `LP#483411 <https://launchpad.net/bugs/483411>`_: suggest always passing
  ``[obj]`` to `show_refs`, `show_backrefs`, since obj might be a
  list/tuple.

- Fixes `LP#514422 <https://launchpad.net/bugs/514422>`_: `show_refs`,
  `show_backrefs` don't create files in the current working directory any
  more.  Instead they accept a filename argument, which can be a .dot file or a
  .png file.  If None or not specified, those functions will try to spawn xdot
  as before.

- New extra_info argument to graph-generating functions (patch by Thouis Jones,
  `LP#558914 <https://launchpad.net/bugs/558914>`_).

- setup.py should work with distutils now (`LP#604430
  <https://launchpad.net/bugs/604430>`_, thanks to Randy Heydon).


1.2 (2009-03-25)
----------------

- Project website, public source repository, uploaded to PyPI.

- No code changes.


1.1 (2008-09-10)
----------------

- New function: `show_refs` for showing forward references.

- New functions: `typestats` and `show_most_common_types`.

- Object boxes are less crammed with useless information (such as IDs).

- Spawns `xdot <https://pypi.python.org/pypi/xdot>`_ if it is available.


1.0 (2008-06-14)
----------------

- First public release.



  * [odo-0.5.0](https://github.com/blaze/odo) Odo
===

|Build Status| |Doc Status|

.. image:: https://binstar.org/blaze/odo/badges/build.svg
   :target: https://binstar.org/blaze/odo/builds

.. image:: https://binstar.org/blaze/odo/badges/version.svg
   :target: https://binstar.org/blaze/odo

Data migration in Python

Documentation_

Example
-------

Odo migrates data between different containers

.. code-block:: python

   >>> from odo import odo
   >>> odo((1, 2, 3), list)
   [1, 2, 3]

It operates on small, in-memory containers (as above) and large, out-of-core
containers (as below)

.. code-block:: python

   >>> odo('myfile.hdf5::/data', 'postgresql://user:pass@host::my-table')
   Table('my-table', MetaData(bind=Engine(postgresql://user:****@host)), ...)

Odo leverages the existing Python ecosystem.  The example above uses
``sqlalchemy`` for SQL interation and ``h5py`` for HDF5 interaction.


Method
------

Odo migrates data using network of small data conversion functions between
type pairs. That network is below:

.. image:: https://raw.githubusercontent.com/blaze/odo/master/docs/source/images/conversions.png
   :alt: odo conversions

Each node is a container type (like ``pandas.DataFrame`` or
``sqlalchemy.Table``) and each directed edge is a function that transforms or
appends one container into or onto another.  We annotate these functions/edges
with relative costs.

This network approach allows ``odo`` to select the shortest path between any
two types (thank you networkx_).  For performance reasons these functions often
leverage non-Pythonic systems like NumPy arrays or native ``CSV->SQL`` loading
functions.  Odo is not dependent on only Python iterators.

This network approach is also robust.  When libraries go missing or runtime
errors occur ``odo`` can work around these holes and find new paths.

This network approach is extensible.  It is easy to write small functions and
register them to the overall graph.  In the following example showing how we
convert from ``pandas.DataFrame`` to a ``numpy.ndarray``.

.. code-block:: python

   from odo import convert

   @convert.register(np.ndarray, pd.DataFrame, cost=1.0)
   def dataframe_to_numpy(df, **kwargs):
       return df.to_records(index=False)

We decorate ``convert`` functions with the target and source types as well as a
relative cost.  This decoration establishes a contract that the underlying
function must fulfill, in this case with the fast ``DataFrame.to_records``
method.  Similar functions exist for ``append``, to add to existing data, and
``resource`` for URI resolution.

* ``convert``: Transform dataset into new container
* ``append``: Add dataset onto existing container
* ``resource``: Given a URI find the appropriate data resource
* ``odo``: Call one of the above based on inputs.
  E.g. ``odo((1, 2, 3), list) -> convert(list, (1, 2, 3))``
  while ``L = []; odo((1, 2, 3), L) -> append(L, (1, 2, 3))``

Finally, ``odo`` is also aware of which containers must reside in memory and
which do not.  In the graph above the *red-colored* nodes are robust to
larger-than-memory datasets.  Transformations between two out-of-core datasets
operate only on the subgraph of the red nodes.


LICENSE
-------

New BSD. See `License File <https://github.com/blaze/odo/blob/master/LICENSE.txt>`__.

History
-------

Odo was factored out from the Blaze_ project.


.. _Blaze: http://blaze.pydata.org/
.. _networkx: https://networkx.github.io/
.. _Documentation: https://odo.readthedocs.org/en/latest/
.. |Build Status| image:: https://travis-ci.org/blaze/odo.png
   :target: https://travis-ci.org/blaze/odo
.. |Doc Status| image:: https://readthedocs.org/projects/odo/badge/?version=latest
   :target: https://readthedocs.org/projects/odo/?badge=latest
   :alt: Documentation Status
  * [olefile-0.46](https://www.decalage.info/python/olefileio) olefile
=======

|Build Status TravisCI| |Build Status AppVeyor| |Coverage Status|
|Documentation Status| |PyPI| |Can I Use Python 3?| |Say Thanks!|

`olefile <https://www.decalage.info/olefile>`__ is a Python package to
parse, read and write `Microsoft OLE2
files <http://en.wikipedia.org/wiki/Compound_File_Binary_Format>`__
(also called Structured Storage, Compound File Binary Format or Compound
Document File Format), such as Microsoft Office 97-2003 documents,
vbaProject.bin in MS Office 2007+ files, Image Composer and FlashPix
files, Outlook messages, StickyNotes, several Microscopy file formats,
McAfee antivirus quarantine files, etc.

**Quick links:** `Home page <https://www.decalage.info/olefile>`__ -
`Download/Install <http://olefile.readthedocs.io/en/latest/Install.html>`__
- `Documentation <http://olefile.readthedocs.io/en/latest>`__ - `Report
Issues/Suggestions/Questions <https://github.com/decalage2/olefile/issues>`__
- `Contact the author <https://www.decalage.info/contact>`__ -
`Repository <https://github.com/decalage2/olefile>`__ - `Updates on
Twitter <https://twitter.com/decalage2>`__

News
----

Follow all updates and news on Twitter: https://twitter.com/decalage2

-  **2018-09-09 v0.46**: OleFileIO can now be used as a context manager
   (with...as), to close the file automatically (see
   `doc <https://olefile.readthedocs.io/en/latest/Howto.html#open-an-ole-file-from-disk>`__).
   Improved handling of malformed files, fixed several bugs.
-  2018-01-24 v0.45: olefile can now overwrite streams of any size,
   improved handling of malformed files, fixed several
   `bugs <https://github.com/decalage2/olefile/milestone/4?closed=1>`__,
   end of support for Python 2.6 and 3.3.
-  2017-01-06 v0.44: several bugfixes, removed support for Python 2.5
   (olefile2), added support for incomplete streams and incorrect
   directory entries (to read malformed documents), added getclsid,
   improved `documentation <http://olefile.readthedocs.io/en/latest>`__
   with API reference.
-  2017-01-04: moved the documentation to
   `ReadTheDocs <http://olefile.readthedocs.io/en/latest>`__
-  2016-05-20: moved olefile repository to
   `GitHub <https://github.com/decalage2/olefile>`__
-  2016-02-02 v0.43: fixed issues
   `#26 <https://github.com/decalage2/olefile/issues/26>`__ and
   `#27 <https://github.com/decalage2/olefile/issues/27>`__, better
   handling of malformed files, use python logging.
-  see
   `changelog <https://github.com/decalage2/olefile/blob/master/CHANGELOG.md>`__
   for more detailed information and the latest changes.

Download/Install
----------------

If you have pip or setuptools installed (pip is included in Python
2.7.9+), you may simply run **pip install olefile** or **easy_install
olefile** for the first installation.

To update olefile, run **pip install -U olefile**.

Otherwise, see http://olefile.readthedocs.io/en/latest/Install.html

Features
--------

-  Parse, read and write any OLE file such as Microsoft Office 97-2003
   legacy document formats (Word .doc, Excel .xls, PowerPoint .ppt,
   Visio .vsd, Project .mpp), Image Composer and FlashPix files, Outlook
   messages, StickyNotes, Zeiss AxioVision ZVI files, Olympus FluoView
   OIB files, etc
-  List all the streams and storages contained in an OLE file
-  Open streams as files
-  Parse and read property streams, containing metadata of the file
-  Portable, pure Python module, no dependency

olefile can be used as an independent package or with PIL/Pillow.

olefile is mostly meant for developers. If you are looking for tools to
analyze OLE files or to extract data (especially for security purposes
such as malware analysis and forensics), then please also check my
`python-oletools <https://www.decalage.info/python/oletools>`__, which
are built upon olefile and provide a higher-level interface.

Documentation
-------------

Please see the `online
documentation <http://olefile.readthedocs.io/en/latest>`__ for more
information.

Real-life examples
------------------

A real-life example: `using OleFileIO_PL for malware analysis and
forensics <http://blog.gregback.net/2011/03/using-remnux-for-forensic-puzzle-6/>`__.

See also `this
paper <https://computer-forensics.sans.org/community/papers/gcfa/grow-forensic-tools-taxonomy-python-libraries-helpful-forensic-analysis_6879>`__
about python tools for forensics, which features olefile.

License
-------

olefile (formerly OleFileIO_PL) is copyright (c) 2005-2018 Philippe
Lagadec (https://www.decalage.info)

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

-  Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.
-  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

--------------

olefile is based on source code from the OleFileIO module of the Python
Imaging Library (PIL) published by Fredrik Lundh under the following
license:

The Python Imaging Library (PIL) is

-  Copyright (c) 1997-2009 by Secret Labs AB
-  Copyright (c) 1995-2009 by Fredrik Lundh

By obtaining, using, and/or copying this software and/or its associated
documentation, you agree that you have read, understood, and will comply
with the following terms and conditions:

Permission to use, copy, modify, and distribute this software and its
associated documentation for any purpose and without fee is hereby
granted, provided that the above copyright notice appears in all copies,
and that both that copyright notice and this permission notice appear in
supporting documentation, and that the name of Secret Labs AB or the
author not be used in advertising or publicity pertaining to
distribution of the software without specific, written prior permission.

SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO
THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS. IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR
ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER
RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF
CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

.. |Build Status TravisCI| image:: https://travis-ci.org/decalage2/olefile.svg?branch=master
   :target: https://travis-ci.org/decalage2/olefile
.. |Build Status AppVeyor| image:: https://ci.appveyor.com/api/projects/status/github/decalage2/olefile?svg=true
   :target: https://ci.appveyor.com/project/decalage2/olefile
.. |Coverage Status| image:: https://coveralls.io/repos/github/decalage2/olefile/badge.svg?branch=master
   :target: https://coveralls.io/github/decalage2/olefile?branch=master
.. |Documentation Status| image:: http://readthedocs.org/projects/olefile/badge/?version=latest
   :target: http://olefile.readthedocs.io/en/latest/?badge=latest
.. |PyPI| image:: https://img.shields.io/pypi/v/olefile.svg
   :target: https://pypi.org/project/olefile/
.. |Can I Use Python 3?| image:: https://caniusepython3.com/project/olefile.svg
   :target: https://caniusepython3.com/project/olefile
.. |Say Thanks!| image:: https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg
   :target: https://saythanks.io/to/decalage2
  * [oncodrivefml-2.2.0](https://bitbucket.org/bbglab/oncodrivefml) 
  * [ont-fast5-api-1.4.7](https://github.com/nanoporetech/ont_fast5_api) .. image:: img/ONT_logo.png
  :width: 800
  :alt:  .


ont_fast5_api
===============================================================================

``ont_fast5_api`` is a simple interface to HDF5 files of the Oxford Nanopore
fast5 file format.

- Source code: https://github.com/nanoporetech/ont_fast5_api
- Fast5 File Schema: https://github.com/nanoporetech/ont_h5_validator

It provides:

- Concrete implementation of the fast5 file schema using the generic h5py library
- Plain-english-named methods to interact with and reflect the fast5 file schema
- Tools to convert between `multi_read` and `single_read` formats

Getting Started
===============================================================================
The ``ont_fast5_api`` is available on PyPI and can be installed via pip::

    pip install ont-fast5-api

Alternatively, it is available on github where it can be built from source::

    git clone https://github.com/nanoporetech/ont_fast5_api
    cd ont_fast5_api
    python setup.py install

Dependencies
-------------------------------------------------------------------------------
``ont_fast5_api`` is a pure python project and should run on most python
versions and operating systems.

It requires:

- `h5py <http://www.h5py.org>`_: 2.2.1 or higher
- `NumPy <https://www.numpy.org>`_: 1.8.1 or higher
- `six <https://github.com/benjaminp/six>`_: 1.9 or higher
- `progressbar33 <https://github.com/germangh/python-progressbar>`_: 2.3.1 or higher

Interface - get_fast5_file
===============================================================================

The ont_fast5_api provides a simple interface to access the data structures in fast5
files of either single- or multi- read format using the same method calls.

For example to print the raw data from all reads in a file::

    from ont_fast5_api.fast5_interface import get_fast5_file

    def print_all_raw_data():
        fast5_filepath = "test/data/single_reads/read0.fast5" # This can be a single- or multi-read file
        with get_fast5_file(fast5_filepath, mode="r") as f5:
            for read_id in f5.get_read_ids():
                read = f5.get_read(read_id)
                raw_data = read.get_raw_data()
                print(read_id, raw_data)

Interface - Console Scripts
===============================================================================
The ``ont_fast5_api`` provides terminal/command-line ``console_scripts`` for
converting between files in the Oxford Nanopore ``single_read`` and
``multi_read`` fast5 formats. These are provided to ensure compatibility between
tools which expect either the ``single_read`` or ``multi_read`` fast5 file
formats.

The scripts are added during installation and can be called from the
terminal/command-line or from within python.

single_to_multi_fast5
-------------------------------------------------------------------------------
This script converts folders containing ``single_read_fast5`` files into
``multi_read_fast5_files``::

    single_to_multi_fast5
        -i, --input_path <(path) folder containing single_read_fast5 files>
        -s, --save_path <(path) to folder where multi_read fast5 files will be output>
        [optional] -t, --threads <(int) number of CPU threads to use; default=1>
        [optional] -f, --filename_base <(string) name for new multi_read file; default="batch" (see note-1)>
        [optional] -n, --batch_size <(int) number of single_reads to include in each multi_read file; default=4000>
        [optional] --recursive <(bool) if included, rescursively search sub-directories for single_read files; default=False>

*note-1:* newly created ``multi_read`` files require a name. This is the
``filename_base`` with the batch count and ``.fast5`` appended to it; e.g.
``-f batch`` yields ``batch_0.fast5, batch_1.fast5, ...``

**example usage**::

    single_to_multi_fast5 --input_path /data/reads --save_path /data/multi_reads
        --filename_base batch_output --batch_size 100 --recursive

Where ``/data/reads`` and/or its subfolders contain ``single_read`` fast5
files. The output will be ``multi_read`` fast5 files each containing 100 reads,
in the folder: ``/data/multi_reads`` with the names: ``batch_output_0.fast5``,
``batch_output_1.fast5`` etc.

multi_to_single_fast5
-------------------------------------------------------------------------------
This script converts folders containing ``multi_read_fast5`` files into
``single_read_fast5`` files::

    multi_to_single_fast5
        -i, --input_path <(path) folder containing multi_read_fast5 files>
        -s, --save_path <(path) to folder where single_read fast5 files will be output>
        [optional] -t, --threads <(int) number of CPU threads to use; default=1>
        [optional] --recursive <(bool) if included, rescursively search sub-directories for multi_read files; default=False>

**example usage**::

    multi_to_single_fast5 --input_path /data/multi_reads --save_path /data/single_reads
        --recursive

Where ``/data/multi_reads`` and/or its subfolders contain ``multi_read``  fast5
files. The output will be ``single_read`` fast5 files in the folder 
``/data/single_reads`` with one subfolder per ``multi_read`` input file

fast5_subset
-------------------------------------------------------------------------------
This script extracts reads from ``multi_read_fast5_file(s)`` based on a list of read_ids::

    fast5_subset
        -i, --input <(path) to folder containing multi_read_fast5 files or an individual multi_read_fast5 file> 
        -s, --save_path <(path) to folder where multi_read fast5 files will be output>
        -l,--read_id_list <(file) either sequencing_summary.txt file or a file containing a list of read_ids>
        [optional] -f, --filename_base <(string) name for new multi_read file; default="batch" (see note-1)>
        [optional] -n, --batch_size <(int) number of single_reads to include in each multi_read file; default=4000>
        [optional] --recursive <(bool) if included, rescursively search sub-directories for single_read files; default=False>

**example usage**::

    fast5_subset --input /data/multi_reads --save_path /data/subset
        --read_id_list read_id_list.txt --batch_size 100 --recursive

Where ``/data/multi_reads`` and/or its subfolders contain ``multi_read`` fast5
files and ``read_id_list.txt`` is a text file either containing 1 read_id per line
or a tsv file with a column named ``read_id``.
The output will be ``multi_read`` fast5 files each containing 100 reads,
in the folder: ``/data/multi_reads`` with the names: ``batch_output_0.fast5``,
``batch_output_1.fast5`` etc.


Glossary of Terms:
==============================================================================

**HDF5 file format** - a portable file format for storing and managing
data. It is designed for flexible and efficient I/O and for high volume and
complex data
**Fast5** - an implementation of the HDF5 file format, with specific data
schemas for Oxford Nanopore sequencing data
**Single read fast5** - A  fast5 file containing all the data pertaining to a
single Oxford Nanopore read. This may include raw signal data, run metadata,
fastq-basecalls and any other additional analyses
**Multi read fast5** - A fast5 file containing data pertaining to a multiple
Oxford Nanopore reads.



  * [opencv_python-4.1.0.25](https://github.com/skvark/opencv-python) [![Downloads](http://pepy.tech/badge/opencv-python)](http://pepy.tech/project/opencv-python)

## OpenCV on Wheels

**Unofficial** pre-built OpenCV packages for Python.

### Installation and Usage

1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.
2. Select the correct package for your environment:

    There are four different packages and you should **select only one of them**. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (`cv2`). If you installed multiple different packages in the same environment, uninstall them all with ``pip uninstall`` and reinstall only one package.

    **a.** Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)

    - run ``pip install opencv-python`` if you need only main modules
    - run ``pip install opencv-contrib-python`` if you need both main and contrib modules (check extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))

    **b.** Packages for server (headless) environments

    These packages do not contain any GUI functionality. They are smaller and suitable for more restricted environments.

    - run ``pip install opencv-python-headless`` if you need only main modules
    - run ``pip install opencv-contrib-python-headless`` if you need both main and contrib modules (check extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))

3. Import the package:

    ``import cv2``

    All packages contain haarcascade files. ``cv2.data.haarcascades`` can be used as a shortcut to the data folder. For example:

    ``cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")``

5. Read [OpenCV documentation](https://docs.opencv.org/master/)

6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.

Frequently Asked Questions
--------------------------

**Q: Do I need to install also OpenCV separately?**

A: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.

**Q: Pip fails with ``Could not find a version that satisfies the requirement ...``?**

A: Most likely the issue is related to too old pip and can be fixed by running ``pip install --upgrade pip``. Note that the wheel (especially manylinux) format does not currently support properly ARM architecture so there are no packages for ARM based platforms in PyPI. However, ``opencv-python`` packages for Raspberry Pi can be found from https://www.piwheels.org/.

**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**

A: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.

Windows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions).

If the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/skvark/opencv-python/issues/36) for a manual fix.

If you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\Users\username\AppData\Local\Programs\Python\PythonXX\Lib\site-packages\cv2``) file with it to debug missing DLL issues.

**Q: I have some other import errors?**

A: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).

**Q: Why the packages do not include non-free algorithms?**

A: Non-free algorithms such as SIFT and SURF are not included in these packages because they are patented and therefore cannot be distributed as built binaries. See this issue for more info: https://github.com/skvark/opencv-python/issues/126

**Q: Why the package and import are different (opencv-python vs. cv2)?**

A: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.

## Documentation for opencv-python

[![AppVeyor CI test status (Windows)](https://img.shields.io/appveyor/ci/skvark/opencv-python.svg?maxAge=3600&label=Windows)](https://ci.appveyor.com/project/skvark/opencv-python)
[![Travis CI test status (Linux and OS X)](https://img.shields.io/travis/skvark/opencv-python.svg?maxAge=3600&label=Linux+macOS)](https://travis-ci.org/skvark/opencv-python)

The aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.

### Build process

The project is structured like a normal Python package with a standard ``setup.py`` file.
The build process for a single entry in the build matrices is as follows (see for example ``appveyor.yml`` file):

0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against

1. Checkout repository and submodules

   -  OpenCV is included as submodule and the version is updated
      manually by maintainers when a new OpenCV release has been made
   -  Contrib modules are also included as a submodule

2. Find OpenCV version from the sources
3. Install Python dependencies

   - ``setup.py`` installs the dependencies itself, so you need to run it in an environment
     where you have the rights to install modules with Pip for the running Python

4. Build OpenCV

   -  tests are disabled, otherwise build time increases too much
   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)
   -  Linux builds run in manylinux Docker containers (CentOS 5)

5. Rearrange OpenCV's build result, add our custom files and generate wheel

6. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly

7. Install the generated wheel
8. Test that Python can import the library and run some sanity checks
9. Use twine to upload the generated wheel to PyPI (only in release builds)

Steps 1--5 are handled by ``setup.py bdist_wheel``.

The build can be customized with environment variables.
In addition to any variables that OpenCV's build accepts, we recognize:

- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version
- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.

### Licensing

Opencv-python package (scripts in this repository) is available under MIT license.

OpenCV itself is available under [3-clause BSD License](https://github.com/opencv/opencv/blob/master/LICENSE).

Third party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).

All wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).

Linux and MacOS wheels ship with [Qt 4.8.7](http://doc.qt.io/qt-4.8/lgpl.html) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).

### Versioning

``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string.

### Releases

A release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:

``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``

### Development builds

Every commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:

``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``

These artifacts can't be and will not be uploaded to PyPI.

### Manylinux wheels

Linux wheels are built using [manylinux](https://github.com/pypa/python-manylinux-demo). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.

The default ``manylinux`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.

### Supported Python versions

Python 2.7 is the only supported version in 2.x series. Python 2.7 support will be dropped in the end of 2019.

Python 3.x releases follow Numpy releases. For example Python 3.3 is no longer supported by Numpy so support for it has been dropped in ``opencv-python``, too.

Currently, builds for following Python versions are provided:

- 2.7
- 3.4
- 3.5
- 3.6
- 3.7



  * [openpyxl-2.6.2](https://openpyxl.readthedocs.io) .. image:: https://coveralls.io/repos/bitbucket/openpyxl/openpyxl/badge.svg?branch=default
    :target: https://coveralls.io/bitbucket/openpyxl/openpyxl?branch=default
    :alt: coverage status

Introduction
------------

openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.

It was born from lack of existing library to read/write natively from Python
the Office Open XML format.

All kudos to the PHPExcel team as openpyxl was initially based on PHPExcel.


Security
--------

By default openpyxl does not guard against quadratic blowup or billion laughs
xml attacks. To guard against these attacks install defusedxml.

Mailing List
------------

The user list can be found on http://groups.google.com/group/openpyxl-users


Sample code::

    from openpyxl import Workbook
    wb = Workbook()

    # grab the active worksheet
    ws = wb.active

    # Data can be assigned directly to cells
    ws['A1'] = 42

    # Rows can also be appended
    ws.append([1, 2, 3])

    # Python types will automatically be converted
    import datetime
    ws['A2'] = datetime.datetime.now()

    # Save the file
    wb.save("sample.xlsx")


Documentation
-------------

The documentation is at: https://openpyxl.readthedocs.io

* installation methods
* code examples
* instructions for contributing

Release notes: https://openpyxl.readthedocs.io/en/stable/changes.html
  * [openstackdocstheme-1.31.0](https://docs.openstack.org/openstackdocstheme/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/openstackdocstheme.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

OpenStack Foundation Sphinx Themes
==================================

Theme and extension support for Sphinx documentation that is published by
OpenStack Foundation projects.

OpenStack docs.openstack.org Sphinx Theme
-----------------------------------------

Theme support for Sphinx documentation that is published to
docs.openstack.org and developer.openstack.org.

Intended for use by OpenStack `projects governed by the Technical Committee`_.

.. _`projects governed by the Technical Committee`: https://governance.openstack.org/tc/reference/projects/index.html

StarlingX docs.starlingx.io Sphinx Theme
-----------------------------------------

Theme support for Sphinx documentation that is published to
docs.starlingx.io.

Intended for use by StarlingX `projects governed by the Technical Steering Committee`_.

.. _`projects governed by the Technical Steering Committee`: https://docs.starlingx.io/governance/reference/projects/index.html

References
----------

* Free software: Apache License, Version 2.0
* Documentation: https://docs.openstack.org/openstackdocstheme/latest/
* Release notes: https://docs.openstack.org/releasenotes/openstackdocstheme/
* Source: https://opendev.org/openstack/openstackdocstheme
* Bugs: https://launchpad.net/openstack-doc-tools




  * [openstacksdk-0.32.0](https://docs.openstack.org/openstacksdk/) openstacksdk
============

openstacksdk is a client library for building applications to work
with OpenStack clouds. The project aims to provide a consistent and
complete set of interactions with OpenStack's many services, along with
complete documentation, examples, and tools.

It also contains an abstraction interface layer. Clouds can do many things, but
there are probably only about 10 of them that most people care about with any
regularity. If you want to do complicated things, the per-service oriented
portions of the SDK are for you. However, if what you want is to be able to
write an application that talks to clouds no matter what crazy choices the
deployer has made in an attempt to be more hipster than their self-entitled
narcissist peers, then the Cloud Abstraction layer is for you.

More information about its history can be found at
https://docs.openstack.org/openstacksdk/latest/contributor/history.html

openstack
=========

List servers using objects configured with the ``clouds.yaml`` file:

.. code-block:: python

    import openstack

    # Initialize and turn on debug logging
    openstack.enable_logging(debug=True)

    # Initialize cloud
    conn = openstack.connect(cloud='mordred')

    for server in conn.compute.servers():
        print(server.to_dict())

Cloud Layer
===========

``openstacksdk`` contains a higher-level layer based on logical operations.

.. code-block:: python

    import openstack

    # Initialize and turn on debug logging
    openstack.enable_logging(debug=True)

    for server in conn.list_servers():
        print(server.to_dict())

The benefit is mostly seen in more complicated operations that take multiple
steps and where the steps vary across providers:

.. code-block:: python

    import openstack

    # Initialize and turn on debug logging
    openstack.enable_logging(debug=True)

    # Initialize connection
    # Cloud configs are read with openstack.config
    conn = openstack.connect(cloud='mordred')

    # Upload an image to the cloud
    image = conn.create_image(
        'ubuntu-trusty', filename='ubuntu-trusty.qcow2', wait=True)

    # Find a flavor with at least 512M of RAM
    flavor = conn.get_flavor_by_ram(512)

    # Boot a server, wait for it to boot, and then do whatever is needed
    # to get a public ip for it.
    conn.create_server(
        'my-server', image=image, flavor=flavor, wait=True, auto_ip=True)

openstack.config
================

``openstack.config`` will find cloud configuration for as few as 1 clouds and
as many as you want to put in a config file. It will read environment variables
and config files, and it also contains some vendor specific default values so
that you don't have to know extra info to use OpenStack

* If you have a config file, you will get the clouds listed in it
* If you have environment variables, you will get a cloud named `envvars`
* If you have neither, you will get a cloud named `defaults` with base defaults

Sometimes an example is nice.

Create a ``clouds.yaml`` file:

.. code-block:: yaml

     clouds:
      mordred:
        region_name: Dallas
        auth:
          username: 'mordred'
          password: XXXXXXX
          project_name: 'shade'
          auth_url: 'https://identity.example.com'

Please note: ``openstack.config`` will look for a file called ``clouds.yaml``
in the following locations:

* Current Directory
* ``~/.config/openstack``
* ``/etc/openstack``

More information at https://docs.openstack.org/openstacksdk/latest/user/config/configuration.html

Links
=====

* `Issue Tracker <https://storyboard.openstack.org/#!/project/openstack/openstacksdk>`_
* `Code Review <https://review.opendev.org/#/q/status:open+project:openstack/openstacksdk,n,z>`_
* `Documentation <https://docs.openstack.org/openstacksdk/latest/>`_
* `PyPI <https://pypi.org/project/openstacksdk/>`_
* `Mailing list <http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-discuss>`_
* `Release Notes <https://docs.openstack.org/releasenotes/openstacksdk>`_




  * [orderedmultidict-1.0.1](https://github.com/gruns/orderedmultidict) A multivalue dictionary is a dictionary that can store multiple values for the
same key. An ordered multivalue dictionary is a multivalue dictionary that
retains the order of insertions and deletions.

omdict retains method parity with dict.

Information and documentation at https://github.com/gruns/orderedmultidict.


  * [os-brick-2.9.1](https://docs.openstack.org/os-brick/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/os-brick.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

=====
brick
=====

.. image:: https://img.shields.io/pypi/v/os-brick.svg
    :target: https://pypi.org/project/os-brick/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/os-brick.svg
    :target: https://pypi.org/project/os-brick/
    :alt: Downloads

OpenStack Cinder brick library for managing local volume attaches


Features
--------

* Discovery of volumes being attached to a host for many transport protocols.
* Removal of volumes from a host.

Hacking
-------

Hacking on brick requires python-gdbm (for Debian derived distributions),
Python 2.7 and Python 3.4. A recent tox is required, as is a recent virtualenv
(13.1.0 or newer).

If "tox -e py34" fails with the error "db type could not be determined", remove
the .testrepository/ directory and then run "tox -e py34".

For any other information, refer to the developer documents:
  https://docs.openstack.org/os-brick/latest/
OR refer to the parent project, Cinder:
  https://docs.openstack.org/cinder/latest/
Release notes for the project can be found at:
  https://docs.openstack.org/releasenotes/os-brick

* License: Apache License, Version 2.0
* Source: https://opendev.org/openstack/os-brick
* Bugs: https://bugs.launchpad.net/os-brick




  * [os-client-config-1.32.0](https://docs.openstack.org/os-client-config/latest) ================
os-client-config
================

.. image:: https://governance.openstack.org/tc/badges/os-client-config.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. warning::
  `os-client-config` has been superceded by `openstacksdk`_. While
  `os-client-config` will continue to exist, it is highly recommended that
  users transition to using `openstacksdk`_ directly.

`os-client-config` is a library for collecting client configuration for
using an OpenStack cloud in a consistent and comprehensive manner. It
will find cloud config for as few as 1 cloud and as many as you want to
put in a config file. It will read environment variables and config files,
and it also contains some vendor specific default values so that you don't
have to know extra info to use OpenStack

* If you have a config file, you will get the clouds listed in it
* If you have environment variables, you will get a cloud named `envvars`
* If you have neither, you will get a cloud named `defaults` with base defaults

Source
------

* Free software: Apache license
* Documentation: http://docs.openstack.org/os-client-config/latest
* Source: http://opendev.org/openstack/os-client-config
* Bugs: https://storyboard.openstack.org/#!/project/openstack/os-client-config
* Release Notes https://docs.openstack.org/releasenotes/os-client-config

.. _openstacksdk: http://docs.openstack.org/openstacksdk/latest




  * [os-service-types-1.7.0](https://docs.openstack.org/os-service-types/latest/) ================
os-service-types
================

Python library for consuming OpenStack sevice-types-authority data

The `OpenStack Service Types Authority`_ contains information about official
OpenStack services and their historical ``service-type`` aliases.

The data is in JSON and the latest data should always be used. This simple
library exists to allow for easy consumption of the data, along with a built-in
version of the data to use in case network access is for some reason not
possible and local caching of the fetched data.

* Free software: Apache license
* Documentation: https://docs.openstack.org/os-service-types/latest/
* Source: https://opendev.org/openstack/os-service-types
* Bugs: https://storyboard.openstack.org/#!/project/904
* Release notes: https://docs.openstack.org/releasenotes/os-service-types/

.. _OpenStack Service Types Authority: https://service-types.openstack.org/




  * [os-testr-1.1.0](https://docs.openstack.org/os-testr/latest/) ========
os-testr
========

.. image:: https://img.shields.io/pypi/v/os-testr.svg
    :target: https://pypi.org/project/os-testr/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/os-testr.svg
    :target: https://pypi.org/project/os-testr/
    :alt: Downloads

A testr wrapper to provide functionality for OpenStack projects.

* Free software: Apache license
* Documentation: https://docs.openstack.org/os-testr/
* Source: https://opendev.org/openstack/os-testr
* Bugs: https://bugs.launchpad.net/os-testr

Features
--------

.. warning::
   ``ostestr`` command is deprecated. Use `stestr`_ command instead like
   following

   0. Install `stestr`_ (This step is already done if you're using ostestr.)
   1. You can use ``stestr run ...`` instead of ``ostestr ...``
   2. You can use ``stestr list ...`` instead of ``ostestr --list ...``

   For more sub commands and options, please refer to `stestr help` or the
   `stestr`_ document.

* ``ostestr``: a testr wrapper that uses subunit-trace for output and builds
  some helpful extra functionality around testr
* ``subunit-trace``: an output filter for a subunit stream which provides
  useful information about the run
* ``subunit2html``: generates a test results html page from a subunit stream
* ``generate-subunit``: generate a subunit stream for a single test

.. _stestr: https://stestr.readthedocs.io/




  * [os_diskconfig_python_novaclient_ext-0.1.3](UNKNOWN) ===================================
os-diskconfig-python-novaclient-ext
===================================


Disk Config extension for python-novaclient.


Install
=======

::

  pip install os_diskconfig_python_novaclient_ext


Usage
=====

This extension will automatically be detected by novaclient and will provide
the --disk-config option for `boot`, `zone-boot`, `rebuild`, and `resize`
commands.


Note
====

This is the *client* extension, for disk-config to work, the Nova service must
also be using the *server-side* extension as well.

  * [os_networksv2_python_novaclient_ext-0.26](https://github.com/rackerlabs/os_networksv2_python_novaclient_ext) =================================
os_networksv2_python_novaclient_ext
=================================

Adds network extension support to python-novaclient.

This extension is autodiscovered once installed. To use::

    pip install os_networksv2_python_novaclient_ext
    nova network                     Show a network
    nova network-create              Create a network
    nova network-delete              Delete a network
    nova network-list                List networks
    nova network-show                Show a network
  * [os_virtual_interfacesv2_python_novaclient_ext-0.20](https://github.com/rackerlabs/os_virtual_interfacesv2_ext) =============================================
os_virtual_interfacesv2_python_novaclient_ext
=============================================

Adds virtual interface extension support to python-novaclient.

This extension is autodiscovered once installed. To use::

    pip install os_virtual_interfacesv2_ext
    nova virtual-interface-create    Add a new virtual interface to an instance
    nova virtual-interface-delete    Removes the specified virtual interface from
                                     an instance
    nova virtual-interface-list      Lists the virtual interfaces for an instance
  * [osc-lib-1.13.0](https://docs.openstack.org/osc-lib/latest/) =======
osc-lib
=======

.. image:: https://img.shields.io/pypi/v/osc-lib.svg
    :target: https://pypi.org/project/osc-lib/
    :alt: Latest Version

OpenStackClient (aka OSC) is a command-line client for OpenStack. osc-lib
is a package of common support modules for writing OSC plugins.

* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - part of OpenStackClient
* `Bugs`_ - issue tracking
* `Source`_
* `Developer` - getting started as a developer
* `Contributing` - contributing code
* `Testing` - testing code
* IRC: #openstack-sdks on Freenode (irc.freenode.net)
* License: Apache 2.0

.. _PyPi: https://pypi.org/project/osc-lib
.. _Online Documentation: http://docs.openstack.org/osc-lib/latest/
.. _Launchpad project: https://launchpad.net/python-openstackclient
.. _Bugs: https://storyboard.openstack.org/#!/project_group/80
.. _Source: https://opendev.org/openstack/osc-lib
.. _Developer: http://docs.openstack.org/project-team-guide/project-setup/python.html
.. _Contributing: http://docs.openstack.org/infra/manual/developers.html
.. _Testing: http://docs.openstack.org/osc-lib/latest/contributor/#testing
.. _Release Notes: https://docs.openstack.org/releasenotes/osc-lib

Getting Started
===============

osc-lib can be installed from PyPI using pip::

    pip install osc-lib

Transition From OpenStackclient
===============================

This library was extracted from the main OSC repo after the OSC 2.4.0 release.
The following are the changes to imports that will cover the majority of
transition to using osc-lib:

* openstackclient.api.api -> osc_lib.api.api
* openstackclient.api.auth -> osc_lib.api.auth
* openstackclient.api.utils -> osc_lib.api.utils
* openstackclient.common.command -> osc_lib.command.command
* openstackclient.common.commandmanager -> osc_lib.command.commandmanager
* openstackclient.common.exceptions -> osc_lib.exceptions
* openstackclient.common.logs -> osc_lib.logs
* openstackclient.common.parseractions -> osc_lib.cli.parseractions
* openstackclient.common.session -> osc_lib.session
* openstackclient.common.utils -> osc_lib.utils
* openstackclient.i18n -> osc_lib.i18n
* openstackclient.shell -> osc_lib.shell

Also, some of the test fixtures and modules may be used:

* openstackclient.tests.fakes -> osc_lib.tests.fakes
* openstackclient.tests.utils -> osc_lib.tests.utils




  * [oscpy-0.4.0](https://github.com/kivy/oscpy) ### OSCPy

[![Coverage Status](https://coveralls.io/repos/github/kivy/oscpy/badge.svg?branch=master)](https://coveralls.io/github/kivy/oscpy?branch=master)
[![Build Status](https://travis-ci.org/kivy/oscpy.svg?branch=master)](https://travis-ci.org/kivy/oscpy)

A modern implementation of OSC for python2/3.

#### What is OSC.

OpenSoundControl is an UDP based network protocol, that is designed for fast
dispatching of time-sensitive messages, as the name suggests, it was designed
as a replacement for MIDI, but applies well to other situations. The protocol is
simple to use, OSC addresses look like http URLs, and accept various basic
types, such as string, float, int, etc. You can think of it basically as an
http POST, with less overhead.

You can learn more about OSC on [OpenSoundControl.org](http://opensoundcontrol.org/introduction-osc)

#### Goals

- python2.7/3.6+ compatibility (can be relaxed more on the python3 side
  if needed, but nothing before 2.7 will be supported)
- fast
- easy to use
- robust (returns meaningful errors in case of malformed messages,
  always do the right thing on correct messages)
- separation of concerns (message parsing vs communication)
- sync and async compatibility (threads, asyncio, trio…)
- clean and easy to read code

#### Features

- serialize and parse OSC data types/Messages/Bundles
- a thread based udp server to open sockets and bind callbacks on osc addresses on them
- a simple client

#### Install
```sh
pip install oscpy
```

#### Usage

Server (thread)

```python
from oscpy.server import OSCThreadServer
from time import sleep

def callback(values):
    print("got values: {}".format(values))

osc = OSCThreadServer()
sock = osc.listen(address='0.0.0.0', port=8000, default=True)
osc.bind(b'/address', callback)
sleep(1000)
osc.stop()
```

or you can use the decorator API.

Server (thread)

```python
from oscpy.server import OSCThreadServer
from time import sleep

osc = OSCThreadServer()
sock = osc.listen(address='0.0.0.0', port=8000, default=True)

@osc.address(b'/address')
def callback(values):
    print("got values: {}".format(values))

sleep(1000)
osc.stop()
```

Servers are also client, in the sense they can send messages and answer to
messages from other servers

```python
from oscpy.server import OSCThreadServer
from time import sleep

osc_1 = OSCThreadServer()
osc_1.listen(default=True)

@osc_1.address(b'/ping')
def ping(*values):
    print("ping called")
    if True in values:
        cont.append(True)
    else:
        osc_1.answer(b'/pong')

osc_2 = OSCThreadServer()
osc_2.listen(default=True)

@osc_2.address(b'/pong')
def pong(*values):
    print("pong called")
    osc_2.answer(b'/ping', [True])

osc_2.send_message(b'/ping', [], *osc_1.getaddress())

timeout = time() + 1
while not cont:
    if time() > timeout:
        raise OSError('timeout while waiting for success message.')
```


Server (async) (TODO!)

```python
from oscpy.server import OSCThreadServer

with OSCAsyncServer(port=8000) as OSC:
    for address, values in OSC.listen():
       if address == b'/example':
            print("got {} on /example".format(values))
       else:
            print("unknown address {}".format(address))
```

Client

```python
from oscpy.client import OSCClient

osc = OSCClient(address, port)
for i in range(10):
    osc.send_message(b'/ping', [i])
```

#### Unicode

By default, the server and client take bytes (encoded strings), not unicode
strings, for osc addresses as well as osc strings. However, you can pass an
`encoding` parameter to have your strings automatically encoded and decoded by
them, so your callbacks will get unicode strings (unicode in python2, str in
python3).

```python
osc = OSCThreadServer(encoding='utf8')
osc.listen(default=True)

values = []

@osc.address(u'/encoded')
def encoded(*val):
    for v in val:
        assert not isinstance(v, bytes)
    values.append(val)

send_message(
    u'/encoded',
    [u'hello world', u'ééééé ààààà'],
    *osc.getaddress(), encoding='utf8')
```

(`u` literals added here for clarity).

#### CLI

OSCPy provides an "oscli" util, to help with debugging:
- `oscli dump` to listen for messages and dump them
- `oscli send` to send messages or bundles to a server

See `oscli -h` for more information.

#### TODO

- real support for timetag (currently only supports optionally
  dropping late bundles, not delaying those with timetags in the future)
- support for additional argument types
- an asyncio-oriented server implementation
- examples & documentation

#### Contributing

Check out our [contribution guide](CONTRIBUTING.md) and feel free to improve OSCPy.

#### License

OSCPy is released under the terms of the MIT License.
Please see the [LICENSE.txt](LICENSE.txt) file.



  * [oslo.concurrency-3.29.1](https://docs.openstack.org/oslo.concurrency/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.concurrency.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

================
oslo.concurrency
================

.. image:: https://img.shields.io/pypi/v/oslo.concurrency.svg
    :target: https://pypi.org/project/oslo.concurrency/
    :alt: Latest Version

The oslo.concurrency library has utilities for safely running multi-thread,
multi-process applications using locking mechanisms and for running
external processes.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.concurrency/latest/
* Source: https://opendev.org/openstack/oslo.concurrency
* Bugs: https://bugs.launchpad.net/oslo.concurrency
* Release Notes: https://docs.openstack.org/releasenotes/oslo.concurrency/




  * [oslo.config-6.11.0](https://docs.openstack.org/oslo.config/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.config.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

==========================
Oslo Configuration Library
==========================

.. image:: https://img.shields.io/pypi/v/oslo.config.svg
    :target: https://pypi.org/project/oslo.config/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.config.svg
    :target: https://pypi.org/project/oslo.config/
    :alt: Downloads

The Oslo configuration API supports parsing command line arguments and
.ini style configuration files.

* License: Apache License, Version 2.0
* Documentation: https://docs.openstack.org/oslo.config/latest/
* Source: https://git.openstack.org/cgit/openstack/oslo.config
* Bugs: https://bugs.launchpad.net/oslo.config
* Release notes:  https://docs.openstack.org/releasenotes/oslo.config/




  * [oslo.context-2.22.1](https://docs.openstack.org/oslo.context/latest/) ====================
Oslo Context Library
====================

The Oslo context library has helpers to maintain useful information
about a request context. The request context is usually populated in
the WSGI pipeline and used by various modules such as logging.

* License: Apache License, Version 2.0
* Documentation: https://docs.openstack.org/oslo.context/latest/
* Source: https://opendev.org/openstack/oslo.context
* Bugs: https://bugs.launchpad.net/oslo.context
* Release notes:  https://docs.openstack.org/releasenotes/oslo.context/

Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.context.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

.. image:: https://img.shields.io/pypi/v/oslo.context.svg
    :target: https://pypi.org/project/oslo.context/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.context.svg
    :target: https://pypi.org/project/oslo.context/
    :alt: Downloads




  * [oslo.db-5.0.1](https://docs.openstack.org/oslo.db/latest) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.db.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

===============================================
 oslo.db -- OpenStack Database Pattern Library
===============================================

.. image:: https://img.shields.io/pypi/v/oslo.db.svg
    :target: https://pypi.org/project/oslo.db/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.db.svg
    :target: https://pypi.org/project/oslo.db/
    :alt: Downloads

The oslo db (database) handling library, provides database
connectivity to different database backends and various other helper
utils.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.db/latest
* Source: https://opendev.org/openstack/oslo.db
* Bugs: https://bugs.launchpad.net/oslo.db
* Release notes:  https://docs.openstack.org/releasenotes/oslo.db/




  * [oslo.i18n-3.23.1](https://docs.openstack.org/oslo.i18n/latest) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.i18n.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

==================================================
 oslo.i18n -- Oslo Internationalization Utilities
==================================================

.. image:: https://img.shields.io/pypi/v/oslo.i18n.svg
    :target: https://pypi.org/project/oslo.i18n/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.i18n.svg
    :target: https://pypi.org/project/oslo.i18n/
    :alt: Downloads

The oslo.i18n library contain utilities for working with
internationalization (i18n) features, especially translation for text
strings in an application or library.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.i18n/latest
* Source: https://opendev.org/openstack/oslo.i18n
* Bugs: https://bugs.launchpad.net/oslo.i18n
* Release notes:  https://docs.openstack.org/releasenotes/oslo.i18n/




  * [oslo.log-3.44.0](https://docs.openstack.org/oslo.log/latest) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.log.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

================================
oslo.log -- Oslo Logging Library
================================

.. image:: https://img.shields.io/pypi/v/oslo.log.svg
    :target: https://pypi.org/project/oslo.log/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.log.svg
    :target: https://pypi.org/project/oslo.log/
    :alt: Downloads

The oslo.log (logging) configuration library provides standardized
configuration for all openstack projects. It also provides custom
formatters, handlers and support for context specific
logging (like resource id's etc).

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.log/latest/
* Source: https://opendev.org/openstack/oslo.log
* Bugs: https://bugs.launchpad.net/oslo.log
* Release notes: https://docs.openstack.org/releasenotes/oslo.log/




  * [oslo.privsep-1.33.1](https://docs.openstack.org/oslo.privsep/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.privsep.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

============
oslo.privsep
============

.. image:: https://img.shields.io/pypi/v/oslo.privsep.svg
    :target: https://pypi.org/project/oslo.privsep/
    :alt: Latest Version

OpenStack library for privilege separation

This library helps applications perform actions which require more or
less privileges than they were started with in a safe, easy to code
and easy to use manner. For more information on why this is generally
a good idea please read over the `principle of least privilege`_ and
the `specification`_ which created this library.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.privsep/latest/
* Source: https://opendev.org/openstack/oslo.privsep
* Bugs: https://bugs.launchpad.net/oslo.privsep
* Release Notes: https://docs.openstack.org/releasenotes/oslo.privsep

.. _principle of least privilege: https://en.wikipedia.org/wiki/\
                                  Principle_of_least_privilege
.. _specification: https://specs.openstack.org/openstack/\
                   oslo-specs/specs/liberty/privsep.html




  * [oslo.serialization-2.29.1](https://docs.openstack.org/oslo.serialization/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.serialization.svg
    :target: https://governance.openstack.org/tc/ference/tags/index.html

.. Change things from this point on

===================
 oslo.serialization
===================

.. image:: https://img.shields.io/pypi/v/oslo.serialization.svg
    :target: https://pypi.org/project/oslo.serialization/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.serialization.svg
    :target: https://pypi.org/project/oslo.serialization/
    :alt: Downloads

The oslo.serialization library provides support for representing objects
in transmittable and storable formats, such as Base64, JSON and MessagePack.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.serialization/latest/
* Source: https://opendev.org/openstack/oslo.serialization
* Bugs: https://bugs.launchpad.net/oslo.serialization
* Release notes: https://docs.openstack.org/releasenotes/oslo.serialization/




  * [oslo.service-1.40.0](https://docs.openstack.org/oslo.service/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.service.svg
    :target: https://governance.openstack.org/tc/ference/tags/index.html

.. Change things from this point on

========================================================
 oslo.service -- Library for running OpenStack services
========================================================

.. image:: https://img.shields.io/pypi/v/oslo.service.svg
    :target: https://pypi.org/project/oslo.service/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.service.svg
    :target: https://pypi.org/project/oslo.service/
    :alt: Downloads

oslo.service provides a framework for defining new long-running
services using the patterns established by other OpenStack
applications. It also includes utilities long-running applications
might need for working with SSL or WSGI, performing periodic
operations, interacting with systemd, etc.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.service/latest/
* Source: https://opendev.org/openstack/oslo.service
* Bugs: https://bugs.launchpad.net/oslo.service
* Release notes: https://docs.openstack.org/releasenotes/oslo.service/




  * [oslo.utils-3.41.0](https://docs.openstack.org/oslo.utils/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslo.utils.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

==========
oslo.utils
==========

.. image:: https://img.shields.io/pypi/v/oslo.utils.svg
    :target: https://pypi.org/project/oslo.utils/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/oslo.utils.svg
    :target: https://pypi.org/project/oslo.utils/
    :alt: Downloads

The oslo.utils library provides support for common utility type functions,
such as encoding, exception handling, string manipulation, and time handling.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslo.utils/latest/
* Source: https://opendev.org/openstack/oslo.utils
* Bugs: https://bugs.launchpad.net/oslo.utils
* Release notes: https://docs.openstack.org/releasenotes/oslo.utils/




  * [oslosphinx-4.18.0](http://docs.openstack.org/developer/oslosphinx) =============================
 OpenStack Sphinx Extensions
=============================

oslosphinx is obsolete. The openstackdocstheme package should be used
instead. oslosphinx will be maintained for the pike, ocata, and newton
release series and completely remove after that pike is marked
end-of-life.

The contents of this repository are still available in the Git source
code management system.  To see the contents of this repository before
it reached its end of life, please check out the previous commit with
"git checkout HEAD^1", or check out one of the supported stable
branches.

For any further questions, please email
openstack-dev@lists.openstack.org or join #openstack-oslo on Freenode.




  * [oslotest-3.8.0](https://docs.openstack.org/oslotest/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/oslotest.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

=====================================================
oslotest -- OpenStack Testing Framework and Utilities
=====================================================

The Oslo Test framework provides common fixtures, support for debugging, and
better support for mocking results.

* Free software: Apache license
* Documentation: https://docs.openstack.org/oslotest/latest/
* Source: https://opendev.org/openstack/oslotest
* Bugs: https://bugs.launchpad.net/oslotest
* Release notes: https://docs.openstack.org/releasenotes/oslotest




  * [packaging-19.1](https://github.com/pypa/packaging) packaging
=========

Core utilities for Python packages.

The ``packaging`` project includes the following: version handling, specifiers,
markers, requirements, tags, utilities.

Documentation
-------------

The `documentation`_ provides information and the API for the following:

- Version Handling
- Specifiers
- Markers
- Requirements
- Tags
- Utilities

Installation
------------

Use ``pip`` to install these utilities::

    pip install packaging

Discussion
----------

If you run into bugs, you can file them in our `issue tracker`_.

You can also join ``#pypa`` on Freenode to ask questions or get involved.


.. _`documentation`: https://packaging.pypa.io/
.. _`issue tracker`: https://github.com/pypa/packaging/issues


Code of Conduct
---------------

Everyone interacting in the packaging project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/

Contributing
------------

The ``CONTRIBUTING.rst`` file outlines how to contribute to this project as
well as how to report a potential security issue. The documentation for this
project also covers information about `project development`_ and `security`_.

.. _`project development`: https://packaging.pypa.io/en/latest/development/
.. _`security`: https://packaging.pypa.io/en/latest/security/

Project History
---------------

Please review the ``CHANGELOG.rst`` file or the `Changelog documentation`_ for
recent changes and project history.

.. _`Changelog documentation`: https://packaging.pypa.io/en/latest/changelog/

Changelog
---------

19.2 - 2019-09-18
~~~~~~~~~~~~~~~~~

* Remove dependency on ``attrs`` (`#178 <https://github.com/pypa/packaging/issues/178>`__, `#179 <https://github.com/pypa/packaging/issues/179>`__)

* Use appropriate fallbacks for CPython ABI tag (`#181 <https://github.com/pypa/packaging/issues/181>`__, `#185 <https://github.com/pypa/packaging/issues/185>`__)

* Add manylinux2014 support (`#186 <https://github.com/pypa/packaging/issues/186>`__)

* Improve ABI detection (`#181 <https://github.com/pypa/packaging/issues/181>`__)

* Properly handle debug wheels for Python 3.8 (`#172 <https://github.com/pypa/packaging/issues/172>`__)

* Improve detection of debug builds on Windows (`#194 <https://github.com/pypa/packaging/issues/194>`__)

19.1 - 2019-07-30
~~~~~~~~~~~~~~~~~

* Add the ``packaging.tags`` module. (`#156 <https://github.com/pypa/packaging/issues/156>`__)

* Correctly handle two-digit versions in ``python_version`` (`#119 <https://github.com/pypa/packaging/issues/119>`__)


19.0 - 2019-01-20
~~~~~~~~~~~~~~~~~

* Fix string representation of PEP 508 direct URL requirements with markers.

* Better handling of file URLs

  This allows for using ``file:///absolute/path``, which was previously
  prevented due to the missing ``netloc``.

  This allows for all file URLs that ``urlunparse`` turns back into the
  original URL to be valid.


18.0 - 2018-09-26
~~~~~~~~~~~~~~~~~

* Improve error messages when invalid requirements are given. (`#129 <https://github.com/pypa/packaging/issues/129>`__)


17.1 - 2017-02-28
~~~~~~~~~~~~~~~~~

* Fix ``utils.canonicalize_version`` when supplying non PEP 440 versions.


17.0 - 2017-02-28
~~~~~~~~~~~~~~~~~

* Drop support for python 2.6, 3.2, and 3.3.

* Define minimal pyparsing version to 2.0.2 (`#91 <https://github.com/pypa/packaging/issues/91>`__).

* Add ``epoch``, ``release``, ``pre``, ``dev``, and ``post`` attributes to
  ``Version`` and ``LegacyVersion`` (`#34 <https://github.com/pypa/packaging/issues/34>`__).

* Add ``Version().is_devrelease`` and ``LegacyVersion().is_devrelease`` to
  make it easy to determine if a release is a development release.

* Add ``utils.canonicalize_version`` to canonicalize version strings or
  ``Version`` instances (`#121 <https://github.com/pypa/packaging/issues/121>`__).


16.8 - 2016-10-29
~~~~~~~~~~~~~~~~~

* Fix markers that utilize ``in`` so that they render correctly.

* Fix an erroneous test on Python RC releases.


16.7 - 2016-04-23
~~~~~~~~~~~~~~~~~

* Add support for the deprecated ``python_implementation`` marker which was
  an undocumented setuptools marker in addition to the newer markers.


16.6 - 2016-03-29
~~~~~~~~~~~~~~~~~

* Add support for the deprecated, PEP 345 environment markers in addition to
  the newer markers.


16.5 - 2016-02-26
~~~~~~~~~~~~~~~~~

* Fix a regression in parsing requirements with whitespaces between the comma
  separators.


16.4 - 2016-02-22
~~~~~~~~~~~~~~~~~

* Fix a regression in parsing requirements like ``foo (==4)``.


16.3 - 2016-02-21
~~~~~~~~~~~~~~~~~

* Fix a bug where ``packaging.requirements:Requirement`` was overly strict when
  matching legacy requirements.


16.2 - 2016-02-09
~~~~~~~~~~~~~~~~~

* Add a function that implements the name canonicalization from PEP 503.


16.1 - 2016-02-07
~~~~~~~~~~~~~~~~~

* Implement requirement specifiers from PEP 508.


16.0 - 2016-01-19
~~~~~~~~~~~~~~~~~

* Relicense so that packaging is available under *either* the Apache License,
  Version 2.0 or a 2 Clause BSD license.

* Support installation of packaging when only distutils is available.

* Fix ``==`` comparison when there is a prefix and a local version in play.
  (`#41 <https://github.com/pypa/packaging/issues/41>`__).

* Implement environment markers from PEP 508.


15.3 - 2015-08-01
~~~~~~~~~~~~~~~~~

* Normalize post-release spellings for rev/r prefixes. `#35 <https://github.com/pypa/packaging/issues/35>`__


15.2 - 2015-05-13
~~~~~~~~~~~~~~~~~

* Fix an error where the arbitary specifier (``===``) was not correctly
  allowing pre-releases when it was being used.

* Expose the specifier and version parts through properties on the
  ``Specifier`` classes.

* Allow iterating over the ``SpecifierSet`` to get access to all of the
  ``Specifier`` instances.

* Allow testing if a version is contained within a specifier via the ``in``
  operator.


15.1 - 2015-04-13
~~~~~~~~~~~~~~~~~

* Fix a logic error that was causing inconsistent answers about whether or not
  a pre-release was contained within a ``SpecifierSet`` or not.


15.0 - 2015-01-02
~~~~~~~~~~~~~~~~~

* Add ``Version().is_postrelease`` and ``LegacyVersion().is_postrelease`` to
  make it easy to determine if a release is a post release.

* Add ``Version().base_version`` and ``LegacyVersion().base_version`` to make
  it easy to get the public version without any pre or post release markers.

* Support the update to PEP 440 which removed the implied ``!=V.*`` when using
  either ``>V`` or ``<V`` and which instead special cased the handling of
  pre-releases, post-releases, and local versions when using ``>V`` or ``<V``.


14.5 - 2014-12-17
~~~~~~~~~~~~~~~~~

* Normalize release candidates as ``rc`` instead of ``c``.

* Expose the ``VERSION_PATTERN`` constant, a regular expression matching
  a valid version.


14.4 - 2014-12-15
~~~~~~~~~~~~~~~~~

* Ensure that versions are normalized before comparison when used in a
  specifier with a less than (``<``) or greater than (``>``) operator.


14.3 - 2014-11-19
~~~~~~~~~~~~~~~~~

* **BACKWARDS INCOMPATIBLE** Refactor specifier support so that it can sanely
  handle legacy specifiers as well as PEP 440 specifiers.

* **BACKWARDS INCOMPATIBLE** Move the specifier support out of
  ``packaging.version`` into ``packaging.specifiers``.


14.2 - 2014-09-10
~~~~~~~~~~~~~~~~~

* Add prerelease support to ``Specifier``.
* Remove the ability to do ``item in Specifier()`` and replace it with
  ``Specifier().contains(item)`` in order to allow flags that signal if a
  prerelease should be accepted or not.
* Add a method ``Specifier().filter()`` which will take an iterable and returns
  an iterable with items that do not match the specifier filtered out.


14.1 - 2014-09-08
~~~~~~~~~~~~~~~~~

* Allow ``LegacyVersion`` and ``Version`` to be sorted together.
* Add ``packaging.version.parse()`` to enable easily parsing a version string
  as either a ``Version`` or a ``LegacyVersion`` depending on it's PEP 440
  validity.


14.0 - 2014-09-05
~~~~~~~~~~~~~~~~~

* Initial release.


.. _`master`: https://github.com/pypa/packaging/



  * [pandas-0.23.4](http://pandas.pydata.org) 
**pandas** is a Python package providing fast, flexible, and expressive data
structures designed to make working with structured (tabular, multidimensional,
potentially heterogeneous) and time series data both easy and intuitive. It
aims to be the fundamental high-level building block for doing practical,
**real world** data analysis in Python. Additionally, it has the broader goal
of becoming **the most powerful and flexible open source data analysis /
manipulation tool available in any language**. It is already well on its way
toward this goal.

pandas is well suited for many different kinds of data:

  - Tabular data with heterogeneously-typed columns, as in an SQL table or
    Excel spreadsheet
  - Ordered and unordered (not necessarily fixed-frequency) time series data.
  - Arbitrary matrix data (homogeneously typed or heterogeneous) with row and
    column labels
  - Any other form of observational / statistical data sets. The data actually
    need not be labeled at all to be placed into a pandas data structure

The two primary data structures of pandas, Series (1-dimensional) and DataFrame
(2-dimensional), handle the vast majority of typical use cases in finance,
statistics, social science, and many areas of engineering. For R users,
DataFrame provides everything that R's ``data.frame`` provides and much
more. pandas is built on top of `NumPy <http://www.numpy.org>`__ and is
intended to integrate well within a scientific computing environment with many
other 3rd party libraries.

Here are just a few of the things that pandas does well:

  - Easy handling of **missing data** (represented as NaN) in floating point as
    well as non-floating point data
  - Size mutability: columns can be **inserted and deleted** from DataFrame and
    higher dimensional objects
  - Automatic and explicit **data alignment**: objects can be explicitly
    aligned to a set of labels, or the user can simply ignore the labels and
    let `Series`, `DataFrame`, etc. automatically align the data for you in
    computations
  - Powerful, flexible **group by** functionality to perform
    split-apply-combine operations on data sets, for both aggregating and
    transforming data
  - Make it **easy to convert** ragged, differently-indexed data in other
    Python and NumPy data structures into DataFrame objects
  - Intelligent label-based **slicing**, **fancy indexing**, and **subsetting**
    of large data sets
  - Intuitive **merging** and **joining** data sets
  - Flexible **reshaping** and pivoting of data sets
  - **Hierarchical** labeling of axes (possible to have multiple labels per
    tick)
  - Robust IO tools for loading data from **flat files** (CSV and delimited),
    Excel files, databases, and saving / loading data from the ultrafast **HDF5
    format**
  - **Time series**-specific functionality: date range generation and frequency
    conversion, moving window statistics, moving window linear regressions,
    date shifting and lagging, etc.

Many of these principles are here to address the shortcomings frequently
experienced using other languages / scientific research environments. For data
scientists, working with data is typically divided into multiple stages:
munging and cleaning data, analyzing / modeling it, then organizing the results
of the analysis into a form suitable for plotting or tabular display. pandas is
the ideal tool for all of these tasks.



  * [pandocfilters-1.4.2](http://github.com/jgm/pandocfilters) pandocfilters
=============

A python module for writing `pandoc <http://pandoc.org/>`_ filters

What are pandoc filters?
--------------------------
Pandoc filters
are pipes that read a JSON serialization of the Pandoc AST
from stdin, transform it in some way, and write it to stdout.
They can be used with pandoc (>= 1.12) either using pipes ::

    pandoc -t json -s | ./caps.py | pandoc -f json

or using the ``--filter`` (or ``-F``) command-line option. ::

    pandoc --filter ./caps.py -s

For more on pandoc filters, see the pandoc documentation under ``--filter``
and `the tutorial on writing filters`__.

__ http://johnmacfarlane.net/pandoc/scripting.html

Compatibility
----------------
Pandoc 1.16 introduced link and image `attributes` to the existing
`caption` and `target` arguments, requiring a change in pandocfilters
that breaks backwards compatibility. Consequently, you should use:

- pandocfilters version <= 1.2.4 for pandoc versions 1.12--1.15, and
- pandocfilters version >= 1.3.0 for pandoc versions >= 1.16.

Pandoc 1.17.3 (pandoc-types 1.17.*) introduced a new JSON format.
pandocfilters 1.4.0 should work with both the old and the new
format.

Installing
--------------
Run this inside the present directory::

    python setup.py install

Or install from PyPI::

    pip install pandocfilters

Available functions
----------------------
The main functions ``pandocfilters`` exports are

-  ``walk(x, action, format, meta)``

   Walk a tree, applying an action to every object. Returns a modified
   tree. An action is a function of the form
   ``action(key, value, format, meta)``, where:

   -  ``key`` is the type of the pandoc object (e.g. 'Str', 'Para')
   -  ``value`` is the contents of the object (e.g. a string for 'Str', a list of
      inline elements for 'Para')
   -  ``format`` is the target output format (as supplied by the
      ``format`` argument of ``walk``)
   -  ``meta`` is the document's metadata

   The return of an action is either:

   -  ``None``: this means that the object should remain unchanged
   -  a pandoc object: this will replace the original object
   -  a list of pandoc objects: these will replace the original object;
      the list is merged with the neighbors of the orignal objects
      (spliced into the list the original object belongs to); returning
      an empty list deletes the object

-  ``toJSONFilter(action)``

   Like ``toJSONFilters``, but takes a single action as argument.

-  ``toJSONFilters(actions)``

   Generate a JSON-to-JSON filter from stdin to stdout

   The filter:

   -  reads a JSON-formatted pandoc document from stdin
   -  transforms it by walking the tree and performing the actions
   -  returns a new JSON-formatted pandoc document to stdout

   The argument ``actions`` is a list of functions of the form
   ``action(key, value, format, meta)``, as described in more detail
   under ``walk``.

   This function calls ``applyJSONFilters``, with the ``format``
   argument provided by the first command-line argument, if present.
   (Pandoc sets this by default when calling filters.)

-  ``applyJSONFilters(actions, source, format="")``

   Walk through JSON structure and apply filters

   This:

   -  reads a JSON-formatted pandoc document from a source string
   -  transforms it by walking the tree and performing the actions
   -  returns a new JSON-formatted pandoc document as a string

   The ``actions`` argument is a list of functions (see ``walk`` for a
   full description).

   The argument ``source`` is a string encoded JSON object.

   The argument ``format`` is a string describing the output format.

   Returns a the new JSON-formatted pandoc document.

-  ``stringify(x)``

   Walks the tree x and returns concatenated string content, leaving out
   all formatting.

-  ``attributes(attrs)``

   Returns an attribute list, constructed from the dictionary attrs.

How to use
----------
Most users will only need ``toJSONFilter``.  Here is a simple example
of its use::

    #!/usr/bin/env python

    """
    Pandoc filter to convert all regular text to uppercase.
    Code, link URLs, etc. are not affected.
    """

    from pandocfilters import toJSONFilter, Str

    def caps(key, value, format, meta):
      if key == 'Str':
        return Str(value.upper())

    if __name__ == "__main__":
      toJSONFilter(caps)

Examples
--------

The examples subdirectory in the source repository contains the
following filters. These filters should provide a useful starting point
for developing your own pandocfilters.

``abc.py``
    Pandoc filter to process code blocks with class ``abc`` containing ABC
    notation into images. Assumes that abcm2ps and ImageMagick's convert
    are in the path. Images are put in the abc-images directory.

``caps.py``
    Pandoc filter to convert all regular text to uppercase. Code, link
    URLs, etc. are not affected.

``comments.py``
    Pandoc filter that causes everything between
    ``<!-- BEGIN COMMENT -->`` and ``<!-- END COMMENT -->`` to be ignored.
    The comment lines must appear on lines by themselves, with blank
    lines surrounding

``deemph.py``
    Pandoc filter that causes emphasized text to be displayed in ALL
    CAPS.

``deflists.py``
    Pandoc filter to convert definition lists to bullet lists with the
    defined terms in strong emphasis (for compatibility with standard
    markdown).

``gabc.py``
    Pandoc filter to convert code blocks with class "gabc" to LaTeX
    \\gabcsnippet commands in LaTeX output, and to images in HTML output.

``graphviz.py``
    Pandoc filter to process code blocks with class ``graphviz`` into
    graphviz-generated images.

``lilypond.py``
    Pandoc filter to process code blocks with class "ly" containing
    Lilypond notation.

``metavars.py``
    Pandoc filter to allow interpolation of metadata fields into a
    document. ``%{fields}`` will be replaced by the field's value, assuming
    it is of the type ``MetaInlines`` or ``MetaString``.

``myemph.py``
    Pandoc filter that causes emphasis to be rendered using the custom
    macro ``\myemph{...}`` rather than ``\emph{...}`` in latex. Other output
    formats are unaffected.

``plantuml.py``
    Pandoc filter to process code blocks with class ``plantuml`` to images.
    Needs `plantuml.jar` from http://plantuml.com/.

``theorem.py``
    Pandoc filter to convert divs with ``class="theorem"`` to LaTeX theorem
    environments in LaTeX output, and to numbered theorems in HTML
    output.

``tikz.py``
    Pandoc filter to process raw latex tikz environments into images.
    Assumes that pdflatex is in the path, and that the standalone
    package is available. Also assumes that ImageMagick's convert is in
    the path. Images are put in the ``tikz-images`` directory.


  * [paramiko-2.4.1](https://github.com/paramiko/paramiko/) 
This is a library for making SSH2 connections (client or server).
Emphasis is on using SSH2 as an alternative to SSL for making secure
connections between python scripts.  All major ciphers and hash methods
are supported.  SFTP client and server mode are both supported too.

Required packages:
    Cryptography

To install the development version, ``pip install -e
git+https://github.com/paramiko/paramiko/#egg=paramiko``.



  * [parse-1.12.0](https://github.com/r1chardj0n3s/parse) Parse strings using a specification based on the Python format() syntax.

   ``parse()`` is the opposite of ``format()``

The module is set up to only export ``parse()``, ``search()``, ``findall()``,
and ``with_pattern()`` when ``import \*`` is used:

>>> from parse import *

From there it's a simple thing to parse a string:

>>> parse("It's {}, I love it!", "It's spam, I love it!")
<Result ('spam',) {}>
>>> _[0]
'spam'

Or to search a string for some pattern:

>>> search('Age: {:d}\n', 'Name: Rufus\nAge: 42\nColor: red\n')
<Result (42,) {}>

Or find all the occurrences of some pattern in a string:

>>> ''.join(r.fixed[0] for r in findall(">{}<", "<p>the <b>bold</b> text</p>"))
'the bold text'

If you're going to use the same pattern to match lots of strings you can
compile it once:

>>> from parse import compile
>>> p = compile("It's {}, I love it!")
>>> print(p)
<Parser "It's {}, I love it!">
>>> p.parse("It's spam, I love it!")
<Result ('spam',) {}>

("compile" is not exported for ``import *`` usage as it would override the
built-in ``compile()`` function)

The default behaviour is to match strings case insensitively. You may match with
case by specifying `case_sensitive=True`:

>>> parse('SPAM', 'spam', case_sensitive=True) is None
True


Format Syntax
-------------

A basic version of the `Format String Syntax`_ is supported with anonymous
(fixed-position), named and formatted fields::

   {[field name]:[format spec]}

Field names must be a valid Python identifiers, including dotted names;
element indexes imply dictionaries (see below for example).

Numbered fields are also not supported: the result of parsing will include
the parsed fields in the order they are parsed.

The conversion of fields to types other than strings is done based on the
type in the format specification, which mirrors the ``format()`` behaviour.
There are no "!" field conversions like ``format()`` has.

Some simple parse() format string examples:

>>> parse("Bring me a {}", "Bring me a shrubbery")
<Result ('shrubbery',) {}>
>>> r = parse("The {} who say {}", "The knights who say Ni!")
>>> print(r)
<Result ('knights', 'Ni!') {}>
>>> print(r.fixed)
('knights', 'Ni!')
>>> r = parse("Bring out the holy {item}", "Bring out the holy hand grenade")
>>> print(r)
<Result () {'item': 'hand grenade'}>
>>> print(r.named)
{'item': 'hand grenade'}
>>> print(r['item'])
hand grenade
>>> 'item' in r
True

Note that `in` only works if you have named fields. Dotted names and indexes
are possible though the application must make additional sense of the result:

>>> r = parse("Mmm, {food.type}, I love it!", "Mmm, spam, I love it!")
>>> print(r)
<Result () {'food.type': 'spam'}>
>>> print(r.named)
{'food.type': 'spam'}
>>> print(r['food.type'])
spam
>>> r = parse("My quest is {quest[name]}", "My quest is to seek the holy grail!")
>>> print(r)
<Result () {'quest': {'name': 'to seek the holy grail!'}}>
>>> print(r['quest'])
{'name': 'to seek the holy grail!'}
>>> print(r['quest']['name'])
to seek the holy grail!

If the text you're matching has braces in it you can match those by including
a double-brace ``{{`` or ``}}`` in your format string, just like format() does.


Format Specification
--------------------

Most often a straight format-less ``{}`` will suffice where a more complex
format specification might have been used.

Most of `format()`'s `Format Specification Mini-Language`_ is supported:

   [[fill]align][0][width][.precision][type]

The differences between `parse()` and `format()` are:

- The align operators will cause spaces (or specified fill character) to be
  stripped from the parsed value. The width is not enforced; it just indicates
  there may be whitespace or "0"s to strip.
- Numeric parsing will automatically handle a "0b", "0o" or "0x" prefix.
  That is, the "#" format character is handled automatically by d, b, o
  and x formats. For "d" any will be accepted, but for the others the correct
  prefix must be present if at all.
- Numeric sign is handled automatically.
- The thousands separator is handled automatically if the "n" type is used.
- The types supported are a slightly different mix to the format() types.  Some
  format() types come directly over: "d", "n", "%", "f", "e", "b", "o" and "x".
  In addition some regular expression character group types "D", "w", "W", "s"
  and "S" are also available.
- The "e" and "g" types are case-insensitive so there is not need for
  the "E" or "G" types.

===== =========================================== ========
Type  Characters Matched                          Output
===== =========================================== ========
l     Letters (ASCII)                             str
w     Letters, numbers and underscore             str
W     Not letters, numbers and underscore         str
s     Whitespace                                  str
S     Non-whitespace                              str
d     Digits (effectively integer numbers)        int
D     Non-digit                                   str
n     Numbers with thousands separators (, or .)  int
%     Percentage (converted to value/100.0)       float
f     Fixed-point numbers                         float
F     Decimal numbers                             Decimal
e     Floating-point numbers with exponent        float
      e.g. 1.1e-10, NAN (all case insensitive)
g     General number format (either d, f or e)    float
b     Binary numbers                              int
o     Octal numbers                               int
x     Hexadecimal numbers (lower and upper case)  int
ti    ISO 8601 format date/time                   datetime
      e.g. 1972-01-20T10:21:36Z ("T" and "Z"
      optional)
te    RFC2822 e-mail format date/time             datetime
      e.g. Mon, 20 Jan 1972 10:21:36 +1000
tg    Global (day/month) format date/time         datetime
      e.g. 20/1/1972 10:21:36 AM +1:00
ta    US (month/day) format date/time             datetime
      e.g. 1/20/1972 10:21:36 PM +10:30
tc    ctime() format date/time                    datetime
      e.g. Sun Sep 16 01:03:52 1973
th    HTTP log format date/time                   datetime
      e.g. 21/Nov/2011:00:07:11 +0000
ts    Linux system log format date/time           datetime
      e.g. Nov  9 03:37:44
tt    Time                                        time
      e.g. 10:21:36 PM -5:30
===== =========================================== ========

Some examples of typed parsing with ``None`` returned if the typing
does not match:

>>> parse('Our {:d} {:w} are...', 'Our 3 weapons are...')
<Result (3, 'weapons') {}>
>>> parse('Our {:d} {:w} are...', 'Our three weapons are...')
>>> parse('Meet at {:tg}', 'Meet at 1/2/2011 11:00 PM')
<Result (datetime.datetime(2011, 2, 1, 23, 0),) {}>

And messing about with alignment:

>>> parse('with {:>} herring', 'with     a herring')
<Result ('a',) {}>
>>> parse('spam {:^} spam', 'spam    lovely     spam')
<Result ('lovely',) {}>

Note that the "center" alignment does not test to make sure the value is
centered - it just strips leading and trailing whitespace.

Width and precision may be used to restrict the size of matched text
from the input. Width specifies a minimum size and precision specifies
a maximum. For example:

>>> parse('{:.2}{:.2}', 'look')           # specifying precision
<Result ('lo', 'ok') {}>
>>> parse('{:4}{:4}', 'look at that')     # specifying width
<Result ('look', 'at that') {}>
>>> parse('{:4}{:.4}', 'look at that')    # specifying both
<Result ('look at ', 'that') {}>
>>> parse('{:2d}{:2d}', '0440')           # parsing two contiguous numbers
<Result (4, 40) {}>

Some notes for the date and time types:

- the presence of the time part is optional (including ISO 8601, starting
  at the "T"). A full datetime object will always be returned; the time
  will be set to 00:00:00. You may also specify a time without seconds.
- when a seconds amount is present in the input fractions will be parsed
  to give microseconds.
- except in ISO 8601 the day and month digits may be 0-padded.
- the date separator for the tg and ta formats may be "-" or "/".
- named months (abbreviations or full names) may be used in the ta and tg
  formats in place of numeric months.
- as per RFC 2822 the e-mail format may omit the day (and comma), and the
  seconds but nothing else.
- hours greater than 12 will be happily accepted.
- the AM/PM are optional, and if PM is found then 12 hours will be added
  to the datetime object's hours amount - even if the hour is greater
  than 12 (for consistency.)
- in ISO 8601 the "Z" (UTC) timezone part may be a numeric offset
- timezones are specified as "+HH:MM" or "-HH:MM". The hour may be one or two
  digits (0-padded is OK.) Also, the ":" is optional.
- the timezone is optional in all except the e-mail format (it defaults to
  UTC.)
- named timezones are not handled yet.

Note: attempting to match too many datetime fields in a single parse() will
currently result in a resource allocation issue. A TooManyFields exception
will be raised in this instance. The current limit is about 15. It is hoped
that this limit will be removed one day.

.. _`Format String Syntax`:
  http://docs.python.org/library/string.html#format-string-syntax
.. _`Format Specification Mini-Language`:
  http://docs.python.org/library/string.html#format-specification-mini-language


Result and Match Objects
------------------------

The result of a ``parse()`` and ``search()`` operation is either ``None`` (no match), a
``Result`` instance or a ``Match`` instance if ``evaluate_result`` is False.

The ``Result`` instance has three attributes:

fixed
   A tuple of the fixed-position, anonymous fields extracted from the input.
named
   A dictionary of the named fields extracted from the input.
spans
   A dictionary mapping the names and fixed position indices matched to a
   2-tuple slice range of where the match occurred in the input.
   The span does not include any stripped padding (alignment or width).

The ``Match`` instance has one method:

evaluate_result()
   Generates and returns a ``Result`` instance for this ``Match`` object.



Custom Type Conversions
-----------------------

If you wish to have matched fields automatically converted to your own type you
may pass in a dictionary of type conversion information to ``parse()`` and
``compile()``.

The converter will be passed the field string matched. Whatever it returns
will be substituted in the ``Result`` instance for that field.

Your custom type conversions may override the builtin types if you supply one
with the same identifier.

>>> def shouty(string):
...    return string.upper()
...
>>> parse('{:shouty} world', 'hello world', dict(shouty=shouty))
<Result ('HELLO',) {}>

If the type converter has the optional ``pattern`` attribute, it is used as
regular expression for better pattern matching (instead of the default one).

>>> def parse_number(text):
...    return int(text)
>>> parse_number.pattern = r'\d+'
>>> parse('Answer: {number:Number}', 'Answer: 42', dict(Number=parse_number))
<Result () {'number': 42}>
>>> _ = parse('Answer: {:Number}', 'Answer: Alice', dict(Number=parse_number))
>>> assert _ is None, "MISMATCH"

You can also use the ``with_pattern(pattern)`` decorator to add this
information to a type converter function:

>>> from parse import with_pattern
>>> @with_pattern(r'\d+')
... def parse_number(text):
...    return int(text)
>>> parse('Answer: {number:Number}', 'Answer: 42', dict(Number=parse_number))
<Result () {'number': 42}>

A more complete example of a custom type might be:

>>> yesno_mapping = {
...     "yes":  True,   "no":    False,
...     "on":   True,   "off":   False,
...     "true": True,   "false": False,
... }
>>> @with_pattern(r"|".join(yesno_mapping))
... def parse_yesno(text):
...     return yesno_mapping[text.lower()]


If the type converter ``pattern`` uses regex-grouping (with parenthesis),
you should indicate this by using the optional ``regex_group_count`` parameter
in the ``with_pattern()`` decorator:

>>> @with_pattern(r'((\d+))', regex_group_count=2)
... def parse_number2(text):
...    return int(text)
>>> parse('Answer: {:Number2} {:Number2}', 'Answer: 42 43', dict(Number2=parse_number2))
<Result (42, 43) {}>

Otherwise, this may cause parsing problems with unnamed/fixed parameters.


Potential Gotchas
-----------------

`parse()` will always match the shortest text necessary (from left to right)
to fulfil the parse pattern, so for example:

>>> pattern = '{dir1}/{dir2}'
>>> data = 'root/parent/subdir'
>>> sorted(parse(pattern, data).named.items())
[('dir1', 'root'), ('dir2', 'parent/subdir')]

So, even though `{'dir1': 'root/parent', 'dir2': 'subdir'}` would also fit
the pattern, the actual match represents the shortest successful match for
`dir1`.

----

**Version history (in brief)**:

- 1.12.1 Actually use the `case_sensitive` arg in compile (thanks @jacquev6)
- 1.12.0 Do not assume closing brace when an opening one is found (thanks @mattsep)
- 1.11.1 Revert having unicode char in docstring, it breaks Bamboo builds(?!)
- 1.11.0 Implement `__contains__` for Result instances.
- 1.10.0 Introduce a "letters" matcher, since "w" matches numbers
  also.
- 1.9.1 Fix deprecation warnings around backslashes in regex strings
  (thanks Mickael Schoentgen). Also fix some documentation formatting
  issues.
- 1.9.0 We now honor precision and width specifiers when parsing numbers
  and strings, allowing parsing of concatenated elements of fixed width
  (thanks Julia Signell)
- 1.8.4 Add LICENSE file at request of packagers.
  Correct handling of AM/PM to follow most common interpretation.
  Correct parsing of hexadecimal that looks like a binary prefix.
  Add ability to parse case sensitively.
  Add parsing of numbers to Decimal with "F" (thanks John Vandenberg)
- 1.8.3 Add regex_group_count to with_pattern() decorator to support
  user-defined types that contain brackets/parenthesis (thanks Jens Engel)
- 1.8.2 add documentation for including braces in format string
- 1.8.1 ensure bare hexadecimal digits are not matched
- 1.8.0 support manual control over result evaluation (thanks Timo Furrer)
- 1.7.0 parse dict fields (thanks Mark Visser) and adapted to allow
  more than 100 re groups in Python 3.5+ (thanks David King)
- 1.6.6 parse Linux system log dates (thanks Alex Cowan)
- 1.6.5 handle precision in float format (thanks Levi Kilcher)
- 1.6.4 handle pipe "|" characters in parse string (thanks Martijn Pieters)
- 1.6.3 handle repeated instances of named fields, fix bug in PM time
  overflow
- 1.6.2 fix logging to use local, not root logger (thanks Necku)
- 1.6.1 be more flexible regarding matched ISO datetimes and timezones in
  general, fix bug in timezones without ":" and improve docs
- 1.6.0 add support for optional ``pattern`` attribute in user-defined types
  (thanks Jens Engel)
- 1.5.3 fix handling of question marks
- 1.5.2 fix type conversion error with dotted names (thanks Sebastian Thiel)
- 1.5.1 implement handling of named datetime fields
- 1.5 add handling of dotted field names (thanks Sebastian Thiel)
- 1.4.1 fix parsing of "0" in int conversion (thanks James Rowe)
- 1.4 add __getitem__ convenience access on Result.
- 1.3.3 fix Python 2.5 setup.py issue.
- 1.3.2 fix Python 3.2 setup.py issue.
- 1.3.1 fix a couple of Python 3.2 compatibility issues.
- 1.3 added search() and findall(); removed compile() from ``import *``
  export as it overwrites builtin.
- 1.2 added ability for custom and override type conversions to be
  provided; some cleanup
- 1.1.9 to keep things simpler number sign is handled automatically;
  significant robustification in the face of edge-case input.
- 1.1.8 allow "d" fields to have number base "0x" etc. prefixes;
  fix up some field type interactions after stress-testing the parser;
  implement "%" type.
- 1.1.7 Python 3 compatibility tweaks (2.5 to 2.7 and 3.2 are supported).
- 1.1.6 add "e" and "g" field types; removed redundant "h" and "X";
  removed need for explicit "#".
- 1.1.5 accept textual dates in more places; Result now holds match span
  positions.
- 1.1.4 fixes to some int type conversion; implemented "=" alignment; added
  date/time parsing with a variety of formats handled.
- 1.1.3 type conversion is automatic based on specified field types. Also added
  "f" and "n" types.
- 1.1.2 refactored, added compile() and limited ``from parse import *``
- 1.1.1 documentation improvements
- 1.1.0 implemented more of the `Format Specification Mini-Language`_
  and removed the restriction on mixing fixed-position and named fields
- 1.0.0 initial release

This code is copyright 2012-2019 Richard Jones <richard@python.org>
See the end of the source file for the license of use.
  * [parso-0.5.1](https://github.com/davidhalter/parso) ###################################################################
parso - A Python Parser
###################################################################


.. image:: https://travis-ci.org/davidhalter/parso.svg?branch=master
    :target: https://travis-ci.org/davidhalter/parso
    :alt: Travis CI build status

.. image:: https://coveralls.io/repos/github/davidhalter/parso/badge.svg?branch=master
    :target: https://coveralls.io/github/davidhalter/parso?branch=master
    :alt: Coverage Status

.. image:: https://raw.githubusercontent.com/davidhalter/parso/master/docs/_static/logo_characters.png

Parso is a Python parser that supports error recovery and round-trip parsing
for different Python versions (in multiple Python versions). Parso is also able
to list multiple syntax errors in your python file.

Parso has been battle-tested by jedi_. It was pulled out of jedi to be useful
for other projects as well.

Parso consists of a small API to parse Python and analyse the syntax tree.

A simple example:

.. code-block:: python

    >>> import parso
    >>> module = parso.parse('hello + 1', version="3.6")
    >>> expr = module.children[0]
    >>> expr
    PythonNode(arith_expr, [<Name: hello@1,0>, <Operator: +>, <Number: 1>])
    >>> print(expr.get_code())
    hello + 1
    >>> name = expr.children[0]
    >>> name
    <Name: hello@1,0>
    >>> name.end_pos
    (1, 5)
    >>> expr.end_pos
    (1, 9)

To list multiple issues:

.. code-block:: python

    >>> grammar = parso.load_grammar()
    >>> module = grammar.parse('foo +\nbar\ncontinue')
    >>> error1, error2 = grammar.iter_errors(module)
    >>> error1.message
    'SyntaxError: invalid syntax'
    >>> error2.message
    "SyntaxError: 'continue' not properly in loop"

Resources
=========

- `Testing <https://parso.readthedocs.io/en/latest/docs/development.html#testing>`_
- `PyPI <https://pypi.python.org/pypi/parso>`_
- `Docs <https://parso.readthedocs.org/en/latest/>`_
- Uses `semantic versioning <https://semver.org/>`_

Installation
============

    pip install parso

Future
======

- There will be better support for refactoring and comments. Stay tuned.
- There's a WIP PEP8 validator. It's however not in a good shape, yet.

Known Issues
============

- `async`/`await` are already used as keywords in Python3.6.
- `from __future__ import print_function` is not ignored.


Acknowledgements
================

- Guido van Rossum (@gvanrossum) for creating the parser generator pgen2
  (originally used in lib2to3).
- `Salome Schneider <https://www.crepes-schnaegg.ch/cr%C3%AApes-schn%C3%A4gg/kunst-f%C3%BCrs-cr%C3%AApes-mobil/>`_
  for the extremely awesome parso logo.


.. _jedi: https://github.com/davidhalter/jedi


.. :changelog:

Changelog
---------

0.5.1 (2019-07-13)
++++++++++++++++++

- Fix: Some unicode identifiers were not correctly tokenized
- Fix: Line continuations in f-strings are now working

0.5.0 (2019-06-20)
++++++++++++++++++

- **Breaking Change** comp_for is now called sync_comp_for for all Python
  versions to be compatible with the Python 3.8 Grammar
- Added .pyi stubs for a lot of the parso API
- Small FileIO changes

0.4.0 (2019-04-05)
++++++++++++++++++

- Python 3.8 support
- FileIO support, it's now possible to use abstract file IO, support is alpha

0.3.4 (2019-02-13)
+++++++++++++++++++

- Fix an f-string tokenizer error

0.3.3 (2019-02-06)
+++++++++++++++++++

- Fix async errors in the diff parser
- A fix in iter_errors
- This is a very small bugfix release

0.3.2 (2019-01-24)
+++++++++++++++++++

- 20+ bugfixes in the diff parser and 3 in the tokenizer
- A fuzzer for the diff parser, to give confidence that the diff parser is in a
  good shape.
- Some bugfixes for f-string

0.3.1 (2018-07-09)
+++++++++++++++++++

- Bugfixes in the diff parser and keyword-only arguments

0.3.0 (2018-06-30)
+++++++++++++++++++

- Rewrote the pgen2 parser generator.

0.2.1 (2018-05-21)
+++++++++++++++++++

- A bugfix for the diff parser.
- Grammar files can now be loaded from a specific path.

0.2.0 (2018-04-15)
+++++++++++++++++++

- f-strings are now parsed as a part of the normal Python grammar. This makes
  it way easier to deal with them.

0.1.1 (2017-11-05)
+++++++++++++++++++

- Fixed a few bugs in the caching layer
- Added support for Python 3.7

0.1.0 (2017-09-04)
+++++++++++++++++++

- Pulling the library out of Jedi. Some APIs will definitely change.



  * [partd-1.0.0](http://github.com/dask/partd/) PartD
=====

|Build Status| |Version Status|

Key-value byte store with appendable values

    Partd stores key-value pairs.
    Values are raw bytes.
    We append on old values.

Partd excels at shuffling operations.

Operations
----------

PartD has two main operations, ``append`` and ``get``.


Example
-------

1.  Create a Partd backed by a directory::

        >>> import partd
        >>> p = partd.File('/path/to/new/dataset/')

2.  Append key-byte pairs to dataset::

        >>> p.append({'x': b'Hello ', 'y': b'123'})
        >>> p.append({'x': b'world!', 'y': b'456'})

3.  Get bytes associated to keys::

        >>> p.get('x')         # One key
        b'Hello world!'

        >>> p.get(['y', 'x'])  # List of keys
        [b'123456', b'Hello world!']

4.  Destroy partd dataset::

        >>> p.drop()

That's it.


Implementations
---------------

We can back a partd by an in-memory dictionary::

    >>> p = Dict()

For larger amounts of data or to share data between processes we back a partd
by a directory of files.  This uses file-based locks for consistency.::

    >>> p = File('/path/to/dataset/')

However this can fail for many small writes.  In these cases you may wish to buffer one partd with another, keeping a fixed maximum of data in the buffering partd.  This writes the larger elements of the first partd to the second partd when space runs low::

    >>> p = Buffer(Dict(), File(), available_memory=2e9)  # 2GB memory buffer

You might also want to have many distributed process write to a single partd
consistently.  This can be done with a server

*   Server Process::

        >>> p = Buffer(Dict(), File(), available_memory=2e9)  # 2GB memory buffer
        >>> s = Server(p, address='ipc://server')

*   Worker processes::

        >>> p = Client('ipc://server')  # Client machine talks to remote server


Encodings and Compression
-------------------------

Once we can robustly and efficiently append bytes to a partd we consider
compression and encodings.  This is generally available with the ``Encode``
partd, which accepts three functions, one to apply on bytes as they are
written, one to apply to bytes as they are read, and one to join bytestreams.
Common configurations already exist for common data and compression formats.

We may wish to compress and decompress data transparently as we interact with a
partd.  Objects like ``BZ2``, ``Blosc``, ``ZLib`` and ``Snappy`` exist and take
another partd as an argument.::

    >>> p = File(...)
    >>> p = ZLib(p)

These work exactly as before, the (de)compression happens automatically.

Common data formats like Python lists, numpy arrays, and pandas
dataframes are also supported out of the box.::

    >>> p = File(...)
    >>> p = NumPy(p)
    >>> p.append({'x': np.array([...])})

This lets us forget about bytes and think instead in our normal data types.

Composition
-----------

In principle we want to compose all of these choices together

1.  Write policy:  ``Dict``, ``File``, ``Buffer``, ``Client``
2.  Encoding:  ``Pickle``, ``Numpy``, ``Pandas``, ...
3.  Compression:  ``Blosc``, ``Snappy``, ...

Partd objects compose by nesting.  Here we make a partd that writes pickle
encoded BZ2 compressed bytes directly to disk::

    >>> p = Pickle(BZ2(File('foo')))

We could construct more complex systems that include compression,
serialization, buffering, and remote access.::

    >>> server = Server(Buffer(Dict(), File(), available_memory=2e0))

    >>> client = Pickle(Snappy(Client(server.address)))
    >>> client.append({'x': [1, 2, 3]})

.. |Build Status| image:: https://travis-ci.org/dask/partd.png
   :target: https://travis-ci.org/dask/partd
.. |Version Status| image:: https://img.shields.io/pypi/v/partd.svg
   :target: https://pypi.python.org/pypi/partd/



  * [path.py-12.0.1](https://github.com/jaraco/path.py) .. image:: https://img.shields.io/pypi/v/path.py.svg
   :target: https://pypi.org/project/path.py

.. image:: https://img.shields.io/pypi/pyversions/path.py.svg

.. image:: https://img.shields.io/travis/jaraco/path.py/master.svg
   :target: https://travis-ci.org/jaraco/path.py

.. image:: https://img.shields.io/appveyor/ci/jaraco/path-py/master.svg
   :target: https://ci.appveyor.com/project/jaraco/path-py/branch/master

.. image:: https://readthedocs.org/projects/pathpy/badge/?version=latest
   :target: https://pathpy.readthedocs.io/en/latest/?badge=latest

``path.py`` implements path objects as first-class entities, allowing
common operations on files to be invoked on those path objects directly. For
example:

.. code-block:: python

    from path import Path
    d = Path('/home/guido/bin')
    for f in d.files('*.py'):
        f.chmod(0o755)

    # Globbing
    for f in d.files('*.py'):
        f.chmod(0o755)

    # Changing the working directory:
    with Path("somewhere"):
        # cwd in now `somewhere`
        ...

    # Concatenate paths with /
    foo_txt = Path("bar") / "foo.txt"

``path.py`` is `hosted at Github <https://github.com/jaraco/path.py>`_.

Find `the documentation here <https://pathpy.readthedocs.io>`_.

Guides and Testimonials
=======================

Yasoob wrote the Python 101 `Writing a Cleanup Script
<http://freepythontips.wordpress.com/2014/01/23/python-101-writing-a-cleanup-script/>`_
based on ``path.py``.

Installing
==========

Path.py may be installed using ``setuptools``, ``distribute``, or ``pip``::

    pip install path.py

The latest release is always updated to the `Python Package Index
<http://pypi.python.org/pypi/path.py>`_.

You may also always download the source distribution (zip/tarball), extract
it, and run ``python setup.py`` to install it.

Advantages
==========

Python 3.4 introduced
`pathlib <https://docs.python.org/3/library/pathlib.html>`_,
which shares many characteristics with ``path.py``. In particular,
it provides an object encapsulation for representing filesystem paths.
One may have imagined ``pathlib`` would supersede ``path.py``.

But the implementation and the usage quickly diverge, and ``path.py``
has several advantages over ``pathlib``:

- ``path.py`` implements ``Path`` objects as a subclass of
  ``str``, and as a result these ``Path``
  objects may be passed directly to other APIs that expect simple
  text representations of paths, whereas with ``pathlib``, one
  must first cast values to strings before passing them to
  APIs unaware of ``pathlib``. This shortcoming was `addressed
  by PEP 519 <https://www.python.org/dev/peps/pep-0519/>`_,
  in Python 3.6.
- ``path.py`` goes beyond exposing basic functionality of a path
  and exposes commonly-used behaviors on a path, providing
  methods like ``rmtree`` (from shlib) and ``remove_p`` (remove
  a file if it exists).
- As a PyPI-hosted package, ``path.py`` is free to iterate
  faster than a stdlib package. Contributions are welcome
  and encouraged.
- ``path.py`` provides a uniform abstraction over its Path object,
  freeing the implementer to subclass it readily. One cannot
  subclass a ``pathlib.Path`` to add functionality, but must
  subclass ``Path``, ``PosixPath``, and ``WindowsPath``, even
  if one only wishes to add a ``__dict__`` to the subclass
  instances.  ``path.py`` instead allows the ``Path.module``
  object to be overridden by subclasses, defaulting to the
  ``os.path``. Even advanced uses of ``path.Path`` that
  subclass the model do not need to be concerned with
  OS-specific nuances.

Alternatives
============

In addition to
`pathlib <https://docs.python.org/3/library/pathlib.html>`_, the
`pylib project <https://pypi.org/project/py/>`_ implements a
`LocalPath <https://github.com/pytest-dev/py/blob/72601dc8bbb5e11298bf9775bb23b0a395deb09b/py/_path/local.py#L106>`_
class, which shares some behaviors and interfaces with ``path.py``.

Development
===========

To install a development version, use the Github links to clone or
download a snapshot of the latest code. Alternatively, if you have git
installed, you may be able to use ``pip`` to install directly from
the repository::

    pip install git+https://github.com/jaraco/path.py.git

Testing
=======

Tests are invoked with `tox <https://pypi.org/project/tox>`_. After
having installed tox, simply invoke ``tox`` in a checkout of the repo
to invoke the tests.

Tests are also run in continuous integration. See the badges above
for links to the CI runs.

Releasing
=========

Tagged releases are automatically published to PyPI by Travis-CI, assuming
the tests pass.



  * [patsy-0.5.1](https://github.com/pydata/patsy) Patsy is a Python library for describing statistical models
(especially linear models, or models that have a linear component) and
building design matrices. Patsy brings the convenience of `R
<http://www.r-project.org/>`_ "formulas" to Python.

.. image:: https://travis-ci.org/pydata/patsy.png?branch=master
   :target: https://travis-ci.org/pydata/patsy
.. image:: https://coveralls.io/repos/pydata/patsy/badge.png?branch=master
   :target: https://coveralls.io/r/pydata/patsy?branch=master

Documentation:
  https://patsy.readthedocs.io/

Downloads:
  http://pypi.python.org/pypi/patsy/

Dependencies:
  * Python (2.6, 2.7, or 3.3+)
  * six
  * numpy

Optional dependencies:
  * nose: needed to run tests
  * scipy: needed for spline-related functions like ``bs``

Install:
  ``pip install patsy`` (or, for traditionalists: ``python setup.py install``)

Code and bug tracker:
  https://github.com/pydata/patsy

Mailing list:
  * pydata@googlegroups.com
  * http://groups.google.com/group/pydata
  * http://news.gmane.org/gmane.comp.python.pydata

License:
  2-clause BSD, see LICENSE.txt for details.



  * [pauvre-0.1923](https://github.com/conchoecia/pauvre) 
'pauvre' is a package for plotting Oxford Nanopore and other long read data.
The name means 'poor' in French, a play on words to the oft-used 'pore' prefix
for similar packages. This package was designed for python 3, but it might work in
python 2. You can visit the gitub page for more detailed information here:
https://github.com/conchoecia/pauvre



  * [paycheck-1.0.2](http://github.com/gcross/paycheck) Hey! PayCheck!
--------------

PayCheck is a half-baked implementation of
`ScalaCheck <http://code.google.com/p/scalacheck/>`_, which itself is an
implementation of
`QuickCheck <http://www.cs.chalmers.se/~rjmh/QuickCheck/>`_ for
Haskell. PayCheck is useful for defining a specification of what a
function
should do, rather than testing its results for a given input.

Thanks to gcross for some of the `more recent
changes <http://github.com/gcross/paycheck/tree/master>`_

Installing PayCheck
~~~~~~~~~~~~~~~~~~~

::

    <code>
    sudo easy_install paycheck
    </code>

That’s it. Get going.

A Quick Example
~~~~~~~~~~~~~~~

Let’s steal an example right from ScalaCheck. Here are the string
functions
ported to PayCheck. See what’s going on? We’re defining the types of the
parameters in with\_checker, then values of that type are getting passed
to the
function.

::

    <code>
    import unittest
    from paycheck import with_checker

    class TestStrings(unittest.TestCase):
        """
        More-or-less a direct port of the string testing example from the ScalaCheck
        doc at: http://code.google.com/p/scalacheck/
        """

        @with_checker(str, str)
        def test_starts_with(self, a, b):
            self.assertTrue((a+b).startswith(a))

        @with_checker(str, str)
        def test_ends_with(self, a, b):
            self.assertTrue((a+b).endswith(b))

        # Is this really always true?
        @with_checker(str, str)
        def test_concat(self, a, b):
            self.assertTrue(len(a+b) > len(a))
            self.assertTrue(len(a+b) > len(b))

        @with_checker(str, str)
        def test_substring2(self, a, b):
            self.assertEquals( (a+b)[len(a):], b )

        @with_checker(str, str, str)
        def test_substring3(self, a, b, c):
            self.assertEquals((a+b+c)[len(a):len(a)+len(b)], b)

    if __name__ == '__main__':
        unittest.main()
    </code>

Then give the ol’ test a run. You’ll likely see a problem:

::

    <code>
    $ python test_strings.py
    F....
    ======================================================================
    FAIL: test_concat (__main__.TestStrings)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File "paycheck/checker.py", line 11, in wrapper
        test_func(self, *v)
      File "test_strings.py", line 20, in test_concat
        self.assertTrue(len(a+b) > len(a))
    AssertionError: Failed for input ('UGzo2LP<(9Gl_*o*GH$H<+{wPiNk?', '')

    ----------------------------------------------------------------------
    Ran 5 tests in 0.051s

    FAILED (failures=1)
    </code>

As predicted, test_concat has bombed; note that PayCheck is nice
enough to tell you which inputs caused the problem. In this case, we
see that propety test_concat fails for the empty string, which is caused
by the fact that we used “>” instead of “>=”.

Nested and More Complex Types
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Container and nested types are specified to PayCheck “by analogy”.
That is, unlike scalar types, which are specified by giving the type
that you want, container types are specified by creating a non-empty
container of the type you want which contains a specification of the
type that you want generated containers to contain; note that the
contained element is itself allowed to be a container, allowing for
arbitrarily nested types. PayCheck will infer from your containers
the type of container and contained elements that you want to
generate. This includes dictionaries: it will look at the first
key/value mapping that it sees and infer from it both the key type and
the value type. Containers will be generated with between 0 and
paycheck.generator.LIST\_LEN elements.

The following examples illustrate how this works:

::

    <code>
    import unittest
    from paycheck import with_checker

    class TestTypes(unittest.TestCase):

        @with_checker(int)
        def test_int(self, i):
            self.assertTrue(isinstance(i, int))

        @with_checker([int])
        def test_get_list(self, list_of_ints):
            self.assertTrue(isinstance(list_of_ints, list))
            for i in list_of_ints:
                self.assertTrue(isinstance(i, int))

        @with_checker([{str: int}])
        def test_list_of_dict_of_int_string(self, list_of_dict_of_int_string):
            self.assertTrue(isinstance(list_of_dict_of_int_string, list))

            for dict_of_int_string in list_of_dict_of_int_string:
                self.assertTrue(isinstance(dict_of_int_string, dict))

                for key, value in dict_of_int_string.items():
                    self.assertTrue(isinstance(key, str))
                    self.assertTrue(isinstance(value, int))

    if __name__ == '__main__':
        unittest.main()
    </code>
  * [pbr-4.2.0](https://docs.openstack.org/pbr/latest/) Introduction
============

.. image:: https://img.shields.io/pypi/v/pbr.svg
    :target: https://pypi.python.org/pypi/pbr/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/dm/pbr.svg
    :target: https://pypi.python.org/pypi/pbr/
    :alt: Downloads

PBR is a library that injects some useful and sensible default behaviors
into your setuptools run. It started off life as the chunks of code that
were copied between all of the `OpenStack`_ projects. Around the time that
OpenStack hit 18 different projects each with at least 3 active branches,
it seemed like a good time to make that code into a proper reusable library.

PBR is only mildly configurable. The basic idea is that there's a decent
way to run things and if you do, you should reap the rewards, because then
it's simple and repeatable. If you want to do things differently, cool! But
you've already got the power of Python at your fingertips, so you don't
really need PBR.

PBR builds on top of the work that `d2to1`_ started to provide for declarative
configuration. `d2to1`_ is itself an implementation of the ideas behind
`distutils2`_. Although `distutils2`_ is now abandoned in favor of work towards
`PEP 426`_ and Metadata 2.0, declarative config is still a great idea and
specifically important in trying to distribute setup code as a library
when that library itself will alter how the setup is processed. As Metadata
2.0 and other modern Python packaging PEPs come out, PBR aims to support
them as quickly as possible.

* License: Apache License, Version 2.0
* Documentation: https://docs.openstack.org/pbr/latest/
* Source: https://git.openstack.org/cgit/openstack-dev/pbr
* Bugs: https://bugs.launchpad.net/pbr
* Change Log: https://docs.openstack.org/pbr/latest/user/history.html

.. _d2to1: https://pypi.python.org/pypi/d2to1
.. _distutils2: https://pypi.python.org/pypi/Distutils2
.. _PEP 426: http://legacy.python.org/dev/peps/pep-0426/
.. _OpenStack: https://www.openstack.org/




  * [pefile-2019.4.18](https://github.com/erocarrera/pefile) pefile, Portable Executable reader module

All the PE file basic structures are available with their default names as
attributes of the instance returned.

Processed elements such as the import table are made available with lowercase
names, to differentiate them from the upper case basic structure names.

pefile has been tested against many edge cases such as corrupted and malformed
PEs as well as malware, which often attempts to abuse the format way beyond its
standard use. To the best of my knowledge most of the abuse is handled
gracefully.

Copyright (c) 2005-2019 Ero Carrera <ero.carrera@gmail.com>
  * [pep517-0.5.0](https://github.com/takluyver/pep517) `PEP 517 <https://www.python.org/dev/peps/pep-0517/>`_ specifies a standard
API for systems which build Python packages.

This package contains wrappers around the hooks specified by PEP 517. It
provides:

- A mechanism to call the hooks in a subprocess, so they are isolated from
  the current process.
- Fallbacks for the optional hooks, so that frontends can call the hooks without
  checking which are defined.
- Higher-level functions which install the build dependencies into a
  temporary environment and build a wheel/sdist using them.

Run the tests with ``py.test``.

High level usage, with build requirements handled:

.. code-block:: python

    import os
    from pep517.envbuild import build_wheel, build_sdist

    src = 'path/to/source'  # Folder containing 'pyproject.toml'
    destination = 'also/a/folder'
    whl_filename = build_wheel(src, destination)
    assert os.path.isfile(os.path.join(destination, whl_filename))

    targz_filename = build_sdist(src, destination)
    assert os.path.isfile(os.path.join(destination, targz_filename))

Lower level usage—you are responsible for ensuring build requirements are
available:

.. code-block:: python

    import os
    import toml
    from pep517.wrappers import Pep517HookCaller

    src = 'path/to/source'  # Folder containing 'pyproject.toml'
    with open(os.path.join(src, 'pyproject.toml')) as f:
        build_sys = toml.load(f)['build-system']

    print(build_sys['requires'])  # List of static requirements

    hooks = Pep517HookCaller(
        src, 
        build_backend=build_sys['build_backend'],
        backend_path=build_sys.get('backend-path'),
    )

    config_options = {}   # Optional parameters for backend
    # List of dynamic requirements:
    print(hooks.get_requires_for_build_wheel(config_options))

    destination = 'also/a/folder'
    whl_filename = hooks.build_wheel(destination, config_options)
    assert os.path.isfile(os.path.join(destination, whl_filename))

To test the build backend for a project, run in a system shell:

.. code-block:: shell

    python3 -m pep517.check path/to/source  # source dir containing pyproject.toml

To build a backend into source and/or binary distributions, run in a shell:

.. code-block:: shell

    python -m pep517.build path/to/source  # source dir containing pyproject.toml

This 'build' module should be considered experimental while the PyPA `decides
on the best place for this functionality
<https://github.com/pypa/packaging-problems/issues/219>`_.

  * [persistent-4.5.0](https://github.com/zopefoundation/persistent/) ``persistent``:  automatic persistence for Python objects
=========================================================

.. image:: https://travis-ci.org/zopefoundation/persistent.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/persistent

.. image:: https://coveralls.io/repos/github/zopefoundation/persistent/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/persistent?branch=master

.. image:: https://readthedocs.org/projects/persistent/badge/?version=latest
        :target: http://persistent.readthedocs.org/en/latest/
        :alt: Documentation Status

.. image:: https://img.shields.io/pypi/v/persistent.svg
        :target: https://pypi.org/project/persistent
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/persistent.svg
        :target: https://pypi.org/project/persistent
        :alt: Python versions

This package contains a generic persistence implementation for Python. It
forms the core protocol for making objects interact "transparently" with
a database such as the ZODB.

Please see the Sphinx documentation (``docs/index.rst``) for further
information, or view the documentation at Read The Docs, for either
the latest (``http://persistent.readthedocs.io/en/latest/``) or stable
release (``http://persistent.readthedocs.io/en/stable/``).

.. note::

   Use of this standalone ``persistent`` release is not recommended or
   supported with ZODB < 3.11.  ZODB 3.10 and earlier bundle their own
   version of  the ``persistent`` package.


``persistent`` Changelog
========================

4.5.0 (2019-05-09)
------------------

- Fully test the C implementation of the PickleCache, and fix
  discrepancies between it and the Python implementation:

  - The C implementation now raises ``ValueError`` instead of
    ``AssertionError`` for certain types of bad inputs.
  - The Python implementation uses the C wording for error messages.
  - The C implementation properly implements ``IPickleCache``; methods
    unique to the Python implementation were moved to
    ``IExtendedPickleCache``.
  - The Python implementation raises ``AttributeError`` if a
    persistent class doesn't have a ``p_jar`` attribute.

  See `issue 102
  <https://github.com/zopefoundation/persistent/issues/102>`_.

- Allow sweeping cache without ``cache_size``. ``cache_size_bytes``
  works with ``cache_size=0``, no need to set ``cache_size`` to a
  large value.

- Require ``CFFI`` on CPython for pure-Python operation. This drops
  support for Jython (which was untested). See `issue 77
  <https://github.com/zopefoundation/persistent/issues/77>`_.

- Fix DeprecationWarning about ``PY_SSIZE_T_CLEAN``.
  See `issue 108 <https://github.com/zopefoundation/persistent/issues/108>`_.

- Drop support for Python 3.4.


4.4.3 (2018-10-22)
------------------

- Fix the repr of the persistent objects to include the module name
  when using the C extension. This matches the pure-Python behaviour
  and the behaviour prior to 4.4.0. See `issue 92
  <https://github.com/zopefoundation/persistent/issues/92>`_.

- Change the repr of persistent objects to format the OID as in
  integer in hexadecimal notation if it is an 8-byte byte string, as
  ZODB does. This eliminates some issues in doctests. See `issue 95
  <https://github.com/zopefoundation/persistent/pull/95>`_.


4.4.2 (2018-08-28)
------------------

- Explicitly use unsigned constants for packing and unpacking C
  timestamps, fixing an arithmetic issue for GCC when optimizations
  are enabled and ``-fwrapv`` is *not* enabled. See `issue 86
  <https://github.com/zopefoundation/persistent/issues/86>`_.


4.4.1 (2018-08-23)
------------------

- Fix installation of source packages on PyPy. See `issue 88
  <https://github.com/zopefoundation/persistent/issues/88>`_.


4.4.0 (2018-08-22)
------------------

- Use unsigned constants when doing arithmetic on C timestamps,
  possibly avoiding some overflow issues with some compilers or
  compiler settings. See `issue 86
  <https://github.com/zopefoundation/persistent/issues/86>`_.

- Change the default representation of ``Persistent`` objects to
  include the representation of their OID and jar, if set. Also add
  the ability for subclasses to implement ``_p_repr()`` instead of
  overriding ``__repr__`` for better exception handling. See `issue 11
  <https://github.com/zopefoundation/persistent/issues/11>`_.

- Reach and maintain 100% test coverage.

- Simplify ``__init__.py``, including removal of an attempted legacy
  import of ``persistent.TimeStamp``. See `PR 80
  <https://github.com/zopefoundation/persistent/pull/80>`_.

- Add support for Python 3.7 and drop support for Python 3.3.

- Build the CFFI modules (used on PyPy or when PURE_PYTHON is set) `at
  installation or wheel building time
  <https://cffi.readthedocs.io/en/latest/cdef.html#ffibuilder-set-source-preparing-out-of-line-modules>`_
  when CFFI is available. This replaces `the deprecated way
  <https://cffi.readthedocs.io/en/latest/overview.html#abi-versus-api>`_
  of building them at import time. If binary wheels are distributed,
  it eliminates the need to have a functioning C compiler to use PyPy.
  See `issue 75
  <https://github.com/zopefoundation/persistent/issues/75>`_.

- Fix deleting the ``_p_oid`` of a pure-Python persistent object when
  it is in a cache.

- Fix deleting special (``_p``) attributes of a pure-Python persistent
  object that overrides ``__delattr__`` and correctly calls ``_p_delattr``.

- Remove some internal compatibility shims that are no longer
  necessary. See `PR 82 <https://github.com/zopefoundation/persistent/pull/82>`_.

- Make the return value of ``TimeStamp.second()`` consistent across C
  and Python implementations when the ``TimeStamp`` was created from 6
  arguments with floating point seconds. Also make it match across
  trips through ``TimeStamp.raw()``. Previously, the C version could
  initially have erroneous rounding and too much false precision,
  while the Python version could have too much precision. The raw/repr
  values have not changed. See `issue 41
  <https://github.com/zopefoundation/persistent/issues/41>`_.


4.3.0 (2018-07-30)
------------------

- Fix the possibility of a rare crash in the C extension when
  deallocating items. See https://github.com/zopefoundation/persistent/issues/66

- Change cPickleCache's comparison of object sizes to determine
  whether an object can go in the cache to use ``PyObject_TypeCheck()``.
  This matches what the pure Python implementation does and is a
  stronger test that the object really is compatible with the cache.
  Previously, an object could potentially include ``cPersistent_HEAD``
  and *not* set ``tp_base`` to ``cPersistenceCAPI->pertype`` and still
  be eligible for the pickle cache; that is no longer the case. See
  `issue 69 <https://github.com/zopefoundation/persistent/issues/69>`_.


4.2.4.2 (2017-04-23)
--------------------

- Packaging-only release: fix Python 2.7 ``manylinux`` wheels.


4.2.4.1 (2017-04-21)
--------------------

- Packaging-only release:  get ``manylinux`` wheel built automatically.


4.2.4 (2017-03-20)
------------------

- Avoid raising a ``SystemError: error return without exception set``
  when loading an object with slots whose jar generates an exception
  (such as a ZODB ``POSKeyError``) in ``setstate``.


4.2.3 (2017-03-08)
------------------

- Fix the hashcode of Python ``TimeStamp`` objects on 64-bit Python on
  Windows. See https://github.com/zopefoundation/persistent/pull/55

- Stop calling ``gc.collect`` every time ``PickleCache.incrgc`` is called (every
  transaction boundary) in pure-Python mode (PyPy). This means that
  the reported size of the cache may be wrong (until the next GC), but
  it is much faster. This should not have any observable effects for
  user code.

- Stop clearing the dict and slots of objects added to
  ``PickleCache.new_ghost`` (typically these values are passed to
  ``__new__`` from the pickle data) in pure-Python mode (PyPy). This
  matches the behaviour of the C code.

- Add support for Python 3.6.

- Fix ``__setstate__`` interning when ``state`` parameter is not a built-in dict


4.2.2 (2016-11-29)
------------------

- Drop use of ``ctypes`` for determining maximum integer size, to increase
  pure-Python compatibility. See https://github.com/zopefoundation/persistent/pull/31

- Ensure that ``__slots__`` attributes are cleared when a persistent
  object is ghostified.  (This excluses classes that override
  ``__new__``.  See
  https://github.com/zopefoundation/persistent/wiki/Notes_on_state_new_and_slots
  if you're curious.)


4.2.1 (2016-05-26)
------------------

- Fix the hashcode of C ``TimeStamp`` objects on 64-bit Python 3 on
  Windows.


4.2.0 (2016-05-05)
------------------

- Fixed the Python(/PYPY) implementation ``TimeStamp.timeTime`` method
  to have subsecond precision.

- When testing ``PURE_PYTHON`` environments under ``tox``, avoid poisoning
  the user's global wheel cache.

- Add support for Python 3.5.

- Drop support for Python 2.6 and 3.2.


4.1.1 (2015-06-02)
------------------

- Fix manifest and re-upload to fix stray files included in 4.1.0.


4.1.0 (2015-05-19)
------------------

- Make the Python implementation of ``Persistent`` and ``PickleCache``
  behave more similarly to the C implementation. In particular, the
  Python version can now run the complete ZODB and ZEO test suites.

- Fix the hashcode of the Python ``TimeStamp`` on 32-bit platforms.


4.0.9 (2015-04-08)
------------------

- Make the C and Python ``TimeStamp`` objects behave more alike. The
  Python version now produces the same ``repr`` and ``.raw()`` output as
  the C version, and has the same hashcode. In addition, the Python
  version is now supports ordering and equality like the C version.

- Intern keys of object state in ``__setstate__`` to reduce memory usage
  when unpickling multiple objects with the same attributes.

- Add support for PyPy3.

- 100% branch coverage.


4.0.8 (2014-03-20)
------------------

- Add support for Python 3.4.

- In pure-Python ``Persistent``, avoid loading state in ``_p_activate``
  for non-ghost objects (which could corrupt their state).  (PR #9)

- In pure-Python, and don't throw ``POSKeyError`` if ``_p_activate`` is
  called on an object that has never been committed.  (PR #9)

- In pure-Python ``Persistent``, avoid calling a subclass's ``__setattr__``
  at instance creation time. (PR #8)

- Make it possible to delete ``_p_jar`` / ``_p_oid`` of a pure-Python
  ``Persistent`` object which has been removed from the jar's cache
  (fixes aborting a ZODB Connection that has added objects). (PR #7)


4.0.7 (2014-02-20)
------------------

- Avoid a KeyError from ``_p_accessed()`` on newly-created objects under
  pure-Python:  these objects may be assigned to a jar, but not yet added
  to its cache.  (PR #6)

- Avoid a failure in ``Persistent.__setstate__`` when the state dict
  contains exactly two keys.  (PR #5)

- Fix a hang in ``picklecache`` invalidation if OIDs are manually passed
  out-of-order. (PR #4)

- Add ``PURE_PYTHON`` environment variable support:  if set, the C
  extensions will not be built, imported, or tested.


4.0.6 (2013-01-03)
------------------

- Updated Trove classifiers.


4.0.5 (2012-12-14)
------------------

- Fixed the C-extensions under Py3k (previously they compiled but were
  not importable).


4.0.4 (2012-12-11)
------------------

- Added support for Python 3.3.

- C extenstions now build under Python 3.2, passing the same tests as
  the pure-Python reference implementation.


4.0.3 (2012-11-19)
------------------

- Fixed: In the C implimentation, an integer was compared with a
  pointer, with undefined results and a compiler warning.

- Fixed: the Python implementation of the ``_p_estimated_size`` propety
  didn't support deletion.

- Simplified implementation of the ``_p_estimated_size`` property to
  only accept integers.  A TypeError is raised if an incorrect type is
  provided.


4.0.2 (2012-08-27)
------------------

- Correct initialization functions in renamed ``_timestamp`` extension.


4.0.1 (2012-08-26)
------------------

- Worked around test failure due to overflow to long on 32-bit systems.

- Renamed ``TimeStamp`` extension module to avoid clash with pure-Python
  ``timestamp`` module on case-insensitive filesystems.

  N.B:  the canonical way to import the ``TimeStamp`` class is now::

    from persistent.timestamp import TimeStamp

  which will yield the class from the extension module (if available),
  falling back to the pure-Python reference implementation.


4.0.0 (2012-08-11)
------------------

Platform Changes
################

- Added explicit support for Python 3.2 and PyPy.

  - Note that the C implementations of Persistent, PickleCache, and Timestamp
    are not built (yet) on these platforms.

- Dropped support for Python < 2.6.

Testing Changes
###############

- 100% unit test coverage.

- Removed all ``ZODB``-dependent tests:

  - Rewrote some to avoid the dependency

  - Cloned the remainder into new ``ZODB.tests`` modules.

- Refactored some doctests refactored as unittests.

- Completed pure-Python reference implementations of 'Persistent',
  'PickleCache', and 'TimeStamp'.

- All covered platforms tested under ``tox``.

- Added support for continuous integration using ``tox`` and ``jenkins``.

- Added ``setup.py dev`` alias (installs ``nose`` and ``coverage``).

- Dropped dependency on ``zope.testing`` / ``zope.testrunner``:  tests now
  run with ``setup.py test``.

Documentation Changes
#####################

- Refactored many Doctests as Sphinx documentation (snippets are exercised
  via 'tox').

- Added ``setup.py docs`` alias (installs ``Sphinx`` and
  ``repoze.sphinx.autointerface``).



  * [pexpect-4.7.0](https://pexpect.readthedocs.io/) 
Pexpect is a pure Python module for spawning child applications; controlling
them; and responding to expected patterns in their output. Pexpect works like
Don Libes' Expect. Pexpect allows your script to spawn a child application and
control it as if a human were typing commands.

Pexpect can be used for automating interactive applications such as ssh, ftp,
passwd, telnet, etc. It can be used to a automate setup scripts for duplicating
software package installations on different servers. It can be used for
automated software testing. Pexpect is in the spirit of Don Libes' Expect, but
Pexpect is pure Python.

The main features of Pexpect require the pty module in the Python standard
library, which is only available on Unix-like systems. Some features—waiting
for patterns from file descriptors or subprocesses—are also available on
Windows.



  * [pickleshare-0.7.5](https://github.com/pickleshare/pickleshare) PickleShare - a small 'shelve' like datastore with concurrency support

Like shelve, a PickleShareDB object acts like a normal dictionary. Unlike shelve,
many processes can access the database simultaneously. Changing a value in 
database is immediately visible to other processes accessing the same database.

Concurrency is possible because the values are stored in separate files. Hence
the "database" is a directory where *all* files are governed by PickleShare.

Example usage::

    from pickleshare import *
    db = PickleShareDB('~/testpickleshare')
    db.clear()
    print("Should be empty:",db.items())
    db['hello'] = 15
    db['aku ankka'] = [1,2,313]
    db['paths/are/ok/key'] = [1,(5,46)]
    print(db.keys())

This module is certainly not ZODB, but can be used for low-load
(non-mission-critical) situations where tiny code size trumps the 
advanced features of a "real" object database.

Installation guide: pip install pickleshare



  * [pifpaf-2.2.2](https://github.com/jd/pifpaf) ==========
 Pifpaf
==========

.. image:: https://travis-ci.org/jd/pifpaf.png?branch=master
    :target: https://travis-ci.org/jd/pifpaf
    :alt: Build Status

.. image:: https://badge.fury.io/py/pifpaf.svg
    :target: https://badge.fury.io/py/pifpaf

Pifpaf is a suite of `fixtures`_ and a command-line tool that allows to start
and stop daemons for a quick throw-away usage. This is typically useful when
needing these daemons to run `integration testing`_. It originaly evolved from
its precussor `overtest`_.

.. _fixtures: https://pypi.python.org/pypi/fixtures
.. _overtest: https://github.com/jd/overtest

Supported daemons
=================

Pifpaf currently supports:

* `PostgreSQL`_
* `MySQL`_
* `Memcached`_
* `InfluxDB`_
* `Etcd`_ (with clustering)
* `Redis`_ (with sentinel mode)
* `Elasticsearch`_
* `ZooKeeper`_
* `Gnocchi`_
* `Aodh`_
* `Ceph`_
* `RabbitMQ`_ (with clustering)
* `FakeS3`_
* `Consul`_
* `Keystone`_
* `CouchDB`_
* `S3rver`_
* `MongoDB`_
* `OpenStack Swift`_
* `Vault`_

.. _Consul: https://www.consul.io/
.. _PostgreSQL: http://postgresql.org
.. _MySQL: http://mysql.org
.. _Memcached: http://memcached.org
.. _InfluxDB: http://influxdb.org
.. _Etcd: https://coreos.com/etcd/
.. _Redis: http://redis.io/
.. _Elasticsearch: https://www.elastic.co/
.. _ZooKeeper: https://zookeeper.apache.org/
.. _Gnocchi: http://gnocchi.xyz
.. _Aodh: http://launchpad.net/aodh
.. _Ceph: http://ceph.com
.. _RabbitMQ: https://www.rabbitmq.com/
.. _FakeS3: https://github.com/jubos/fake-s3
.. _Keystone: https://launchpad.net/keystone
.. _CouchDB: http://couchdb.apache.org/
.. _S3rver: https://www.npmjs.com/package/s3rver
.. _MongoDB: https://www.mongodb.com
.. _OpenStack Swift: https://docs.openstack.org/developer/swift/
.. _Vault: https://www.vaultproject.io/

Usage
=====
To use Pifpaf, simply call the `pifpaf run $daemon <command>` program that you
need. It will setup the temporary environment and export a few environment
variable for you to accesss it::

  $ pifpaf run postgresql psql template1
  Expanded display is used automatically.
  Line style is unicode.
  SET
  psql (9.4.5)
  Type "help" for help.

  template1=# \l
                                List of databases
     Name    │ Owner │ Encoding │   Collate   │    Ctype    │ Access privileges
  ───────────┼───────┼──────────┼─────────────┼─────────────┼───────────────────
   postgres  │ jd    │ UTF8     │ en_US.UTF-8 │ en_US.UTF-8 │
   template0 │ jd    │ UTF8     │ en_US.UTF-8 │ en_US.UTF-8 │ =c/jd            ↵
             │       │          │             │             │ jd=CTc/jd
   template1 │ jd    │ UTF8     │ en_US.UTF-8 │ en_US.UTF-8 │ =c/jd            ↵
             │       │          │             │             │ jd=CTc/jd
  (3 rows)

  template1=# \q
  $

You can also run it with no command line provided::

  $ eval `pifpaf run memcached`
  $ env | grep PIFPAF
  PIFPAF_PID=13387
  PIFPAF_DAEMON=memcached
  PIFPAF_URL=memcached://localhost:11212
  PIFPAF_MEMCACHED_URL=memcached://localhost:11212
  $ pifpaf_stop

Killing the daemon whose PID is contained in `$PIFPAF_PID` will stop the
launched daemon and clean the test environment. You can kill it yourself or use
the defined function `pifpaf_stop`.

Environment variables
=====================
Pifpaf exports a few environment variable:

* `PIFPAF_DAEMON` which contains the name of the daemon launched
* `PIFPAF_URL` which contains the URL to the daemon
* `PIFPAF_PID` the PID of the pifpaf daemon
* `PIFPAF_$daemon_*` variables, which contains daemon specific variables,
  such as port, database name, URL, etc.

.. _integration testing: https://en.wikipedia.org/wiki/Integration_testing


Running several programs at once
================================
Pifpaf provides the ability to change the prefix of its environment variable,
allowing you to nest several Pifpaf instances and therefore running several
daemons at once::

  $ pifpaf --env-prefix STORAGE run memcached -- pifpaf --env-prefix INDEX run postgresql $SHELL
  $ env | grep STORAGE
  STORAGE_DATA=/var/folders/7k/pwdhb_mj2cv4zyr0kyrlzjx40000gq/T/tmpVreJ0J
  STORAGE_MEMCACHED_PORT=11212
  STORAGE_URL=memcached://localhost:11212
  STORAGE_PID=71019
  STORAGE_DAEMON=memcached
  STORAGE_MEMCACHED_URL=memcached://localhost:11212
  $ env | grep INDEX
  INDEX_DATA=/var/folders/7k/pwdhb_mj2cv4zyr0kyrlzjx40000gq/T/tmphAG7tf
  INDEX_URL=postgresql://localhost/postgres?host=/var/folders/7k/pwdhb_mj2cv4zyr0kyrlzjx40000gq/T/tmphAG7tf&port=9824
  INDEX_PID=71021
  INDEX_DAEMON=postgresql
  INDEX_POSTGRESQL_URL=postgresql://localhost/postgres?host=/var/folders/7k/pwdhb_mj2cv4zyr0kyrlzjx40000gq/T/tmphAG7tf&port=9824
  $ echo $PIFPAF_URLS
  memcached://localhost:11212;postgresql://localhost/postgres?host=/var/folders/7k/pwdhb_mj2cv4zyr0kyrlzjx40000gq/T/tmpQ2BWFH&port=9824

The `PIFPAF_URLS` environment variable will contain the list of all URLs
detected and set-up by Pifpaf. You can override this variable name with the
`--global-urls-variable` option.

How it works under the hood
===========================

Pifpaf will start the asked daemon using the current Posix user. The data file
of the daemon will be placed in a temporary directory. The system-wide
configured daemon that might exists is not touched at all.

Pifpaf expected to find daemon binaries on your system (like `mysql`, `mysqld`,
`pg_config`, `pg_ctl`, `rabbitmq-server`, etc).

When the Python fixture is cleaned or when Pifpaf is terminated, the daemon is
stopped and the temporary directory removed.

.. image:: pifpaf.jpg




  * [pip-18.0](https://pip.pypa.io/) pip - The Python Package Installer
==================================

.. image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.org/project/pip/

.. image:: https://readthedocs.org/projects/pip/badge/?version=latest
   :target: https://pip.pypa.io/en/latest

pip is the `package installer`_ for Python. You can use pip to install packages from the `Python Package Index`_ and other indexes.

Please take a look at our documentation for how to install and use pip:

* `Installation`_
* `Usage`_

Updates are released regularly, with a new version every 3 months. More details can be found in our documentation:

* `Release notes`_
* `Release process`_

If you find bugs, need help, or want to talk to the developers please use our mailing lists or chat rooms:

* `Issue tracking`_
* `Discourse channel`_
* `User IRC`_

If you want to get involved head over to GitHub to get the source code, look at our development documentation and feel free to jump on the developer mailing lists and chat rooms:

* `GitHub page`_
* `Dev documentation`_
* `Dev mailing list`_
* `Dev IRC`_

Code of Conduct
---------------

Everyone interacting in the pip project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _package installer: https://packaging.python.org/en/latest/current/
.. _Python Package Index: https://pypi.org
.. _Installation: https://pip.pypa.io/en/stable/installing.html
.. _Usage: https://pip.pypa.io/en/stable/
.. _Release notes: https://pip.pypa.io/en/stable/news.html
.. _Release process: https://pip.pypa.io/en/latest/development/release-process/
.. _GitHub page: https://github.com/pypa/pip
.. _Dev documentation: https://pip.pypa.io/en/latest/development
.. _Issue tracking: https://github.com/pypa/pip/issues
.. _Discourse channel: https://discuss.python.org/c/packaging
.. _Dev mailing list: https://groups.google.com/forum/#!forum/pypa-dev
.. _User IRC: https://webchat.freenode.net/?channels=%23pypa
.. _Dev IRC: https://webchat.freenode.net/?channels=%23pypa-dev
.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/



  * [pip-api-0.0.12](http://github.com/di/pip-api) 
Since [`pip`](https://pypi.org/p/pip) is a command-line-tool, [it does not have
an official, supported, _importable_
API](https://pip.pypa.io/en/latest/user_guide/#using-pip-from-your-program).

However, this does not mean that people haven't tried to `import pip`, usually
to end up with much headache when `pip`'s maintainers do routine refactoring.

This project attempts to provide an importable `pip` API, which is _fully
compliant_ with the recommended method of using `pip` from your program.

## Supported Commands

Not all commands are supported in all versions of `pip` and on all platforms.
If the command you are trying to use is not compatible, `pip_api` will raise a
`pip_api.exceptions.Incompatible` exception for your program to catch.

### Available with all `pip` versions:
* `pip_api.version()`
  > Returns the `pip` version as a string, e.g. `"9.0.1"`

* `pip_api.installed_distributions()`
  > Returns a list of all installed distributions as a `Distribution` object with the following attributes:
  > * `Distribution.name` (`string`): The name of the installed distribution
  > * `Distribution.version` ([`packaging.version.Version`](https://packaging.pypa.io/en/latest/version/#packaging.version.Version)): The version of the installed distribution
  > * `Distribution.location` (`string`): The location of the installed distribution
  > * `Distribution.editable` (`bool`): Whether the distribution is editable or not

* `pip_api.parse_requirements(filename, options=None, include_invalid=False)`
  > Takes a path to a filename of a Requirements file. Returns a mapping from package name to a [`packaging.requirements.Requirement`](https://packaging.pypa.io/en/latest/requirements/#packaging.requirements.Requirement) object with the following attributes:
  > * `Requirement.name` (`string`): The name of the requirement.
  > * `Requirement.extras` (`set`): A set of extras that the requirement specifies.
  > * `Requirement.specifier` ([`packaging.specifiers.SpecifierSet`](https://packaging.pypa.io/en/latest/specifiers/#packaging.specifiers.SpecifierSet)): A `SpecifierSet` of the version specified by the requirement.
  > * `Requirement.marker` ([`packaging.markers.Marker`](https://packaging.pypa.io/en/latest/markers/#packaging.markers.Marker)): A `Marker` of the marker for the requirement. Can be None.`
  > Optionally takes an `options` parameter to override the regex used to skip requirements lines.
  > Optionally takes an `include_invalid` parameter to return an `UnparsedRequirement` in the event that a requirement cannot be parsed correctly.

### Available with `pip>=8.0.0`:
* `pip_api.hash(filename, algorithm='sha256')`
  > Returns the resulting as a string.
  > Valid `algorithm` parameters are `'sha256'`, `'sha384'`, and `'sha512'`



  * [pip-shims-0.3.3](https://github.com/sarugaku/pip-shims) ===============================================================================
pip-shims: Shims for importing packages from pip's internals.
===============================================================================

.. image:: https://img.shields.io/pypi/v/pip-shims.svg
    :target: https://pypi.python.org/pypi/pip-shims

.. image:: https://img.shields.io/pypi/l/pip-shims.svg
    :target: https://pypi.python.org/pypi/pip-shims

.. image:: https://travis-ci.com/sarugaku/pip-shims.svg?branch=master
    :target: https://travis-ci.com/sarugaku/pip-shims

.. image:: https://img.shields.io/pypi/pyversions/pip-shims.svg
    :target: https://pypi.python.org/pypi/pip-shims

.. image:: https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg
    :target: https://saythanks.io/to/techalchemy

.. image:: https://readthedocs.org/projects/pip-shims/badge/?version=latest
    :target: https://pip-shims.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status


Warning
********

.. warning::
   The authors of `pip`_ **do not condone** the use of this package. Relying on pip's
   internals is a **dangerous** idea for your software as they are broken intentionally
   and regularly.  This package may not always be completely updated up PyPI, so relying
   on it may break your code! User beware!

.. _pip: https://github.com/pypa/pip


Installation
*************

Install from `PyPI`_:

  ::

    $ pipenv install --pre pip-shims

Install from `Github`_:

  ::

    $ pipenv install -e git+https://github.com/sarugaku/pip-shims.git#egg=pip-shims


.. _PyPI: https://www.pypi.org/project/pip-shims
.. _Github: https://github.com/sarugaku/pip-shims


.. _`Summary`:

Summary
********

**pip-shims** is a set of compatibilty access shims to the `pip`_ internal API. **pip-shims**
provides compatibility with pip versions 8.0 through the current release (18.x).  The shims
are provided using a lazy import strategy by hacking a module by overloading a class instance's *getattr* method.
This library exists due to my constant writing of the same set of import shims across
many different libraries, including `pipenv`_, `pip-tools`_, `requirementslib`_, and
`passa`_.

.. _passa: https://github.com/sarugaku/passa
.. _pip: https://github.com/pypa/pip
.. _pipenv: https://github.com/pypa/pipenv
.. _pip-tools: https://github.com/jazzband/pip-tools
.. _requirementslib: https://github.com/sarugaku/requirementslib


.. _`Usage`:

Usage
******

Importing a shim
/////////////////

You can use **pip-shims** to expose elements of **pip**'s internal API by importing them:

  ::

    from pip_shims import Wheel
    mywheel = Wheel('/path/to/my/wheel.whl')


Available Shims
****************

**pip-shims** provides the following compatibility shims:

======================== ========================================== =======================================
Import Path               Import Name                                Former Path
======================== ========================================== =======================================
req.constructors          _strip_extras                              req.req_install
cli                       cmdoptions                                 cmdoptions
cli.base_command          Command                                    basecommand
cli.parser                ConfigOptionParser                         baseparser
commands.freeze           DEV_PKGS
exceptions                DistributionNotFound
utils.hashes              FAVORITE_HASH
models                    FormatControl                              index
utils.misc                get_installed_distributions                utils
utils.compat              stdlib_pkgs                                compat
cli.cmdoptions            index_group                                cmdoptions
req.req_install           InstallRequirement
req.constructors          install_req_from_line                      req.req_install.InstallRequirement
req.constructors          install_req_from_editable                  req.req_install.InstallRequirement
req.req_uninstall         UninstallPathSet
distributions             make_distribution_for_install_requirement  operations.prepare.make_abstract_dist
distributions.base        AbstractDistribution
distributions.source      SourceDistribution
distributions.installed   InstalledDistribution
distributions.wheel       WheelDistribution
download                  is_archive_file
download                  is_file_url
utils.misc                is_installable_dir                         utils
index                     Link
operations.prepare        make_abstract_dist                         req.req_set
cli.cmdoptions            make_option_group                          cmdoptions
index                     CandidateEvaluator
index                     PackageFinder
req.req_file              parse_requirements
index                     parse_version
download                  path_to_url
__version__               pip_version
exceptions                PipError
exceptions                InstallationError
exceptions                UninstallationError
exceptions                DistributionNotFound
exceptions                RequirementsFileParseError
exceptions                BestVersionAlreadyInstalled
exceptions                BadCommand
exceptions                CommandError
exceptions                PreviousBuildDirError
operations.prepare        RequirementPreparer
operations.freeze         FrozenRequirement                          <`__init__`>
req.req_set               RequirementSet
req.req_tracker           RequirementTracker
resolve                   Resolver
download                  SafeFileCache
download                  url_to_path
download                  unpack_url
locations                 USER_CACHE_DIR
vcs.versioncontrol        VcsSupport                                 vcs.VcsSupport
wheel                     Wheel
wheel                     WheelBuilder
cache                     WheelCache                                 wheel
======================== ========================================== =======================================



  * [pipenv-2018.11.26](https://github.com/pypa/pipenv) Pipenv: Python Development Workflow for Humans
==============================================

[![image](https://img.shields.io/pypi/v/pipenv.svg)](https://python.org/pypi/pipenv)
[![image](https://img.shields.io/pypi/l/pipenv.svg)](https://python.org/pypi/pipenv)
[![image](https://badge.buildkite.com/79c7eccf056b17c3151f3c4d0e4c4b8b724539d84f1e037b9b.svg?branch=master)](https://code.kennethreitz.org/source/pipenv/)
[![VSTS build status (Windows)](https://dev.azure.com/pypa/pipenv/_apis/build/status/pipenv%20CI%20(Windows)?branchName=master&label=Windows)](https://dev.azure.com/pypa/pipenv/_build/latest?definitionId=9&branchName=master)
[![VSTS build status (Linux)](https://dev.azure.com/pypa/pipenv/_apis/build/status/pipenv%20CI%20(Linux)?branchName=master&label=Linux)](https://dev.azure.com/pypa/pipenv/_build/latest?definitionId=10&branchName=master)
[![image](https://img.shields.io/pypi/pyversions/pipenv.svg)](https://python.org/pypi/pipenv)
[![image](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/kennethreitz)

------------------------------------------------------------------------

**Pipenv** is a tool that aims to bring the best of all packaging worlds
(bundler, composer, npm, cargo, yarn, etc.) to the Python world.
*Windows is a first--class citizen, in our world.*

It automatically creates and manages a virtualenv for your projects, as
well as adds/removes packages from your `Pipfile` as you
install/uninstall packages. It also generates the ever--important
`Pipfile.lock`, which is used to produce deterministic builds.

![image](http://media.kennethreitz.com.s3.amazonaws.com/pipenv.gif)

The problems that Pipenv seeks to solve are multi-faceted:

-   You no longer need to use `pip` and `virtualenv` separately. They
    work together.
-   Managing a `requirements.txt` file [can be
    problematic](https://www.kennethreitz.org/essays/a-better-pip-workflow),
    so Pipenv uses the upcoming `Pipfile` and `Pipfile.lock` instead,
    which is superior for basic use cases.
-   Hashes are used everywhere, always. Security. Automatically expose
    security vulnerabilities.
-   Give you insight into your dependency graph (e.g. `$ pipenv graph`).
-   Streamline development workflow by loading `.env` files.

You can quickly play with Pipenv right in your browser:

[![Try in browser](https://cdn.rawgit.com/rootnroll/library/assets/try.svg)](https://rootnroll.com/d/pipenv/)

Installation
------------

If you\'re on MacOS, you can install Pipenv easily with Homebrew:

    $ brew install pipenv

Or, if you\'re using Fedora 28:

    $ sudo dnf install pipenv

Otherwise, refer to the [documentation](https://docs.pipenv.org/install/) for instructions.

✨🍰✨

☤ User Testimonials
-------------------

**Jannis Leidel**, former pip maintainer---

:   *Pipenv is the porcelain I always wanted to build for pip. It fits
    my brain and mostly replaces virtualenvwrapper and manual pip calls
    for me. Use it.*

**David Gang**---

:   *This package manager is really awesome. For the first time I know
    exactly what my dependencies are which I installed and what the
    transitive dependencies are. Combined with the fact that installs
    are deterministic, makes this package manager first class, like
    cargo*.

**Justin Myles Holmes**---

:   *Pipenv is finally an abstraction meant to engage the mind instead
    of merely the filesystem.*

☤ Features
----------

-   Enables truly *deterministic builds*, while easily specifying *only
    what you want*.
-   Generates and checks file hashes for locked dependencies.
-   Automatically install required Pythons, if `pyenv` is available.
-   Automatically finds your project home, recursively, by looking for a
    `Pipfile`.
-   Automatically generates a `Pipfile`, if one doesn\'t exist.
-   Automatically creates a virtualenv in a standard location.
-   Automatically adds/removes packages to a `Pipfile` when they are
    un/installed.
-   Automatically loads `.env` files, if they exist.

The main commands are `install`, `uninstall`, and `lock`, which
generates a `Pipfile.lock`. These are intended to replace
`$ pip install` usage, as well as manual virtualenv management (to
activate a virtualenv, run `$ pipenv shell`).

### Basic Concepts

-   A virtualenv will automatically be created, when one doesn\'t exist.
-   When no parameters are passed to `install`, all packages
    `[packages]` specified will be installed.
-   To initialize a Python 3 virtual environment, run
    `$ pipenv --three`.
-   To initialize a Python 2 virtual environment, run `$ pipenv --two`.
-   Otherwise, whatever virtualenv defaults to will be the default.

### Other Commands

-   `shell` will spawn a shell with the virtualenv activated.
-   `run` will run a given command from the virtualenv, with any
    arguments forwarded (e.g. `$ pipenv run python`).
-   `check` asserts that PEP 508 requirements are being met by the
    current environment.
-   `graph` will print a pretty graph of all your installed
    dependencies.

### Shell Completion

For example, with fish, put this in your
`~/.config/fish/completions/pipenv.fish`:

    eval (pipenv --completion)

Alternatively, with bash, put this in your `.bashrc` or `.bash_profile`:

    eval "$(pipenv --completion)"

Magic shell completions are now enabled! There is also a [fish
plugin](https://github.com/fisherman/pipenv), which will automatically
activate your subshells for you!

Fish is the best shell. You should use it.

☤ Usage
-------

    $ pipenv
    Usage: pipenv [OPTIONS] COMMAND [ARGS]...

    Options:
      --where          Output project home information.
      --venv           Output virtualenv information.
      --py             Output Python interpreter information.
      --envs           Output Environment Variable options.
      --rm             Remove the virtualenv.
      --bare           Minimal output.
      --completion     Output completion (to be eval'd).
      --man            Display manpage.
      --three / --two  Use Python 3/2 when creating virtualenv.
      --python TEXT    Specify which version of Python virtualenv should use.
      --site-packages  Enable site-packages for the virtualenv.
      --version        Show the version and exit.
      -h, --help       Show this message and exit.


    Usage Examples:
       Create a new project using Python 3.7, specifically:
       $ pipenv --python 3.7

       Remove project virtualenv (inferred from current directory):
       $ pipenv --rm

       Install all dependencies for a project (including dev):
       $ pipenv install --dev

       Create a lockfile containing pre-releases:
       $ pipenv lock --pre

       Show a graph of your installed dependencies:
       $ pipenv graph

       Check your installed dependencies for security vulnerabilities:
       $ pipenv check

       Install a local setup.py into your virtual environment/Pipfile:
       $ pipenv install -e .

       Use a lower-level pip command:
       $ pipenv run pip freeze

    Commands:
      check      Checks for security vulnerabilities and against PEP 508 markers
                 provided in Pipfile.
      clean      Uninstalls all packages not specified in Pipfile.lock.
      graph      Displays currently–installed dependency graph information.
      install    Installs provided packages and adds them to Pipfile, or (if no
                 packages are given), installs all packages from Pipfile.
      lock       Generates Pipfile.lock.
      open       View a given module in your editor.
      run        Spawns a command installed into the virtualenv.
      shell      Spawns a shell within the virtualenv.
      sync       Installs all packages specified in Pipfile.lock.
      uninstall  Un-installs a provided package and removes it from Pipfile.

Locate the project:

    $ pipenv --where
    /Users/kennethreitz/Library/Mobile Documents/com~apple~CloudDocs/repos/kr/pipenv/test

Locate the virtualenv:

    $ pipenv --venv
    /Users/kennethreitz/.local/share/virtualenvs/test-Skyy4vre

Locate the Python interpreter:

    $ pipenv --py
    /Users/kennethreitz/.local/share/virtualenvs/test-Skyy4vre/bin/python

Install packages:

    $ pipenv install
    Creating a virtualenv for this project...
    ...
    No package provided, installing all dependencies.
    Virtualenv location: /Users/kennethreitz/.local/share/virtualenvs/test-EJkjoYts
    Installing dependencies from Pipfile.lock...
    ...

    To activate this project's virtualenv, run the following:
    $ pipenv shell

Installing from git:

You can install packages with pipenv from git and other version control systems using URLs formatted according to the following rule:

    <vcs_type>+<scheme>://<location>/<user_or_organization>/<repository>@<branch_or_tag>#<package_name>

The only optional section is the `@<branch_or_tag>` section.  When using git over SSH, you may use the shorthand vcs and scheme alias `git+git@<location>:<user_or_organization>/<repository>@<branch_or_tag>#<package_name>`. Note that this is translated to `git+ssh://git@<location>` when parsed.

Valid values for `<vcs_type>` include `git`, `bzr`, `svn`, and `hg`.  Valid values for `<scheme>` include `http,`, `https`, `ssh`, and `file`.  In specific cases you also have access to other schemes: `svn` may be combined with `svn` as a scheme, and `bzr` can be combined with `sftp` and `lp`.

Note that it is **strongly recommended** that you install any version-controlled dependencies in editable mode, using `pipenv install -e`, in order to ensure that dependency resolution can be performed with an up to date copy of the repository each time it is performed, and that it includes all known dependencies.

Below is an example usage which installs the git repository located at `https://github.com/requests/requests.git` from tag `v2.19.1` as package name `requests`:

    $ pipenv install -e git+https://github.com/requests/requests.git@v2.19#egg=requests
    Creating a Pipfile for this project...
    Installing -e git+https://github.com/requests/requests.git@v2.19.1#egg=requests...
    [...snipped...]
    Adding -e git+https://github.com/requests/requests.git@v2.19.1#egg=requests to Pipfile's [packages]...
    [...]

You can read more about [pip's implementation of vcs support here](https://pip.pypa.io/en/stable/reference/pip_install/#vcs-support).

Install a dev dependency:

    $ pipenv install pytest --dev
    Installing pytest...
    ...
    Adding pytest to Pipfile's [dev-packages]...

Show a dependency graph:

    $ pipenv graph
    requests==2.18.4
      - certifi [required: >=2017.4.17, installed: 2017.7.27.1]
      - chardet [required: >=3.0.2,<3.1.0, installed: 3.0.4]
      - idna [required: >=2.5,<2.7, installed: 2.6]
      - urllib3 [required: <1.23,>=1.21.1, installed: 1.22]

Generate a lockfile:

    $ pipenv lock
    Assuring all dependencies from Pipfile are installed...
    Locking [dev-packages] dependencies...
    Locking [packages] dependencies...
    Note: your project now has only default [packages] installed.
    To install [dev-packages], run: $ pipenv install --dev

Install all dev dependencies:

    $ pipenv install --dev
    Pipfile found at /Users/kennethreitz/repos/kr/pip2/test/Pipfile. Considering this to be the project home.
    Pipfile.lock out of date, updating...
    Assuring all dependencies from Pipfile are installed...
    Locking [dev-packages] dependencies...
    Locking [packages] dependencies...

Uninstall everything:

    $ pipenv uninstall --all
    No package provided, un-installing all dependencies.
    Found 25 installed package(s), purging...
    ...
    Environment now purged and fresh!

Use the shell:

    $ pipenv shell
    Loading .env environment variables…
    Launching subshell in virtual environment. Type 'exit' or 'Ctrl+D' to return.
    $ ▯

☤ Documentation
---------------

Documentation resides over at [pipenv.org](http://pipenv.org/).



  * [pipreqs-0.4.9](https://github.com/bndr/pipreqs) ===============================
``pipreqs`` - Generate requirements.txt file for any project based on imports
===============================

.. image:: https://img.shields.io/travis/bndr/pipreqs.svg
        :target: https://travis-ci.org/bndr/pipreqs
      
        
.. image:: https://img.shields.io/pypi/v/pipreqs.svg
        :target: https://pypi.python.org/pypi/pipreqs

        
.. image:: https://img.shields.io/coveralls/bndr/pipreqs.svg 
        :target: https://coveralls.io/r/bndr/pipreqs
  
        
.. image:: https://img.shields.io/pypi/l/pipreqs.svg 
        :target: https://pypi.python.org/pypi/pipreqs

        

Installation
------------

::

    pip install pipreqs

Usage
-----

::

    Usage:
        pipreqs [options] <path>

    Options:
        --use-local           Use ONLY local package info instead of querying PyPI
        --pypi-server <url>   Use custom PyPi server
        --proxy <url>         Use Proxy, parameter will be passed to requests library. You can also just set the
                              environments parameter in your terminal:
                              $ export HTTP_PROXY="http://10.10.1.10:3128"
                              $ export HTTPS_PROXY="https://10.10.1.10:1080"
        --debug               Print debug information
        --ignore <dirs>...    Ignore extra directories
        --encoding <charset>  Use encoding parameter for file open
        --savepath <file>     Save the list of requirements in the given file
        --print               Output the list of requirements in the standard output
        --force               Overwrite existing requirements.txt
        --diff <file>         Compare modules in requirements.txt to project imports.
        --clean <file>        Clean up requirements.txt by removing modules that are not imported in project.
Example
-------

::

    $ pipreqs /home/project/location
    Successfully saved requirements file in /home/project/location/requirements.txt

Contents of requirements.txt

::

    wheel==0.23.0
    Yarg==0.1.9
    docopt==0.6.2
    
Why not pip freeze?
-------------------

- ``pip freeze`` only saves the packages that are installed with ``pip install`` in your environment. 
- pip freeze saves all packages in the environment including those that you don't use in your current project. (if you don't have virtualenv)
- and sometimes you just need to create requirements.txt for a new project without installing modules.




History
-------

0.4.8
--------------------

* Implement '--clean' and '--diff' (kxrd)
* Exclude concurrent{,.futures} from stdlib if py2 (kxrd)

0.4.7
--------------------

* BUG: remove package/version duplicates
* Style: pep8

0.4.5
---------------------

* Fixed the --pypi-server option

0.4.4 (2016-07-14)
---------------------

* Remove Spaces in output
* Add package to output even without version

0.4.2 (2016-02-10)
---------------------

* Fix duplicated lines in requirements.txt (Dmitry Pribysh)

0.4.1 (2016-02-05)
---------------------

* Added ignore option (Nick Rhinehart)

0.4.0 (2016-01-28)
---------------------

* Walk Abstract Syntax Tree to find imports (Kay Sackey)

0.3.9 (2016-01-20)
---------------------

* Fix regex for docstring comments (#35)

0.3.8 (2016-01-12)
---------------------

* Add more package mapping
* fix(pipreqs/mapping): remove pylab reference to matplotlib
* Remove comments """ before going through imports
* Update proxy documentation

0.3.1 (2015-10-20)
---------------------

* fixed lint warnings (EJ Lee)
* add --encoding parameter for open() (EJ Lee)
* support windows directory separator (EJ Lee)

0.3.0 (2015-09-29)
---------------------

* Add --proxy option
* Add --pypi-server option

0.2.9 (2015-09-24)
---------------------

* Ignore irreverent directory when generating requirement.txt (Lee Wei)
* Modify logging level of "Requirement.txt already exists" to warning (Lee Wei)

0.2.8 (2015-05-11)
---------------------

* Add --force option as a protection for overwrites

0.2.6 (2015-05-11)
---------------------

* Fix exception when 'import' is used inside package name #17
* Add more tests

0.2.5 (2015-05-11)
---------------------

* Fix exception when 'import' is used in comments #17
* Fix duplicate entries in requirements.txt

0.2.4 (2015-05-10)
---------------------

* Refactoring
* fix "import as"

0.2.3 (2015-05-09)
---------------------

* Fix multiple alias imports on the same line (Tiago Costa)
* More package mappings

0.2.2 (2015-05-08)
---------------------

* Add ImportName -> PackageName mapping
* More tests

0.2.1 (2015-05-08)
---------------------

* Fix for TypeError for implicit conversion

0.2.0 (2015-05-06)
---------------------

* Add --use-local option
* Exclude relative imports. (Dongwon Shin)
* Use "latest_release_id" instead of "release_ids[-1]" (Dongwon Shin)

0.1.9 (2015-05-01)
---------------------

* Output tuning (Harri Berglund)
* Use str.partition() to simplify the logic (cclaus)

0.1.8 (2015-04-26)
---------------------

* Fixed problems with local imports (Dongwon Shin)
* Fixed problems with imports with 'as' (Dongwon Shin)
* Fix indentation, pep8 Styling. (Michael Borisov)
* Optimize imports and adding missing import for sys module. (Michael Borisov)

0.1.7 (2015-04-24)
---------------------

* Add more assertions in tests
* Add more verbose output
* Add recursive delete to Makefile clean
* Update Readme

0.1.6 (2015-04-22)
---------------------

* py3 print function

0.1.5 (2015-04-22)
---------------------

* Add Readme, Add Examples
* Add Stdlib into package

0.1.1 (2015-04-22)
---------------------

* Fix regex matching for imports
* Release on Pypi

0.1.0 (2015-04-22)
---------------------

* First release on Github.

  * [pkgconfig-1.5.1](https://github.com/matze/pkgconfig) pkgconfig
=========

.. image:: https://travis-ci.org/matze/pkgconfig.png?branch=master
    :target: https://travis-ci.org/matze/pkgconfig

``pkgconfig`` is a Python module to interface with the ``pkg-config``
command line tool and supports Python 2.6+ and 3.3+.

It can be used to

-  find all pkg-config packages ::

       >>> packages = pkgconfig.list_all()

-  check if a package exists ::

       >>> pkgconfig.exists('glib-2.0')
       True

-  check if a package meets certain version requirements ::

       >>> pkgconfig.installed('glib-2.0', '< 2.26')
       False

-  return the version ::
       >>> pkgconfig.modversion('glib-2.0')
       '2.56.3'

-  query CFLAGS and LDFLAGS ::

       >>> pkgconfig.cflags('glib-2.0')
       '-I/usr/include/glib-2.0 -I/usr/lib/glib-2.0/include'

       >>> pkgconfig.libs('glib-2.0')
       '-lglib-2.0'

-  get all variables defined for a package::

        >>> pkgconfig.variables('glib-2.0')
        {u'exec_prefix': u'/usr'}

-  parse the output to build extensions with setup.py ::

       >>> d = pkgconfig.parse('glib-2.0 gtk+-2.0')
       >>> d['libraries']
       [u'gtk+-2.0', u'glib-2.0']

   The ``pkgconfig.parse`` function returns a dictonary of lists.
   The lists returned are accurate representations of the equivalent
   ``pkg-config`` call's result, both in content and order.

If ``pkg-config`` is not on the path, raises ``EnvironmentError``.

The ``pkgconfig`` module is licensed under the MIT license.


Changelog
---------

Version 1.5.0
~~~~~~~~~~~~~

- Use poetry instead of setuptools directly
- Fix #42: raise exception if package is missing
- Fix version parsing for openssl-like version numbers, fixes #32
- Fix #31: expose --modversion
- Fix #30: strip whitespace from variable names

Version 1.4.0
~~~~~~~~~~~~~

- Add boolean ``static`` keyword to output private libraries as well
- Raise original ``OSError`` as well

Version 1.3.1
~~~~~~~~~~~~~

- Fix compatibility problems with Python 2.6

Version 1.3.0
~~~~~~~~~~~~~

- Add variables() API to query defined variables
- Disable Python 3.2 and enable Python 3.5 and 3.6 tests
- Fix #16: handle spaces of values in .pc files correctly

Version 1.2.1 and 1.2.2
~~~~~~~~~~~~~~~~~~~~~~~

Bug fix releases released on December 1st and 2nd 2016.

- Include the ``data`` folder in the distribution in order to run tests
- Improve the tests


Version 1.2.0
~~~~~~~~~~~~~

Released on November 30th 2016.

- Potential break: switch from result set to list
- Expose --list-all query
- Added support for PKG_CONFIG environment variable


Version 1.1.0
~~~~~~~~~~~~~

Released on November 6th 2013.

- Multiple packages can now be parsed with a single call to ``.parse``.


Version 1.0.0
~~~~~~~~~~~~~

First release on September 8th 2013.

  * [pkginfo-1.5.0.1](https://code.launchpad.net/~tseaver/pkginfo/trunk) ``pkginfo`` README
==================

This package provides an API for querying the distutils metadata written in
the ``PKG-INFO`` file inside a source distriubtion (an ``sdist``) or a
binary distribution (e.g., created by running ``bdist_egg``).  It can
also query the ``EGG-INFO`` directory of an installed distribution, and
the ``*.egg-info`` stored in a "development checkout"
(e.g, created by running ``setup.py develop``).


Please see the `pkginfo docs <http://packages.python.org/pkginfo>`_
for detailed documentation.


``pkginfo`` Changelog
=====================

1.5.0.1 (2019-01-08)
--------------------

- Fix broken 'sdist'. LP #1639585.

1.5.0 (2019-01-07)
------------------

- Fix 'console_scripts' entry point syntax. LP #1810734.

- Add support for JSON output from the CLI.

- Add support for installed wheels.  E.g., 'dist-info/' dirs.

- Add support for Python 3.6 and 3.7.

- Drop support for Python 3.3.

1.4.2 (2018-03-14)
------------------

- Use relative imports in pkginfo modules.  Supports vendoring of the
  package into setuptools.

- Add support for ``Provides-Extra`` and ``Description-Content-Type`` fields.
  Per https://packaging.python.org/specifications/.  See:  PEP 566.

- Remove support for old setuptools leaving ``PKG-INFO`` in the root of
  the project directory.

1.4.1 (2016-11-07)
------------------

- Packaging only change (invalid sdist built for 1.4.0).

1.4.0 (2016-11-04)
------------------

- Relicense under MIT license:  the PSF license is not suitable for
  third-party libraries.

1.3.2 (2016-05-24)
------------------

- Packaging-only change (automate fix for wheel built for 1.3.1).

1.3.1 (2016-05-24)
------------------

- Packaging-only change (invalid wheel built for 1.3.0).

1.3.0 (2016-05-23)
------------------

- Update homepage URL to point to Launchpad, rather than PyPI.

- Add support for building wheels.

- Add support for Python 3.5.

- Drop support for Python 2.6 and 3.2.

1.2.1 (2014-01-02)
------------------

- Add overlooked Trove classifier for Python 3.4.

1.2 (2014-01-02)
----------------

- Add support for Python 3.4, PyPy3.

- Add 100% coverage for ``pkginfo.commandline`` module.

1.2b1 (2013-12-05)
------------------

- Add support for the "wheel" distribution format, along with minimal
  metadata 2.0 support (not including new PEP 426 JSON properties).
  Code (re-)borrowed from Donald Stuft's ``twine`` package.

1.1 (2013-10-09)
----------------

- Fix tests to pass with current PyPy releases.

1.1b1 (2013-05-05)
------------------

- Support "develop" packages which keep their ``*.egg-info`` in a subdirectory.
  See https://bugs.launchpad.net/pkginfo/+bug/919147.

- Add support for "unpacked SDists" (thanks to Mike Lundy for the patch).

1.0 (2013-05-05)
----------------

- No changes from 1.0b2.

1.0b2 (2012-12-28)
------------------

- Suppress resource warning leaks reported against clients.

- Fix 'commandline' module under Py3k.

1.0b1 (2012-12-28)
------------------

- Add support for Python 3.2 and 3.3, including testing them under ``tox``.

- Add support for PyPy, including testing it under ``tox``.

- Test supported Python versions under ``tox``.

- Drop support for Python 2.5.

- Add a ``setup.py dev`` alias:  runs ``setup.py develop`` and installs
  testing extras (``nose`` and ``coverage``).

0.9.1 (2012-10-22)
------------------

- Fix test failure under Python >= 2.7, which is enforcing
  'metadata_version == 1.1' because we have classifiers.


0.9 (2012-04-25)
----------------

- Fix introspection of installed namespace packages.
  They may be installed as eggs or via dist-installed 'egg-info' files.
  https://bugs.launchpad.net/pkginfo/+bug/934311

- Avoid a regression in 0.8 under Python 2.6 / 2.7 when parsing unicode.
  https://bugs.launchpad.net/pkginfo/+bug/733827/comments/3


0.8 (2011-03-12)
----------------

- Work around Python 2.7's breakage of StringIO.  Fixes
  https://bugs.launchpad.net/pkginfo/+bug/733827

- Fix bug in introspection of installed packages missing the
  ``__package__`` attribute.


0.7 (2010-11-04)
----------------

- Preserve newlines in the ``description`` field.  Thanks to Sridhar
  Ratnakumar for the patch.

- 100% test coverage.


0.6 (2010-06-01)
----------------

- Replace use of ``StringIO.StringIO`` with ``io.StringIO``, where available
  (Python >= 2.6).

- Replace use of ``rfc822`` stdlib module with ``email.parser``, when
  available (Python >= 2.5).  Ensured that distributions "unfold" wrapped
  continuation lines, stripping any leading / trailing whitespace, no matter
  which module was used for parsing.

- Remove bogus testing dependency on ``zope.testing``.

- Add tests that the "environment markers" spelled out in the approved
  PEP 345 are captured.

- Add ``Project-URL`` for ``1.2`` PKG-INFO metdata (defined in the accepted
  version of PEP 345).


0.5 (2009-09-11)
----------------

- Marked package as non-zip-safe.

- Fix Trove metadata misspelling.

- Restore compatibility with Python 2.4.

- Note that the introspection of installed packages / modules works only
  in Python 2.6 or later.

- Add ``Index`` class as an abstraction over a collection of distributions.

- Add ``download_url_prefix`` argument to ``pkginfo`` script.  If passed,
  the script will use the prefix to synthesize a ``download_url`` for
  distributions which do not supply that value directly.


0.4.1 (2009-05-07)
------------------

- Fix bugs in handling of installed packages which lack ``__file__``
  or ``PKG-INFO``.


0.4 (2009-05-07)
----------------

- Extend the console script to allow output as CSV or INI.  Also, added
  arguments to specify the metadata version and other parsing / output
  policies.

- Add support for the different metadata versions specified in PEPs
  241, 314, and 345.  Distributions now parse and expose only the attributes
  corresponding to their metadata version, which defaults to the version
  parsed from the ``PKG-INFO`` file.  The programmer can override that version
  when creating the distribution object.


0.3 (2009-05-07)
----------------

- Add support for introspection of "development eggs" (checkouts with
  ``PKG-INFO``, perhaps created via ``setup.py develop``).

- Add a console script, ``pkginfo``, which takes one or more paths
  on the command line and writes out the associated information.  Thanks
  to ``runeh`` for the patch!

- Add ``get_metadata`` helper function, which dispatches a given path or
  module across the available distribution types, and returns a distribution
  object.  Thanks to ``runeh`` for the patch!

- Make distribution objects support iteration over the metadata fields.
  Thanks to ``runeh`` for the patch!

- Make ``Distribution`` and subclasses new-style classes.  Thanks to ``runeh``
  for the patch!


0.2 (2009-04-14)
----------------

- Add support for introspection of ``bdist_egg`` binary distributions.


0.1.1 (2009-04-10)
------------------

- Fix packaging errors.


0.1 (2009-04-10)
----------------

- Initial release.



  * [plette-0.2.2](https://github.com/sarugaku/plette) ===================================================
Plette: Structured Pipfile and Pipfile.lock models.
===================================================

.. image:: https://img.shields.io/pypi/v/plette.svg
    :target: https://pypi.org/pypi/plette

.. image:: https://img.shields.io/pypi/l/plette.svg
    :target: https://pypi.org/pypi/plette

.. image:: https://api.travis-ci.com/sarugaku/plette.svg?branch=master
    :target: https://travis-ci.com/sarugaku/plette

.. image:: https://img.shields.io/pypi/pyversions/plette.svg
    :target: https://pypi.org/pypi/plette

.. image:: https://readthedocs.org/projects/plette/badge/?version=master
    :target: http://plette.readthedocs.io/en/master/?badge=master
    :alt: Documentation Status

Plette implements Pipfle and Pipfile.lock parsers, generators, and optional
validators to let the user work with them in a structured manner.


Installation
============

Install with pip::

    pip install plette

Install with the optional validation feature::

    pip install plette[validation]


`Read the documentation <https://plette.readthedocs.io>`__.



  * [plotly-4.0.0](https://plot.ly/python/) The main plotly distribution package



  * [pluggy-0.12.0](https://github.com/pytest-dev/pluggy) ====================================================
pluggy - A minimalist production ready plugin system
====================================================

|pypi| |conda-forge| |versions| |travis| |appveyor| |gitter| |black| |codecov|

This is the core framework used by the `pytest`_, `tox`_, and `devpi`_ projects.

Please `read the docs`_ to learn more!

A definitive example
====================
.. code-block:: python

    import pluggy

    hookspec = pluggy.HookspecMarker("myproject")
    hookimpl = pluggy.HookimplMarker("myproject")


    class MySpec(object):
        """A hook specification namespace.
        """

        @hookspec
        def myhook(self, arg1, arg2):
            """My special little hook that you can customize.
            """


    class Plugin_1(object):
        """A hook implementation namespace.
        """

        @hookimpl
        def myhook(self, arg1, arg2):
            print("inside Plugin_1.myhook()")
            return arg1 + arg2


    class Plugin_2(object):
        """A 2nd hook implementation namespace.
        """

        @hookimpl
        def myhook(self, arg1, arg2):
            print("inside Plugin_2.myhook()")
            return arg1 - arg2


    # create a manager and add the spec
    pm = pluggy.PluginManager("myproject")
    pm.add_hookspecs(MySpec)

    # register plugins
    pm.register(Plugin_1())
    pm.register(Plugin_2())

    # call our ``myhook`` hook
    results = pm.hook.myhook(arg1=1, arg2=2)
    print(results)


.. badges

.. |pypi| image:: https://img.shields.io/pypi/v/pluggy.svg
    :target: https://pypi.org/pypi/pluggy

.. |versions| image:: https://img.shields.io/pypi/pyversions/pluggy.svg
    :target: https://pypi.org/pypi/pluggy

.. |travis| image:: https://img.shields.io/travis/pytest-dev/pluggy/master.svg
    :target: https://travis-ci.org/pytest-dev/pluggy

.. |appveyor| image:: https://img.shields.io/appveyor/ci/pytestbot/pluggy/master.svg
    :target: https://ci.appveyor.com/project/pytestbot/pluggy

.. |conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/pluggy.svg
    :target: https://anaconda.org/conda-forge/pytest

.. |gitter| image:: https://badges.gitter.im/pytest-dev/pluggy.svg
    :alt: Join the chat at https://gitter.im/pytest-dev/pluggy
    :target: https://gitter.im/pytest-dev/pluggy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

.. |codecov| image:: https://codecov.io/gh/pytest-dev/pluggy/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/pytest-dev/pluggy
    :alt: Code coverage Status

.. links
.. _pytest:
    http://pytest.org
.. _tox:
    https://tox.readthedocs.org
.. _devpi:
    http://doc.devpi.net
.. _read the docs:
   https://pluggy.readthedocs.io/en/latest/


=========
Changelog
=========

.. towncrier release notes start

pluggy 0.13.0 (2019-09-10)
==========================

Trivial/Internal Changes
------------------------

- `#222 <https://github.com/pytest-dev/pluggy/issues/222>`_: Replace ``importlib_metadata`` backport with ``importlib.metadata`` from the
  standard library on Python 3.8+.


pluggy 0.12.0 (2019-05-27)
==========================

Features
--------

- `#215 <https://github.com/pytest-dev/pluggy/issues/215>`_: Switch from ``pkg_resources`` to ``importlib-metadata`` for entrypoint detection for improved performance and import time.  This time with ``.egg`` support.


pluggy 0.11.0 (2019-05-07)
==========================

Bug Fixes
---------

- `#205 <https://github.com/pytest-dev/pluggy/issues/205>`_: Revert changes made in 0.10.0 release breaking ``.egg`` installs.


pluggy 0.10.0 (2019-05-07)
==========================

Features
--------

- `#199 <https://github.com/pytest-dev/pluggy/issues/199>`_: Switch from ``pkg_resources`` to ``importlib-metadata`` for entrypoint detection for improved performance and import time.


pluggy 0.9.0 (2019-02-21)
=========================

Features
--------

- `#189 <https://github.com/pytest-dev/pluggy/issues/189>`_: ``PluginManager.load_setuptools_entrypoints`` now accepts a ``name`` parameter that when given will
  load only entry points with that name.

  ``PluginManager.load_setuptools_entrypoints`` also now returns the number of plugins loaded by the
  call, as opposed to the number of all plugins loaded by all calls to this method.



Bug Fixes
---------

- `#187 <https://github.com/pytest-dev/pluggy/issues/187>`_: Fix internal ``varnames`` function for PyPy3.


pluggy 0.8.1 (2018-11-09)
=========================

Trivial/Internal Changes
------------------------

- `#166 <https://github.com/pytest-dev/pluggy/issues/166>`_: Add ``stacklevel=2`` to implprefix warning so that the reported location of warning is the caller of PluginManager.


pluggy 0.8.0 (2018-10-15)
=========================

Features
--------

- `#177 <https://github.com/pytest-dev/pluggy/issues/177>`_: Add ``get_hookimpls()`` method to hook callers.



Trivial/Internal Changes
------------------------

- `#165 <https://github.com/pytest-dev/pluggy/issues/165>`_: Add changelog in long package description and documentation.


- `#172 <https://github.com/pytest-dev/pluggy/issues/172>`_: Add a test exemplifying the opt-in nature of spec defined args.


- `#57 <https://github.com/pytest-dev/pluggy/issues/57>`_: Encapsulate hook specifications in a type for easier introspection.


pluggy 0.7.1 (2018-07-28)
=========================

Deprecations and Removals
-------------------------

- `#116 <https://github.com/pytest-dev/pluggy/issues/116>`_: Deprecate the ``implprefix`` kwarg to ``PluginManager`` and instead
  expect users to start using explicit ``HookimplMarker`` everywhere.



Features
--------

- `#122 <https://github.com/pytest-dev/pluggy/issues/122>`_: Add ``.plugin`` member to ``PluginValidationError`` to access failing plugin during post-mortem.


- `#138 <https://github.com/pytest-dev/pluggy/issues/138>`_: Add per implementation warnings support for hookspecs allowing for both
  deprecation and future warnings of legacy and (future) experimental hooks
  respectively.



Bug Fixes
---------

- `#110 <https://github.com/pytest-dev/pluggy/issues/110>`_: Fix a bug where ``_HookCaller.call_historic()`` would call the ``proc``
  arg even when the default is ``None`` resulting in a ``TypeError``.

- `#160 <https://github.com/pytest-dev/pluggy/issues/160>`_: Fix problem when handling ``VersionConflict`` errors when loading setuptools plugins.



Improved Documentation
----------------------

- `#123 <https://github.com/pytest-dev/pluggy/issues/123>`_: Document how exceptions are handled and how the hook call loop
  terminates immediately on the first error which is then delivered
  to any surrounding wrappers.


- `#136 <https://github.com/pytest-dev/pluggy/issues/136>`_: Docs rework including a much better introduction and comprehensive example
  set for new users. A big thanks goes out to @obestwalter for the great work!



Trivial/Internal Changes
------------------------

- `#117 <https://github.com/pytest-dev/pluggy/issues/117>`_: Break up the main monolithic package modules into separate modules by concern


- `#131 <https://github.com/pytest-dev/pluggy/issues/131>`_: Automate ``setuptools`` wheels building and PyPi upload using TravisCI.


- `#153 <https://github.com/pytest-dev/pluggy/issues/153>`_: Reorganize tests more appropriately by modules relating to each
  internal component/feature. This is in an effort to avoid (future)
  duplication and better separation of concerns in the test set.


- `#156 <https://github.com/pytest-dev/pluggy/issues/156>`_: Add ``HookImpl.__repr__()`` for better debugging.


- `#66 <https://github.com/pytest-dev/pluggy/issues/66>`_: Start using ``towncrier`` and a custom ``tox`` environment to prepare releases!


pluggy 0.7.0 (Unreleased)
=========================

* `#160 <https://github.com/pytest-dev/pluggy/issues/160>`_: We discovered a deployment issue so this version was never released to PyPI, only the tag exists.

pluggy 0.6.0 (2017-11-24)
=========================

- Add CI testing for the features, release, and master
  branches of ``pytest`` (PR `#79`_).
- Document public API for ``_Result`` objects passed to wrappers
  (PR `#85`_).
- Document and test hook LIFO ordering (PR `#85`_).
- Turn warnings into errors in test suite (PR `#89`_).
- Deprecate ``_Result.result`` (PR `#88`_).
- Convert ``_Multicall`` to a simple function distinguishing it from
  the legacy version (PR `#90`_).
- Resolve E741 errors (PR `#96`_).
- Test and bug fix for unmarked hook collection (PRs `#97`_ and
  `#102`_).
- Drop support for EOL Python 2.6 and 3.3 (PR `#103`_).
- Fix ``inspect`` based arg introspection on py3.6 (PR `#94`_).

.. _#79: https://github.com/pytest-dev/pluggy/pull/79
.. _#85: https://github.com/pytest-dev/pluggy/pull/85
.. _#88: https://github.com/pytest-dev/pluggy/pull/88
.. _#89: https://github.com/pytest-dev/pluggy/pull/89
.. _#90: https://github.com/pytest-dev/pluggy/pull/90
.. _#94: https://github.com/pytest-dev/pluggy/pull/94
.. _#96: https://github.com/pytest-dev/pluggy/pull/96
.. _#97: https://github.com/pytest-dev/pluggy/pull/97
.. _#102: https://github.com/pytest-dev/pluggy/pull/102
.. _#103: https://github.com/pytest-dev/pluggy/pull/103


pluggy 0.5.2 (2017-09-06)
=========================

- fix bug where ``firstresult`` wrappers were being sent an incorrectly configured
  ``_Result`` (a list was set instead of a single value). Add tests to check for
  this as well as ``_Result.force_result()`` behaviour. Thanks to `@tgoodlet`_
  for the PR `#72`_.

- fix incorrect ``getattr``  of ``DeprecationWarning`` from the ``warnings``
  module. Thanks to `@nicoddemus`_ for the PR `#77`_.

- hide ``pytest`` tracebacks in certain core routines. Thanks to
  `@nicoddemus`_ for the PR `#80`_.

.. _#72: https://github.com/pytest-dev/pluggy/pull/72
.. _#77: https://github.com/pytest-dev/pluggy/pull/77
.. _#80: https://github.com/pytest-dev/pluggy/pull/80


pluggy 0.5.1 (2017-08-29)
=========================

- fix a bug and add tests for case where ``firstresult`` hooks return
  ``None`` results. Thanks to `@RonnyPfannschmidt`_ and `@tgoodlet`_
  for the issue (`#68`_) and PR (`#69`_) respectively.

.. _#69: https://github.com/pytest-dev/pluggy/pull/69
.. _#68: https://github.com/pytest-dev/pluggy/issues/68


pluggy 0.5.0 (2017-08-28)
=========================

- fix bug where callbacks for historic hooks would not be called for
  already registered plugins.  Thanks `@vodik`_ for the PR
  and `@hpk42`_ for further fixes.

- fix `#17`_ by considering only actual functions for hooks
  this removes the ability to register arbitrary callable objects
  which at first glance is a reasonable simplification,
  thanks `@RonnyPfannschmidt`_ for report and pr.

- fix `#19`_: allow registering hookspecs from instances.  The PR from
  `@tgoodlet`_ also modernized the varnames implementation.

- resolve `#32`_: split up the test set into multiple modules.
  Thanks to `@RonnyPfannschmidt`_ for the PR and `@tgoodlet`_ for
  the initial request.

- resolve `#14`_: add full sphinx docs. Thanks to `@tgoodlet`_ for
  PR `#39`_.

- add hook call mismatch warnings. Thanks to `@tgoodlet`_ for the
  PR `#42`_.

- resolve `#44`_: move to new-style classes. Thanks to `@MichalTHEDUDE`_
  for PR `#46`_.

- add baseline benchmarking/speed tests using ``pytest-benchmark``
  in PR `#54`_.  Thanks to `@tgoodlet`_.

- update the README to showcase the API. Thanks to `@tgoodlet`_ for the
  issue and PR `#55`_.

- deprecate ``__multicall__`` and add a faster call loop implementation.
  Thanks to `@tgoodlet`_ for PR `#58`_.

- raise a comprehensible error when a ``hookimpl`` is called with positional
  args. Thanks to `@RonnyPfannschmidt`_ for the issue and `@tgoodlet`_ for
  PR `#60`_.

- fix the ``firstresult`` test making it more complete
  and remove a duplicate of that test. Thanks to `@tgoodlet`_
  for PR `#62`_.

.. _#62: https://github.com/pytest-dev/pluggy/pull/62
.. _#60: https://github.com/pytest-dev/pluggy/pull/60
.. _#58: https://github.com/pytest-dev/pluggy/pull/58
.. _#55: https://github.com/pytest-dev/pluggy/pull/55
.. _#54: https://github.com/pytest-dev/pluggy/pull/54
.. _#46: https://github.com/pytest-dev/pluggy/pull/46
.. _#44: https://github.com/pytest-dev/pluggy/issues/44
.. _#42: https://github.com/pytest-dev/pluggy/pull/42
.. _#39: https://github.com/pytest-dev/pluggy/pull/39
.. _#32: https://github.com/pytest-dev/pluggy/pull/32
.. _#19: https://github.com/pytest-dev/pluggy/issues/19
.. _#17: https://github.com/pytest-dev/pluggy/issues/17
.. _#14: https://github.com/pytest-dev/pluggy/issues/14


pluggy 0.4.0 (2016-09-25)
=========================

- add ``has_plugin(name)`` method to pluginmanager.  thanks `@nicoddemus`_.

- fix `#11`_: make plugin parsing more resilient against exceptions
  from ``__getattr__`` functions. Thanks `@nicoddemus`_.

- fix issue `#4`_: specific ``HookCallError`` exception for when a hook call
  provides not enough arguments.

- better error message when loading setuptools entrypoints fails
  due to a ``VersionConflict``.  Thanks `@blueyed`_.

.. _#11: https://github.com/pytest-dev/pluggy/issues/11
.. _#4: https://github.com/pytest-dev/pluggy/issues/4


pluggy 0.3.1 (2015-09-17)
=========================

- avoid using deprecated-in-python3.5 getargspec method. Thanks
  `@mdboom`_.


pluggy 0.3.0 (2015-05-07)
=========================

initial release

.. contributors
.. _@hpk42: https://github.com/hpk42
.. _@tgoodlet: https://github.com/tgoodlet
.. _@MichalTHEDUDE: https://github.com/MichalTHEDUDE
.. _@vodik: https://github.com/vodik
.. _@RonnyPfannschmidt: https://github.com/RonnyPfannschmidt
.. _@blueyed: https://github.com/blueyed
.. _@nicoddemus: https://github.com/nicoddemus
.. _@mdboom: https://github.com/mdboom



  * [ply-3.11](http://www.dabeaz.com/ply/) 
PLY is yet another implementation of lex and yacc for Python. Some notable
features include the fact that its implemented entirely in Python and it
uses LALR(1) parsing which is efficient and well suited for larger grammars.

PLY provides most of the standard lex/yacc features including support for empty 
productions, precedence rules, error recovery, and support for ambiguous grammars. 

PLY is extremely easy to use and provides very extensive error checking. 
It is compatible with both Python 2 and Python 3.

  * porechop 0.2.0
  * [positional-1.2.1]() ==========
positional
==========

A decorator which enforces only some args may be passed positionally. This library is minimally maintained and should only be used in cases of Python 2 to Python 3 conversions. Please write only Python 3 code going forward.

|PyPi|

|Build Status|

|Documentation Status|

The Basics
==========

`positional` provides a decorator which enforces only some args may be passed
positionally. The idea and some of the code was taken from the oauth2 client
of the google-api client.

The decorator makes it easy to support Python 3 style key-word only
parameters. For example, in Python 3 it is possible to write:

.. code:: python

    >>> def fn(pos1, *, kwonly1, kwonly2=None):
    ...     ...

All named parameters after `*` must be a keyword:

.. code:: python

    >>> fn(10, 'kw1', 'kw2')  # Raises exception.
    >>> fn(10, kwonly1='kw1', kwonly2='kw2')  # Ok.

To replicate this behaviour with the positional decorator you simply specify
how many arguments may be passed positionally.

First to import the decorator we typically use:

.. code:: python

    >> from positional import positional

Replicating the Example above:

.. code:: python

    >>> @positional(1)
    ... fn(pos1, kwonly1=None, kwonly2=None):
    ...     ...

If no default value is provided to a keyword argument, it becomes a required
keyword argument:

.. code:: python

    >>> @positional(0)
    ... def fn(required_kw):
    ...     ...

This must be called with the keyword parameter:

.. code:: python

    >>> fn() # Raises exception
    >>> fn(10) # Raises Exception
    >>> fn(required_kw=10) # OK

When defining instance or class methods always remember that in python the
first positional argument passed is the instance; you will need to account for
`self` and `cls`:

.. code:: python

    >>> class MyClass(object):
    ...
    ...     @positional(2)
    ...     def my_method(self, pos1, kwonly1=None):
    ...         ...
    ...
    ...     @classmethod
    ...     @positional(2)
    ...     def my_method(cls, pos1, kwonly1=None):
    ...         ...



If you would prefer not to account for `self` and `cls` you can use the
`method` and `classmethod` helpers which do not consider the initial
positional argument. So the following class is exactly the same as the one
above:

.. code:: python

    >>> class MyClass(object):
    ...
    ...     @positional.method(1)
    ...     def my_method(self, pos1, kwonly1=None):
    ...         ...
    ...
    ...     @positional.classmethod(1)
    ...     def my_method(cls, pos1, kwonly1=None):
    ...         ...


If a value isn't provided to the decorator then it will enforce that
every variable without a default value will be required to be a kwarg:

.. code:: python

    >>> @positional()
    ... def fn(pos1, kwonly1=None):
    ...     ...
    ...
    >>> fn(10)  # Ok.
    >>> fn(10, 20)  # Raises exception.
    >>> fn(10, kwonly1=20)  # Ok.

This behaviour will work with the `positional.method` and
`positional.classmethod` helper functions as well:

.. code:: python

    >>> class MyClass(object):
    ...
    ...    @positional.classmethod()
    ...    def my_method(cls, pos1, kwonly1=None):
    ...        ...
    ...
    >>> MyClass.my_method(10)  # Ok.
    >>> MyClass.my_method(10, 20)  # Raises exception.
    >>> MyClass.my_method(10, kwonly1=20)  # Ok.

For compatibility reasons you may wish to not always raise an exception so
a WARN mode is available. Rather than raise an exception a warning will be
emitted.

.. code:: python

    >>> @positional(1, enforcement=positional.WARN):
    ... def fn(pos1, kwonly=1):
    ...     ...

Available modes are:

- positional.EXCEPT - the default, raise an exception.
- positional.WARN - emit a warning.


.. |Build Status| image:: https://travis-ci.org/morganfainberg/positional.svg?branch=master
   :target: https://travis-ci.org/morganfainberg/positional
.. |Documentation Status| image:: https://readthedocs.org/projects/positional/badge/?version=latest
   :target: http://positional.readthedocs.org/en/latest/?badge=latest
.. |PyPi| image:: https://badge.fury.io/py/positional.png
   :target: http://badge.fury.io/py/positional


  * [presto-0.5.11](http://presto.readthedocs.io) pRESTO - The REpertoire Sequencing TOolkit
================================================================================

pRESTO is a toolkit for processing raw reads from high-throughput sequencing of
B cell and T cell repertoires.

Dramatic improvements in high-throughput sequencing technologies now enable
large-scale characterization of lymphocyte repertoires, defined as the
collection of trans-membrane antigen-receptor proteins located on the surface of
B cells and T cells. The REpertoire Sequencing TOolkit (pRESTO) is composed of a
suite of utilities to handle all stages of sequence processing prior to germline
segment assignment. pRESTO is designed to handle either single reads or
paired-end reads. It includes features for quality control, primer masking,
annotation of reads with sequence embedded barcodes, generation of
unique molecular identifier (UMI) consensus sequences, assembly of paired-end 
reads and identification of duplicate sequences. Numerous options for sequence 
sorting, sampling and conversion operations are also included.
  * [pretend-1.0.9](https://github.com/alex/pretend) pretend
=======

.. image:: https://secure.travis-ci.org/alex/pretend.png
    :target: https://travis-ci.org/alex/pretend

Pretend is a library to make stubbing with Python easier.

What is stubbing?
-----------------

Stubbing is a technique for writing tests. You may hear the term mixed up with
mocks, fakes, or doubles. Basically a stub is an object that returns pre-canned
responses, rather than doing any computation.

Martin Fowler does a good job explaining the terms in his `Mocks Aren't Stubs`_
article.

.. _`Mocks Aren't Stubs`: http://martinfowler.com/articles/mocksArentStubs.html

How do I install ``pretend``?
-----------------------------

It's easy with ``pip``!

.. code:: bash

    $ pip install pretend

How do I use ``pretend``?
-------------------------

It's easy, the ``stub`` function makes it easy to create a stub:

.. code:: pycon

    >>> from pretend import stub
    >>> x = stub(country_code="US")
    >>> some_function(x)

Here ``x`` will be an object with a single attribute ``country_code`` which has
the value ``"US"``. Unlike mocks, ``x`` will not respond to any other attribute
or methods, nor does it have any methods for making assertions about what you
accessed.

If you want to add a method to the stub, simply provide a function to it:

.. code:: pycon

    >>> from pretend import stub
    >>> x = stub(country_code=lambda: "US")
    >>> x.country_code()
    'US'

It's important to note that functions on stubs *do not* take a ``self``
argument, this is because stubs should be returning pre-canned values, not
doing computations.

Exceptions with ``pretend``
---------------------------

Sometimes a method you want to stub doesn't return a value, but instead raises
an exception. To make this easy, ``pretend`` provides a helper function,
``raiser``, it can be used like so:

.. code:: pycon

    >>> from pretend import stub, raiser
    >>> x = stub(func=raiser(ValueError))
    >>> x.func()
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "pretend.py", line 74, in inner
        raise exc
    ValueError

Why is stubbing better?
-----------------------

Ideally stubbing tests how your system responds to a particular input, rather
than which API is used. Stubbing still requires you to write tests that check
the results of a computation, rather than looking for side effects. This
doesn't always work though, so you do sometimes still need mocking (e.g.
sometimes you really want to check for a side effect.)

How do I get my stub into place?
--------------------------------

If you come from other mocking libraries you're probably used to a ``patch``
method to put a mock in place. ``pretend`` doesn't include anything like this,
a) we believe it's better, where possible, to pass stubs as arguments rather
than monkey patch them into place, b) we believe that when you do need to
monkey patch something into place you should use something provided by your
testing tool. ``py.test`` includes `such a tool`_.

.. _`such a tool`: http://pytest.org/latest/monkeypatch.html

What if I really need to record the calls?
------------------------------------------

If you really really need to, ``pretend`` includes a ``call_recorder`` utility:

.. code:: pycon

    >>> from pretend import call_recorder, call
    >>> f = call_recorder(lambda a: a + 2)
    >>> f(3)
    5
    >>> assert f.calls == [call(3)]

Who wrote this?
---------------

``pretend`` is by Alex Gaynor, who was just tired of not having a good stubbing
tool for Python. The name is from Idan Gazit.



  * [process-tests-2.0.2](https://github.com/ionelmc/python-process-tests) ============================
    python-process-tests
============================

.. image:: https://badge.fury.io/py/process-tests.png
    :alt: PYPI Package
    :target: https://pypi.python.org/pypi/process-tests

Tools for testing processes.

Usage
=====

::

    from process_tests import ProcessTestCase
    from process_tests import TestProcess

    class MyTestCase(ProcessTestCase):
        def test_simple(self):
            with TestProcess('mydaemon', 'arg1', 'arg2') as proc:
                with self.dump_on_error(proc.read):
                    self.wait_for_strings(proc.read, 10, # wait 10 seconds for process to output lines with these strings
                        'Started',
                        'Working',
                        'Done',
                    )


Features
========

* TODO

Examples
========

* https://github.com/ionelmc/python-redis-lock/blob/master/tests/test_redis_lock.py
* https://github.com/ionelmc/python-manhole/blob/master/tests/test_manhole.py
* https://github.com/ionelmc/python-stampede/blob/master/tests/test_stampede.py
* https://github.com/ionelmc/python-remote-pdb/blob/master/tests/test_remote_pdb.py

TODO
====

* tests
* docs

Requirements
============

:OS: Any
:Runtime: Python 2.6, 2.7, 3.2, 3.3 or PyPy

Similar projects
================

* TODO



  * [profilehooks-1.11.0](https://mg.pov.lt/profilehooks/) profilehooks
============

.. image:: https://travis-ci.org/mgedmin/profilehooks.svg?branch=master
   :target: https://travis-ci.org/mgedmin/profilehooks

.. image:: https://ci.appveyor.com/api/projects/status/github/mgedmin/profilehooks?branch=master&svg=true
   :target: https://ci.appveyor.com/project/mgedmin/profilehooks

.. image:: https://coveralls.io/repos/mgedmin/profilehooks/badge.svg?branch=master
   :target: https://coveralls.io/r/mgedmin/profilehooks


It's a collection of decorators for profiling functions.  E.g. to profile a
single function::

    from profilehooks import profile

    @profile
    def my_function(args, etc):
        pass

The results will be printed when the program exits (or you can use
``@profile(immediate=True)``).

If you're interested in coarse timings and don't want to pay for the overhead
of profiling, use ::

    from profilehooks import timecall

    @timecall       # or @timecall(immediate=True)
    def my_function(args, etc):
        pass

Finally, you may be interested in seeing line coverage for a single function ::

    from profilehooks import coverage

    @coverage
    def my_function(args, etc):
        pass

Also functions can be available in Python console or module if run it with -m arg ::

     $ python -m profilehooks
     >>> profile
     <function profile at 0x1005c6488>

     $ python -m profilehooks yourmodule

Full documentation is available through ``pydoc profilehooks`` after
installation.

The home page for this module is https://mg.pov.lt/profilehooks.  It has
screensho, uh, that is, more examples.


Changelog
=========

1.11.0 (2019-04-23)
-------------------

- New options: ``@timecall(log_name='logger', log_level=DEBUG)``.
  https://github.com/mgedmin/profilehooks/pull/20.

- Add Python 3.7 support.

- Drop Python 3.3 and 3.4 support.


1.10.0 (2017-12-09)
-------------------

- ``@timecall()`` now defaults to the highest-precision timer
  (``timeit.default_timer()``) instead of ``time.time()``:
  https://github.com/mgedmin/profilehooks/pull/11


1.9.0 (2017-01-02)
------------------

- Drop claim of Python 3.2 compatibility.  Everything still works, except I'm
  no longer running automated tests on 3.2, so things might regress.

- Drop Python 2.6 compatibility.

- Add Python 3.6 compatibility.


1.8.1 (2015-11-21)
------------------

- Include PID in temporary filenames:
  https://github.com/mgedmin/profilehooks/issues/6.

- Claim Python 3.5 compatibility.


1.8.0 (2015-03-25)
------------------

- New option: ``@profile(stdout=False)`` to suppress output to sys.stdout.


1.7.1 (2014-12-02)
------------------

- Make ``@profile(profiler='hotshot')`` work again.  This was probably broken
  in 1.0 or 1.1, but nobody complained.

- Fix missing space in the output of ``@profile(skip=N)``.

- Make ``@coverage_with_hotshot`` output match ``@coverage`` output precisely.

- 100% test coverage.

- Claim Python 3.4 and PyPy compatibility.


1.7 (2013-10-16)
----------------

- Explicitly claim Python 3.3 compatibility.

- Fix Python 3.x bug with @coverage (stop using sys.maxint):
  https://github.com/mgedmin/profilehooks/issues/2.


1.6 (2012-06-05)
----------------

- Added Python 3.2 compatibility, dropped Python 2.3, 2.4 and 2.5 compatibility.

- Migrated the source repository to https://github.com/mgedmin/profilehooks

- Added a changelog.


1.5 (2010-08-13)
----------------

- New argument to @timecall: timer (defaults to time.time).
  Example: @timecall(timer=time.clock)

- Better documentation.


1.4 (2009-03-31)
----------------

- Added support for cProfile, make it the default profiler when available.
  Previously profilehooks supported profile and hotshot only.


1.3 (2008-06-10)
----------------

- Store profile results (when you pass filename to @profile) in pstats format
  instead of pickles.  Contributed by Florian Schulze.


1.2 (2008-03-07)
----------------

- New argument to: @timecall: immediate (defaults to False).

- Added a test suite.


1.1 (2007-11-07)
----------------

- First release to PyPI, with a setup.py and everything.

- New arguments to @profile: dirs, sort, entries.  Contributed by Hanno
  Schlichting.

- Preserve function attributes such as __doc__ and __module__ when decorating
  them.

- Pydoc-friendly docstring wrapping and other docstring improvements.


1.0 (2006-12-06)
----------------

- Changed licence from GPL to MIT.

- New decorator: @timecall

- New arguments to @profile: skip, filename, immediate.

- Added support for profile, after becoming convinced hotshot was unreliable.
  Made it the default profiler.


0.1 (2004-12-30)
----------------

- First public release (it didn't actually have a version number), announced on
  my blog: https://mg.pov.lt/blog/profiling.html

- @profile and @coverage decorators that didn't accept any arguments.

- hotshot was the only profiler supported for @profile, while @coverage used
  trace.py




  * [progressbar33-2.4](http://github.com/germangh/python-progressbar) Text progress bar library for Python.

A text progress bar is typically used to display the progress of a long
running operation, providing a visual cue that processing is underway.

The ProgressBar class manages the current progress, and the format of the line
is given by a number of widgets. A widget is an object that may display
differently depending on the state of the progress bar. There are three types
of widgets:
 - a string, which always shows itself

 - a ProgressBarWidget, which may return a different value every time its
   update method is called

 - a ProgressBarWidgetHFill, which is like ProgressBarWidget, except it
   expands to fill the remaining width of the line.

The progressbar module is very easy to use, yet very powerful. It will also
automatically enable features like auto-resizing when the system supports it.
  * [prometheus_client-0.7.1](https://github.com/prometheus/client_python) See https://github.com/prometheus/client_python/blob/master/README.md for documentation.
  * [prompt_toolkit-2.0.9](https://github.com/jonathanslenders/python-prompt-toolkit) Python Prompt Toolkit
=====================

|Build Status|  |AppVeyor|  |PyPI|  |RTD|  |License|  |Codecov|

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/logo_400px.png

*``prompt_toolkit`` is a library for building powerful interactive command line
applications in Python.*

Read the `documentation on readthedocs
<http://python-prompt-toolkit.readthedocs.io/en/stable/>`_.

NOTICE: prompt_toolkit 2.0
**************************

Please notice that this is prompt_toolkit 2.0. It is incompatible with the 1.0
branch, but much better in many regards. Many applications are still using
prompt_toolkit 1.0, but upgrading is strongly recommended. Feel free to open a
new issue if you don't manage to upgrade to prompt_toolkit 2.0.


Ptpython
********

`ptpython <http://github.com/prompt-toolkit/ptpython/>`_ is an interactive
Python Shell, build on top of prompt_toolkit.

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/ptpython.png


prompt_toolkit features
***********************

``prompt_toolkit`` could be a replacement for `GNU readline
<https://tiswww.case.edu/php/chet/readline/rltop.html>`_, but it can be much
more than that.

Some features:

- Pure Python.
- Syntax highlighting of the input while typing. (For instance, with a Pygments lexer.)
- Multi-line input editing.
- Advanced code completion.
- Both Emacs and Vi key bindings. (Similar to readline.)
- Even some advanced Vi functionality, like named registers and digraphs.
- Reverse and forward incremental search.
- Runs on all Python versions from 2.6 up to 3.7.
- Works well with Unicode double width characters. (Chinese input.)
- Selecting text for copy/paste. (Both Emacs and Vi style.)
- Support for `bracketed paste <https://cirw.in/blog/bracketed-paste>`_.
- Mouse support for cursor positioning and scrolling.
- Auto suggestions. (Like `fish shell <http://fishshell.com/>`_.)
- Multiple input buffers.
- No global state.
- Lightweight, the only dependencies are Pygments, six and wcwidth.
- Runs on Linux, OS X, FreeBSD, OpenBSD and Windows systems.
- And much more...

Feel free to create tickets for bugs and feature requests, and create pull
requests if you have nice patches that you would like to share with others.


About Windows support
*********************

``prompt_toolkit`` is cross platform, and everything that you build on top
should run fine on both Unix and Windows systems. On Windows, it uses a
different event loop (``WaitForMultipleObjects`` instead of ``select``), and
another input and output system. (Win32 APIs instead of pseudo-terminals and
VT100.)

It's worth noting that the implementation is a "best effort of what is
possible". Both Unix and Windows terminals have their limitations. But in
general, the Unix experience will still be a little better.

For Windows, it's recommended to use either `cmder
<http://cmder.net/>`_ or `conemu <https://conemu.github.io/>`_.


Installation
************

::

    pip install prompt_toolkit

For Conda, do:

::

    conda install -c https://conda.anaconda.org/conda-forge prompt_toolkit


Getting started
***************

The most simple example of the library would look like this:

.. code:: python

    from prompt_toolkit import prompt

    if __name__ == '__main__':
        answer = prompt('Give me some input: ')
        print('You said: %s' % answer)

For more complex examples, have a look in the ``examples`` directory. All
examples are chosen to demonstrate only one thing. Also, don't be afraid to
look at the source code. The implementation of the ``prompt`` function could be
a good start.

Note for Python 2: all strings are expected to be unicode strings. So, either
put a small ``u`` in front of every string or put ``from __future__ import
unicode_literals`` at the start of the above example.


Projects using prompt_toolkit
*****************************

Shells:

- `ptpython <http://github.com/prompt-toolkit/ptpython/>`_: Python REPL
- `ptpdb <http://github.com/jonathanslenders/ptpdb/>`_: Python debugger (pdb replacement)
- `pgcli <https://www.pgcli.com/>`_: Postgres client.
- `mycli <https://www.mycli.net/>`_: MySql client.
- `litecli <https://litecli.com/>`_: SQLite client.
- `wharfee <http://wharfee.com/>`_: A Docker command line.
- `xonsh <http://xon.sh/>`_: A Python-ish, BASHwards-compatible shell.
- `saws <https://github.com/donnemartin/saws>`_: A Supercharged AWS Command Line Interface.
- `cycli <https://github.com/nicolewhite/cycli>`_:  A Command Line Interface for Cypher.
- `crash <https://github.com/crate/crash>`_:  Crate command line client.
- `vcli <https://github.com/dbcli/vcli>`_: Vertica client.
- `aws-shell <https://github.com/awslabs/aws-shell>`_: An integrated shell for working with the AWS CLI.
- `softlayer-python <https://github.com/softlayer/softlayer-python>`_: A command-line interface to manage various SoftLayer products and services.
- `ipython <http://github.com/ipython/ipython/>`_: The IPython REPL
- `click-repl <https://github.com/click-contrib/click-repl>`_: Subcommand REPL for click apps.
- `haxor-news <https://github.com/donnemartin/haxor-news>`_: A Hacker News CLI.
- `gitsome <https://github.com/donnemartin/gitsome>`_: A Git/Shell Autocompleter with GitHub Integration.
- `http-prompt <https://github.com/eliangcs/http-prompt>`_: An interactive command-line HTTP client.
- `coconut <http://coconut-lang.org/>`_: Functional programming in Python.
- `Ergonomica <https://github.com/ergonomica/ergonomica>`_: A Bash alternative written in Python.
- `Kube-shell <https://github.com/cloudnativelabs/kube-shell>`_: Kubernetes shell: An integrated shell for working with the Kubernetes CLI
- `mssql-cli <https://github.com/dbcli/mssql-cli>`_: A command-line client for Microsoft SQL Server.
- `robotframework-debuglibrary <https://github.com/xyb/robotframework-debuglibrary>`_: A debug library and REPL for RobotFramework.
- `ptrepl <https://github.com/imomaliev/ptrepl>`_: Run any command as REPL
- `clipwdmgr <https://github.com/samisalkosuo/clipasswordmgr>`_: Command Line Password Manager.
- `slacker <https://github.com/netromdk/slacker>`_: Easy access to the Slack API and admin of workspaces via REPL.
- `EdgeDB <https://edgedb.com/>`_: The next generation object-relational database.
- `pywit <https://github.com/wit-ai/pywit>`_: Python library for Wit.ai.
- `objection <https://github.com/sensepost/objection>`_: Runtime Mobile Exploration.
- `habu <https://github.com/portantier/habu>`_: Python Network Hacking Toolkit.
- `nawano <https://github.com/rbw/nawano>`_: Nano cryptocurrency wallet
- `athenacli <https://github.com/dbcli/athenacli>`_: A CLI for AWS Athena.
- `vulcano <https://github.com/dgarana/vulcano>`_: A framework for creating command-line applications that also runs in REPL mode.
- `kafka-shell <https://github.com/devshawn/kafka-shell>`_: A supercharged shell for Apache Kafka.

Full screen applications:

- `pymux <http://github.com/prompt-toolkit/pymux/>`_: A terminal multiplexer (like tmux) in pure Python.
- `pyvim <http://github.com/prompt-toolkit/pyvim/>`_: A Vim clone in pure Python.
- `freud <http://github.com/stloma/freud/>`_: REST client backed by SQLite for storing servers
- `pypager <https://github.com/prompt-toolkit/pypager>`_: A $PAGER in pure Python (like "less").
- `kubeterminal <https://github.com/samisalkosuo/kubeterminal>`_: Kubectl helper tool.

Libraries:

- `ptterm <https://github.com/prompt-toolkit/ptterm>`_: A terminal emulator
  widget for prompt_toolkit.
- `PyInquirer <https://github.com/CITGuru/PyInquirer/>`_: A Python library that
  wants to make it easy for existing Inquirer.js users to write immersive
  command line applications in Python.

(Want your own project to be listed here? Please create a GitHub issue.)


Philosophy
**********

The source code of ``prompt_toolkit`` should be readable, concise and
efficient. We prefer short functions focusing each on one task and for which
the input and output types are clearly specified. We mostly prefer composition
over inheritance, because inheritance can result in too much functionality in
the same object. We prefer immutable objects where possible (objects don't
change after initialization). Reusability is important. We absolutely refrain
from having a changing global state, it should be possible to have multiple
independent instances of the same code in the same process. The architecture
should be layered: the lower levels operate on primitive operations and data
structures giving -- when correctly combined -- all the possible flexibility;
while at the higher level, there should be a simpler API, ready-to-use and
sufficient for most use cases. Thinking about algorithms and efficiency is
important, but avoid premature optimization.


Special thanks to
*****************

- `Pygments <http://pygments.org/>`_: Syntax highlighter.
- `wcwidth <https://github.com/jquast/wcwidth>`_: Determine columns needed for a wide characters.

.. |Build Status| image:: https://api.travis-ci.org/prompt-toolkit/python-prompt-toolkit.svg?branch=master
    :target: https://travis-ci.org/prompt-toolkit/python-prompt-toolkit#

.. |PyPI| image:: https://img.shields.io/pypi/v/prompt_toolkit.svg
    :target: https://pypi.python.org/pypi/prompt-toolkit/
    :alt: Latest Version

.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/32r7s2skrgm9ubva?svg=true
    :target: https://ci.appveyor.com/project/prompt-toolkit/python-prompt-toolkit/

.. |RTD| image:: https://readthedocs.org/projects/python-prompt-toolkit/badge/
    :target: https://python-prompt-toolkit.readthedocs.io/en/master/

.. |License| image:: https://img.shields.io/github/license/prompt-toolkit/python-prompt-toolkit.svg
    :target: https://github.com/prompt-toolkit/python-prompt-toolkit/blob/master/LICENSE

.. |Codecov| image:: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/branch/master/graphs/badge.svg?style=flat
    :target: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/


Other libraries and implementations in other languages
******************************************************

- `go-prompt <https://github.com/c-bata/go-prompt>`_: building a powerful
  interactive prompt in Go, inspired by python-prompt-toolkit.
- `urwid <http://urwid.org/>`_: Console user interface library for Python.

  * [protobuf-3.9.0](https://developers.google.com/protocol-buffers/) Protocol Buffers are Google's data interchange format



  * [prov-1.5.3](https://github.com/trungdong/prov) ============
Introduction
============


.. image:: https://badge.fury.io/py/prov.svg
  :target: http://badge.fury.io/py/prov
  :alt: Latest Release
.. image:: https://travis-ci.org/trungdong/prov.svg
  :target: https://travis-ci.org/trungdong/prov
  :alt: Build Status
.. image:: https://img.shields.io/coveralls/trungdong/prov.svg
  :target: https://coveralls.io/r/trungdong/prov?branch=master
  :alt: Coverage Status
.. image:: https://landscape.io/github/trungdong/prov/master/landscape.svg?style=flat
  :target: https://landscape.io/github/trungdong/prov/master
  :alt: Code Health
.. image:: https://img.shields.io/pypi/wheel/prov.svg
  :target: https://pypi.python.org/pypi/prov/
  :alt: Wheel Status
.. image:: https://img.shields.io/pypi/pyversions/prov.svg
  :target: https://pypi.python.org/pypi/prov/
  :alt: Supported Python version
.. image:: https://img.shields.io/pypi/l/prov.svg
  :target: https://pypi.python.org/pypi/prov/
  :alt: License


A library for W3C Provenance Data Model supporting PROV-O (RDF), PROV-XML, PROV-JSON import/export

* Free software: MIT license
* Documentation: http://prov.readthedocs.io/.

Features
--------

* An implementation of the `W3C PROV Data Model <http://www.w3.org/TR/prov-dm/>`_ in Python.
* In-memory classes for PROV assertions, which can then be output as `PROV-N <http://www.w3.org/TR/prov-n/>`_
* Serialization and deserialization support: `PROV-O <http://www.w3.org/TR/prov-o/>`_ (RDF), `PROV-XML <http://www.w3.org/TR/prov-xml/>`_ and `PROV-JSON <http://www.w3.org/Submission/prov-json/>`_.
* Exporting PROV documents into various graphical formats (e.g. PDF, PNG, SVG).
* Convert a PROV document to a `Networkx MultiDiGraph <https://networkx.readthedocs.io/en/stable/reference/classes.multigraph.html>`_ and back.


Uses
^^^^

See `a short tutorial  <http://trungdong.github.io/prov-python-short-tutorial.html>`_ for using this package.

This package is used extensively by `ProvStore <https://provenance.ecs.soton.ac.uk/store/>`_,
a free online repository for provenance documents.


  * [psutil-5.6.3](https://github.com/giampaolo/psutil) .. image:: https://pepy.tech/badge/psutil/month
    :target: https://pepy.tech/project/psutil
    :alt: Downloads

.. image:: https://img.shields.io/github/stars/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/stargazers
    :alt: Github stars

.. image:: https://img.shields.io/github/forks/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/network/members
    :alt: Github forks

.. image:: https://img.shields.io/github/contributors/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/graphs/contributors
    :alt: Contributors

.. image:: https://img.shields.io/travis/giampaolo/psutil/master.svg?maxAge=3600&label=Linux%20/%20macOS
    :target: https://travis-ci.org/giampaolo/psutil
    :alt: Linux tests (Travis)

.. image:: https://img.shields.io/appveyor/ci/giampaolo/psutil/master.svg?maxAge=3600&label=Windows
    :target: https://ci.appveyor.com/project/giampaolo/psutil
    :alt: Windows tests (Appveyor)

.. image:: https://coveralls.io/repos/github/giampaolo/psutil/badge.svg?branch=master
    :target: https://coveralls.io/github/giampaolo/psutil?branch=master
    :alt: Test coverage (coverall.io)

.. image:: https://readthedocs.org/projects/psutil/badge/?version=latest
    :target: http://psutil.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/pypi/v/psutil.svg?label=pypi
    :target: https://pypi.org/project/psutil
    :alt: Latest version

.. image:: https://img.shields.io/pypi/pyversions/psutil.svg
    :target: https://pypi.org/project/psutil
    :alt: Supported Python versions

.. image:: https://img.shields.io/pypi/l/psutil.svg
    :target: https://github.com/giampaolo/psutil/blob/master/LICENSE
    :alt: License

-----

Quick links
===========

- `Home page <https://github.com/giampaolo/psutil>`_
- `Install <https://github.com/giampaolo/psutil/blob/master/INSTALL.rst>`_
- `Documentation <http://psutil.readthedocs.io>`_
- `Download <https://pypi.org/project/psutil/#files>`_
- `Forum <http://groups.google.com/group/psutil/topics>`_
- `StackOverflow <https://stackoverflow.com/questions/tagged/psutil>`_
- `Blog <http://grodola.blogspot.com/search/label/psutil>`_
- `Development guide <https://github.com/giampaolo/psutil/blob/master/docs/DEVGUIDE.rst>`_
- `What's new <https://github.com/giampaolo/psutil/blob/master/HISTORY.rst>`_


Summary
=======

psutil (process and system utilities) is a cross-platform library for
retrieving information on **running processes** and **system utilization**
(CPU, memory, disks, network, sensors) in Python.
It is useful mainly for **system monitoring**, **profiling and limiting process
resources** and **management of running processes**.
It implements many functionalities offered by UNIX command line tools such as:
ps, top, lsof, netstat, ifconfig, who, df, kill, free, nice, ionice, iostat,
iotop, uptime, pidof, tty, taskset, pmap.
psutil currently supports the following platforms:

- **Linux**
- **Windows**
- **macOS**
- **FreeBSD, OpenBSD**, **NetBSD**
- **Sun Solaris**
- **AIX**

...both **32-bit** and **64-bit** architectures. Supported Python versions are **2.6**, **2.7** and **3.4+**. `PyPy <http://pypy.org/>`__ is also known to work.


Author
======

psutil was created and is maintained by
`Giampaolo Rodola <http://grodola.blogspot.com/p/about.html>`__ and it
received many useful `contributions <https://github.com/giampaolo/psutil/blob/master/CREDITS>`__
over the years.
A lot of time and effort went into making psutil as it is right now.
If you feel psutil is useful to you or your business and want to support its
future development consider making a small donation:

.. image:: http://www.paypal.com/en_US/i/btn/x-click-but04.gif
    :target: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=A9ZS7PKKRM3S8
    :alt: Donate via PayPal

Don't want to donate money? Then maybe you could `write me a recommendation on Linkedin <https://www.linkedin.com/in/grodola>`_.


Example applications
====================

+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+
| .. image:: https://github.com/giampaolo/psutil/blob/master/docs/_static/procinfo-small.png     | .. image:: https://github.com/giampaolo/psutil/blob/master/docs/_static/top-small.png      |
|    :target: https://github.com/giampaolo/psutil/blob/master/docs/_static/procinfo.png          |     :target: https://github.com/giampaolo/psutil/blob/master/docs/_static/top.png          |
+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+
| .. image:: https://github.com/giampaolo/psutil/blob/master/docs/_static/procsmem-small.png     | .. image:: https://github.com/giampaolo/psutil/blob/master/docs/_static/pmap-small.png     |
|     :target: https://github.com/giampaolo/psutil/blob/master/docs/_static/procsmem.png         |     :target: https://github.com/giampaolo/psutil/blob/master/docs/_static/pmap.png         |
+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+

Also see `scripts directory <https://github.com/giampaolo/psutil/tree/master/scripts>`__
and `doc recipes <http://psutil.readthedocs.io/#recipes/>`__.

Projects using psutil
=====================

psutil has roughly the following monthly downloads:

.. image:: https://pepy.tech/badge/psutil/month
    :target: https://pepy.tech/project/psutil
    :alt: Downloads

There are over
`10.000 open source projects <https://libraries.io/pypi/psutil/dependent_repositories?page=1>`__
on github which depend from psutil.
Here's some I find particularly interesting:

- https://github.com/google/grr
- https://github.com/facebook/osquery/
- https://github.com/nicolargo/glances
- https://github.com/Jahaja/psdash
- https://github.com/ajenti/ajenti
- https://github.com/home-assistant/home-assistant/


Portings
========

- Go: https://github.com/shirou/gopsutil
- C: https://github.com/hamon-in/cpslib
- Node: https://github.com/christkv/node-psutil
- Rust: https://github.com/borntyping/rust-psutil
- Ruby: https://github.com/spacewander/posixpsutil
- Nim: https://github.com/johnscillieri/psutil-nim


Example usages
==============

This represents pretty much the whole psutil API.

CPU
---

.. code-block:: python

    >>> import psutil
    >>>
    >>> psutil.cpu_times()
    scputimes(user=3961.46, nice=169.729, system=2150.659, idle=16900.540, iowait=629.59, irq=0.0, softirq=19.42, steal=0.0, guest=0, nice=0.0)
    >>>
    >>> for x in range(3):
    ...     psutil.cpu_percent(interval=1)
    ...
    4.0
    5.9
    3.8
    >>>
    >>> for x in range(3):
    ...     psutil.cpu_percent(interval=1, percpu=True)
    ...
    [4.0, 6.9, 3.7, 9.2]
    [7.0, 8.5, 2.4, 2.1]
    [1.2, 9.0, 9.9, 7.2]
    >>>
    >>> for x in range(3):
    ...     psutil.cpu_times_percent(interval=1, percpu=False)
    ...
    scputimes(user=1.5, nice=0.0, system=0.5, idle=96.5, iowait=1.5, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
    scputimes(user=1.0, nice=0.0, system=0.0, idle=99.0, iowait=0.0, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
    scputimes(user=2.0, nice=0.0, system=0.0, idle=98.0, iowait=0.0, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
    >>>
    >>> psutil.cpu_count()
    4
    >>> psutil.cpu_count(logical=False)
    2
    >>>
    >>> psutil.cpu_stats()
    scpustats(ctx_switches=20455687, interrupts=6598984, soft_interrupts=2134212, syscalls=0)
    >>>
    >>> psutil.cpu_freq()
    scpufreq(current=931.42925, min=800.0, max=3500.0)
    >>>
    >>> psutil.getloadavg()  # also on Windows (emulated)
    (3.14, 3.89, 4.67)

Memory
------

.. code-block:: python

    >>> psutil.virtual_memory()
    svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712, active=4748992512, inactive=2758115328, buffers=790724608, cached=3500347392, shared=787554304)
    >>> psutil.swap_memory()
    sswap(total=2097147904, used=296128512, free=1801019392, percent=14.1, sin=304193536, sout=677842944)
    >>>

Disks
-----

.. code-block:: python

    >>> psutil.disk_partitions()
    [sdiskpart(device='/dev/sda1', mountpoint='/', fstype='ext4', opts='rw,nosuid'),
     sdiskpart(device='/dev/sda2', mountpoint='/home', fstype='ext, opts='rw')]
    >>>
    >>> psutil.disk_usage('/')
    sdiskusage(total=21378641920, used=4809781248, free=15482871808, percent=22.5)
    >>>
    >>> psutil.disk_io_counters(perdisk=False)
    sdiskio(read_count=719566, write_count=1082197, read_bytes=18626220032, write_bytes=24081764352, read_time=5023392, write_time=63199568, read_merged_count=619166, write_merged_count=812396, busy_time=4523412)
    >>>

Network
-------

.. code-block:: python

    >>> psutil.net_io_counters(pernic=True)
    {'eth0': netio(bytes_sent=485291293, bytes_recv=6004858642, packets_sent=3251564, packets_recv=4787798, errin=0, errout=0, dropin=0, dropout=0),
     'lo': netio(bytes_sent=2838627, bytes_recv=2838627, packets_sent=30567, packets_recv=30567, errin=0, errout=0, dropin=0, dropout=0)}
    >>>
    >>> psutil.net_connections()
    [sconn(fd=115, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=48776), raddr=addr(ip='93.186.135.91', port=80), status='ESTABLISHED', pid=1254),
     sconn(fd=117, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=43761), raddr=addr(ip='72.14.234.100', port=80), status='CLOSING', pid=2987),
     ...]
    >>>
    >>> psutil.net_if_addrs()
    {'lo': [snicaddr(family=<AddressFamily.AF_INET: 2>, address='127.0.0.1', netmask='255.0.0.0', broadcast='127.0.0.1', ptp=None),
            snicaddr(family=<AddressFamily.AF_INET6: 10>, address='::1', netmask='ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', broadcast=None, ptp=None),
            snicaddr(family=<AddressFamily.AF_LINK: 17>, address='00:00:00:00:00:00', netmask=None, broadcast='00:00:00:00:00:00', ptp=None)],
     'wlan0': [snicaddr(family=<AddressFamily.AF_INET: 2>, address='192.168.1.3', netmask='255.255.255.0', broadcast='192.168.1.255', ptp=None),
               snicaddr(family=<AddressFamily.AF_INET6: 10>, address='fe80::c685:8ff:fe45:641%wlan0', netmask='ffff:ffff:ffff:ffff::', broadcast=None, ptp=None),
               snicaddr(family=<AddressFamily.AF_LINK: 17>, address='c4:85:08:45:06:41', netmask=None, broadcast='ff:ff:ff:ff:ff:ff', ptp=None)]}
    >>>
    >>> psutil.net_if_stats()
    {'lo': snicstats(isup=True, duplex=<NicDuplex.NIC_DUPLEX_UNKNOWN: 0>, speed=0, mtu=65536),
     'wlan0': snicstats(isup=True, duplex=<NicDuplex.NIC_DUPLEX_FULL: 2>, speed=100, mtu=1500)}
    >>>

Sensors
-------

.. code-block:: python

    >>> import psutil
    >>> psutil.sensors_temperatures()
    {'acpitz': [shwtemp(label='', current=47.0, high=103.0, critical=103.0)],
     'asus': [shwtemp(label='', current=47.0, high=None, critical=None)],
     'coretemp': [shwtemp(label='Physical id 0', current=52.0, high=100.0, critical=100.0),
                  shwtemp(label='Core 0', current=45.0, high=100.0, critical=100.0)]}
    >>>
    >>> psutil.sensors_fans()
    {'asus': [sfan(label='cpu_fan', current=3200)]}
    >>>
    >>> psutil.sensors_battery()
    sbattery(percent=93, secsleft=16628, power_plugged=False)
    >>>

Other system info
-----------------

.. code-block:: python

    >>> import psutil
    >>> psutil.users()
    [suser(name='giampaolo', terminal='pts/2', host='localhost', started=1340737536.0, pid=1352),
     suser(name='giampaolo', terminal='pts/3', host='localhost', started=1340737792.0, pid=1788)]
    >>>
    >>> psutil.boot_time()
    1365519115.0
    >>>

Process management
------------------

.. code-block:: python

    >>> import psutil
    >>> psutil.pids()
    [1, 2, 3, 4, 5, 6, 7, 46, 48, 50, 51, 178, 182, 222, 223, 224, 268, 1215, 1216, 1220, 1221, 1243, 1244,
     1301, 1601, 2237, 2355, 2637, 2774, 3932, 4176, 4177, 4185, 4187, 4189, 4225, 4243, 4245, 4263, 4282,
     4306, 4311, 4312, 4313, 4314, 4337, 4339, 4357, 4358, 4363, 4383, 4395, 4408, 4433, 4443, 4445, 4446,
     5167, 5234, 5235, 5252, 5318, 5424, 5644, 6987, 7054, 7055, 7071]
    >>>
    >>> p = psutil.Process(7055)
    >>> p
    psutil.Process(pid=7055, name='python', started='09:04:44')
    >>> p.name()
    'python'
    >>> p.exe()
    '/usr/bin/python'
    >>> p.cwd()
    '/home/giampaolo'
    >>> p.cmdline()
    ['/usr/bin/python', 'main.py']
    >>>
    >>> p.pid
    7055
    >>> p.ppid()
    7054
    >>> p.children(recursive=True)
    [psutil.Process(pid=29835, name='python2.7', started='11:45:38'),
     psutil.Process(pid=29836, name='python2.7', started='11:43:39')]
    >>>
    >>> p.parent()
    psutil.Process(pid=4699, name='bash', started='09:06:44')
    >>> p.parents()
    [psutil.Process(pid=4699, name='bash', started='09:06:44'),
     psutil.Process(pid=4689, name='gnome-terminal-server', started='0:06:44'),
     psutil.Process(pid=1, name='systemd', started='05:56:55')]
    >>>
    >>> p.status()
    'running'
    >>> p.username()
    'giampaolo'
    >>> p.create_time()
    1267551141.5019531
    >>> p.terminal()
    '/dev/pts/0'
    >>>
    >>> p.uids()
    puids(real=1000, effective=1000, saved=1000)
    >>> p.gids()
    pgids(real=1000, effective=1000, saved=1000)
    >>>
    >>> p.cpu_times()
    pcputimes(user=1.02, system=0.31, children_user=0.32, children_system=0.1)
    >>> p.cpu_percent(interval=1.0)
    12.1
    >>> p.cpu_affinity()
    [0, 1, 2, 3]
    >>> p.cpu_affinity([0, 1])  # set
    >>> p.cpu_num()
    1
    >>>
    >>> p.memory_info()
    pmem(rss=10915840, vms=67608576, shared=3313664, text=2310144, lib=0, data=7262208, dirty=0)
    >>> p.memory_full_info()  # "real" USS memory usage (Linux, macOS, Win only)
    pfullmem(rss=10199040, vms=52133888, shared=3887104, text=2867200, lib=0, data=5967872, dirty=0, uss=6545408, pss=6872064, swap=0)
    >>> p.memory_percent()
    0.7823
    >>> p.memory_maps()
    [pmmap_grouped(path='/lib/x8664-linux-gnu/libutil-2.15.so', rss=32768, size=2125824, pss=32768, shared_clean=0, shared_dirty=0, private_clean=20480, private_dirty=12288, referenced=32768, anonymous=12288, swap=0),
     pmmap_grouped(path='/lib/x8664-linux-gnu/libc-2.15.so', rss=3821568, size=3842048, pss=3821568, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=3821568, referenced=3575808, anonymous=3821568, swap=0),
     pmmap_grouped(path='[heap]',  rss=32768, size=139264, pss=32768, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=32768, referenced=32768, anonymous=32768, swap=0),
     pmmap_grouped(path='[stack]', rss=2465792, size=2494464, pss=2465792, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=2465792, referenced=2277376, anonymous=2465792, swap=0),
     ...]
    >>>
    >>> p.io_counters()
    pio(read_count=478001, write_count=59371, read_bytes=700416, write_bytes=69632, read_chars=456232, write_chars=517543)
    >>>
    >>> p.open_files()
    [popenfile(path='/home/giampaolo/svn/psutil/setup.py', fd=3, position=0, mode='r', flags=32768),
     popenfile(path='/var/log/monitd', fd=4, position=235542, mode='a', flags=33793)]
    >>>
    >>> p.connections()
    [pconn(fd=115, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=48776), raddr=addr(ip='93.186.135.91', port=80), status='ESTABLISHED'),
     pconn(fd=117, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=43761), raddr=addr(ip='72.14.234.100', port=80), status='CLOSING')]
    >>>
    >>> p.num_threads()
    4
    >>> p.num_fds()
    8
    >>> p.threads()
    [pthread(id=5234, user_time=22.5, system_time=9.2891),
     pthread(id=5237, user_time=0.0707, system_time=1.1)]
    >>>
    >>> p.num_ctx_switches()
    pctxsw(voluntary=78, involuntary=19)
    >>>
    >>> p.nice()
    0
    >>> p.nice(10)  # set
    >>>
    >>> p.ionice(psutil.IOPRIO_CLASS_IDLE)  # IO priority (Win and Linux only)
    >>> p.ionice()
    pionice(ioclass=<IOPriority.IOPRIO_CLASS_IDLE: 3>, value=0)
    >>>
    >>> p.rlimit(psutil.RLIMIT_NOFILE, (5, 5))  # set resource limits (Linux only)
    >>> p.rlimit(psutil.RLIMIT_NOFILE)
    (5, 5)
    >>>
    >>> p.environ()
    {'LC_PAPER': 'it_IT.UTF-8', 'SHELL': '/bin/bash', 'GREP_OPTIONS': '--color=auto',
    'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/usr/share/upstart/xdg:/etc/xdg',
     ...}
    >>>
    >>> p.as_dict()
    {'status': 'running', 'num_ctx_switches': pctxsw(voluntary=63, involuntary=1), 'pid': 5457, ...}
    >>> p.is_running()
    True
    >>> p.suspend()
    >>> p.resume()
    >>>
    >>> p.terminate()
    >>> p.wait(timeout=3)
    0
    >>>
    >>> psutil.test()
    USER         PID %CPU %MEM     VSZ     RSS TTY        START    TIME  COMMAND
    root           1  0.0  0.0   24584    2240            Jun17   00:00  init
    root           2  0.0  0.0       0       0            Jun17   00:00  kthreadd
    ...
    giampaolo  31475  0.0  0.0   20760    3024 /dev/pts/0 Jun19   00:00  python2.4
    giampaolo  31721  0.0  2.2  773060  181896            00:04   10:30  chrome
    root       31763  0.0  0.0       0       0            00:05   00:00  kworker/0:1
    >>>

Further process APIs
--------------------

.. code-block:: python

    >>> import psutil
    >>> for proc in psutil.process_iter(attrs=['pid', 'name']):
    ...     print(proc.info)
    ...
    {'pid': 1, 'name': 'systemd'}
    {'pid': 2, 'name': 'kthreadd'}
    {'pid': 3, 'name': 'ksoftirqd/0'}
    ...
    >>>
    >>> psutil.pid_exists(3)
    True
    >>>
    >>> def on_terminate(proc):
    ...     print("process {} terminated".format(proc))
    ...
    >>> # waits for multiple processes to terminate
    >>> gone, alive = psutil.wait_procs(procs_list, timeout=3, callback=on_terminate)
    >>>

Popen wrapper:

.. code-block:: python

    >>> import psutil
    >>> from subprocess import PIPE
    >>> p = psutil.Popen(["/usr/bin/python", "-c", "print('hello')"], stdout=PIPE)
    >>> p.name()
    'python'
    >>> p.username()
    'giampaolo'
    >>> p.communicate()
    ('hello\n', None)
    >>> p.wait(timeout=2)
    0
    >>>

Windows services
----------------

.. code-block:: python

    >>> list(psutil.win_service_iter())
    [<WindowsService(name='AeLookupSvc', display_name='Application Experience') at 38850096>,
     <WindowsService(name='ALG', display_name='Application Layer Gateway Service') at 38850128>,
     <WindowsService(name='APNMCP', display_name='Ask Update Service') at 38850160>,
     <WindowsService(name='AppIDSvc', display_name='Application Identity') at 38850192>,
     ...]
    >>> s = psutil.win_service_get('alg')
    >>> s.as_dict()
    {'binpath': 'C:\\Windows\\System32\\alg.exe',
     'description': 'Provides support for 3rd party protocol plug-ins for Internet Connection Sharing',
     'display_name': 'Application Layer Gateway Service',
     'name': 'alg',
     'pid': None,
     'start_type': 'manual',
     'status': 'stopped',
     'username': 'NT AUTHORITY\\LocalService'}



  * [psycopg2-2.8.3](http://initd.org/psycopg/) Psycopg is the most popular PostgreSQL database adapter for the Python
programming language.  Its main features are the complete implementation of
the Python DB API 2.0 specification and the thread safety (several threads can
share the same connection).  It was designed for heavily multi-threaded
applications that create and destroy lots of cursors and make a large number
of concurrent "INSERT"s or "UPDATE"s.

Psycopg 2 is mostly implemented in C as a libpq wrapper, resulting in being
both efficient and secure.  It features client-side and server-side cursors,
asynchronous communication and notifications, "COPY TO/COPY FROM" support.
Many Python types are supported out-of-the-box and adapted to matching
PostgreSQL data types; adaptation can be extended and customized thanks to a
flexible objects adaptation system.

Psycopg 2 is both Unicode and Python 3 friendly.


Documentation
-------------

Documentation is included in the ``doc`` directory and is `available online`__.

.. __: http://initd.org/psycopg/docs/

For any other resource (source code repository, bug tracker, mailing list)
please check the `project homepage`__.


Installation
------------

Building Psycopg requires a few prerequisites (a C compiler, some development
packages): please check the install_ and the faq_ documents in the ``doc`` dir
or online for the details.

If prerequisites are met, you can install psycopg like any other Python
package, using ``pip`` to download it from PyPI_::

    $ pip install psycopg2

or using ``setup.py`` if you have downloaded the source package locally::

    $ python setup.py build
    $ sudo python setup.py install

You can also obtain a stand-alone package, not requiring a compiler or
external libraries, by installing the `psycopg2-binary`_ package from PyPI::

    $ pip install psycopg2-binary

The binary package is a practical choice for development and testing but in
production it is advised to use the package built from sources.

.. _PyPI: https://pypi.org/project/psycopg2/
.. _psycopg2-binary: https://pypi.org/project/psycopg2-binary/
.. _install: http://initd.org/psycopg/docs/install.html#install-from-source
.. _faq: http://initd.org/psycopg/docs/faq.html#faq-compile

.. __: http://initd.org/psycopg/


:Linux/OSX: |travis|
:Windows: |appveyor|

.. |travis| image:: https://travis-ci.org/psycopg/psycopg2.svg?branch=master
    :target: https://travis-ci.org/psycopg/psycopg2
    :alt: Linux and OSX build status

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/psycopg/psycopg2?branch=master&svg=true
    :target: https://ci.appveyor.com/project/psycopg/psycopg2/branch/master
    :alt: Windows build status



  * [ptyprocess-0.6.0](https://github.com/pexpect/ptyprocess) Launch a subprocess in a pseudo terminal (pty), and interact with both the
process and its pty.

Sometimes, piping stdin and stdout is not enough. There might be a password
prompt that doesn't read from stdin, output that changes when it's going to a
pipe rather than a terminal, or curses-style interfaces that rely on a terminal.
If you need to automate these things, running the process in a pseudo terminal
(pty) is the answer.

Interface::

    p = PtyProcessUnicode.spawn(['python'])
    p.read(20)
    p.write('6+6\n')
    p.read(20)

  * [pure-sasl-0.6.1](http://github.com/thobbs/pure-sasl) This package provides a reasonably high-level SASL client written
in pure Python.  New mechanisms may be integrated easily, but by default,
support for PLAIN, ANONYMOUS, EXTERNAL, CRAM-MD5, DIGEST-MD5, and GSSAPI are
provided.
  * [purl-1.5](https://github.com/codeinthehole/purl) ================================
purl - A simple Python URL class
================================

A simple, immutable URL class with a clean API for interrogation and
manipulation.  Supports Pythons 2.7, 3.3, 3.4, 3.5, 3.6 and pypy.

Also supports template URLs as per `RFC 6570`_

Contents:

.. contents:: :local:
    :depth: 1

.. image:: https://secure.travis-ci.org/codeinthehole/purl.png
    :target: https://travis-ci.org/codeinthehole/purl

.. image:: https://img.shields.io/pypi/v/purl.svg
    :target: https://crate.io/packages/purl/

.. _`RFC 6570`: http://tools.ietf.org/html/rfc6570

Docs
----

http://purl.readthedocs.org/en/latest/

Install
-------

From PyPI (stable)::

    $ pip install purl

From Github (unstable)::

    $ pip install git+git://github.com/codeinthehole/purl.git#egg=purl

Use
---

Construct:

.. code:: python

    >>> from purl import URL

    # String constructor
    >>> from_str = URL('https://www.google.com/search?q=testing')

    # Keyword constructor
    >>> from_kwargs = URL(scheme='https', host='www.google.com', path='/search', query='q=testing')

    # Combine
    >>> from_combo = URL('https://www.google.com').path('search').query_param('q', 'testing')

URL objects are immutable - all mutator methods return a new instance.

Interrogate:

.. code:: python

    >>> u = URL('https://www.google.com/search?q=testing')
    >>> u.scheme()
    'https'
    >>> u.host()
    'www.google.com'
    >>> u.domain()
    'www.google.com'
    >>> u.username()
    >>> u.password()
    >>> u.netloc()
    'www.google.com'
    >>> u.port()
    >>> u.path()
    '/search'
    >>> u.query()
    'q=testing'
    >>> u.fragment()
    ''
    >>> u.path_segment(0)
    'search'
    >>> u.path_segments()
    ('search',)
    >>> u.query_param('q')
    'testing'
    >>> u.query_param('q', as_list=True)
    ['testing']
    >>> u.query_param('lang', default='GB')
    'GB'
    >>> u.query_params()
    {'q': ['testing']}
    >>> u.has_query_param('q')
    True
    >>> u.has_query_params(('q', 'r'))
    False
    >>> u.subdomains()
    ['www', 'google', 'com']
    >>> u.subdomain(0)
    'www'

Note that each accessor method is overloaded to be a mutator method too, similar
to the jQuery API.  Eg:

.. code:: python

    >>> u = URL.from_string('https://github.com/codeinthehole')

    # Access
    >>> u.path_segment(0)
    'codeinthehole'

    # Mutate (creates a new instance)
    >>> new_url = u.path_segment(0, 'tangentlabs')
    >>> new_url is u
    False
    >>> new_url.path_segment(0)
    'tangentlabs'

Hence, you can build a URL up in steps:

.. code:: python

    >>> u = URL().scheme('http').domain('www.example.com').path('/some/path').query_param('q', 'search term')
    >>> u.as_string()
    'http://www.example.com/some/path?q=search+term'

Along with the above overloaded methods, there is also a ``add_path_segment``
method for adding a segment at the end of the current path:

.. code:: python

    >>> new_url = u.add_path_segment('here')
    >>> new_url.as_string()
    'http://www.example.com/some/path/here?q=search+term'

Couple of other things:

* Since the URL class is immutable it can be used as a key in a dictionary
* It can be pickled and restored
* It supports equality operations
* It supports equality operations

URL templates can be used either via a ``Template`` class:

.. code:: python

    >>> from purl import Template
    >>> tpl = Template("http://example.com{/list*}")
    >>> url = tpl.expand({'list': ['red', 'green', 'blue']})
    >>> url.as_string()
    'http://example.com/red/green/blue'

or the ``expand`` function:

.. code:: python

    >>> from purl import expand
    >>> expand(u"{/list*}", {'list': ['red', 'green', 'blue']})
    '/red/green/blue'

A wide variety of expansions are possible - refer to the RFC_ for more details.

.. _RFC: http://tools.ietf.org/html/rfc6570

Changelog
---------

v1.5 - 2019-03-10
~~~~~~~~~~~~~~~~~

* Allow `@` in passwords.

v1.4 - 2018-03-11
~~~~~~~~~~~~~~~~~

* Allow usernames and passwords to be removed from URLs.

v1.3.1
~~~~~~

* Ensure paths always have a leading slash.

v1.3
~~~~

* Allow absolute URLs to be converted into relative.

v1.2
~~~~

* Support password-less URLs.
* Allow slashes to be passed as path segments.

v1.1
~~~~

* Support setting username and password via mutator methods

v1.0.3
~~~~~~

* Handle some unicode compatibility edge-cases

v1.0.2
~~~~~~

* Fix template expansion bug with no matching variables being passed in. This
  ensures ``purl.Template`` works correctly with the URLs returned from the
  Github API.

v1.0.1
~~~~~~

* Fix bug with special characters in paths not being escaped.

v1.0
~~~~

* Slight tidy up. Document support for PyPy and Python 3.4.

v0.8
~~~~

* Support for RFC 6570 URI templates

v0.7
~~~~

* All internal strings are unicode.
* Support for unicode chars in path, fragment, query, auth added.

v0.6
~~~~

* Added ``append_query_param`` method
* Added ``remove_query_param`` method

v0.5
~~~~

* Added support for Python 3.2/3.3 (thanks @pmcnr and @mitchellrj)

v0.4.1
~~~~~~

* Added API docs
* Added to readthedocs.org

v0.4
~~~~

* Modified constructor to accept full URL string as first arg
* Added ``add_path_segment`` method

v0.3.2
~~~~~~

* Fixed bug port number in string when using from_string constructor

v0.3.1
~~~~~~

* Fixed bug with passing lists to query param setter methods

v0.3
~~~~

* Added support for comparison and equality
* Added support for pickling
* Added ``__slots__`` so instances can be used as keys within dictionaries

Contribute
----------

Clone, create a virtualenv then install purl and the packages required for
testing::

    $ git clone git@github.com:codeinthehole/purl.git
    $ cd purl
    $ mkvirtualenv purl  # requires virtualenvwrapper
    (purl) $ make

Ensure tests pass using::

    (purl) $ ./runtests.sh

or::

    $ tox

  * [py-1.8.0](http://py.readthedocs.io/) .. image:: https://img.shields.io/pypi/v/py.svg
    :target: https://pypi.org/project/py

.. image:: https://img.shields.io/conda/vn/conda-forge/py.svg
    :target: https://anaconda.org/conda-forge/py

.. image:: https://img.shields.io/pypi/pyversions/pytest.svg
  :target: https://pypi.org/project/py

.. image:: https://img.shields.io/travis/pytest-dev/py.svg
   :target: https://travis-ci.org/pytest-dev/py

.. image:: https://ci.appveyor.com/api/projects/status/10keglan6uqwj5al/branch/master?svg=true
   :target: https://ci.appveyor.com/project/pytestbot/py


**NOTE**: this library is in **maintenance mode** and should not be used in new code.

The py lib is a Python development support library featuring
the following tools and modules:

* ``py.path``:  uniform local and svn path objects  -> please use pathlib/pathlib2 instead
* ``py.apipkg``:  explicit API control and lazy-importing -> please use the standalone package instead
* ``py.iniconfig``:  easy parsing of .ini files -> please use the standalone package instead
* ``py.code``: dynamic code generation and introspection (deprecated, moved to ``pytest`` as a implementation detail).

**NOTE**: prior to the 1.4 release this distribution used to
contain py.test which is now its own package, see http://pytest.org

For questions and more information please visit http://py.readthedocs.org

Bugs and issues: https://github.com/pytest-dev/py

Authors: Holger Krekel and others, 2004-2017



  * [py2bit-0.3.0](https://github.com/deeptools/py2bit) 
  * [pyArango-1.3.2](https://github.com/tariqdaouda/pyArango) Python Object Wrapper for ArangoDB_ with built-in validation
===========================================================

pyArango aims to be an easy to use driver for ArangoDB with built in validation. Collections are treated as types that apply to the documents within. You can be 100% permissive or enforce schemas and validate fields on set, on save or on both.

pyArango supports graphs, indexes and probably everything that arangodb_ can do.

pyArango is developed by `Tariq Daouda`_, the full source code is available from github_.

.. _Tariq Daouda: http://bioinfo.iric.ca/~daoudat/
.. _github: https://github.com/tariqdaouda/pyArango
.. _arangodb: http://www.arangodb.com
.. _ArangoDB: http://www.arangodb.com

For the latest news about pyArango, you can follow me on twitter `@tariqdaouda`_.
If you have any issues with it, please file a github issue.

.. _@tariqdaouda: https://www.twitter.com/tariqdaouda
  * [pyBigWig-0.3.17](https://github.com/dpryan79/pyBigWig) 
  * [pyOpenSSL-19.0.0](https://pyopenssl.org/) ========================================================
pyOpenSSL -- A Python wrapper around the OpenSSL library
========================================================

.. image:: https://readthedocs.org/projects/pyopenssl/badge/?version=stable
   :target: https://pyopenssl.org/en/stable/
   :alt: Stable Docs

.. image:: https://travis-ci.org/pyca/pyopenssl.svg?branch=master
   :target: https://travis-ci.org/pyca/pyopenssl
   :alt: Build status

.. image:: https://codecov.io/github/pyca/pyopenssl/branch/master/graph/badge.svg
   :target: https://codecov.io/github/pyca/pyopenssl
   :alt: Test coverage

**Note:** The Python Cryptographic Authority **strongly suggests** the use of `pyca/cryptography`_
where possible. If you are using pyOpenSSL for anything other than making a TLS connection 
**you should move to cryptography and drop your pyOpenSSL dependency**.

High-level wrapper around a subset of the OpenSSL library. Includes

* ``SSL.Connection`` objects, wrapping the methods of Python's portable sockets
* Callbacks written in Python
* Extensive error-handling mechanism, mirroring OpenSSL's error codes

... and much more.

You can find more information in the documentation_.
Development takes place on GitHub_.


Discussion
==========

If you run into bugs, you can file them in our `issue tracker`_.

We maintain a cryptography-dev_ mailing list for both user and development discussions.

You can also join ``#cryptography-dev`` on Freenode to ask questions or get involved.


.. _documentation: https://pyopenssl.org/
.. _`issue tracker`: https://github.com/pyca/pyopenssl/issues
.. _cryptography-dev: https://mail.python.org/mailman/listinfo/cryptography-dev
.. _GitHub: https://github.com/pyca/pyopenssl
.. _`pyca/cryptography`: https://github.com/pyca/cryptography


Release Information
===================

19.0.0 (2019-01-21)
-------------------


Backward-incompatible changes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- ``X509Store.add_cert`` no longer raises an error if you add a duplicate cert.
  `#787 <https://github.com/pyca/pyopenssl/pull/787>`_


Deprecations:
^^^^^^^^^^^^^

*none*


Changes:
^^^^^^^^

- pyOpenSSL now works with OpenSSL 1.1.1.
  `#805 <https://github.com/pyca/pyopenssl/pull/805>`_
- pyOpenSSL now handles NUL bytes in ``X509Name.get_components()``
  `#804 <https://github.com/pyca/pyopenssl/pull/804>`_


`Full changelog <https://pyopenssl.org/en/stable/changelog.html>`_.




  * [py_expression_eval-0.3.4](https://github.com/AxiaCore/py-expression-eval/) # Python Mathematical Expression Evaluator

[![Build Status](https://travis-ci.org/Axiacore/py-expression-eval.svg?branch=master)](https://travis-ci.org/Axiacore/py-expression-eval)

[![PyPi version](https://img.shields.io/pypi/v/py_expression_eval.svg)](https://pypi.python.org/pypi/py_expression_eval/)
[![PyPi downloads](https://img.shields.io/pypi/dm/py_expression_eval.svg)](https://pypi.python.org/pypi/py_expression_eval/)

[![Coverage Status](https://coveralls.io/repos/github/Axiacore/py-expression-eval/badge.svg?branch=master)](https://coveralls.io/github/Axiacore/py-expression-eval?branch=master)

Based on js-expression-eval, by Matthew Crumley (email@matthewcrumley.com, http://silentmatt.com/)
https://github.com/silentmatt/js-expression-eval

Ported to Python and modified by Vera Mazhuga (ctrl-alt-delete@live.com, http://vero4ka.info/)

You are free to use and modify this code in anyway you find useful. Please leave this comment in the code
to acknowledge its original source. If you feel like it, I enjoy hearing about projects that use my code,
but don't feel like you have to let me know or ask permission.

## Installation

    pip install py_expression_eval

## Tests

    python setup.py test

## Documentation

All the classes and methods of ``py-expression-eval`` were written as similar as possible to their analogues from   [js-expression-eval](https://github.com/silentmatt/js-expression-eval) to make it easier to use for validation on back-end side.

### Parser


``Parser`` is the main class of the library that contains the methods to parse, evaluate and simplify mathematical expressions. In order to use the library you need to create an instance of this class:

```python
> from py_expression_eval import Parser
> parser = Parser()
```

Once you instantiated ``Parser`` class, you can create ``Expression`` object using ``parse`` method:

```python
> parser.parse('2 * 3')
Out: <py_expression_eval.Expression instance at 0x7f40cc4e5ef0>
```

### Parser.Expression

``evaluate()`` takes a dictionary with variables as a parameter and returns the value of the expression:

```python
> parser.parse('2 * 3').evaluate({})
Out: 6
> parser.parse('2 * 3.0').evaluate({})
Out: 6.0
> parser.parse('2 * x').evaluate({'x': 7})
Out: 14
> parser.parse('2 * x').evaluate({'x': 7.0})
Out: 14.0
```

``substitute()`` creates a new expression where specified variables are replaces with a new expression. For example, to replace ``x`` with ``3 + x`` in ``2 * x`` expression we use the following code:

```python
> parser.parse('2 * x').substitute('x', '3 + x').toString()
Out: '(2*(3+x))'
```

``variables()`` returns a list of the variables for the expression:

```python
> parser.parse('2 * x + y').variables()
Out: ['x', 'y']
```

``simplify()`` simplifies the expression. For example,

```python
> parser.parse('2 * 3 * x + y').simplify({}).toString()
Out: '((6*x)+y)'
> parser.parse('2 * 3 * x + y').simplify({'x': -1}).toString()
Out: '(-6+y)'
> parser.parse('cos(PI) + x').simplify({}).toString()
Out: '(-1.0+x)'
```

``toString()`` converts the expression to a string.

### Available operators, constants and functions

| Expression | Example | Output
| ---------- | ------- | ------ 
| +          | ``parser.parse('2 + 2').evaluate({})`` | 4
| -          | ``parser.parse('3 - 1').evaluate({})`` | 2
| `*`          | ``parser.parse('2 * 3').evaluate({})`` | 6
| /          | ``parser.parse('5 / 2').evaluate({})`` | 2.5
| %          | ``parser.parse('5 % 2').evaluate({})`` | 1
| ^          | ``parser.parse('5 ^ 2').evaluate({})`` | 25.0
| PI         | ``parser.parse('PI').evaluate({})`` | 3.141592653589793
| E          | ``parser.parse('E').evaluate({})`` | 2.718281828459045
| sin(x)     | ``parser.parse('sin(0)').evaluate({})`` | 0.0
| cos(x)     | ``parser.parse('cos(PI)').evaluate({})`` | - 1.0
| tan(x)     | ``parser.parse('tan(0)').evaluate({})`` | 0.0
| asin(x)     | ``parser.parse('asin(0)').evaluate({})`` | 0.0
| acos(x)     | ``parser.parse('acos(-1)').evaluate({})`` | 3.141592653589793
| atan(x)    | ``parser.parse('atan(PI)').evaluate({})`` | 1.2626272556789118
| log(x)    | ``parser.parse('log(1)').evaluate({})`` | 0.0
| log(x, base) | ``parser.parse('log(16, 2)').evaluate({})`` | 4.0
| abs(x)    | ``parser.parse('abs(-1)').evaluate({})`` | 1
| ceil(x)    | ``parser.parse('ceil(2.7)').evaluate({})`` | 3.0
| floor(x)    | ``parser.parse('floor(2.7)').evaluate({})`` | 2.0
| round(x)    | ``parser.parse('round(2.7)').evaluate({})`` | 3.0
| exp(x)    | ``parser.parse('exp(2)').evaluate({})`` | 7.38905609893065

## Examples

```python
from py_expression_eval import Parser

parser = Parser()
parser.parse('2 * 3').evaluate({})  # 6
parser.parse('2 ^ x').evaluate({'x': 3})  # 8.0
parser.parse('2 * x + 1').evaluate({'x': 3})  # 7
parser.parse('2 + 3 * x').evaluate({'x': 4})  # 14
parser.parse('(2 + 3) * x').evaluate({'x': 4}) # 20
parser.parse('2-3^x').evaluate({'x': 4})  # -79.0
parser.parse('-2-3^x').evaluate({'x': 4})  # -83.0
parser.parse('-3^x').evaluate({'x': 4})  # -81.0
parser.parse('(-3)^x').evaluate({'x': 4})  # 81.0
parser.parse('2*x + y').evaluate({'x': 4, 'y': 1})  # 9
parser.parse('round(log(2.7))').evaluate({}) # 1.0

# substitute
expr = parser.parse('2 * x + 1')
expr2 = expr.substitute('x', '4 * x')  # ((2*(4*x))+1)
expr2.evaluate({'x': 3})  # 25

# simplify
expr = parser.parse('x * (y * atan(1))').simplify({'y': 4})
expr.toString()  # x*3.141592
expr.evaluate({'x': 2})  # 6.283185307179586

# get variables
expr = parser.parse('x * (y * atan(1))')
expr.variables()  # ['x', 'y']
expr.simplify({'y': 4}).variables()  # ['x']
```

Available operations
--------------------

```python
from py_expression_eval import Parser

parser = Parser()
parser.parse('2 + 3').evaluate({})  # 5
parser.parse('2 - 3').evaluate({})  # -1
parser.parse('2 * 3').evaluate({})  # 6
parser.parse('2 / 3').evaluate({})  # 0.6666666666666666
parser.parse('2 % 3').evaluate({})  # 2
parser.parse('-2').evaluate({})  # -2
parser.parse('abs(-2)').evaluate({}) # 2

parser.parse('ceil(1.4)').evaluate({})  # 2.0
parser.parse('floor(1.4)').evaluate({})  # 1.0
parser.parse('round(1.4)').evaluate({})  # 1.0

parser.parse('2^3').evaluate({})  # 8.0
parser.parse('sqrt(16)').evaluate({}) # 4.0

parser.parse('sin(3.14)').evaluate({})  # 0.0015926529164868282
parser.parse('cos(3.14)').evaluate({})  # -0.9999987317275395
parser.parse('tan(3.14)').evaluate({})  # -0.0015926549364072232

parser.parse('asin(1)').evaluate({})  # 1.5707963267948966
parser.parse('acos(1)').evaluate({})  # 0.0
parser.parse('atan(1)').evaluate({})  # 0.7853981633974483

parser.parse('log(2.7)').evaluate({})  # 0.9932517730102834
parser.parse('exp(1)').evaluate({})  # 2.718281828459045

parser.parse('log(E)').evaluate({})  # 1.0
parser.parse('cos(PI)').evaluate({})  # -1.0

parser.parse('x||y').evaluate({'x': 2, 'y': 3})  # '23'
```

## Upload package to PyPi





  * [pyarrow-0.14.1](https://arrow.apache.org/) <!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->

## Python library for Apache Arrow

[![pypi](https://img.shields.io/pypi/v/pyarrow.svg)](https://pypi.org/project/pyarrow/) [![conda-forge](https://img.shields.io/conda/vn/conda-forge/pyarrow.svg)](https://anaconda.org/conda-forge/pyarrow)

This library provides a Python API for functionality provided by the Arrow C++
libraries, along with tools for Arrow integration and interoperability with
pandas, NumPy, and other software in the Python ecosystem.

## Installing

Across platforms, you can install a recent version of pyarrow with the conda
package manager:

```shell
conda install pyarrow -c conda-forge
```

On Linux, macOS, and Windows, you can also install binary wheels from PyPI with
pip:

```shell
pip install pyarrow
```

If you encounter any issues importing the pip wheels on Windows, you may need
to install the [Visual C++ Redistributable for Visual Studio 2015][6].

## Development

See [Python Development][2] in the documentation subproject.

### Building the documentation

See [documentation build instructions][1] in the documentation subproject.

[1]: https://github.com/apache/arrow/blob/master/docs/source/developers/documentation.rst
[2]: https://github.com/apache/arrow/blob/master/docs/source/developers/python.rst
[3]: https://github.com/pandas-dev/pandas
[5]: https://arrow.apache.org/docs/latest/python/benchmarks.html
[6]: https://www.microsoft.com/en-us/download/details.aspx?id=48145


  * [pyasn1-0.4.4](https://github.com/etingof/pyasn1) Pure-Python implementation of ASN.1 types and DER/BER/CER codecs (X.208)



  * [pyblake2-1.1.2](https://github.com/dchest/pyblake2) 
pyblake2 is an extension module for Python implementing BLAKE2 hash function.

BLAKE2 is a cryptographic hash function, which offers highest security while
being as fast as MD5 or SHA-1, and comes in two flavors:

* BLAKE2b, optimized for 64-bit platforms and produces digests of any size
  between 1 and 64 bytes,

* BLAKE2s, optimized for 8- to 32-bit platforms and produces digests of any
  size between 1 and 32 bytes.

BLAKE2 supports keyed mode (a faster and simpler replacement for HMAC),
salted hashing, personalization, and tree hashing.

Hash objects from this module follow the API of standard library's
`hashlib` objects.

  * [pycares-3.0.0](http://github.com/saghul/pycares) ====================================
pycares: Python interface for c-ares
====================================

.. image:: https://badge.fury.io/py/pycares.png
    :target: http://badge.fury.io/py/pycares

.. image:: https://secure.travis-ci.org/saghul/pycares.png?branch=master
    :target: http://travis-ci.org/saghul/pycares

.. image:: https://ci.appveyor.com/api/projects/status/vx1wbkfq3l7nm1m8?svg=true
    :target: https://ci.appveyor.com/project/saghul/pycares

pycares is a Python module which provides an interface to c-ares.
`c-ares <http://c-ares.haxx.se>`_ is a C library that performs
DNS requests and name resolutions asynchronously.


Documentation
=============

http://readthedocs.org/docs/pycares/


Bundled c-ares
==============

pycares currently bundles c-ares and as of pycares 1.0.0 this is a strong requirement. Upstream
c-ares is not willing to apply `a patch adding TTL support <http://c-ares.haxx.se/mail/c-ares-archive-2013-07/0005.shtml>`_.
I did apply the patch to the bundled c-ares, but unfortunately it breaks the ABI, so attempting
to use a system provided c-ares is not possible.


Installation
============

GNU/Linux, macOS, Windows, others:

::

    pip install pycares

FreeBSD:

::

    cd /usr/ports/dns/py-pycares && make install


IDNA 2008 support
^^^^^^^^^^^^^^^^^

If the ``idna`` package is installed, pycares will support IDNA 2008 encodingm otherwise the builtin idna codec will be used,
which provides IDNA 2003 support.

You can force this at installation time as follows:

::

   pip install pycares[idna]


Running the test suite
======================

There are several ways of running the test ruite:

- Run the test with the current Python interpreter:

  From the toplevel directory, run: ``python tests/tests.py``

- Use Tox to run the test suite in several virtualenvs with several interpreters

  From the toplevel directory, run: ``tox -e py35,py36,py37`` this will run the test suite
  on Python 3.5, 3.6 and 3.7 (you'll need to have them installed beforehand)


Using it from the cli, a la dig
===============================

This module can be used directly from the command line in a similar fashion to dig (limited, of course):

::

   $ python -m pycares google.com
   ;; QUESTION SECTION:
   ;google.com			IN	A

   ;; ANSWER SECTION:
   google.com		300	IN	A	172.217.17.142

   $ python -m pycares mx google.com
   ;; QUESTION SECTION:
   ;google.com			IN	MX

   ;; ANSWER SECTION:
   google.com		600	IN	MX	50 alt4.aspmx.l.google.com
   google.com		600	IN	MX	10 aspmx.l.google.com
   google.com		600	IN	MX	40 alt3.aspmx.l.google.com
   google.com		600	IN	MX	20 alt1.aspmx.l.google.com
   google.com		600	IN	MX	30 alt2.aspmx.l.google.com


Author
======

Saúl Ibarra Corretgé <s@saghul.net>


License
=======

Unless stated otherwise on-file pycares uses the MIT license, check LICENSE file.


Supported Python versions
=========================

Python >= 3.5 are supported. Both CPython and PyPy are supported.


Contributing
============

If you'd like to contribute, fork the project, make a patch and send a pull
request. Have a look at the surrounding code and please, make yours look
alike :-)
  * [pycodestyle-2.5.0](https://pycodestyle.readthedocs.io/) pycodestyle (formerly called pep8) - Python style guide checker
===============================================================

.. image:: https://img.shields.io/travis/PyCQA/pycodestyle.svg
   :target: https://travis-ci.org/PyCQA/pycodestyle
   :alt: Build status

.. image:: https://readthedocs.org/projects/pycodestyle/badge/?version=latest
    :target: https://pycodestyle.readthedocs.io
    :alt: Documentation Status

.. image:: https://img.shields.io/pypi/wheel/pycodestyle.svg
   :target: https://pypi.org/project/pycodestyle/
   :alt: Wheel Status

.. image:: https://badges.gitter.im/PyCQA/pycodestyle.svg
   :alt: Join the chat at https://gitter.im/PyCQA/pycodestyle
   :target: https://gitter.im/PyCQA/pycodestyle?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

pycodestyle is a tool to check your Python code against some of the style
conventions in `PEP 8`_.

.. _PEP 8: http://www.python.org/dev/peps/pep-0008/

.. note::

    This package used to be called ``pep8`` but was renamed to ``pycodestyle``
    to reduce confusion. Further discussion can be found `in the issue where
    Guido requested this
    change <https://github.com/PyCQA/pycodestyle/issues/466>`_, or in the
    lightning talk at PyCon 2016 by @IanLee1521:
    `slides <https://speakerdeck.com/ianlee1521/pep8-vs-pep-8>`_
    `video <https://youtu.be/PulzIT8KYLk?t=36m>`_.

Features
--------

* Plugin architecture: Adding new checks is easy.

* Parseable output: Jump to error location in your editor.

* Small: Just one Python file, requires only stdlib. You can use just
  the ``pycodestyle.py`` file for this purpose.

* Comes with a comprehensive test suite.

Installation
------------

You can install, upgrade, and uninstall ``pycodestyle.py`` with these commands::

  $ pip install pycodestyle
  $ pip install --upgrade pycodestyle
  $ pip uninstall pycodestyle

There's also a package for Debian/Ubuntu, but it's not always the
latest version.

Example usage and output
------------------------

::

  $ pycodestyle --first optparse.py
  optparse.py:69:11: E401 multiple imports on one line
  optparse.py:77:1: E302 expected 2 blank lines, found 1
  optparse.py:88:5: E301 expected 1 blank line, found 0
  optparse.py:222:34: W602 deprecated form of raising exception
  optparse.py:347:31: E211 whitespace before '('
  optparse.py:357:17: E201 whitespace after '{'
  optparse.py:472:29: E221 multiple spaces before operator
  optparse.py:544:21: W601 .has_key() is deprecated, use 'in'

You can also make ``pycodestyle.py`` show the source code for each error, and
even the relevant text from PEP 8::

  $ pycodestyle --show-source --show-pep8 testsuite/E40.py
  testsuite/E40.py:2:10: E401 multiple imports on one line
  import os, sys
           ^
      Imports should usually be on separate lines.

      Okay: import os\nimport sys
      E401: import sys, os


Or you can display how often each error was found::

  $ pycodestyle --statistics -qq Python-2.5/Lib
  232     E201 whitespace after '['
  599     E202 whitespace before ')'
  631     E203 whitespace before ','
  842     E211 whitespace before '('
  2531    E221 multiple spaces before operator
  4473    E301 expected 1 blank line, found 0
  4006    E302 expected 2 blank lines, found 1
  165     E303 too many blank lines (4)
  325     E401 multiple imports on one line
  3615    E501 line too long (82 characters)
  612     W601 .has_key() is deprecated, use 'in'
  1188    W602 deprecated form of raising exception

Links
-----

* `Read the documentation <https://pycodestyle.readthedocs.io/>`_

* `Fork me on GitHub <http://github.com/PyCQA/pycodestyle>`_


Changelog
=========

2.5.0 (2019-01-29)
------------------

New checks:

* E117: Over-indented code blocks
* W505: Maximum doc-string length only when configured with --max-doc-length

Changes:

* Remove support for EOL Python 2.6 and 3.3. PR #720.
* Add E117 error for over-indented code blocks.
* Allow W605 to be silenced by `# noqa` and fix the position reported by W605
* Allow users to omit blank lines around one-liner definitions of classes and
  functions
* Include the function return annotation (``->``) as requiring surrounding
  whitespace only on Python 3
* Verify that only names can follow ``await``. Previously we allowed numbers
  and strings.
* Add support for Python 3.7
* Fix detection of annotated argument defaults for E252
* Cprrect the position reported by W504


2.4.0 (2018-04-10)
------------------

New checks:

* Add W504 warning for checking that a break doesn't happen after a binary
  operator. This check is ignored by default. PR #502.
* Add W605 warning for invalid escape sequences in string literals. PR #676.
* Add W606 warning for 'async' and 'await' reserved keywords being introduced
  in Python 3.7. PR #684.
* Add E252 error for missing whitespace around equal sign in type annotated
  function arguments with defaults values. PR #717.

Changes:

* An internal bisect search has replaced a linear search in order to improve
  efficiency. PR #648.
* pycodestyle now uses PyPI trove classifiers in order to document supported
  python versions on PyPI. PR #654.
* 'setup.cfg' '[wheel]' section has been renamed to '[bdist_wheel]', as
  the former is legacy. PR #653.
* pycodestyle now handles very long lines much more efficiently for python
  3.2+. Fixes #643. PR #644.
* You can now write 'pycodestyle.StyleGuide(verbose=True)' instead of
  'pycodestyle.StyleGuide(verbose=True, paths=['-v'])' in order to achieve
  verbosity. PR #663.
* The distribution of pycodestyle now includes the license text in order to
  comply with open source licenses which require this. PR #694.
* 'maximum_line_length' now ignores shebang ('#!') lines. PR #736.
* Add configuration option for the allowed number of blank lines. It is
  implemented as a top level dictionary which can be easily overwritten. Fixes
  #732. PR #733.

Bugs:

* Prevent a 'DeprecationWarning', and a 'SyntaxError' in future python, caused
  by an invalid escape sequence. PR #625.
* Correctly report E501 when the first line of a docstring is too long.
  Resolves #622. PR #630.
* Support variable annotation when variable start by a keyword, such as class
  variable type annotations in python 3.6. PR #640.
* pycodestyle internals have been changed in order to allow 'python3 -m
  cProfile' to report correct metrics. PR #647.
* Fix a spelling mistake in the description of E722. PR #697.
* 'pycodestyle --diff' now does not break if your 'gitconfig' enables
  'mnemonicprefix'. PR #706.

2.3.1 (2017-01-31)
------------------

Bugs:

* Fix regression in detection of E302 and E306; #618, #620

2.3.0 (2017-01-30)
------------------

New Checks:

* Add E722 warning for bare ``except`` clauses
* Report E704 for async function definitions (``async def``)

Bugs:

* Fix another E305 false positive for variables beginning with "class" or
  "def"
* Fix detection of multiple spaces between ``async`` and ``def``
* Fix handling of variable annotations. Stop reporting E701 on Python 3.6 for
  variable annotations.

2.2.0 (2016-11-14)
------------------

Announcements:

* Added Make target to obtain proper tarball file permissions; #599

Bugs:

* Fixed E305 regression caused by #400; #593

2.1.0 (2016-11-04)
------------------

Announcements:

* Change all references to the pep8 project to say pycodestyle; #530

Changes:

* Report E302 for blank lines before an "async def"; #556
* Update our list of tested and supported Python versions which are 2.6, 2.7,
  3.2, 3.3, 3.4 and 3.5 as well as the nightly Python build and PyPy.
* Report E742 and E743 for functions and classes badly named 'l', 'O', or 'I'.
* Report E741 on 'global' and 'nonlocal' statements, as well as prohibited
  single-letter variables.
* Deprecated use of `[pep8]` section name in favor of `[pycodestyle]`; #591
* Report E722 when bare except clause is used; #579

Bugs:

* Fix opt_type AssertionError when using Flake8 2.6.2 and pycodestyle; #561
* Require two blank lines after toplevel def, class; #536
* Remove accidentally quadratic computation based on the number of colons. This
  will make pycodestyle faster in some cases; #314

2.0.0 (2016-05-31)
------------------

Announcements:

* Repository renamed to `pycodestyle`; Issue #466 / #481.
* Added joint Code of Conduct as member of PyCQA; #483

Changes:

* Added tox test support for Python 3.5 and pypy3
* Added check E275 for whitespace on `from ... import ...` lines; #489 / #491
* Added W503 to the list of codes ignored by default ignore list; #498
* Removed use of project level `.pep8` configuration file; #364

Bugs:

* Fixed bug with treating `~` operator as binary; #383 / #384
* Identify binary operators as unary; #484 / #485

1.7.0 (2016-01-12)
------------------

Announcements:

* Repository moved to PyCQA Organization on GitHub:
  https://github.com/pycqa/pep8

Changes:

* Reverted the fix in #368, "options passed on command line are only ones
  accepted" feature. This has many unintended consequences in pep8 and flake8
  and needs to be reworked when I have more time.
* Added support for Python 3.5. (Issue #420 & #459)
* Added support for multi-line config_file option parsing. (Issue #429)
* Improved parameter parsing. (Issues #420 & #456)

Bugs:

* Fixed BytesWarning on Python 3. (Issue #459)

1.6.2 (2015-02-15)
------------------

Changes:

* Added check for breaking around a binary operator. (Issue #197, Pull #305)

Bugs:

* Restored config_file parameter in process_options(). (Issue #380)


1.6.1 (2015-02-08)
------------------

Changes:

* Assign variables before referenced. (Issue #287)

Bugs:

* Exception thrown due to unassigned ``local_dir`` variable. (Issue #377)


1.6.0 (2015-02-06)
------------------

News:

* Ian Lee <ianlee1521@gmail.com> joined the project as a maintainer.

Changes:

* Report E731 for lambda assignment. (Issue #277)

* Report E704 for one-liner def instead of E701.
  Do not report this error in the default configuration. (Issue #277)

* Replace codes E111, E112 and E113 with codes E114, E115 and E116
  for bad indentation of comments. (Issue #274)

* Report E266 instead of E265 when the block comment starts with
  multiple ``#``. (Issue #270)

* Report E402 for import statements not at the top of the file. (Issue #264)

* Do not enforce whitespaces around ``**`` operator. (Issue #292)

* Strip whitespace from around paths during normalization. (Issue #339 / #343)

* Update ``--format`` documentation. (Issue #198 / Pull Request #310)

* Add ``.tox/`` to default excludes. (Issue #335)

* Do not report E121 or E126 in the default configuration. (Issues #256 / #316)

* Allow spaces around the equals sign in an annotated function. (Issue #357)

* Allow trailing backslash if in an inline comment. (Issue #374)

* If ``--config`` is used, only that configuration is processed. Otherwise,
  merge the user and local configurations are merged. (Issue #368 / #369)

Bug fixes:

* Don't crash if Checker.build_tokens_line() returns None. (Issue #306)

* Don't crash if os.path.expanduser() throws an ImportError. (Issue #297)

* Missing space around keyword parameter equal not always reported, E251.
  (Issue #323)

* Fix false positive E711/E712/E713. (Issues #330 and #336)

* Do not skip physical checks if the newline is escaped. (Issue #319)

* Flush sys.stdout to avoid race conditions with printing. See flake8 bug:
  https://gitlab.com/pycqa/flake8/issues/17 for more details. (Issue #363)


1.5.7 (2014-05-29)
------------------

Bug fixes:

* Skip the traceback on "Broken pipe" signal. (Issue #275)

* Do not exit when an option in ``setup.cfg`` or ``tox.ini``
  is not recognized.

* Check the last line even if it does not end with a newline. (Issue #286)

* Always open files in universal newlines mode in Python 2. (Issue #288)


1.5.6 (2014-04-14)
------------------

Bug fixes:

* Check the last line even if it has no end-of-line. (Issue #273)


1.5.5 (2014-04-10)
------------------

Bug fixes:

* Fix regression with E22 checks and inline comments. (Issue #271)


1.5.4 (2014-04-07)
------------------

Bug fixes:

* Fix negative offset with E303 before a multi-line docstring.
  (Issue #269)


1.5.3 (2014-04-04)
------------------

Bug fixes:

* Fix wrong offset computation when error is on the last char
  of a physical line. (Issue #268)


1.5.2 (2014-04-04)
------------------

Changes:

* Distribute a universal wheel file.

Bug fixes:

* Report correct line number for E303 with comments. (Issue #60)

* Do not allow newline after parameter equal. (Issue #252)

* Fix line number reported for multi-line strings. (Issue #220)

* Fix false positive E121/E126 with multi-line strings. (Issue #265)

* Fix E501 not detected in comments with Python 2.5.

* Fix caret position with ``--show-source`` when line contains tabs.


1.5.1 (2014-03-27)
------------------

Bug fixes:

* Fix a crash with E125 on multi-line strings. (Issue #263)


1.5 (2014-03-26)
----------------

Changes:

* Report E129 instead of E125 for visually indented line with same
  indent as next logical line.  (Issue #126)

* Report E265 for space before block comment. (Issue #190)

* Report E713 and E714 when operators ``not in`` and ``is not`` are
  recommended. (Issue #236)

* Allow long lines in multiline strings and comments if they cannot
  be wrapped. (Issue #224).

* Optionally disable physical line checks inside multiline strings,
  using ``# noqa``. (Issue #242)

* Change text for E121 to report "continuation line under-indented
  for hanging indent" instead of indentation not being a
  multiple of 4.

* Report E131 instead of E121 / E126 if the hanging indent is not
  consistent within the same continuation block.  It helps when
  error E121 or E126 is in the ``ignore`` list.

* Report E126 instead of E121 when the continuation line is hanging
  with extra indentation, even if indentation is not a multiple of 4.

Bug fixes:

* Allow the checkers to report errors on empty files. (Issue #240)

* Fix ignoring too many checks when ``--select`` is used with codes
  declared in a flake8 extension. (Issue #216)

* Fix regression with multiple brackets. (Issue #214)

* Fix ``StyleGuide`` to parse the local configuration if the
  keyword argument ``paths`` is specified. (Issue #246)

* Fix a false positive E124 for hanging indent. (Issue #254)

* Fix a false positive E126 with embedded colon. (Issue #144)

* Fix a false positive E126 when indenting with tabs. (Issue #204)

* Fix behaviour when ``exclude`` is in the configuration file and
  the current directory is not the project directory. (Issue #247)

* The logical checks can return ``None`` instead of an empty iterator.
  (Issue #250)

* Do not report multiple E101 if only the first indentation starts
  with a tab. (Issue #237)

* Fix a rare false positive W602. (Issue #34)


1.4.6 (2013-07-02)
------------------

Changes:

* Honor ``# noqa`` for errors E711 and E712. (Issue #180)

* When both a ``tox.ini`` and a ``setup.cfg`` are present in the project
  directory, merge their contents.  The ``tox.ini`` file takes
  precedence (same as before). (Issue #182)

* Give priority to ``--select`` over ``--ignore``. (Issue #188)

* Compare full path when excluding a file. (Issue #186)

* New option ``--hang-closing`` to switch to the alternative style of
  closing bracket indentation for hanging indent.  Add error E133 for
  closing bracket which is missing indentation. (Issue #103)

* Accept both styles of closing bracket indentation for hanging indent.
  Do not report error E123 in the default configuration. (Issue #103)

Bug fixes:

* Do not crash when running AST checks and the document contains null bytes.
  (Issue #184)

* Correctly report other E12 errors when E123 is ignored. (Issue #103)

* Fix false positive E261/E262 when the file contains a BOM. (Issue #193)

* Fix E701, E702 and E703 not detected sometimes. (Issue #196)

* Fix E122 not detected in some cases. (Issue #201 and #208)

* Fix false positive E121 with multiple brackets. (Issue #203)


1.4.5 (2013-03-06)
------------------

* When no path is specified, do not try to read from stdin.  The feature
  was added in 1.4.3, but it is not supported on Windows.  Use ``-``
  filename argument to read from stdin.  This usage is supported
  since 1.3.4. (Issue #170)

* Do not require ``setuptools`` in setup.py.  It works around an issue
  with ``pip`` and Python 3. (Issue #172)

* Add ``__pycache__`` to the ignore list.

* Change misleading message for E251. (Issue #171)

* Do not report false E302 when the source file has a coding cookie or a
  comment on the first line. (Issue #174)

* Reorganize the tests and add tests for the API and for the command line
  usage and options. (Issues #161 and #162)

* Ignore all checks which are not explicitly selected when ``select`` is
  passed to the ``StyleGuide`` constructor.


1.4.4 (2013-02-24)
------------------

* Report E227 or E228 instead of E225 for whitespace around bitwise, shift
  or modulo operators. (Issue #166)

* Change the message for E226 to make clear that it is about arithmetic
  operators.

* Fix a false positive E128 for continuation line indentation with tabs.

* Fix regression with the ``--diff`` option. (Issue #169)

* Fix the ``TestReport`` class to print the unexpected warnings and
  errors.


1.4.3 (2013-02-22)
------------------

* Hide the ``--doctest`` and ``--testsuite`` options when installed.

* Fix crash with AST checkers when the syntax is invalid. (Issue #160)

* Read from standard input if no path is specified.

* Initiate a graceful shutdown on ``Control+C``.

* Allow changing the ``checker_class`` for the ``StyleGuide``.


1.4.2 (2013-02-10)
------------------

* Support AST checkers provided by third-party applications.

* Register new checkers with ``register_check(func_or_cls, codes)``.

* Allow constructing a ``StyleGuide`` with a custom parser.

* Accept visual indentation without parenthesis after the ``if``
  statement. (Issue #151)

* Fix UnboundLocalError when using ``# noqa`` with continued lines.
  (Issue #158)

* Re-order the lines for the ``StandardReport``.

* Expand tabs when checking E12 continuation lines. (Issue #155)

* Refactor the testing class ``TestReport`` and the specific test
  functions into a separate test module.


1.4.1 (2013-01-18)
------------------

* Allow sphinx.ext.autodoc syntax for comments. (Issue #110)

* Report E703 instead of E702 for the trailing semicolon. (Issue #117)

* Honor ``# noqa`` in addition to ``# nopep8``. (Issue #149)

* Expose the ``OptionParser`` factory for better extensibility.


1.4 (2012-12-22)
----------------

* Report E226 instead of E225 for optional whitespace around common
  operators (``*``, ``**``, ``/``, ``+`` and ``-``).  This new error
  code is ignored in the default configuration because PEP 8 recommends
  to "use your own judgement". (Issue #96)

* Lines with a ``# nopep8`` at the end will not issue errors on line
  length E501 or continuation line indentation E12*. (Issue #27)

* Fix AssertionError when the source file contains an invalid line
  ending ``"\r\r\n"``. (Issue #119)

* Read the ``[pep8]`` section of ``tox.ini`` or ``setup.cfg`` if present.
  (Issue #93 and #141)

* Add the Sphinx-based documentation, and publish it
  on https://pycodestyle.readthedocs.io/. (Issue #105)


1.3.4 (2012-12-18)
------------------

* Fix false positive E124 and E128 with comments. (Issue #100)

* Fix error on stdin when running with bpython. (Issue #101)

* Fix false positive E401. (Issue #104)

* Report E231 for nested dictionary in list. (Issue #142)

* Catch E271 at the beginning of the line. (Issue #133)

* Fix false positive E126 for multi-line comments. (Issue #138)

* Fix false positive E221 when operator is preceded by a comma. (Issue #135)

* Fix ``--diff`` failing on one-line hunk. (Issue #137)

* Fix the ``--exclude`` switch for directory paths. (Issue #111)

* Use ``-`` filename to read from standard input. (Issue #128)


1.3.3 (2012-06-27)
------------------

* Fix regression with continuation line checker. (Issue #98)


1.3.2 (2012-06-26)
------------------

* Revert to the previous behaviour for ``--show-pep8``:
  do not imply ``--first``. (Issue #89)

* Add E902 for IO errors. (Issue #87)

* Fix false positive for E121, and missed E124. (Issue #92)

* Set a sensible default path for config file on Windows. (Issue #95)

* Allow ``verbose`` in the configuration file. (Issue #91)

* Show the enforced ``max-line-length`` in the error message. (Issue #86)


1.3.1 (2012-06-18)
------------------

* Explain which configuration options are expected.  Accept and recommend
  the options names with hyphen instead of underscore. (Issue #82)

* Do not read the user configuration when used as a module
  (except if ``config_file=True`` is passed to the ``StyleGuide`` constructor).

* Fix wrong or missing cases for the E12 series.

* Fix cases where E122 was missed. (Issue #81)


1.3 (2012-06-15)
----------------

.. warning::
   The internal API is backwards incompatible.

* Remove global configuration and refactor the library around
  a ``StyleGuide`` class; add the ability to configure various
  reporters. (Issue #35 and #66)

* Read user configuration from ``~/.config/pep8``
  and local configuration from ``./.pep8``. (Issue #22)

* Fix E502 for backslash embedded in multi-line string. (Issue #68)

* Fix E225 for Python 3 iterable unpacking (PEP 3132). (Issue #72)

* Enable the new checkers from the E12 series in the default
  configuration.

* Suggest less error-prone alternatives for E712 errors.

* Rewrite checkers to run faster (E22, E251, E27).

* Fixed a crash when parsed code is invalid (too many
  closing brackets).

* Fix E127 and E128 for continuation line indentation. (Issue #74)

* New option ``--format`` to customize the error format. (Issue #23)

* New option ``--diff`` to check only modified code.  The unified
  diff is read from STDIN.  Example: ``hg diff | pep8 --diff``
  (Issue #39)

* Correctly report the count of failures and set the exit code to 1
  when the ``--doctest`` or the ``--testsuite`` fails.

* Correctly detect the encoding in Python 3. (Issue #69)

* Drop support for Python 2.3, 2.4 and 3.0. (Issue #78)


1.2 (2012-06-01)
----------------

* Add E121 through E128 for continuation line indentation.  These
  checks are disabled by default.  If you want to force all checks,
  use switch ``--select=E,W``.  Patch by Sam Vilain. (Issue #64)

* Add E721 for direct type comparisons. (Issue #47)

* Add E711 and E712 for comparisons to singletons. (Issue #46)

* Fix spurious E225 and E701 for function annotations. (Issue #29)

* Add E502 for explicit line join between brackets.

* Fix E901 when printing source with ``--show-source``.

* Report all errors for each checker, instead of reporting only the
  first occurrence for each line.

* Option ``--show-pep8`` implies ``--first``.


1.1 (2012-05-24)
----------------

* Add E901 for syntax errors. (Issues #63 and #30)

* Add E271, E272, E273 and E274 for extraneous whitespace around
  keywords. (Issue #57)

* Add ``tox.ini`` configuration file for tests. (Issue #61)

* Add ``.travis.yml`` configuration file for continuous integration.
  (Issue #62)


1.0.1 (2012-04-06)
------------------

* Fix inconsistent version numbers.


1.0 (2012-04-04)
----------------

* Fix W602 ``raise`` to handle multi-char names. (Issue #53)


0.7.0 (2012-03-26)
------------------

* Now ``--first`` prints only the first occurrence of each error.
  The ``--repeat`` flag becomes obsolete because it is the default
  behaviour. (Issue #6)

* Allow specifying ``--max-line-length``. (Issue #36)

* Make the shebang more flexible. (Issue #26)

* Add testsuite to the bundle. (Issue #25)

* Fixes for Jython. (Issue #49)

* Add PyPI classifiers. (Issue #43)

* Fix the ``--exclude`` option. (Issue #48)

* Fix W602, accept ``raise`` with 3 arguments. (Issue #34)

* Correctly select all tests if ``DEFAULT_IGNORE == ''``.


0.6.1 (2010-10-03)
------------------

* Fix inconsistent version numbers. (Issue #21)


0.6.0 (2010-09-19)
------------------

* Test suite reorganized and enhanced in order to check more failures
  with fewer test files.  Read the ``run_tests`` docstring for details
  about the syntax.

* Fix E225: accept ``print >>sys.stderr, "..."`` syntax.

* Fix E501 for lines containing multibyte encoded characters. (Issue #7)

* Fix E221, E222, E223, E224 not detected in some cases. (Issue #16)

* Fix E211 to reject ``v = dic['a'] ['b']``. (Issue #17)

* Exit code is always 1 if any error or warning is found. (Issue #10)

* ``--ignore`` checks are now really ignored, especially in
  conjunction with ``--count``. (Issue #8)

* Blank lines with spaces yield W293 instead of W291: some developers
  want to ignore this warning and indent the blank lines to paste their
  code easily in the Python interpreter.

* Fix E301: do not require a blank line before an indented block. (Issue #14)

* Fix E203 to accept NumPy slice notation ``a[0, :]``. (Issue #13)

* Performance improvements.

* Fix decoding and checking non-UTF8 files in Python 3.

* Fix E225: reject ``True+False`` when running on Python 3.

* Fix an exception when the line starts with an operator.

* Allow a new line before closing ``)``, ``}`` or ``]``. (Issue #5)


0.5.0 (2010-02-17)
------------------

* Changed the ``--count`` switch to print to sys.stderr and set
  exit code to 1 if any error or warning is found.

* E241 and E242 are removed from the standard checks. If you want to
  include these checks, use switch ``--select=E,W``. (Issue #4)

* Blank line is not mandatory before the first class method or nested
  function definition, even if there's a docstring. (Issue #1)

* Add the switch ``--version``.

* Fix decoding errors with Python 3. (Issue #13 [1]_)

* Add ``--select`` option which is mirror of ``--ignore``.

* Add checks E261 and E262 for spaces before inline comments.

* New check W604 warns about deprecated usage of backticks.

* New check W603 warns about the deprecated operator ``<>``.

* Performance improvement, due to rewriting of E225.

* E225 now accepts:

  - no whitespace after unary operator or similar. (Issue #9 [1]_)

  - lambda function with argument unpacking or keyword defaults.

* Reserve "2 blank lines" for module-level logical blocks. (E303)

* Allow multi-line comments. (E302, issue #10 [1]_)


0.4.2 (2009-10-22)
------------------

* Decorators on classes and class methods are OK now.


0.4 (2009-10-20)
----------------

* Support for all versions of Python from 2.3 to 3.1.

* New and greatly expanded self tests.

* Added ``--count`` option to print the total number of errors and warnings.

* Further improvements to the handling of comments and blank lines.
  (Issue #1 [1]_ and others changes.)

* Check all py files in directory when passed a directory (Issue
  #2 [1]_). This also prevents an exception when traversing directories
  with non ``*.py`` files.

* E231 should allow commas to be followed by ``)``. (Issue #3 [1]_)

* Spaces are no longer required around the equals sign for keyword
  arguments or default parameter values.


.. [1] These issues refer to the `previous issue tracker`__.
.. __:  http://github.com/cburroughs/pep8.py/issues


0.3.1 (2009-09-14)
------------------

* Fixes for comments: do not count them when checking for blank lines between
  items.

* Added setup.py for pypi upload and easy_installability.


0.2 (2007-10-16)
----------------

* Loads of fixes and improvements.


0.1 (2006-10-01)
----------------

* First release.



  * [pycosat-0.6.3](https://github.com/ContinuumIO/pycosat) ===========================================
pycosat: bindings to picosat (a SAT solver)
===========================================

`PicoSAT <http://fmv.jku.at/picosat/>`_ is a popular
`SAT <http://en.wikipedia.org/wiki/Boolean_satisfiability_problem>`_ solver
written by Armin Biere in pure C.
This package provides efficient Python bindings to picosat on the C level,
i.e. when importing pycosat, the picosat solver becomes part of the
Python process itself.  For ease of deployment, the picosat source (namely
picosat.c and picosat.h) is included in this project.  These files have
been extracted from the picosat source (picosat-965.tar.gz).

Usage
-----

The ``pycosat`` module has two functions ``solve`` and ``itersolve``,
both of which take an iterable of clauses as an argument. Each clause
is itself represented as an iterable of (non-zero) integers.

The function ``solve`` returns one of the following:
  * one solution (a list of integers)
  * the string "UNSAT" (when the clauses are unsatisfiable)
  * the string "UNKNOWN" (when a solution could not be determined within the
    propagation limit)

The function ``itersolve`` returns an iterator over solutions.  When the
propagation limit is specified, exhausting the iterator may not yield all
possible solutions.

Both functions take the following keyword arguments:
  * ``prop_limit``: the propagation limit (integer)
  * ``vars``: number of variables (integer)
  * ``verbose``: the verbosity level (integer)


Example
-------

Let us consider the following clauses, represented using
the DIMACS `cnf <http://en.wikipedia.org/wiki/Conjunctive_normal_form>`_
format::

   p cnf 5 3
   1 -5 4 0
   -1 5 3 4 0
   -3 -4 0

Here, we have 5 variables and 3 clauses, the first clause being
(x\ :sub:`1`  or not x\ :sub:`5` or x\ :sub:`4`).
Note that the variable x\ :sub:`2` is not used in any of the clauses,
which means that for each solution with x\ :sub:`2` = True, we must
also have a solution with x\ :sub:`2` = False.  In Python, each clause is
most conveniently represented as a list of integers.  Naturally, it makes
sense to represent each solution also as a list of integers, where the sign
corresponds to the Boolean value (+ for True and - for False) and the
absolute value corresponds to i\ :sup:`th` variable::

   >>> import pycosat
   >>> cnf = [[1, -5, 4], [-1, 5, 3, 4], [-3, -4]]
   >>> pycosat.solve(cnf)
   [1, -2, -3, -4, 5]

This solution translates to: x\ :sub:`1` = x\ :sub:`5` = True,
x\ :sub:`2` = x\ :sub:`3` = x\ :sub:`4` = False

To find all solutions, use ``itersolve``::

   >>> for sol in pycosat.itersolve(cnf):
   ...     print sol
   ...
   [1, -2, -3, -4, 5]
   [1, -2, -3, 4, -5]
   [1, -2, -3, 4, 5]
   ...
   >>> len(list(pycosat.itersolve(cnf)))
   18

In this example, there are a total of 18 possible solutions, which had to
be an even number because x\ :sub:`2` was left unspecified in the clauses.

The fact that ``itersolve`` returns an iterator, makes it very elegant
and efficient for many types of operations.  For example, using
the ``itertools`` module from the standard library, here is how one
would construct a list of (up to) 3 solutions::

   >>> import itertools
   >>> list(itertools.islice(pycosat.itersolve(cnf), 3))
   [[1, -2, -3, -4, 5], [1, -2, -3, 4, -5], [1, -2, -3, 4, 5]]


Implementation of itersolve
---------------------------

How does one go from having found one solution to another solution?
The answer is surprisingly simple.  One adds the *inverse* of the already
found solution as a new clause.  This new clause ensures that another
solution is searched for, as it *excludes* the already found solution.
Here is basically a pure Python implementation of ``itersolve`` in terms
of ``solve``::

   def py_itersolve(clauses): # don't use this function!
       while True:            # (it is only here to explain things)
           sol = pycosat.solve(clauses)
           if isinstance(sol, list):
               yield sol
               clauses.append([-x for x in sol])
           else: # no more solutions -- stop iteration
               return

This implementation has several problems.  Firstly, it is quite slow as
``pycosat.solve`` has to convert the list of clauses over and over and over
again.  Secondly, after calling ``py_itersolve`` the list of clauses will
be modified.  In pycosat, ``itersolve`` is implemented on the C level,
making use of the picosat C interface (which makes it much, much faster
than the naive Python implementation above).
  * [pycparser-2.18](https://github.com/eliben/pycparser) 
        pycparser is a complete parser of the C language, written in
        pure Python using the PLY parsing library.
        It parses C code into an AST and can serve as a front-end for
        C compilers or analysis tools.
    
  * [pycrypto-2.6.1](http://www.pycrypto.org/) Python Cryptography Toolkit (pycrypto)
======================================

This is a collection of both secure hash functions (such as SHA256 and
RIPEMD160), and various encryption algorithms (AES, DES, RSA, ElGamal,
etc.).  The package is structured to make adding new modules easy.
This section is essentially complete, and the software interface will
almost certainly not change in an incompatible way in the future; all
that remains to be done is to fix any bugs that show up.  If you
encounter a bug, please report it in the Launchpad bug tracker at

       https://launchpad.net/products/pycrypto/+bugs

An example usage of the SHA256 module is:

>>> from Crypto.Hash import SHA256
>>> hash = SHA256.new()
>>> hash.update('message')
>>> hash.digest()
'\xabS\n\x13\xe4Y\x14\x98+y\xf9\xb7\xe3\xfb\xa9\x94\xcf\xd1\xf3\xfb"\xf7\x1c\xea\x1a\xfb\xf0+F\x0cm\x1d'

An example usage of an encryption algorithm (AES, in this case) is:

>>> from Crypto.Cipher import AES
>>> obj = AES.new('This is a key123', AES.MODE_CBC, 'This is an IV456')
>>> message = "The answer is no"
>>> ciphertext = obj.encrypt(message)
>>> ciphertext
'\xd6\x83\x8dd!VT\x92\xaa`A\x05\xe0\x9b\x8b\xf1'
>>> obj2 = AES.new('This is a key123', AES.MODE_CBC, 'This is an IV456')
>>> obj2.decrypt(ciphertext)
'The answer is no'

One possible application of the modules is writing secure
administration tools.  Another application is in writing daemons and
servers.  Clients and servers can encrypt the data being exchanged and
mutually authenticate themselves; daemons can encrypt private data for
added security.  Python also provides a pleasant framework for
prototyping and experimentation with cryptographic algorithms; thanks
to its arbitrary-length integers, public key algorithms are easily
implemented.

As of PyCrypto 2.1.0, PyCrypto provides an easy-to-use random number
generator:

>>> from Crypto import Random
>>> rndfile = Random.new()
>>> rndfile.read(16)
'\xf7.\x838{\x85\xa0\xd3>#}\xc6\xc2jJU'

A stronger version of Python's standard "random" module is also
provided:

>>> from Crypto.Random import random
>>> random.choice(['dogs', 'cats', 'bears'])
'bears'

Caveat: For the random number generator to work correctly, you must
call Random.atfork() in both the parent and child processes after
using os.fork()


Installation
============

PyCrypto is written and tested using Python version 2.1 through 3.3.  Python
1.5.2 is not supported.

The modules are packaged using the Distutils, so you can simply run
"python setup.py build" to build the package, and "python setup.py
install" to install it.

If the setup.py script crashes with a DistutilsPlatformError
complaining that the file /usr/lib/python2.2/config/Makefile doesn't
exist, this means that the files needed for compiling new Python
modules aren't installed on your system.  Red Hat users often run into
this because they don't have the python2-devel RPM installed.  The fix
is to simply install the requisite RPM.  On Debian/Ubuntu, you need the
python-dev package.

To verify that everything is in order, run "python setup.py test".  It
will test all the cryptographic modules, skipping ones that aren't
available.  If the test script reports an error on your machine,
please report the bug using the bug tracker (URL given above).  If
possible, track down the bug and include a patch that fixes it,
provided that you are able to meet the eligibility requirements at
http://www.pycrypto.org/submission-requirements/.

It is possible to test a single sub-package or a single module only, for instance
when you investigate why certain tests fail and don't want to run the whole
suite each time. Use "python setup.py test --module=name", where 'name'
is either a sub-package (Cipher, PublicKey, etc) or a module (Cipher.DES,
PublicKey.RSA, etc).
To further cut test coverage, pass also the option "--skip-slow-tests".

To install the package under the site-packages directory of
your Python installation, run "python setup.py install".

If you have any comments, corrections, or improvements for this
package, please report them to our mailing list, accessible via the
PyCrypto website:

    http://www.pycrypto.org/
    https://www.dlitz.net/software/pycrypto/
  * [pycryptodome-3.8.2](https://www.pycryptodome.org) PyCryptodome
============

PyCryptodome is a self-contained Python package of low-level
cryptographic primitives.

It supports Python 2.6 and 2.7, Python 3.4 and newer, and PyPy.

You can install it with::

    pip install pycryptodome

All modules are installed under the ``Crypto`` package.

Check the pycryptodomex_ project for the equivalent library that
works under the ``Cryptodome`` package.

PyCryptodome is a fork of PyCrypto. It brings several enhancements
with respect to the last official version of PyCrypto (2.6.1),
for instance:

* Authenticated encryption modes (GCM, CCM, EAX, SIV, OCB)
* Accelerated AES on Intel platforms via AES-NI
* First class support for PyPy
* Elliptic curves cryptography (NIST P-256, P-384 and P-521 curves only)
* Better and more compact API (`nonce` and `iv` attributes for ciphers,
  automatic generation of random nonces and IVs, simplified CTR cipher mode,
  and more)
* SHA-3 (including SHAKE XOFs) and BLAKE2 hash algorithms
* Salsa20 and ChaCha20 stream ciphers
* scrypt and HKDF
* Deterministic (EC)DSA
* Password-protected PKCS#8 key containers
* Shamir's Secret Sharing scheme
* Random numbers get sourced directly from the OS (and not from a CSPRNG in userspace)
* Simplified install process, including better support for Windows
* Cleaner RSA and DSA key generation (largely based on FIPS 186-4)
* Major clean ups and simplification of the code base

PyCryptodome is not a wrapper to a separate C library like *OpenSSL*.
To the largest possible extent, algorithms are implemented in pure Python.
Only the pieces that are extremely critical to performance (e.g. block ciphers)
are implemented as C extensions.

For more information, see the `homepage`_.

All the code can be downloaded from `GitHub`_.

.. _pycryptodomex: https://pypi.python.org/pypi/pycryptodomex
.. _`homepage`: http://www.pycryptodome.org
.. _GitHub: https://github.com/Legrandin/pycryptodome



  * [pycurl-7.43.0.3](http://pycurl.io/) PycURL -- A Python Interface To The cURL library
================================================

PycURL is a Python interface to `libcurl`_, the multiprotocol file
transfer library. Similarly to the urllib_ Python module,
PycURL can be used to fetch objects identified by a URL from a Python program.
Beyond simple fetches however PycURL exposes most of the functionality of
libcurl, including:

- Speed - libcurl is very fast and PycURL, being a thin wrapper above
  libcurl, is very fast as well. PycURL `was benchmarked`_ to be several
  times faster than requests_.
- Features including multiple protocol support, SSL, authentication and
  proxy options. PycURL supports most of libcurl's callbacks.
- Multi_ and share_ interfaces.
- Sockets used for network operations, permitting integration of PycURL
  into the application's I/O loop (e.g., using Tornado_).

.. _was benchmarked: http://stackoverflow.com/questions/15461995/python-requests-vs-pycurl-performance
.. _requests: http://python-requests.org/
.. _Multi: https://curl.haxx.se/libcurl/c/libcurl-multi.html
.. _share: https://curl.haxx.se/libcurl/c/libcurl-share.html
.. _Tornado: http://www.tornadoweb.org/


Requirements
------------

- Python 2.7 or 3.4 through 3.6.
- libcurl 7.19.0 or better.


Installation
------------

Download source and binary distributions from `PyPI`_ or `Bintray`_.
Binary wheels are now available for 32 and 64 bit Windows versions.

Please see `the installation documentation`_ for installation instructions.

.. _PyPI: https://pypi.python.org/pypi/pycurl
.. _Bintray: https://dl.bintray.com/pycurl/pycurl/
.. _the installation documentation: http://pycurl.io/docs/latest/install.html


Documentation
-------------

Documentation for the most recent PycURL release is available on
`PycURL website <http://pycurl.io/docs/latest/>`_.


Support
-------

For support questions please use `curl-and-python mailing list`_.
`Mailing list archives`_ are available for your perusal as well.

Although not an official support venue, `Stack Overflow`_ has been
popular with some PycURL users.

Bugs can be reported `via GitHub`_. Please use GitHub only for bug
reports and direct questions to our mailing list instead.

.. _curl-and-python mailing list: http://cool.haxx.se/mailman/listinfo/curl-and-python
.. _Stack Overflow: http://stackoverflow.com/questions/tagged/pycurl
.. _Mailing list archives: https://curl.haxx.se/mail/list.cgi?list=curl-and-python
.. _via GitHub: https://github.com/pycurl/pycurl/issues


License
-------

PycURL is dual licensed under the LGPL and an MIT/X derivative license
based on the libcurl license. The complete text of the licenses is available
in COPYING-LGPL_ and COPYING-MIT_ files in the source distribution.

.. _libcurl: https://curl.haxx.se/libcurl/
.. _urllib: http://docs.python.org/library/urllib.html
.. _COPYING-LGPL: https://raw.githubusercontent.com/pycurl/pycurl/master/COPYING-LGPL
.. _COPYING-MIT: https://raw.githubusercontent.com/pycurl/pycurl/master/COPYING-MIT
  * [pydicom-1.3.0](https://github.com/pydicom/pydicom) pydicom
=======

[![Build Status](https://travis-ci.org/pydicom/pydicom.svg?branch=master)](https://travis-ci.org/pydicom/pydicom)
[![AppVeyor](https://ci.appveyor.com/api/projects/status/1vjtkr82lumnd3i7?svg=true)](https://ci.appveyor.com/project/glemaitre/pydicom)
[![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
[![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
[![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
[![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2541240.svg)](https://doi.org/10.5281/zenodo.2541240)

pydicom is a pure python package for working with [DICOM](http://medical.nema.org/) files.
It was made for inspecting and modifying DICOM data in an easy "pythonic" way.
The modifications can be written again to a new file.

As a pure python package, pydicom can run anywhere python runs without any other requirements,
although [NumPy](http://www.numpy.org) is needed if manipulating pixel data.

pydicom is not a DICOM server, and is not primarily about viewing images.
It is designed to let you
manipulate data elements in DICOM files with python code.

Limitations -- for files with _compressed_ pixel data, pydicom can decompress
it (with additional libraries installed) and allow you to manipulate the data,
but can only store changed pixel data as uncompressed. Files can always be
read and saved (including compressed pixel data that has not been modified),
but once decompressed, modified pixel data cannot be compressed again.

Documentation
-------------

pydicom documentation is available on GitHub Pages both for the [development
 (master) version](https://pydicom.github.io/pydicom/dev) and for the
[released version](https://pydicom.github.io/pydicom/stable). The
documentation for [the previous 0.9.9 version](https://pydicom.github.io/pydicom/0.9/)
is still there for reference.

See [Getting Started](https://pydicom.github.io/pydicom/stable/getting_started.html)
for installation and basic information, and the
[User Guide](https://pydicom.github.io/pydicom/stable/pydicom_user_guide.html)
for an overview of how to use the pydicom library.
To contribute to pydicom, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
To contribute an example or extension of pydicom that does not belong with
the core software, see our contribution repository,
[contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).



  * [pydocstyle-3.0.0](https://github.com/PyCQA/pydocstyle/) pydocstyle - docstring style checker
====================================


.. image:: https://travis-ci.org/PyCQA/pydocstyle.svg?branch=master
    :target: https://travis-ci.org/PyCQA/pydocstyle

.. image:: https://ci.appveyor.com/api/projects/status/40kkc366bmrrttca/branch/master?svg=true
    :target: https://ci.appveyor.com/project/Nurdok/pydocstyle/branch/master

.. image:: https://readthedocs.org/projects/pydocstyle/badge/?version=latest
    :target: https://readthedocs.org/projects/pydocstyle/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/pypi/pyversions/pydocstyle.svg
    :target: https://pypi.org/project/pydocstyle


**pydocstyle** is a static analysis tool for checking compliance with Python
docstring conventions.

**pydocstyle** supports most of
`PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ out of the box, but it
should not be considered a reference implementation.

**pydocstyle** supports Python 3.4, 3.5, 3.6 and 3.7.


Quick Start
-----------

Install
^^^^^^^

.. code::

    pip install pydocstyle


Run
^^^^

.. code::

    $ pydocstyle test.py
    test.py:18 in private nested class `meta`:
            D101: Docstring missing
    test.py:27 in public function `get_user`:
        D300: Use """triple double quotes""" (found '''-quotes)
    test:75 in public function `init_database`:
        D201: No blank lines allowed before function docstring (found 1)
    ...


Links
-----

* `Read the full documentation here <http://pydocstyle.org>`_.

* `Fork pydocstyle on GitHub <http://github.com/PyCQA/pydocstyle>`_.

* `PyPI project page <https://pypi.python.org/pypi/pydocstyle>`_.



  * [pydocumentdb-2.3.3](https://github.com/Azure/azure-documentdb-python) 🚨🚨🚨 The pydocumentdb package for versions 1.x and 2.x of the Azure Cosmos DB Python SDK for SQL API will be retired August 20, 2020.
See the [release and retirement documentation](https://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-sdk-python#release--retirement-dates) for more information.

Please use the latest version of the Python SDK with new package name, [azure-cosmos](https://pypi.org/project/azure-cosmos/).🚨🚨🚨

## Changes in 2.3.5 : ##
- Updated README regarding package deprecation

## Changes in 2.3.3 : ##

- Added support for proxy
- Added support for reading change feed
- Added support for collection quota headers
- Bugfix for large session tokens issue
- Bugfix for ReadMedia API
- Bugfix in partition key range cache

## Changes in 2.3.2 : ##

- Added support for default retries on connection issues.

## Changes in 2.3.1 : ##

- Updated documentation to reference Azure Cosmos DB instead of Azure DocumentDB.

## Changes in 2.3.0 : ##

- This SDK version requires the latest version of Azure Cosmos DB Emulator available for download from https://aka.ms/cosmosdb-emulator.

## Changes in 2.2.1 : ##

- Bugfix for aggregate dict
- Bugfix for trimming slashes in the resource link
- Added tests for unicode encoding

## Changes in 2.2.0 : ##

- Added support for Request Unit per Minute (RU/m) feature.
- Added support for a new consistency level called ConsistentPrefix.

## Changes in 2.1.0 : ##

- Added support for aggregation queries (COUNT, MIN, MAX, SUM, and AVG).
- Added an option for disabling SSL verification when running against DocumentDB Emulator.
- Removed the restriction of dependent requests module to be exactly 2.10.0.
- Lowered minimum throughput on partitioned collections from 10,100 RU/s to 2500 RU/s.
- Added support for enabling script logging during stored procedure execution.
- REST API version bumped to '2017-01-19' with this release.

## Changes in 2.0.1 : ##

- Made editorial changes to documentation comments.

## Changes in 2.0.0 : ##

- Added support for Python 3.5.
- Added support for connection pooling using the requests module.
- Added support for session consistency.
- Added support for TOP/ORDERBY queries for partitioned collections.

## Changes in 1.9.0 : ##

- Added retry policy support for throttled requests. (Throttled requests receive a request rate too large exception, error code 429.) 
  By default, DocumentDB retries nine times for each request when error code 429 is encountered, honoring the retryAfter time in the response header. 
  A fixed retry interval time can now be set as part of the RetryOptions property on the ConnectionPolicy object if you want to ignore the retryAfter time returned by server between the retries. 
  DocumentDB now waits for a maximum of 30 seconds for each request that is being throttled (irrespective of retry count) and returns the response with error code 429.
  This time can also be overriden in the RetryOptions property on ConnectionPolicy object.

- DocumentDB now returns x-ms-throttle-retry-count and x-ms-throttle-retry-wait-time-ms as the response headers in every request to denote the throttle retry count 
  and the cummulative time the request waited between the retries.

- Removed the RetryPolicy class and the corresponding property (retry_policy) exposed on the document_client class and instead introduced a RetryOptions class 
  exposing the RetryOptions property on ConnectionPolicy class that can be used to override some of the default retry options. 

## Changes in 1.8.0 : ##

- Added the support for geo-replicated database accounts.
- Test fixes to move the global host and masterKey into the individual test classes.

## Changes in 1.7.0 : ##

- Added the support for Time To Live(TTL) feature for documents.

## Changes in 1.6.1 : ##

- Bug fixes related to server side partitioning to allow special characters in partitionkey path.

## Changes in 1.6.0 : ##

- Added the support for server side partitioned collections feature.

## Changes in 1.5.0 : ##

- Added Client-side sharding framework to the SDK. Implemented HashPartionResolver and RangePartitionResolver classes.

## Changes in 1.4.2 : ##

- Implement Upsert. New UpsertXXX methods added to support Upsert feature.
- Implement ID Based Routing. No public API changes, all changes internal.

## Changes in 1.3.0 : ##

- Release skipped to bring version number in alignment with other SDKs

## Changes in 1.2.0 : ##

- Supports GeoSpatial index.
- Validates id property for all resources. Ids for resources cannot contain ?, /, #, \\, characters or end with a space.
- Adds new header "index transformation progress" to ResourceResponse.

## Changes in 1.1.0 : ##

- Implements V2 indexing policy

## Changes in 1.0.1 : ##

- Supports proxy connection
  * [pydot-1.4.1](https://github.com/pydot/pydot) [![Build Status](https://www.travis-ci.com/pydot/pydot.svg?branch=master)](https://www.travis-ci.com/pydot/pydot)
[![PyPI](https://img.shields.io/pypi/v/pydot.svg)](https://pypi.org/project/pydot/)


About
=====

`pydot`:

  - is an interface to [Graphviz][1]
  - can parse and dump into the [DOT language][2] used by GraphViz,
  - is written in pure Python,

and [`networkx`][3] can convert its graphs to `pydot`.
Development occurs at [GitHub][11] (under branch `dev`),
where you can report issues and contribute code.


Installation
============

From [PyPI][4] using [`pip`][5]:

`pip install pydot`

From source:

`python setup.py install`


Dependencies
============

- [`pyparsing`][6]: used only for *loading* DOT files,
  installed automatically during `pydot` installation.

- GraphViz: used to render graphs as PDF, PNG, SVG, etc.
  Should be installed separately, using your system's
  [package manager][7], something similar (e.g., [MacPorts][8]),
  or from [its source][9].


License
=======

Distributed under an [MIT license][10].

[1]: https://www.graphviz.org
[2]: https://en.wikipedia.org/wiki/DOT_%28graph_description_language%29
[3]: https://github.com/networkx/networkx
[4]: https://pypi.python.org/pypi
[5]: https://github.com/pypa/pip
[6]: https://github.com/pyparsing/pyparsing
[7]: https://en.wikipedia.org/wiki/Package_manager
[8]: https://www.macports.org
[9]: https://github.com/ellson/graphviz
[10]: https://github.com/pydot/pydot/blob/master/LICENSE
[11]: https://github.com/pydot/pydot



  * [pyemd-0.5.1](http://github.com/wmayner/pyemd) .. image:: https://img.shields.io/travis/wmayner/pyemd/develop.svg?style=flat-square&maxAge=3600
    :target: https://travis-ci.org/wmayner/pyemd
.. image:: https://img.shields.io/pypi/pyversions/pyemd.svg?style=flat-square&maxAge=86400
    :target: https://wiki.python.org/moin/Python2orPython3
    :alt: Python versions badge

PyEMD: Fast EMD for Python
==========================

PyEMD is a Python wrapper for `Ofir Pele and Michael Werman's implementation
<http://ofirpele.droppages.com/>`_ of the `Earth Mover's
Distance <http://en.wikipedia.org/wiki/Earth_mover%27s_distance>`_ that allows
it to be used with NumPy. **If you use this code, please cite the papers listed
at the end of this document.**


Installation
------------

.. code:: bash

    pip install pyemd


Usage
-----

.. code:: python

    >>> from pyemd import emd
    >>> import numpy as np
    >>> first_histogram = np.array([0.0, 1.0])
    >>> second_histogram = np.array([5.0, 3.0])
    >>> distance_matrix = np.array([[0.0, 0.5],
    ...                             [0.5, 0.0]])
    >>> emd(first_histogram, second_histogram, distance_matrix)
    3.5

You can also get the associated minimum-cost flow:

.. code:: python

    >>> from pyemd import emd_with_flow
    >>> emd_with_flow(first_histogram, second_histogram, distance_matrix)
    (3.5, [[0.0, 0.0], [0.0, 1.0]])

You can also calculate the EMD directly from two arrays of observations:

.. code:: python

    >>> from pyemd import emd_samples
    >>> first_array = [1, 2, 3, 4]
    >>> second_array = [2, 3, 4, 5]
    >>> emd_samples(first_array, second_array, bins=2)
    0.5

Documentation
-------------

emd()
~~~~~

.. code:: python

    emd(first_histogram,
        second_histogram,
        distance_matrix,
        extra_mass_penalty=-1.0)

*Arguments:*

- ``first_histogram`` *(np.ndarray)*: A 1D array of type ``np.float64`` of
  length *N*.
- ``second_histogram`` *(np.ndarray)*: A 1D array of ``np.float64`` of length
  *N*.
- ``distance_matrix`` *(np.ndarray)*: A 2D array of ``np.float64,`` of size at
  least *N* × *N*. This defines the underlying metric, or ground distance, by
  giving the pairwise distances between the histogram bins. It must represent a
  metric; there is no warning if it doesn't.

*Keyword Arguments:*

- ``extra_mass_penalty`` *(float)*: The penalty for extra mass. If you want the
  resulting distance to be a metric, it should be at least half the diameter of
  the space (maximum possible distance between any two points). If you want
  partial matching you can set it to zero (but then the resulting distance is
  not guaranteed to be a metric). The default value is ``-1.0``, which means the
  maximum value in the distance matrix is used.

*Returns:* *(float)* The EMD value.

----

emd_with_flow()
~~~~~~~~~~~~~~~

.. code:: python

    emd_with_flow(first_histogram,
                  second_histogram,
                  distance_matrix,
                  extra_mass_penalty=-1.0)

Arguments are the same as for ``emd()``.

*Returns:* *(tuple(float, list(list(float))))* The EMD value and the associated
minimum-cost flow.

----

emd_samples()
~~~~~~~~~~~~~

.. code:: python

    emd_samples(first_array,
                second_array,
                extra_mass_penalty=-1.0,
                distance='euclidean',
                normalized=True,
                bins='auto',
                range=None)

*Arguments:*

- ``first_array`` *(Iterable)*: A 1D array of samples used to generate a
  histogram.
- ``second_array`` *(Iterable)*: A 1D array of samples used to generate a
  histogram.

*Keyword Arguments:*

- ``extra_mass_penalty`` *(float)*: Same as for ``emd()``.
- ``distance`` *(string or function)*: A string or function implementing
  a metric on a 1D ``np.ndarray``. Defaults to the Euclidean distance. Currently
  limited to 'euclidean' or your own function, which must take a 1D array and
  return a square 2D array of pairwise distances.
- ``normalized`` (*boolean*): If true (default), treat histograms as fractions
  of the dataset. If false, treat histograms as counts. In the latter case the
  EMD will vary greatly by array length.
- ``bins`` *(int or string)*: The number of bins to include in the generated
  histogram. If a string, must be one of the bin selection algorithms accepted
  by ``np.histogram()``. Defaults to ``'auto'``, which gives the maximum of the
  'sturges' and 'fd' estimators.
- ``range`` *(tuple(int, int))*: The lower and upper range of the bins, passed
  to ``numpy.histogram()``. Defaults to the range of the union of
  ``first_array`` and ``second_array``. Note: if the given range is not a
  superset of the default range, no warning will be given.

*Returns:* *(float)* The EMD value between the histograms of ``first_array`` and
``second_array``.

----

Limitations and Caveats
-----------------------

- ``emd()`` and ``emd_with_flow()``:

  - The ``distance_matrix`` is assumed to represent a metric; there is no check
    to ensure that this is true. See the documentation in
    ``pyemd/lib/emd_hat.hpp`` for more information.
  - The histograms and distance matrix must be numpy arrays of type
    ``np.float64``. The original C++ template function can accept any numerical
    C++ type, but this wrapper only instantiates the template with ``double``
    (Cython converts ``np.float64`` to ``double``). If there's demand, I can add
    support for other types.

- ``emd_with_flow()``:

  - The flow matrix does not contain the flows to/from the extra mass bin.

- ``emd_samples()``:

  - Using the default ``bins='auto'`` results in an extra call to
    ``np.histogram()`` to determine the bin lengths, since `the NumPy
    bin-selectors are not exposed in the public API
    <https://github.com/numpy/numpy/issues/10183>`_. For performance, you may
    want to set the bins yourself.


Contributing
------------

To help develop PyEMD, fork the project on GitHub and install the requirements
with ``pip install -r requirements.txt``.

The ``Makefile`` defines some tasks to help with development:

- ``test``: Run the test suite
- ``build`` Generate and compile the Cython extension
- ``clean``: Remove the compiled Cython extension
- ``default``: Run ``build``

Tests for different Python environments can be run with ``tox``.


Credit
------

- All credit for the actual algorithm and implementation goes to `Ofir Pele
  <http://www.ariel.ac.il/sites/ofirpele/>`_ and `Michael Werman
  <http://www.cs.huji.ac.il/~werman/>`_. See the `relevant paper
  <http://www.seas.upenn.edu/~ofirpele/publications/ICCV2009.pdf>`_.
- Thanks to the Cython developers for making this kind of wrapper relatively
  easy to write.

Please cite these papers if you use this code:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ofir Pele and Michael Werman. A linear time histogram metric for improved SIFT
matching. *Computer Vision - ECCV 2008*, Marseille, France, 2008, pp. 495-508.

.. code-block:: latex

    @INPROCEEDINGS{pele2008,
      title={A linear time histogram metric for improved sift matching},
      author={Pele, Ofir and Werman, Michael},
      booktitle={Computer Vision--ECCV 2008},
      pages={495--508},
      year={2008},
      month={October},
      publisher={Springer}
    }

Ofir Pele and Michael Werman. Fast and robust earth mover's distances. *Proc.
2009 IEEE 12th Int. Conf. on Computer Vision*, Kyoto, Japan, 2009, pp. 460-467.

.. code-block:: latex

    @INPROCEEDINGS{pele2009,
      title={Fast and robust earth mover's distances},
      author={Pele, Ofir and Werman, Michael},
      booktitle={2009 IEEE 12th International Conference on Computer Vision},
      pages={460--467},
      year={2009},
      month={September},
      organization={IEEE}
    }



  * [pyensembl-1.7.5](https://github.com/openvax/pyensembl) <a href="https://travis-ci.org/openvax/pyensembl">
    <img src="https://travis-ci.org/openvax/pyensembl.svg?branch=master" alt="Build Status" />
</a>
<a href="https://coveralls.io/github/openvax/pyensembl?branch=master">
    <img src="https://coveralls.io/repos/openvax/pyensembl/badge.svg?branch=master&service=github" alt="Coverage Status" />
</a>
<a href="https://pypi.python.org/pypi/pyensembl/">
    <img src="https://img.shields.io/pypi/v/pyensembl.svg?maxAge=1000" alt="PyPI" />
</a>


PyEnsembl
=======
PyEnsembl is a Python interface to [Ensembl](http://www.ensembl.org) reference genome metadata such as exons and transcripts. PyEnsembl downloads [GTF](https://en.wikipedia.org/wiki/Gene_transfer_format) and [FASTA](https://en.wikipedia.org/wiki/FASTA_format) files from the [Ensembl FTP server](ftp://ftp.ensembl.org) and loads them into a local database. PyEnsembl can also work with custom reference data specified using user-supplied GTF and FASTA files. 

# Example Usage

```python
from pyensembl import EnsemblRelease

# release 77 uses human reference genome GRCh38
data = EnsemblRelease(77)

# will return ['HLA-A']
gene_names = data.gene_names_at_locus(contig=6, position=29945884)

# get all exons associated with HLA-A
exon_ids  = data.exon_ids_of_gene_name('HLA-A')
```

# Installation

You can install PyEnsembl using [pip](https://pip.pypa.io/en/latest/quickstart.html):

```sh
pip install pyensembl
```

This should also install any required packages, such as [datacache](https://github.com/openvax/datacache) and
[BioPython](http://biopython.org/).

Before using PyEnsembl, run the following command to download and install
Ensembl data:

```
pyensembl install --release <list of Ensembl release numbers> --species <species-name>
```

For example, `pyensembl install --release 75 76 --species human` will download and install all
human reference data from Ensembl releases 75 and 76.

Alternatively, you can create the `EnsemblRelease` object from inside a Python
process and call `ensembl_object.download()` followed by `ensembl_object.index()`.

## Cache Location
By default, PyEnsembl uses the platform-specific `Cache` folder
and caches the files into the `pyensembl` sub-directory.
You can override this default by setting the environment key `PYENSEMBL_CACHE_DIR`
as your preferred location for caching:

```sh
export PYENSEMBL_CACHE_DIR=/custom/cache/dir
```

or

```python
import os

os.environ['PYENSEMBL_CACHE_DIR'] = '/custom/cache/dir'
# ... PyEnsembl API usage
```

# Non-Ensembl Data

PyEnsembl also allows arbitrary genomes via the specification
of local file paths or remote URLs to both Ensembl and non-Ensembl GTF
and FASTA files. (Warning: GTF formats can vary, and handling of
non-Ensembl data is still very much in development.)

For example:

```python
data = Genome(
    reference_name='GRCh38',
    annotation_name='my_genome_features',
    gtf_path_or_url='/My/local/gtf/path_to_my_genome_features.gtf')
# parse GTF and construct database of genomic features
data.index()
gene_names = data.gene_names_at_locus(contig=6, position=29945884)
```

# API

The `EnsemblRelease` object has methods to let you access all possible
combinations of the annotation features *gene\_name*, *gene\_id*,
*transcript\_name*, *transcript\_id*, *exon\_id* as well as the location of
these genomic elements (contig, start position, end position, strand).

## Genes

<dl>
<dt>genes(contig=None, strand=None)</dt>
<dd>Returns a list of Gene objects, optionally restricted to a particular contig
or strand.</dd>

<dt>genes_at_locus(contig, position, end=None, strand=None)</dt>
<dd>Returns a list of Gene objects overlapping a particular position on a contig,
optionally extend into a range with the end parameter and restrict to
forward or backward strand by passing strand='+' or strand='-'.</dd>

<dt>gene_by_id(gene_id)</dt>
<dd>Return a Gene object for given Ensembl gene ID (e.g. "ENSG00000068793").</dd>

<dt>gene_names(contig=None, strand=None)</dt>
<dd>Returns all gene names in the annotation database, optionally restricted
to a particular contig or strand.</dd>

<dt>genes_by_name(gene_name)</dt>
<dd>Get all the unqiue genes with the given name (there might be multiple
due to copies in the genome), return a list containing a Gene object for each
distinct ID.</dd>

<dt>gene_by_protein_id(protein_id)</dt>
<dd>Find Gene associated with the given Ensembl protein ID (e.g. "ENSP00000350283")</dd>

<dt>gene_names_at_locus(contig, position, end=None, strand=None)
</dt>
<dd>Names of genes overlapping with the given locus, optionally restricted by strand.
(returns a list to account for overlapping genes)</dd>

<dt>gene_name_of_gene_id(gene_id)
</dt>
<dd>Returns name of gene with given genen ID.</dd>

<dt>gene_name_of_transcript_id(transcript_id)
</dt><dd>Returns name of gene associated with given transcript ID.</dd>

<dt>gene_name_of_transcript_name(transcript_name)
</dt>
<dd>Returns name of gene associated with given transcript name.</dd>

<dt>gene_name_of_exon_id(exon_id)
</dt><dd>Returns name of gene associated with given exon ID.</dd>

<dt>gene_ids(contig=None, strand=None)
</dt>
<dd>Return all gene IDs in the annotation database, optionally restricted by
chromosome name or strand.</dd>

<dt>gene_ids_of_gene_name(gene_name)
</dt>
<dd>Returns all Ensembl gene IDs with the given name.</dd>

</dl>

## Transcripts

<dl>
<dt>transcripts(contig=None, strand=None)</dt>
<dd>Returns a list of Transcript objects for all transcript entries in the
Ensembl database, optionally restricted to a particular contig or strand.</dd>

<dt>transcript_by_id(transcript_id)</dt>
<dd>Construct a Transcript object for given Ensembl transcript ID (e.g. "ENST00000369985")</dd>

<dt>transcripts_by_name(transcript_name)</dt>
<dd>Returns a list of Transcript objects for every transcript matching the given name.</dd>

<dt>transcript_names(contig=None, strand=None)</dt>
<dd>Returns all transcript names in the annotation database.</dd>

<dt>transcript_ids(contig=None, strand=None)</dt>
<dd>Returns all transcript IDs in the annotation database.</dd>

<dt>transcript_ids_of_gene_id(gene_id)</dt>
<dd>Return IDs of all transcripts associated with given gene ID.</dd>

<dt>transcript_ids_of_gene_name(gene_name)</dt>
<dd>Return IDs of all transcripts associated with given gene name.</dd>

<dt>transcript_ids_of_transcript_name(transcript_name)</dt>
<dd>Find all Ensembl transcript IDs with the given name.</dd>

<dt>transcript_ids_of_exon_id(exon_id)</dt>
<dd>Return IDs of all transcripts associatd with given exon ID.</dd>
</dl>

## Exons

<dl>
<dt>exon_ids(contig=None, strand=None)</dt>
<dd>Returns a list of exons IDs in the annotation database, optionally restricted
by the given chromosome and strand.</dd>

<dt>exon_ids_of_gene_id(gene_id)</dt>
<dd>Returns a list of exon IDs associated with a given gene ID.</dd>

<dt>exon_ids_of_gene_name(gene_name)</dt>
<dd>Returns a list of exon IDs associated with a given gene name.</dd>

<dt>exon_ids_of_transcript_id(transcript_id)</dt>
<dd>Returns a list of exon IDs associated with a given transcript ID.</dd>

<dt>exon_ids_of_transcript_name(transcript_name)</dt>
<dd>Returns a list of exon IDs associated with a given transcript name.</dd>
</dl>
  * [pyfaidx-0.5.5.2](http://mattshirley.com) |Travis| |PyPI| |Landscape| |Coverage| |Depsy| |Appveyor|

Description
-----------

Samtools provides a function "faidx" (FAsta InDeX), which creates a
small flat index file ".fai" allowing for fast random access to any
subsequence in the indexed FASTA file, while loading a minimal amount of the
file in to memory. This python module implements pure Python classes for
indexing, retrieval, and in-place modification of FASTA files using a samtools
compatible index. The pyfaidx module is API compatible with the `pygr`_ seqdb module.
A command-line script "`faidx`_" is installed alongside the pyfaidx module, and
facilitates complex manipulation of FASTA files without any programming knowledge.

.. _`pygr`: https://github.com/cjlee112/pygr

If you use pyfaidx in your publication, please cite:

`Shirley MD`_, `Ma Z`_, `Pedersen B`_, `Wheelan S`_. `Efficient "pythonic" access to FASTA files using pyfaidx <https://dx.doi.org/10.7287/peerj.preprints.970v1>`_. PeerJ PrePrints 3:e1196. 2015.

.. _`Shirley MD`: http://github.com/mdshw5
.. _`Ma Z`: http://github.com/azalea
.. _`Pedersen B`: http://github.com/brentp
.. _`Wheelan S`: http://github.com/swheelan

Installation
------------

This package is tested under Linux, MacOS, and Windows using Python 3.2-3.4, 2.7, 2.6, and pypy and is available from the PyPI:

::

    pip install pyfaidx  # add --user if you don't have root

or download a `release <https://github.com/mdshw5/pyfaidx/releases>`_ and:

::

    python setup.py install

If using ``pip install --user`` make sure to add ``/home/$(whoami)/.local/bin`` to your ``$PATH`` if you want to run the ``faidx`` script.

Usage
-----

.. code:: python

    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta')
    >>> genes
    Fasta("tests/data/genes.fasta")  # set strict_bounds=True for bounds checking

Acts like a dictionary.

.. code:: python

    >>> genes.keys() ('AB821309.1', 'KF435150.1', 'KF435149.1', 'NR_104216.1', 'NR_104215.1', 'NR_104212.1', 'NM_001282545.1', 'NM_001282543.1', 'NM_000465.3', 'NM_001282549.1', 'NM_001282548.1', 'XM_005249645.1', 'XM_005249644.1', 'XM_005249643.1', 'XM_005249642.1', 'XM_005265508.1', 'XM_005265507.1', 'XR_241081.1', 'XR_241080.1', 'XR_241079.1')

    >>> genes['NM_001282543.1'][200:230]
    >NM_001282543.1:201-230
    CTCGTTCCGCGCCCGCCATGGAACCGGATG

    >>> genes['NM_001282543.1'][200:230].seq
    'CTCGTTCCGCGCCCGCCATGGAACCGGATG'

    >>> genes['NM_001282543.1'][200:230].name
    'NM_001282543.1'

    # Start attributes are 1-based
    >>> genes['NM_001282543.1'][200:230].start
    201

    # End attributes are 0-based
    >>> genes['NM_001282543.1'][200:230].end
    230

    >>> genes['NM_001282543.1'][200:230].fancy_name
    'NM_001282543.1:201-230'

    >>> len(genes['NM_001282543.1'])
    5466

Note that start and end coordinates of Sequence objects are [1, 0]. This can be changed to [0, 0] by passing ``one_based_attributes=False`` to ``Fasta`` or ``Faidx``. This argument only affects the ``Sequence .start/.end`` attributes, and has no effect on slicing coordinates.

Indexes like a list:

.. code:: python

    >>> genes[0][:50]
    >AB821309.1:1-50
    ATGGTCAGCTGGGGTCGTTTCATCTGCCTGGTCGTGGTCACCATGGCAAC

Slices just like a string:

.. code:: python

    >>> genes['NM_001282543.1'][200:230][:10]
    >NM_001282543.1:201-210
    CTCGTTCCGC

    >>> genes['NM_001282543.1'][200:230][::-1]
    >NM_001282543.1:230-201
    GTAGGCCAAGGTACCGCCCGCGCCTTGCTC

    >>> genes['NM_001282543.1'][200:230][::3]
    >NM_001282543.1:201-230
    CGCCCCTACA

    >>> genes['NM_001282543.1'][:]
    >NM_001282543.1:1-5466
    CCCCGCCCCT........

- Slicing start and end coordinates are 0-based, just like Python sequences.

Complements and reverse complements just like DNA

.. code:: python

    >>> genes['NM_001282543.1'][200:230].complement
    >NM_001282543.1 (complement):201-230
    GAGCAAGGCGCGGGCGGTACCTTGGCCTAC

    >>> genes['NM_001282543.1'][200:230].reverse
    >NM_001282543.1:230-201
    GTAGGCCAAGGTACCGCCCGCGCCTTGCTC

    >>> -genes['NM_001282543.1'][200:230]
    >NM_001282543.1 (complement):230-201
    CATCCGGTTCCATGGCGGGCGCGGAACGAG

``Fasta`` objects can also be accessed using method calls:

.. code:: python

    >>> genes.get_seq('NM_001282543.1', 201, 210)
    >NM_001282543.1:201-210
    CTCGTTCCGC

    >>> genes.get_seq('NM_001282543.1', 201, 210, rc=True)
    >NM_001282543.1 (complement):210-201
    GCGGAACGAG

Spliced sequences can be retrieved from a list of [start, end] coordinates:
**TODO** update this section

.. code:: python

    # new in v0.5.1
    segments = [[1, 10], [50, 70]]
    >>> genes.get_spliced_seq('NM_001282543.1', segments)
    >gi|543583786|ref|NM_001282543.1|:1-70
    CCCCGCCCCTGGTTTCGAGTCGCTGGCCTGC

.. _keyfn:

Custom key functions provide cleaner access:

.. code:: python

    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta', key_function = lambda x: x.split('.')[0])
    >>> genes.keys()
    dict_keys(['NR_104212', 'NM_001282543', 'XM_005249644', 'XM_005249645', 'NR_104216', 'XM_005249643', 'NR_104215', 'KF435150', 'AB821309', 'NM_001282549', 'XR_241081', 'KF435149', 'XR_241079', 'NM_000465', 'XM_005265508', 'XR_241080', 'XM_005249642', 'NM_001282545', 'XM_005265507', 'NM_001282548'])
    >>> genes['NR_104212'][:10]
    >NR_104212:1-10
    CCCCGCCCCT

You can specify a character to split names on, which will generate additional entries:

.. code:: python

    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta', split_char='.', duplicate_action="first") # default duplicate_action="stop"
    >>> genes.keys()
    dict_keys(['.1', 'NR_104212', 'NM_001282543', 'XM_005249644', 'XM_005249645', 'NR_104216', 'XM_005249643', 'NR_104215', 'KF435150', 'AB821309', 'NM_001282549', 'XR_241081', 'KF435149', 'XR_241079', 'NM_000465', 'XM_005265508', 'XR_241080', 'XM_005249642', 'NM_001282545', 'XM_005265507', 'NM_001282548'])

If your `key_function` or `split_char` generates duplicate entries, you can choose what action to take:

.. code:: python

    # new in v0.4.9
    >>> genes = Fasta('tests/data/genes.fasta', split_char="|", duplicate_action="longest")
    >>> genes.keys()
    dict_keys(['gi', '563317589', 'dbj', 'AB821309.1', '', '557361099', 'gb', 'KF435150.1', '557361097', 'KF435149.1', '543583796', 'ref', 'NR_104216.1', '543583795', 'NR_104215.1', '543583794', 'NR_104212.1', '543583788', 'NM_001282545.1', '543583786', 'NM_001282543.1', '543583785', 'NM_000465.3', '543583740', 'NM_001282549.1', '543583738', 'NM_001282548.1', '530384540', 'XM_005249645.1', '530384538', 'XM_005249644.1', '530384536', 'XM_005249643.1', '530384534', 'XM_005249642.1', '530373237','XM_005265508.1', '530373235', 'XM_005265507.1', '530364726', 'XR_241081.1', '530364725', 'XR_241080.1', '530364724', 'XR_241079.1'])

Filter functions (returning True) limit the index:

.. code:: python

    # new in v0.3.8
    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta', filt_function = lambda x: x[0] == 'N')
    >>> genes.keys()
    dict_keys(['NR_104212', 'NM_001282543', 'NR_104216', 'NR_104215', 'NM_001282549', 'NM_000465', 'NM_001282545', 'NM_001282548'])
    >>> genes['XM_005249644']
    KeyError: XM_005249644 not in tests/data/genes.fasta.

Or just get a Python string:

.. code:: python

    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta', as_raw=True)
    >>> genes
    Fasta("tests/data/genes.fasta", as_raw=True)

    >>> genes['NM_001282543.1'][200:230]
    CTCGTTCCGCGCCCGCCATGGAACCGGATG

You can make sure that you always receive an uppercase sequence, even if your fasta file has lower case

.. code:: python

    >>> from pyfaidx import Fasta
    >>> reference = Fasta('tests/data/genes.fasta.lower', sequence_always_upper=True)
    >>> reference['gi|557361099|gb|KF435150.1|'][1:70]

    >gi|557361099|gb|KF435150.1|:2-70
    TGACATCATTTTCCACCTCTGCTCAGTGTTCAACATCTGACAGTGCTTGCAGGATCTCTCCTGGACAAA


You can also perform line-based iteration, receiving the sequence lines as they appear in the FASTA file:

.. code:: python

    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta')
    >>> for line in genes['NM_001282543.1']:
    ...   print(line)
    CCCCGCCCCTCTGGCGGCCCGCCGTCCCAGACGCGGGAAGAGCTTGGCCGGTTTCGAGTCGCTGGCCTGC
    AGCTTCCCTGTGGTTTCCCGAGGCTTCCTTGCTTCCCGCTCTGCGAGGAGCCTTTCATCCGAAGGCGGGA
    CGATGCCGGATAATCGGCAGCCGAGGAACCGGCAGCCGAGGATCCGCTCCGGGAACGAGCCTCGTTCCGC
    ...

Sequence names are truncated on any whitespace. This is a limitation of the indexing strategy. However, full names can be recovered:

.. code:: python

    # new in v0.3.7
    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta')
    >>> for record in genes:
    ...   print(record.name)
    ...   print(record.long_name)
    ...
    gi|563317589|dbj|AB821309.1|
    gi|563317589|dbj|AB821309.1| Homo sapiens FGFR2-AHCYL1 mRNA for FGFR2-AHCYL1 fusion kinase protein, complete cds
    gi|557361099|gb|KF435150.1|
    gi|557361099|gb|KF435150.1| Homo sapiens MDM4 protein variant Y (MDM4) mRNA, complete cds, alternatively spliced
    gi|557361097|gb|KF435149.1|
    gi|557361097|gb|KF435149.1| Homo sapiens MDM4 protein variant G (MDM4) mRNA, complete cds
    ...

    # new in v0.4.9
    >>> from pyfaidx import Fasta
    >>> genes = Fasta('tests/data/genes.fasta', read_long_names=True)
    >>> for record in genes:
    ...   print(record.name)
    ...
    gi|563317589|dbj|AB821309.1| Homo sapiens FGFR2-AHCYL1 mRNA for FGFR2-AHCYL1 fusion kinase protein, complete cds
    gi|557361099|gb|KF435150.1| Homo sapiens MDM4 protein variant Y (MDM4) mRNA, complete cds, alternatively spliced
    gi|557361097|gb|KF435149.1| Homo sapiens MDM4 protein variant G (MDM4) mRNA, complete cds

Records can be accessed efficiently as numpy arrays:

.. code:: python

    # new in v0.5.4
    >>> from pyfaidx import Fasta
    >>> import numpy as np
    >>> genes = Fasta('tests/data/genes.fasta')
    >>> np.asarray(genes['NM_001282543.1'])
    array(['C', 'C', 'C', ..., 'A', 'A', 'A'], dtype='|S1')

Sequence can be buffered in memory using a read-ahead buffer
for fast sequential access:

.. code:: python

    >>> from timeit import timeit
    >>> fetch = "genes['NM_001282543.1'][200:230]"
    >>> read_ahead = "import pyfaidx; genes = pyfaidx.Fasta('tests/data/genes.fasta', read_ahead=10000)"
    >>> no_read_ahead = "import pyfaidx; genes = pyfaidx.Fasta('tests/data/genes.fasta')"
    >>> string_slicing = "genes = {}; genes['NM_001282543.1'] = 'N'*10000"

    >>> timeit(fetch, no_read_ahead, number=10000)
    0.2204863309962093
    >>> timeit(fetch, read_ahead, number=10000)
    0.1121859749982832
    >>> timeit(fetch, string_slicing, number=10000)
    0.0033553699977346696

Read-ahead buffering can reduce runtime by 1/2 for sequential accesses to buffered regions.

.. role:: red

If you want to modify the contents of your FASTA file in-place, you can use the `mutable` argument.
Any portion of the FastaRecord can be replaced with an equivalent-length string.
:red:`Warning`: *This will change the contents of your file immediately and permanently:*

.. code:: python

    >>> genes = Fasta('tests/data/genes.fasta', mutable=True)
    >>> type(genes['NM_001282543.1'])
    <class 'pyfaidx.MutableFastaRecord'>

    >>> genes['NM_001282543.1'][:10]
    >NM_001282543.1:1-10
    CCCCGCCCCT
    >>> genes['NM_001282543.1'][:10] = 'NNNNNNNNNN'
    >>> genes['NM_001282543.1'][:15]
    >NM_001282543.1:1-15
    NNNNNNNNNNCTGGC

The FastaVariant class provides a way to integrate single nucleotide variant calls to generate a consensus sequence.

.. code:: python

    # new in v0.4.0
    >>> consensus = FastaVariant('tests/data/chr22.fasta', 'tests/data/chr22.vcf.gz', het=True, hom=True)
    RuntimeWarning: Using sample NA06984 genotypes.

    >>> consensus['22'].variant_sites
    (16042793, 21833121, 29153196, 29187373, 29187448, 29194610, 29821295, 29821332, 29993842, 32330460, 32352284)

    >>> consensus['22'][16042790:16042800]
    >22:16042791-16042800
    TCGTAGGACA

    >>> Fasta('tests/data/chr22.fasta')['22'][16042790:16042800]
    >22:16042791-16042800
    TCATAGGACA

    >>> consensus = FastaVariant('tests/data/chr22.fasta', 'tests/data/chr22.vcf.gz', sample='NA06984', het=True, hom=True, call_filter='GT == "0/1"')
    >>> consensus['22'].variant_sites
    (16042793, 29187373, 29187448, 29194610, 29821332)

.. _faidx:

It also provides a command-line script:

cli script: faidx
~~~~~~~~~~~~~~~~~

.. code:: bash

    Fetch sequences from FASTA. If no regions are specified, all entries in the
    input file are returned. Input FASTA file must be consistently line-wrapped,
    and line wrapping of output is based on input line lengths.

    positional arguments:
      fasta                 FASTA file
      regions               space separated regions of sequence to fetch e.g.
                            chr1:1-1000

    optional arguments:
      -h, --help            show this help message and exit
      -b BED, --bed BED     bed file of regions
      -o OUT, --out OUT     output file name (default: stdout)
      -i {bed,chromsizes,nucleotide,transposed}, --transform {bed,chromsizes,nucleotide,transposed} transform the requested regions into another format. default: None
      -c, --complement      complement the sequence. default: False
      -r, --reverse         reverse the sequence. default: False
      -a SIZE_RANGE, --size-range SIZE_RANGE
                            selected sequences are in the size range [low, high]. example: 1,1000 default: None
      -n, --no-names        omit sequence names from output. default: False
      -f, --full-names      output full names including description. default: False
      -x, --split-files     write each region to a separate file (names are derived from regions)
      -l, --lazy            fill in --default-seq for missing ranges. default: False
      -s DEFAULT_SEQ, --default-seq DEFAULT_SEQ
                            default base for missing positions and masking. default: None
      -d DELIMITER, --delimiter DELIMITER
                            delimiter for splitting names to multiple values (duplicate names will be discarded). default: None
      -e HEADER_FUNCTION, --header-function HEADER_FUNCTION
                            python function to modify header lines e.g: "lambda x: x.split("|")[0]". default: lambda x: x.split()[0]
      -u {stop,first,last,longest,shortest}, --duplicates-action {stop,first,last,longest,shortest}
                            entry to take when duplicate sequence names are encountered. default: stop
      -g REGEX, --regex REGEX
                            selected sequences are those matching regular expression. default: .*
      -v, --invert-match    selected sequences are those not matching 'regions' argument. default: False
      -m, --mask-with-default-seq
                            mask the FASTA file using --default-seq default: False
      -M, --mask-by-case    mask the FASTA file by changing to lowercase. default: False
      -e HEADER_FUNCTION, --header-function HEADER_FUNCTION
                            python function to modify header lines e.g: "lambda x: x.split("|")[0]". default: None
      --no-rebuild          do not rebuild the .fai index even if it is out of date. default: False
      --version             print pyfaidx version number

Examples:

.. code:: bash

    $ faidx tests/data/genes.fasta NM_001282543.1:201-210 NM_001282543.1:300-320
    >NM_001282543.1:201-210
    CTCGTTCCGC
    >NM_001282543.1:300-320
    GTAATTGTGTAAGTGACTGCA

    $ faidx --full-names tests/data/genes.fasta NM_001282543.1:201-210
    >NM_001282543.1| Homo sapiens BRCA1 associated RING domain 1 (BARD1), transcript variant 2, mRNA
    CTCGTTCCGC

    $ faidx --no-names tests/data/genes.fasta NM_001282543.1:201-210 NM_001282543.1:300-320
    CTCGTTCCGC
    GTAATTGTGTAAGTGACTGCA

    $ faidx --complement tests/data/genes.fasta NM_001282543.1:201-210
    >NM_001282543.1:201-210 (complement)
    GAGCAAGGCG

    $ faidx --reverse tests/data/genes.fasta NM_001282543.1:201-210
    >NM_001282543.1:210-201
    CGCCTTGCTC

    $ faidx --reverse --complement tests/data/genes.fasta NM_001282543.1:201-210
    >NM_001282543.1:210-201 (complement)
    GCGGAACGAG

    $ faidx tests/data/genes.fasta NM_001282543.1
    >NM_001282543.1:1-5466
    CCCCGCCCCT........
    ..................
    ..................
    ..................

    $ faidx --regex "^NM_00128254[35]" genes.fasta
    >NM_001282543.1
    ..................
    ..................
    ..................
    >NM_001282545.1
    ..................
    ..................
    ..................

    $ faidx --lazy tests/data/genes.fasta NM_001282543.1:5460-5480
    >NM_001282543.1:5460-5480
    AAAAAAANNNNNNNNNNNNNN

    $ faidx --lazy --default-seq='Q' tests/data/genes.fasta NM_001282543.1:5460-5480
    >NM_001282543.1:5460-5480
    AAAAAAAQQQQQQQQQQQQQQ

    $ faidx tests/data/genes.fasta --bed regions.bed
    ...

    $ faidx --transform chromsizes tests/data/genes.fasta
    AB821309.1	3510
    KF435150.1	481
    KF435149.1	642
    NR_104216.1	4573
    NR_104215.1	5317
    NR_104212.1	5374
    ...

    $ faidx --transform bed tests/data/genes.fasta
    AB821309.1	1    3510
    KF435150.1	1    481
    KF435149.1	1    642
    NR_104216.1	1   4573
    NR_104215.1	1   5317
    NR_104212.1	1   5374
    ...

    $ faidx --transform nucleotide tests/data/genes.fasta
    name	start	end	A	T	C	G	N
    AB821309.1	1	3510	955	774	837	944	0
    KF435150.1	1	481	149	120	103	109	0
    KF435149.1	1	642	201	163	129	149	0
    NR_104216.1	1	4573	1294	1552	828	899	0
    NR_104215.1	1	5317	1567	1738	968	1044	0
    NR_104212.1	1	5374	1581	1756	977	1060	0
    ...

    faidx --transform transposed tests/data/genes.fasta
    AB821309.1	1	3510	ATGGTCAGCTGGGGTCGTTTCATC...
    KF435150.1	1	481	ATGACATCATTTTCCACCTCTGCT...
    KF435149.1	1	642	ATGACATCATTTTCCACCTCTGCT...
    NR_104216.1	1	4573	CCCCGCCCCTCTGGCGGCCCGCCG...
    NR_104215.1	1	5317	CCCCGCCCCTCTGGCGGCCCGCCG...
    NR_104212.1	1	5374	CCCCGCCCCTCTGGCGGCCCGCCG...
    ...

    $ faidx --split-files tests/data/genes.fasta
    $ ls
    AB821309.1.fasta	NM_001282549.1.fasta	XM_005249645.1.fasta
    KF435149.1.fasta	NR_104212.1.fasta	XM_005265507.1.fasta
    KF435150.1.fasta	NR_104215.1.fasta	XM_005265508.1.fasta
    NM_000465.3.fasta	NR_104216.1.fasta	XR_241079.1.fasta
    NM_001282543.1.fasta	XM_005249642.1.fasta	XR_241080.1.fasta
    NM_001282545.1.fasta	XM_005249643.1.fasta	XR_241081.1.fasta
    NM_001282548.1.fasta	XM_005249644.1.fasta

    $ faidx --delimiter='_' tests/data/genes.fasta 000465.3
    >000465.3
    CCCCGCCCCTCTGGCGGCCCGCCGTCCCAGACGCGGGAAGAGCTTGGCCGGTTTCGAGTCGCTGGCCTGC
    AGCTTCCCTGTGGTTTCCCGAGGCTTCCTTGCTTCCCGCTCTGCGAGGAGCCTTTCATCCGAAGGCGGGA
    .......

    $ faidx --size-range 5500,6000 -i chromsizes tests/data/genes.fasta
    NM_000465.3	5523

    $ faidx -m --bed regions.bed tests/data/genes.fasta
    ### Modifies tests/data/genes.fasta by masking regions using --default-seq character ###

    $ faidx -M --bed regions.bed tests/data/genes.fasta
    ### Modifies tests/data/genes.fasta by masking regions using lowercase characters ###

    $ faidx -e "lambda x: x.split('.')[0]" tests/data/genes.fasta -i bed
    AB821309	1	3510
    KF435150	1	481
    KF435149	1	642
    NR_104216	1	4573
    NR_104215	1	5317
    .......


Similar syntax as ``samtools faidx``


A lower-level Faidx class is also available:

.. code:: python

    >>> from pyfaidx import Faidx
    >>> fa = Faidx('genes.fa')  # can return str with as_raw=True
    >>> fa.index
    OrderedDict([('AB821309.1', IndexRecord(rlen=3510, offset=12, lenc=70, lenb=71)), ('KF435150.1', IndexRecord(rlen=481, offset=3585, lenc=70, lenb=71)),... ])

    >>> fa.index['AB821309.1'].rlen
    3510

    fa.fetch('AB821309.1', 1, 10)  # these are 1-based genomic coordinates
    >AB821309.1:1-10
    ATGGTCAGCT


-  If the FASTA file is not indexed, when ``Faidx`` is initialized the
   ``build_index`` method will automatically run, and
   the index will be written to "filename.fa.fai" with ``write_fai()``.
   where "filename.fa" is the original FASTA file.
-  Start and end coordinates are 1-based.

Support for compressed FASTA
----------------------------

``pyfaidx`` can create and read ``.fai`` indices for FASTA files that have
been compressed using the `bgzip <http://www.htslib.org/doc/tabix.html>`_
tool from `samtools <http://www.htslib.org/>`_. ``bgzip`` writes compressed
data in a ``BGZF`` format. ``BGZF`` is ``gzip`` compatible, consisting of
multiple concatenated ``gzip`` blocks, each with an additional ``gzip``
header making it possible to build an index for rapid random access. I.e.,
files compressed with ``bgzip`` are valid ``gzip`` and so can be read by
``gunzip``.  See `this description
<http://pydoc.net/Python/biopython/1.66/Bio.bgzf/>`_ for more details on
``bgzip``.

Changelog
---------

Please see the `releases <https://github.com/mdshw5/pyfaidx/releases>`_ for a
comprehensive list of version changes.

Known issues
------------

I try to fix as many bugs as possible, but most of this work is supported by a single developer. Please check the `known issues <https://github.com/mdshw5/pyfaidx/issues?utf8=✓&q=is%3Aissue+is%3Aopen+label%3Aknown>`_ for bugs relevant to your work. Pull requests are welcome.


Contributing
------------

Create a new Pull Request with one feature. If you add a new feature, please
create also the relevant test.

To get test running on your machine:
 - Create a new virtualenv and install the `dev-requirements.txt`.
 - Download the test data running:

      python tests/data/download_gene_fasta.py

 - Run the tests with

      nosetests --with-coverage --cover-package=pyfaidx

Acknowledgements
----------------

This project is freely licensed by the author, `Matthew
Shirley <http://mattshirley.com>`_, and was completed under the
mentorship and financial support of Drs. `Sarah
Wheelan <http://sjwheelan.som.jhmi.edu>`_ and `Vasan
Yegnasubramanian <http://yegnalab.onc.jhmi.edu>`_ at the Sidney Kimmel
Comprehensive Cancer Center in the Department of Oncology.

.. |Travis| image:: https://travis-ci.org/mdshw5/pyfaidx.svg?branch=master
    :target: https://travis-ci.org/mdshw5/pyfaidx

.. |PyPI| image:: https://img.shields.io/pypi/v/pyfaidx.svg?branch=master
    :target: https://pypi.python.org/pypi/pyfaidx

.. |Landscape| image:: https://landscape.io/github/mdshw5/pyfaidx/master/landscape.svg
   :target: https://landscape.io/github/mdshw5/pyfaidx/master
   :alt: Code Health

.. |Coverage| image:: https://codecov.io/gh/mdshw5/pyfaidx/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/mdshw5/pyfaidx

.. |Depsy| image:: http://depsy.org/api/package/pypi/pyfaidx/badge.svg
   :target: http://depsy.org/package/python/pyfaidx

.. |Appveyor| image:: https://ci.appveyor.com/api/projects/status/80ihlw30a003596w?svg=true
   :target: https://ci.appveyor.com/project/mdshw5/pyfaidx
  * [pyflakes-2.1.1](https://github.com/PyCQA/pyflakes) ========
Pyflakes
========

A simple program which checks Python source files for errors.

Pyflakes analyzes programs and detects various errors.  It works by
parsing the source file, not importing it, so it is safe to use on
modules with side effects.  It's also much faster.

It is `available on PyPI <https://pypi.org/project/pyflakes/>`_
and it supports all active versions of Python: 2.7 and 3.4 to 3.7.



Installation
------------

It can be installed with::

  $ pip install --upgrade pyflakes


Useful tips:

* Be sure to install it for a version of Python which is compatible
  with your codebase: for Python 2, ``pip2 install pyflakes`` and for
  Python3, ``pip3 install pyflakes``.

* You can also invoke Pyflakes with ``python3 -m pyflakes .`` or
  ``python2 -m pyflakes .`` if you have it installed for both versions.

* If you require more options and more flexibility, you could give a
  look to Flake8_ too.


Design Principles
-----------------
Pyflakes makes a simple promise: it will never complain about style,
and it will try very, very hard to never emit false positives.

Pyflakes is also faster than Pylint_
or Pychecker_. This is
largely because Pyflakes only examines the syntax tree of each file
individually. As a consequence, Pyflakes is more limited in the
types of things it can check.

If you like Pyflakes but also want stylistic checks, you want
flake8_, which combines
Pyflakes with style checks against
`PEP 8`_ and adds
per-project configuration ability.


Mailing-list
------------

Share your feedback and ideas: `subscribe to the mailing-list
<https://mail.python.org/mailman/listinfo/code-quality>`_

Contributing
------------

Issues are tracked on `GitHub <https://github.com/PyCQA/pyflakes/issues>`_.

Patches may be submitted via a `GitHub pull request`_ or via the mailing list
if you prefer. If you are comfortable doing so, please `rebase your changes`_
so they may be applied to master with a fast-forward merge, and each commit is
a coherent unit of work with a well-written log message.  If you are not
comfortable with this rebase workflow, the project maintainers will be happy to
rebase your commits for you.

All changes should include tests and pass flake8_.

.. image:: https://api.travis-ci.org/PyCQA/pyflakes.svg?branch=master
   :target: https://travis-ci.org/PyCQA/pyflakes
   :alt: Build status

.. _Pylint: http://www.pylint.org/
.. _flake8: https://pypi.org/project/flake8/
.. _`PEP 8`: http://legacy.python.org/dev/peps/pep-0008/
.. _Pychecker: http://pychecker.sourceforge.net/
.. _`rebase your changes`: https://git-scm.com/book/en/v2/Git-Branching-Rebasing
.. _`GitHub pull request`: https://github.com/PyCQA/pyflakes/pulls

Changelog
---------

Please see `NEWS.rst <NEWS.rst>`_.



  * [pygithub3-0.5.1](https://github.com/copitux/python-github3) .. image:: https://secure.travis-ci.org/copitux/python-github3.png

Pygithub3
==========

Pygithub3 is a wrapper to the **Github API v3**,
written in Python.

It has been developed with extensibility in mind, because the ``API`` is in a
beta state, trying to achieve a very loosly coupled software.

It should be very easy to extend to support new ``requests`` and ``resources``,
because each of them are managed by itself.

`Pygithub3 docs <http://pygithub3.rtfd.org>`_

`Github API v3 docs <http://developer.github.com/v3/>`_

Fast install
-------------
::

    pip install pygithub3

Fast example
-------------
::

    from pygithub3 import Github

    gh = Github(login='copitux', password='password')

    copitux = gh.users.get()
    kennethreitz = gh.users.get('kennethreitz')

    copitux_repos = gh.repos.list().all()
    kennethreitz_repos = gh.repos.list('kennethreitz').all()

Achievements
-------------

- The core
- `Users service <http://developer.github.com/v3/users/>`_
- `Repos service <http://developer.github.com/v3/repos/>`_
- `Gists service <http://developer.github.com/v3/gists/>`_
- `Git Data service <http://developer.github.com/v3/git/>`_
- `Pull requests service <http://developer.github.com/v3/pulls/>`_
- `Orgs service <http://developer.github.com/v3/orgs/>`_
- `Issues service <http://developer.github.com/v3/issues/>`_
- `Events service <http://developer.github.com/v3/events/>`_

TODO
-----

- Oauth authorization API (service?)
- Proxy methods into resources (e.g copitux.followers)

Contribute
-----------

1. Fork the `repository <https://github.com/copitux/python-github3>`_
2. Write a test to cover new feature or to reproduce bug
3. Code with `pep8 <http://www.python.org/dev/peps/pep-0008/>`_ rules
4. Add yourself to ``AUTHORS``
5. Pull request it to ``develop`` branch

Tests
-----

Run ``make init`` to install test requirements and ``nosetests`` to run tests.
  * [pyinotify-0.9.6](http://github.com/seb-m/pyinotify) # Pyinotify

* License          : MIT
* Project URL      : [http://github.com/seb-m/pyinotify](http://github.com/seb-m/pyinotify)
* Project Wiki     : [http://github.com/seb-m/pyinotify/wiki](http://github.com/seb-m/pyinotify/wiki)
* API Documentation: [http://seb-m.github.com/pyinotify](http://seb-m.github.com/pyinotify)


## Dependencies

* Linux ≥ 2.6.13
* Python ≥ 2.4 (including Python 3.x)


## Install

### Get the current stable version from PyPI and install it with `pip`

    # To install pip follow http://www.pip-installer.org/en/latest/installing.html
    $ sudo pip install pyinotify

### Or install Pyinotify directly from source

    # Choose your Python interpreter: either python, python2.7, python3.2,..
    # Replacing XXX accordingly, type:
    $ sudo pythonXXX setup.py install


## Watch a directory

Install pyinotify and run this command from a shell:

    $ python -m pyinotify -v /my-dir-to-watch

  * [pykerberos-1.2.1]() This Python package is a high-level wrapper for Kerberos (GSSAPI) operations.
The goal is to avoid having to build a module that wraps the entire Kerberos.framework,
and instead offer a limited set of functions that do what is needed for client/server
Kerberos authentication based on <http://www.ietf.org/rfc/rfc4559.txt>.
  * [pylint-2.3.1](https://github.com/PyCQA/pylint) 
README for Pylint - http://pylint.pycqa.org/
============================================

.. image:: https://travis-ci.org/PyCQA/pylint.svg?branch=master
    :target: https://travis-ci.org/PyCQA/pylint

.. image:: https://ci.appveyor.com/api/projects/status/rbvwhakyj1y09atb/branch/master?svg=true
    :alt: AppVeyor Build Status
    :target: https://ci.appveyor.com/project/PCManticore/pylint

.. image:: https://coveralls.io/repos/github/PyCQA/pylint/badge.svg?branch=master
    :target: https://coveralls.io/github/PyCQA/pylint?branch=master


.. image:: https://img.shields.io/pypi/v/pylint.svg
    :alt: Pypi Package version
    :target: https://pypi.python.org/pypi/pylint

.. image:: https://readthedocs.org/projects/pylint/badge/?version=latest
    :target: http://pylint.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

.. |tideliftlogo| image:: doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White_small.png
   :width: 75
   :height: 60
   :alt: Tidelift

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for pylint is available as part of the `Tidelift
       Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-pylint?utm_source=pypi-pylint&utm_medium=referral&utm_campaign=readme


======
Pylint
======

**It's not just a linter that annoys you!**

Pylint is a Python static code analysis tool which looks for programming errors,
helps enforcing a coding standard, sniffs for code smells and offers simple refactoring
suggestions.

It's highly configurable, having special pragmas to control its errors and warnings
from within your code, as well as from an extensive configuration file.
It is also possible to write your own plugins for adding your own checks or for
extending pylint in one way or another.

It's a free software distributed under the GNU General Public Licence unless
otherwise specified.

Development is hosted on GitHub: https://github.com/PyCQA/pylint/

You can use the code-quality@python.org mailing list to discuss about
Pylint. Subscribe at https://mail.python.org/mailman/listinfo/code-quality/
or read the archives at https://mail.python.org/pipermail/code-quality/

Pull requests are amazing and most welcome.

Install
-------

Pylint can be simply installed by running::

    pip install pylint

If you are using Python 3.6+, upgrade to get full support for your version::

    pip install pylint --upgrade

If you want to install from a source distribution, extract the tarball and run
the following command ::

    python setup.py install


Do make sure to do the same for astroid, which is used internally by pylint.

For debian and rpm packages, use your usual tools according to your Linux distribution.

More information about installation and available distribution format
can be found here_.

Documentation
-------------

The documentation lives at http://pylint.pycqa.org/.

Pylint is shipped with following additional commands:

* pyreverse: an UML diagram generator
* symilar: an independent similarities checker
* epylint: Emacs and Flymake compatible Pylint


Testing
-------

We use tox_ for running the test suite. You should be able to install it with::

    pip install tox pytest


To run the test suite for a particular Python version, you can do::

    tox -e py37


To run individual tests with ``tox``, you can do::

    tox -e py37 -- -k name_of_the_test


We use pytest_ for testing ``pylint``, which you can use without using ``tox`` for a faster development cycle.

If you want to run tests on a specific portion of the code with pytest_, (pytest-cov_) and your local python version::

    # ( pip install pytest-cov )
    # Everything:
    python3 -m pytest tests/
    # Everything in tests/message with coverage for the relevant code:
    python3 -m pytest tests/message/ --cov=pylint.message
    coverage html
    # Only the functional test "missing_kwoa_py3":
    python3 -m pytest "tests/test_functional.py::test_functional[missing_kwoa_py3]"


Do not forget to clone astroid_ and install the last version::


    git clone https://github.com/PyCQA/astroid.git

    # From source
    python3 astroid/setup.py build sdist
    pip3 install astroid/dist/astroid*.tar.gz

    # Using an editable installation
    cd astroid
    python3 -m pip install -e .


For more detailed information, check the documentation.

.. _here: http://pylint.pycqa.org/en/latest/user_guide/installation.html
.. _tox: https://tox.readthedocs.io/en/latest/
.. _pytest: https://docs.pytest.org/en/latest/
.. _pytest-cov: https://pypi.org/project/pytest-cov/
.. _astroid: https://github.com/PyCQA/astroid

License
-------

pylint is, with a few exceptions listed below, `GPLv2 <COPYING>`_.

The icon files are licensed under the `CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0/>`_ license:

- `doc/logo.png <doc/logo.png>`_
- `doc/logo.svg <doc/logo.svg>`_



  * [pymongo-3.8.0](http://github.com/mongodb/mongo-python-driver) =======
PyMongo
=======
:Info: See `the mongo site <http://www.mongodb.org>`_ for more information. See `GitHub <http://github.com/mongodb/mongo-python-driver>`_ for the latest source.
:Author: Mike Dirolf
:Maintainer: Bernie Hackett <bernie@mongodb.com>

About
=====

The PyMongo distribution contains tools for interacting with MongoDB
database from Python.  The ``bson`` package is an implementation of
the `BSON format <http://bsonspec.org>`_ for Python. The ``pymongo``
package is a native Python driver for MongoDB. The ``gridfs`` package
is a `gridfs
<http://www.mongodb.org/display/DOCS/GridFS+Specification>`_
implementation on top of ``pymongo``.

PyMongo supports MongoDB 2.6, 3.0, 3.2, 3.4, 3.6, 4.0 and 4.2.

Support / Feedback
==================

For issues with, questions about, or feedback for PyMongo, please look into
our `support channels <http://www.mongodb.org/about/support>`_. Please
do not email any of the PyMongo developers directly with issues or
questions - you're more likely to get an answer on the `mongodb-user
<http://groups.google.com/group/mongodb-user>`_ list on Google Groups.

Bugs / Feature Requests
=======================

Think you’ve found a bug? Want to see a new feature in PyMongo? Please open a
case in our issue management tool, JIRA:

- `Create an account and login <https://jira.mongodb.org>`_.
- Navigate to `the PYTHON project <https://jira.mongodb.org/browse/PYTHON>`_.
- Click **Create Issue** - Please provide as much information as possible about the issue type and how to reproduce it.

Bug reports in JIRA for all driver projects (i.e. PYTHON, CSHARP, JAVA) and the
Core Server (i.e. SERVER) project are **public**.

How To Ask For Help
-------------------

Please include all of the following information when opening an issue:

- Detailed steps to reproduce the problem, including full traceback, if possible.
- The exact python version used, with patch level::

  $ python -c "import sys; print(sys.version)"

- The exact version of PyMongo used, with patch level::

  $ python -c "import pymongo; print(pymongo.version); print(pymongo.has_c())"

- The operating system and version (e.g. Windows 7, OSX 10.8, ...)
- Web framework or asynchronous network library used, if any, with version (e.g.
  Django 1.7, mod_wsgi 4.3.0, gevent 1.0.1, Tornado 4.0.2, ...)

Security Vulnerabilities
------------------------

If you’ve identified a security vulnerability in a driver or any other
MongoDB project, please report it according to the `instructions here
<http://docs.mongodb.org/manual/tutorial/create-a-vulnerability-report>`_.

Installation
============

PyMongo can be installed with `pip <http://pypi.python.org/pypi/pip>`_::

  $ python -m pip install pymongo

Or ``easy_install`` from
`setuptools <http://pypi.python.org/pypi/setuptools>`_::

  $ python -m easy_install pymongo

You can also download the project source and do::

  $ python setup.py install

Do **not** install the "bson" package from pypi. PyMongo comes with its own
bson package; doing "easy_install bson" installs a third-party package that
is incompatible with PyMongo.

Dependencies
============

PyMongo supports CPython 2.7, 3.4+, PyPy, and PyPy3.5+.

Optional dependencies:

GSSAPI authentication requires `pykerberos
<https://pypi.python.org/pypi/pykerberos>`_ on Unix or `WinKerberos
<https://pypi.python.org/pypi/winkerberos>`_ on Windows. The correct
dependency can be installed automatically along with PyMongo::

  $ python -m pip install pymongo[gssapi]

Support for mongodb+srv:// URIs requires `dnspython
<https://pypi.python.org/pypi/dnspython>`_::

  $ python -m pip install pymongo[srv]

TLS / SSL support may require `ipaddress
<https://pypi.python.org/pypi/ipaddress>`_ and `certifi
<https://pypi.python.org/pypi/certifi>`_ or `wincertstore
<https://pypi.python.org/pypi/wincertstore>`_ depending on the Python
version in use. The necessary dependencies can be installed along with
PyMongo::

  $ python -m pip install pymongo[tls]

Wire protocol compression with snappy requires `python-snappy
<https://pypi.org/project/python-snappy>`_::

  $ python -m pip install pymongo[snappy]

Wire protocol compression with zstandard requires `zstandard
<https://pypi.org/project/zstandard>`_::

  $ python -m pip install pymongo[zstd]

You can install all dependencies automatically with the following
command::

  $ python -m pip install pymongo[snappy,gssapi,srv,tls,zstd]

Other optional packages:

- `backports.pbkdf2 <https://pypi.python.org/pypi/backports.pbkdf2/>`_,
  improves authentication performance with SCRAM-SHA-1 and SCRAM-SHA-256.
  It especially improves performance on Python versions older than 2.7.8.
- `monotonic <https://pypi.python.org/pypi/monotonic>`_ adds support for
  a monotonic clock, which improves reliability in environments
  where clock adjustments are frequent. Not needed in Python 3.


Additional dependencies are:

- (to generate documentation) sphinx_

Examples
========
Here's a basic example (for more see the *examples* section of the docs):

.. code-block:: python

  >>> import pymongo
  >>> client = pymongo.MongoClient("localhost", 27017)
  >>> db = client.test
  >>> db.name
  u'test'
  >>> db.my_collection
  Collection(Database(MongoClient('localhost', 27017), u'test'), u'my_collection')
  >>> db.my_collection.insert_one({"x": 10}).inserted_id
  ObjectId('4aba15ebe23f6b53b0000000')
  >>> db.my_collection.insert_one({"x": 8}).inserted_id
  ObjectId('4aba160ee23f6b543e000000')
  >>> db.my_collection.insert_one({"x": 11}).inserted_id
  ObjectId('4aba160ee23f6b543e000002')
  >>> db.my_collection.find_one()
  {u'x': 10, u'_id': ObjectId('4aba15ebe23f6b53b0000000')}
  >>> for item in db.my_collection.find():
  ...     print(item["x"])
  ...
  10
  8
  11
  >>> db.my_collection.create_index("x")
  u'x_1'
  >>> for item in db.my_collection.find().sort("x", pymongo.ASCENDING):
  ...     print(item["x"])
  ...
  8
  10
  11
  >>> [item["x"] for item in db.my_collection.find().limit(2).skip(1)]
  [8, 11]

Documentation
=============

You will need sphinx_ installed to generate the
documentation. Documentation can be generated by running **python
setup.py doc**. Generated documentation can be found in the
*doc/build/html/* directory.

Testing
=======

The easiest way to run the tests is to run **python setup.py test** in
the root of the distribution.

To verify that PyMongo works with Gevent's monkey-patching::

    $ python green_framework_test.py gevent

Or with Eventlet's::

    $ python green_framework_test.py eventlet

.. _sphinx: http://sphinx.pocoo.org/
  * [pymssql-2.1.4](http://pymssql.org) pymssql - DB-API interface to Microsoft SQL Server
==================================================

.. image:: https://travis-ci.org/pymssql/pymssql.svg?branch=master
        :target: https://travis-ci.org/pymssql/pymssql

.. image:: https://circleci.com/gh/pymssql/pymssql.svg?style=svg
        :target: https://circleci.com/gh/pymssql/pymssql

.. image:: https://ci.appveyor.com/api/projects/status/ts4q4nptm15ac6j7/branch/master?svg=true
        :target: https://ci.appveyor.com/project/level12/pymssql/branch/master

.. image:: http://img.shields.io/pypi/dm/pymssql.svg
        :target: https://pypi.python.org/pypi/pymssql/

.. image:: http://img.shields.io/pypi/v/pymssql.svg
        :target: https://pypi.python.org/pypi/pymssql/

A simple database interface for `Python`_ that builds on top of `FreeTDS`_ to
provide a Python DB-API (`PEP-249`_) interface to `Microsoft SQL Server`_.

.. _Microsoft SQL Server: http://www.microsoft.com/sqlserver/
.. _Python: http://www.python.org/
.. _PEP-249: http://www.python.org/dev/peps/pep-0249/
.. _FreeTDS: http://www.freetds.org/

Detailed information on pymssql is available on the website:

http://pymssql.org

New development is happening on GitHub at:

https://github.com/pymssql/pymssql

There is a Google Group for discussion at:

https://groups.google.com/forum/?fromgroups#!forum/pymssql


Do you use pymssql?
-------------------

Can you take a minute and fill out this survey to help us prioritize development tasks?

https://www.surveymonkey.com/s/KMQ8BM5


.. image:: https://d2weczhvl823v0.cloudfront.net/pymssql/pymssql/trend.png
   :alt: Bitdeli badge
   :target: https://bitdeli.com/free



Recent Changes
==============

Version 2.1.4 - 2018-08-28
==============================

General
-------

- Drop support for versions of FreeTDS older than 0.91.

- Add Python 3.7 support

- Drop Python 3.3 support

Features
--------

- Support for new in SQL Server 2008 ``DATE``, ``TIME`` and ``DATETIME2`` data
  types (GH-156). The following conditions need to be additionally met so
  values of these column types can be returned from the database as their
  native corresponding Python data types instead of as strings:

  * Underlying FreeTDS must be 0.95 or newer.
  * TDS protocol version in use must be 7.3 or newer.

  Thanks Ed Avis for the implementation. (GH-331)

Bug fixes
---------

- Fix ``tds_version``  ``_mssql`` connection property value for TDS version.
  7.1 is actually 7.1 and not 8.0.

Version 2.1.3 - 2016-06-22 - Ramiro Morales
===========================================

- We now publish Linux PEP 513 manylinux wheels on PyPI.
- Windows official binaries: Rollback changes to Windows binaries we had
  implemented in pymssql 2.1.2; go back to using:

  * A statically linked version of FreeTDS (v0.95.95)
  * No SSL support

Version 2.1.2 - 2016-02-10 - Ramiro Morales
===========================================

.. attention:: Windows users: You need to download and install additional DLLs

    pymssql version 2.1.2 includes a change in the official Windows binaries:
    FreeTDS isn't statically linked as it happened up to release 2.1.1, as that
    FreeTDS copy lacked SSL support.

    Please see http://pymssql.org/en/latest/freetds.html#windows for futher
    details.

    We are trying to find a balance between security and convenience and will
    be evaluating the situation for future releases. Your feedback is greatly
    welcome.

Features
--------

- Add ability to set TDS protocol version from pymssql when connecting to SQL
  Server. For the remaining pymssql 2.1.x releases its default value will be 7.1
  (GH-323)

- Add Dockerfile and a Docker image and instructions on how to use it (GH-258).
  This could be a convenient way to use pymssql without having to build stuff.
  See http://pymssql.readthedocs.org/en/latest/intro.html#docker
  Thanks Marc Abramowitz.

- Floating point values are now accepted as Stored Procedure arguments
  (GH-287). Thanks Runzhou Li (Leo) for the report and Bill Adams for the
  implementation.

- Send pymssql version in the appname TDS protocol login record field when the
  application doesn't provide one (GH-354)

Bug fixes
---------

- Fix a couple of very common causes of segmentation faults in presence of
  network a partition between a pymssql-based app and SQL Server (GH-147,
  GH-271) Thanks Marc Abramowitz. See also GH-373.

- Fix failures and inconsistencies in query parameter interpolation when
  UTF-8-encoded literals are present (GH-185). Thanks Bill Adams. Also, GH-291.

- Fix ``login_timeout`` parameter of ``pymssql.connect()`` (GH-318)

- Fixed some cases of ``cursor.rowcont`` having a -1 value after iterating
  over the value returned by pymssql cursor ``fetchmany()`` and ``fetchone()``
  methods (GH-141)

- Remove automatic treatment of string literals passed in queries that start
  with ``'0x'`` as hexadecimal values (GH-286)

- Fix build fatal error when using Cython >= 0.22 (GH-311)

Internals
---------

- Add Appveyor hosted CI setup for running tests on Windows (GH-347)

- Travis CI: Use newer, faster, container-based infrastructure. Also, test
  against more than one FreeTDS version.

- Make it possible to build official release files (sdist, wheels) on Travis &
  AppVeyor.

Version 2.1.1 - 2014-11-25 - Ramiro Morales
===========================================

Features
--------

- Custom message handlers (GH-139)

  The DB-Library API includes a callback mechanism so applications can provide
  functions known as *message handlers* that get passed informative messages
  sent by the server which then can be logged, shown to the user, etc.

  ``_mssql`` now allows you to install your own *message handlers* written in
  Python. See the ``_msssql`` examples and reference sections of the
  documentation for more details.

  Thanks Marc Abramowitz.

- Compatibility with Azure

  It is now possible to transparently connect to `SQL Server instances`_
  accessible as part of the Azure_ cloud services.

  .. note:: If you need to connect to Azure make sure you use FreeTDS 0.91 or
            newer.

- Customizable per-connection initialization SQL clauses (both in ``pymssql``
  and ``_mssql``) (GH-97)

  It is now possible to customize the SQL statements sent right after the
  connection is established (e.g. ``'SET ANSI_NULLS ON;'``). Previously
  it was a hard-coded list of queries. See the ``_mssql.MSSQLConnection``
  documentation for more details.

  Thanks Marc Abramowitz.

- Added ability to handle instances of ``uuid.UUID`` passed as parameters for
  SQL queries both in ``pymssql`` and ``_mssql``. (GH-209)

  Thanks Marat Mavlyutov.

- Allow using `SQL Server autocommit mode`_ from ``pymssql`` at connection
  opening time. This allows e.g. DDL statements like ``DROP DATABASE`` to be
  executed. (GH-210)

  Thanks Marat Mavlyutov.

- Documentation: Explicitly mention minimum versions supported of Python (2.6)
  and SQL Server (2005).

- Incremental enhancements to the documentation.

.. _SQL Server instances: http://www.windowsazure.com/en-us/services/sql-database/
.. _Azure: https://www.windowsazure.com/
.. _SQL Server autocommit mode: http://msdn.microsoft.com/en-us/library/ms187878%28v=sql.105%29.aspx

Bug fixes
---------

- Handle errors when calling Stored Procedures via the ``.callproc()`` pymssql
  cursor method. Now it will raise a DB-API ``DatabaseException``; previously
  it allowed a ``_mssql.MSSQLDatabaseException`` exception to surface.

- Fixes in ``tds_version`` ``_mssql`` connections property value

  Made it work with TDS protocol version 7.2. (GH-211)

  The value returned for TDS version 7.1 is still 8.0 for backward
  compatibility (this is because such feature got added in times when
  Microsoft documentation labeled the two protocol versions that followed 7.0
  as 8.0 and 9.0; later it changed them to 7.1 and 7.2 respectively) and will
  be corrected in a future release (2.2).

- PEP 249 compliance (GH-251)

  Added type constructors to increase compatibility with other libraries.

  Thanks Aymeric Augustin.

- pymssql: Made handling of integer SP params more robust (GH-237)

- Check lower bound value when convering integer values from to Python to SQL
  (GH-238)

Internals
---------

- Completed migration of the test suite from nose to py.test.

- Added a few more test cases to our suite.

- Tests: Modified a couple of test cases so the full suite can be run against
  SQL Server 2005.

- Added testing of successful build of documentation to Travis CI script.

- Build process: Cleanup intermediate and ad-hoc anciliary files (GH-231,
  GH-273)

- setup.py: Fixed handling of release tarballs contents so no extraneous files
  are shipped and the documentation tree is actually included. Also, removed
  unused code.

Version 2.1.0 - 2014-02-25 - `Marc Abramowitz <http://marc-abramowitz.com/>`_
=============================================================================

Features
--------

- Sphinx-based documentation (GH-149)

  Read it online at http://pymssql.org/

  Thanks, Ramiro Morales!

  See:

  * https://github.com/pymssql/pymssql/pull/149
  * https://github.com/pymssql/pymssql/pull/162
  * https://github.com/pymssql/pymssql/pull/164
  * https://github.com/pymssql/pymssql/pull/165
  * https://github.com/pymssql/pymssql/pull/166
  * https://github.com/pymssql/pymssql/pull/167
  * https://github.com/pymssql/pymssql/pull/169
  * https://github.com/pymssql/pymssql/pull/174
  * https://github.com/pymssql/pymssql/pull/175

- "Green" support (GH-135)

  Lets you use pymssql with cooperative multi-tasking systems like
  gevent and have pymssql call a callback when it is waiting for a
  response from the server. You can set this callback to yield to
  another greenlet, coroutine, etc. For example, for gevent, you could
  do::

      def wait_callback(read_fileno):
          gevent.socket.wait_read(read_fileno)

      pymssql.set_wait_callback(wait_callback)

  The above is useful if you're say, running a gunicorn server with the
  gevent worker. With this callback in place, when you send a query to
  SQL server and are waiting for a response, you can yield to other
  greenlets and process other requests. This is super useful when you
  have high concurrency and/or slow database queries and lets you use
  less gunicorn worker processes and still handle high concurrency.

  See https://github.com/pymssql/pymssql/pull/135

- Better error messages.

  E.g.: For a connection failure, instead of:

      pymssql.OperationalError: (20009, 'Net-Lib error during Connection
      refused')

  the dberrstr is also included, resulting in:

      pymssql.OperationalError: (20009, 'DB-Lib error message 20009,
      severity 9:\nUnable to connect: Adaptive Server is unavailable or
      does not exist\nNet-Lib error during Connection refused\n')

  See:
  * https://github.com/pymssql/pymssql/pull/151

  In the area of error messages, we also made this change:

  execute: Raise ColumnsWithoutNamesError when as_dict=True and missing
  column names (GH-160)

  because the previous behavior was very confusing; instead of raising
  an exception, we would just return row dicts with those columns
  missing. This prompted at least one question on the mailing list
  (https://groups.google.com/forum/?fromgroups#!topic/pymssql/JoZpmNZFtxM),
  so we thought it was better to handle this explicitly by raising an
  exception, so the user would understand what went wrong.

  See:
  * https://github.com/pymssql/pymssql/pull/160
  * https://github.com/pymssql/pymssql/pull/168

- Performance improvements

  You are most likely to notice a difference from these when you are
  fetching a large number of rows.

  * Reworked row fetching (GH-159)

    There was a rather large amount of type conversion occuring when
    fetching a row from pymssql. The number of conversions required have
    been cut down significantly with these changes.
    Thanks Damien, Churchill!

    See:
    * https://github.com/pymssql/pymssql/pull/158
    * https://github.com/pymssql/pymssql/pull/159

  * Modify get_row() to use the CPython tuple API (GH-178)

    This drops the previous method of building up a row tuple and switches
    to using the CPython API, which allows you to create a correctly sized
    tuple at the beginning and simply fill it in. This appears to offer
    around a 10% boost when fetching rows from a table where the data is
    already in memory.
    Thanks Damien, Churchill!

    See:
    * https://github.com/pymssql/pymssql/pull/178

- MSSQLConnection: Add `with` (context manager) support (GH-171)

  This adds `with` statement support for MSSQLConnection in the `_mssql`
  module -- e.g.::

      with mssqlconn() as conn:
          conn.execute_query("SELECT @@version AS version")

  We already have `with` statement support for the `pymssql` module.
  See:

  * https://github.com/pymssql/pymssql/pull/171

- Allow passing in binary data (GH-179)

  Use the bytesarray type added in Python 2.6 to signify that this is
  binary data and to quote it accordingly. Also modify the handling of
  str/bytes types checking the first 2 characters for b'0x' and insert
  that as binary data.
  See:

  * https://github.com/pymssql/pymssql/pull/179

- Add support for binding uuid.UUID instances to stored procedures input
  params (GH-143)
  Thanks, Ramiro Morales!

  See:
  * https://github.com/pymssql/pymssql/pull/143
  * https://github.com/pymssql/pymssql/commit/1689c83878304f735eb38b1c63c31e210b028ea7

- The version number is now stored in one place, in pymssql_version.h
  This makes it easier to update the version number and not forget any
  places, like I did with pymssql 2.0.1

  * See https://github.com/pymssql/pymssql/commit/fd317df65fa62691c2af377e4661defb721b2699

- Improved support for using py.test as test runner (GH-183)

  * See: https://github.com/pymssql/pymssql/pull/183

- Improved PEP-8 and pylint compliance

Bug Fixes
---------

- GH-142 ("Change how ``*.pyx`` files are included in package") - this
  should prevent pymssql.pyx and _mssql.pyx from getting copied into the
  root of your virtualenv. Thanks, @Arfrever!

  * See: https://github.com/pymssql/pymssql/issues/142

- GH-145 ("Prevent error string growing with repeated failed connection
  attempts.")

  See:

  * https://github.com/pymssql/pymssql/issues/145
  * https://github.com/pymssql/pymssql/pull/146

- GH-151 ("err_handler: Don't clobber dberrstr with oserrstr")

  * https://github.com/pymssql/pymssql/pull/151

- GH-152 ("_mssql.pyx: Zero init global last_msg_* vars")
  See: https://github.com/pymssql/pymssql/pull/152

- GH-177 ("binary columns sometimes are processed as varchar")
  Better mechanism for pymssql to detect that user is passing binary
  data.

  See: https://github.com/pymssql/pymssql/issues/177

- buffer overflow fix (GH-182)

  * See: https://github.com/pymssql/pymssql/pull/181
  * See: https://github.com/pymssql/pymssql/pull/182

- Return uniqueidentifer columns as uuid.UUID objects on Python 3


See `ChangeLog`_ for older history...

.. _PyPI: https://pypi.python.org/pypi/pymssql/2.0.0
.. _Travis CI: https://travis-ci.org/pymssql/pymssql
.. _Cython: http://cython.org/
.. _ChangeLog: https://github.com/pymssql/pymssql/blob/master/ChangeLog



  * [pynast-1.2.2](http://qiime.org/pynast) PyNAST: Python Nearest Alignment Space Termination tool
=======================================================


The official PyNAST source code repository. For documentation on installing and using PyNAST, see http://qiime.org/pynast. This documentation will refer to the latest release version of PyNAST. If you're working with a development version of PyNAST, you should refer to the documentation in PyNAST/doc/.

See the [QIIME GitHub organization](https://github.com/qiime) for related software projects and data.

Stay up-to-date on PyNAST news
------------------------------
Subscribing to the [QIIME blog](http://qiime.wordpress.com) is the best way to keep up-to-date on news related to PyNAST. You can subscribe via RSS or e-mail on the front page of the blog. This is a very low traffic list, with currently around one e-mail per month or less.

The QIIME blog is the primary means by which we will communicate information on bugs, new releases, and news to our users, so we highly recommend subscribing. We won't share subscriber information with anyone ever.

Citing PyNAST
-------------
If you make use of [PyNAST](http://qiime.org/pynast) in published work, please cite:

PyNAST: a flexible tool for aligning sequences to a template alignment.
J. Gregory Caporaso, Kyle Bittinger, Frederic D. Bushman, Todd Z. DeSantis, Gary L. Andersen, and Rob Knight. 
January 15, 2010, DOI 10.1093/bioinformatics/btp636. Bioinformatics 26: 266-267.

Need help?
----------
Requests for help with PyNAST should be directed to the [QIIME Forum](http://forum.qiime.org).
  * [pyodbc-4.0.27](https://github.com/mkleehammer/pyodbc) A Python DB API 2 module for ODBC. This project provides an up-to-date, convenient interface to ODBC using native data types like datetime and decimal.



  * [pypandoc-1.4](https://github.com/bebraw/pypandoc) pypandoc
========

|Build Status| |Appveyor Build Status| |GitHub Releases| |PyPI version|
|conda version| |Development Status| |Python version| |License|

Pypandoc provides a thin wrapper for `pandoc <https://pandoc.org>`__, a
universal document converter.

Installation
------------

Pypandoc uses pandoc, so it needs an available installation of pandoc.
For some common cases (wheels, conda packages), pypandoc already
includes pandoc (and pandoc-citeproc) in it's prebuilt package.

If pandoc is already installed (i.e. pandoc is in the ``PATH``),
pypandoc uses the version with the higher version number, and if both
are the same, the already installed version. See `Specifying the
location of pandoc
binaries <#specifying-the-location-of-pandoc-binaries>`__ for more.

To use pandoc filters, you must have the relevant filters installed on
your machine.

Installing via pip
~~~~~~~~~~~~~~~~~~

Install via ``pip install pypandoc``.

Prebuilt `wheels for Windows and Mac OS
X <https://pypi.python.org/pypi/pypandoc/>`__ include pandoc. If there
is no prebuilt binary available, you have to `install pandoc
yourself <#installing-pandoc-manually>`__.

If you use Linux and have `your own
wheelhouse <https://wheel.readthedocs.org/en/latest/#usage>`__, you can
build a wheel which include pandoc with
``python setup.py download_pandoc; python setup.py bdist_wheel``. Be
aware that this works only on 64bit intel systems, as we only download
it from the `official
releases <https://github.com/jgm/pandoc/releases>`__.

Installing via conda
~~~~~~~~~~~~~~~~~~~~

Pypandoc is included in
`conda-forge <https://conda-forge.github.io/>`__. The conda packages
will also install the pandoc package, so pandoc is available in the
installation.

Install via ``conda install -c conda-forge pypandoc``.

You can also add the channel to your conda config via
``conda config --add channels conda-forge``. This makes it possible to
use ``conda install pypandoc`` directly and also lets you update via
``conda update pypandoc``.

Installing pandoc
~~~~~~~~~~~~~~~~~

If you don't get pandoc installed via a prebuild wheel which includes
pandoc or via the conda package dependencies, you need to install pandoc
by yourself.

Installing pandoc via pypandoc
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Installing via pypandoc is possible on Windows, Mac OS X or Linux
(Intel-based, 64-bit):

.. code:: python

    # expects an installed pypandoc: pip install pypandoc
    from pypandoc.pandoc_download import download_pandoc
    # see the documentation how to customize the installation path
    # but be aware that you then need to include it in the `PATH`
    download_pandoc()

The default install location is included in the search path for pandoc,
so you don't need to add it to the ``PATH``.

By default, the latest pandoc version is installed. If you want to
specify your own version, say 1.19.1, use
``download_pandoc(version='1.19.1')`` instead.

Installing pandoc manually
^^^^^^^^^^^^^^^^^^^^^^^^^^

Installing manually via the system mechanism is also possible. Such
installation mechanism make pandoc available on many more platforms:

-  Ubuntu/Debian: ``sudo apt-get install pandoc``
-  Fedora/Red Hat: ``sudo yum install pandoc``
-  Arch: ``sudo pacman -S pandoc``
-  Mac OS X with Homebrew:
   ``brew install pandoc pandoc-citeproc Caskroom/cask/mactex``
-  Machine with Haskell: ``cabal-install pandoc``
-  Windows: There is an installer available
   `here <https://pandoc.org/installing.html>`__
-  `FreeBSD port <https://www.freshports.org/textproc/pandoc/>`__
-  Or see `Pandoc - Installing
   pandoc <https://pandoc.org/installing.html>`__

Be aware that not all install mechanisms put pandoc in the ``PATH``, so
you either have to change the ``PATH`` yourself or set the full ``PATH``
to pandoc in ``PYPANDOC_PANDOC``. See the next section for more
information.

Specifying the location of pandoc binaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can point to a specific pandoc version by setting the environment
variable ``PYPANDOC_PANDOC`` to the full ``PATH`` to the pandoc binary
(``PYPANDOC_PANDOC=/home/x/whatever/pandoc`` or
``PYPANDOC_PANDOC=c:\pandoc\pandoc.exe``). If this environment variable
is set, this is the only place where pandoc is searched for.

In certain cases, e.g. pandoc is installed but a web server with its own
user cannot find the binaries, it is useful to specify the location at
runtime:

.. code:: python

    import os
    os.environ.setdefault('PYPANDOC_PANDOC', '/home/x/whatever/pandoc')

Usage
-----

There are two basic ways to use pypandoc: with input files or with input
strings.

.. code:: python

    import pypandoc

    # With an input file: it will infer the input format from the filename
    output = pypandoc.convert_file('somefile.md', 'rst')

    # ...but you can overwrite the format via the `format` argument:
    output = pypandoc.convert_file('somefile.txt', 'rst', format='md')

    # alternatively you could just pass some string. In this case you need to
    # define the input format:
    output = pypandoc.convert_text('#some title', 'rst', format='md')
    # output == 'some title\r\n==========\r\n\r\n'

``convert_text`` expects this string to be unicode or utf-8 encoded
bytes. ``convert_*`` will always return a unicode string.

It's also possible to directly let pandoc write the output to a file.
This is the only way to convert to some output formats (e.g. odt, docx,
epub, epub3, pdf). In that case ``convert_*()`` will return an empty
string.

.. code:: python

    import pypandoc

    output = pypandoc.convert_file('somefile.md', 'docx', outputfile="somefile.docx")
    assert output == ""

In addition to ``format``, it is possible to pass ``extra_args``. That
makes it possible to access various pandoc options easily.

.. code:: python

    output = pypandoc.convert_text(
        '<h1>Primary Heading</h1>',
        'md', format='html',
        extra_args=['--atx-headers'])
    # output == '# Primary Heading\r\n'
    output = pypandoc.convert(
        '# Primary Heading',
        'html', format='md',
        extra_args=['--base-header-level=2'])
    # output == '<h2 id="primary-heading">Primary Heading</h2>\r\n'

pypandoc now supports easy addition of `pandoc
filters <https://pandoc.org/scripting.html>`__.

.. code:: python

    filters = ['pandoc-citeproc']
    pdoc_args = ['--mathjax',
                 '--smart']
    output = pd.convert_file(source=filename,
                             to='html5',
                             format='md',
                             extra_args=pdoc_args,
                             filters=filters)

Please pass any filters in as a list and not as a string.

Please refer to ``pandoc -h`` and the `official
documentation <https://pandoc.org/MANUAL.html>`__ for further details.

    Note: the old way of using ``convert(input, output)`` is deprecated
    as in some cases it wasn't possible to determine whether the input
    should be used as a filename or as text.

Dealing with Formatting Arguments
---------------------------------

Pandoc supports custom formatting though ``-V`` parameter. In order to
use it through pypandoc, use code such as this:

.. code:: python

    output = pypandoc.convert_file('demo.md', 'pdf', outputfile='demo.pdf',
      extra_args=['-V', 'geometry:margin=1.5cm'])

    Note: it's important to separate ``-V`` and its argument within a
    list like that or else it won't work. This gotcha has to do with the
    way
    ```subprocess.Popen`` <https://docs.python.org/2/library/subprocess.html#subprocess.Popen>`__
    works.

Getting Pandoc Version
----------------------

As it can be useful sometimes to check what pandoc version is available
at your system or which particular pandoc binary is used by pypandoc.
For that, pypandoc provides the following utility functions. Example:

::

    print(pypandoc.get_pandoc_version())
    print(pypandoc.get_pandoc_path())
    print(pypandoc.get_pandoc_formats())

Related
-------

-  `pydocverter <https://github.com/msabramo/pydocverter>`__ is a client
   for a service called `Docverter <https://www.docverter.com>`__, which
   offers pandoc as a service (plus some extra goodies).
-  See `pyandoc <https://pypi.python.org/pypi/pyandoc/>`__ for an
   alternative implementation of a pandoc wrapper from Kenneth Reitz.
   This one hasn't been active in a while though.
-  See `panflute <https://github.com/sergiocorreia/panflute>`__ which
   provides ``convert_text`` similar to pypandoc's. Its focus is on
   writing and running pandoc filters though.

Contributing
------------

Contributions are welcome. When opening a PR, please keep the following
guidelines in mind:

1. Before implementing, please open an issue for discussion.
2. Make sure you have tests for the new logic.
3. Make sure your code passes ``flake8 pypandoc/*.py tests.py``
4. Add yourself to contributors at ``README.md`` unless you are already
   there. In that case tweak your contributions.

Note that for citeproc tests to pass you'll need to have
`pandoc-citeproc <https://github.com/jgm/pandoc-citeproc>`__ installed.
If you installed a prebuilt wheel or conda package, it is already
included.

Contributors
------------

-  `Valentin Haenel <https://github.com/esc>`__ - String conversion fix
-  `Daniel Sanchez <https://github.com/ErunamoJAZZ>`__ - Automatic
   parsing of input/output formats
-  `Thomas G. <https://github.com/coldfix>`__ - Python 3 support
-  `Ben Jao Ming <https://github.com/benjaoming>`__ - Fail gracefully if
   pandoc is missing
-  `Ross Crawford-d'Heureuse <https://github.com/rosscdh>`__ - Encode
   input in UTF-8 and add Django example
-  `Michael Chow <https://github.com/machow>`__ - Decode output in UTF-8
-  `Janusz Skonieczny <https://github.com/wooyek>`__ - Support Windows
   newlines and allow encoding to be specified.
-  `gabeos <https://github.com/gabeos>`__ - Fix help parsing
-  `Marc Abramowitz <https://github.com/msabramo>`__ - Make ``setup.py``
   fail hard if pandoc is missing, Travis, Dockerfile, PyPI badge, Tox,
   PEP-8, improved documentation
-  `Daniel L. <https://github.com/mcktrtl>`__ - Add ``extra_args``
   example to README
-  `Amy Guy <https://github.com/rhiaro>`__ - Exception handling for
   unicode errors
-  `Florian Eßer <https://github.com/flesser>`__ - Allow Markdown
   extensions in output format
-  `Philipp Wendler <https://github.com/PhilippWendler>`__ - Allow
   Markdown extensions in input format
-  `Jan Schulz <https://github.com/JanSchulz>`__ - Handling output to a
   file, Travis to work on newer version of pandoc, return code
   checking, get\_pandoc\_version. Helped to fix the Travis build, new
   ``convert_*`` API
-  `Aaron Gonzales <https://github.com/xysmas>`__ - Added better filter
   handling
-  `David Lukes <https://github.com/dlukes>`__ - Enabled input from
   non-plain-text files and made sure tests clean up template files
   correctly if they fail
-  `valholl <https://github.com/valholl>`__ - Set up licensing
   information correctly and include examples to distribution version
-  `Cyrille Rossant <https://github.com/rossant>`__ - Fixed bug by
   trimming out stars in the list of pandoc formats. Helped to fix the
   Travis build.
-  `Paul Osborne <https://github.com/posborne>`__ - Don't require pandoc
   to install pypandoc.
-  `Felix Yan <https://github.com/felixonmars>`__ - Added installation
   instructions for Arch Linux.
-  `Kolen Cheung <https://github.com/ickc>`__ - Implement
   ``_get_pandoc_urls`` for installing arbitrary version as well as the
   latest version of pandoc. Minor: README, Travis, setup.py.

License
-------

Pypandoc is available under MIT license. See LICENSE for more details.
Pandoc itself is `available under the GPL2
license <https://github.com/jgm/pandoc/blob/master/COPYING.md>`__.

.. |Build Status| image:: https://travis-ci.org/bebraw/pypandoc.svg?branch=master
   :target: https://travis-ci.org/bebraw/pypandoc
.. |Appveyor Build Status| image:: https://ci.appveyor.com/api/projects/status/github/bebraw/pypandoc?svg=true
   :target: https://ci.appveyor.com/project/bebraw/pypandoc
.. |GitHub Releases| image:: https://img.shields.io/github/tag/bebraw/pypandoc.svg?label=github+release
   :target: https://github.com/bebraw/pypandoc/releases
.. |PyPI version| image:: https://badge.fury.io/py/pypandoc.svg
   :target: https://pypi.python.org/pypi/pypandoc/
.. |conda version| image:: https://anaconda.org/conda-forge/pypandoc/badges/version.svg
   :target: https://anaconda.org/conda-forge/pypandoc/
.. |Development Status| image:: https://img.shields.io/pypi/status/pypandoc.svg
   :target: https://pypi.python.org/pypi/pypandoc/
.. |Python version| image:: https://img.shields.io/pypi/pyversions/pypandoc.svg
   :target: https://pypi.python.org/pypi/pypandoc/
.. |License| image:: https://img.shields.io/pypi/l/pypandoc.svg
  * [pyparsing-2.2.0](https://github.com/pyparsing/pyparsing/) 
  * [pyperclip-1.7.0](https://github.com/asweigart/pyperclip) Pyperclip is a cross-platform Python module for copy and paste clipboard functions. It works with Python 2 and 3.

`pip install pyperclip`

Al Sweigart al@inventwithpython.com
BSD License

Example Usage
=============

    >>> import pyperclip
    >>> pyperclip.copy('The text to be copied to the clipboard.')
    >>> pyperclip.paste()
    'The text to be copied to the clipboard.'


Currently only handles plaintext.

On Windows, no additional modules are needed.

On Mac, this module makes use of the pbcopy and pbpaste commands, which should come with the os.

On Linux, this module makes use of the xclip or xsel commands, which should come with the os. Otherwise run "sudo apt-get install xclip" or "sudo apt-get install xsel" (Note: xsel does not always seem to work.)

Otherwise on Linux, you will need the gtk or PyQt4 modules installed.
  * [pyrsistent-0.15.4](http://github.com/tobgu/pyrsistent/) Pyrsistent
==========
.. image:: https://travis-ci.org/tobgu/pyrsistent.png?branch=master
    :target: https://travis-ci.org/tobgu/pyrsistent

.. image:: https://badge.fury.io/py/pyrsistent.svg
    :target: https://badge.fury.io/py/pyrsistent

.. image:: https://coveralls.io/repos/tobgu/pyrsistent/badge.svg?branch=master&service=github
    :target: https://coveralls.io/github/tobgu/pyrsistent?branch=master


.. _Pyrthon: https://www.github.com/tobgu/pyrthon/

Pyrsistent is a number of persistent collections (by some referred to as functional data structures). Persistent in 
the sense that they are immutable.

All methods on a data structure that would normally mutate it instead return a new copy of the structure containing the
requested updates. The original structure is left untouched.

This will simplify the reasoning about what a program does since no hidden side effects ever can take place to these
data structures. You can rest assured that the object you hold a reference to will remain the same throughout its
lifetime and need not worry that somewhere five stack levels below you in the darkest corner of your application
someone has decided to remove that element that you expected to be there.

Pyrsistent is influenced by persistent data structures such as those found in the standard library of Clojure. The
data structures are designed to share common elements through path copying.
It aims at taking these concepts and make them as pythonic as possible so that they can be easily integrated into any python
program without hassle.

If you want to go all in on persistent data structures and use literal syntax to define them in your code rather
than function calls check out Pyrthon_.

Examples
--------
.. _Sequence: collections_
.. _Hashable: collections_
.. _Mapping: collections_
.. _Mappings: collections_
.. _Set: collections_
.. _collections: https://docs.python.org/3/library/collections.abc.html
.. _documentation: http://pyrsistent.readthedocs.org/

The collection types and key features currently implemented are:

* PVector_, similar to a python list
* PMap_, similar to dict
* PSet_, similar to set
* PRecord_, a PMap on steroids with fixed fields, optional type and invariant checking and much more
* PClass_, a Python class fixed fields, optional type and invariant checking and much more
* `Checked collections`_, PVector, PMap and PSet with optional type and invariance checks and more
* PBag, similar to collections.Counter
* PList, a classic singly linked list
* PDeque, similar to collections.deque
* Immutable object type (immutable) built on the named tuple
* freeze_ and thaw_ functions to convert between pythons standard collections and pyrsistent collections.
* Flexible transformations_ of arbitrarily complex structures built from PMaps and PVectors.

Below are examples of common usage patterns for some of the structures and features. More information and
full documentation for all data structures is available in the documentation_.

.. _PVector:

PVector
~~~~~~~
With full support for the Sequence_ protocol PVector is meant as a drop in replacement to the built in list from a readers
point of view. Write operations of course differ since no in place mutation is done but naming should be in line
with corresponding operations on the built in list.

Support for the Hashable_ protocol also means that it can be used as key in Mappings_.

Appends are amortized O(1). Random access and insert is log32(n) where n is the size of the vector.

.. code:: python

    >>> from pyrsistent import v, pvector

    # No mutation of vectors once created, instead they
    # are "evolved" leaving the original untouched
    >>> v1 = v(1, 2, 3)
    >>> v2 = v1.append(4)
    >>> v3 = v2.set(1, 5)
    >>> v1
    pvector([1, 2, 3])
    >>> v2
    pvector([1, 2, 3, 4])
    >>> v3
    pvector([1, 5, 3, 4])

    # Random access and slicing
    >>> v3[1]
    5
    >>> v3[1:3]
    pvector([5, 3])

    # Iteration
    >>> list(x + 1 for x in v3)
    [2, 6, 4, 5]
    >>> pvector(2 * x for x in range(3))
    pvector([0, 2, 4])

.. _PMap:

PMap
~~~~
With full support for the Mapping_ protocol PMap is meant as a drop in replacement to the built in dict from a readers point
of view. Support for the Hashable_ protocol also means that it can be used as key in other Mappings_.

Random access and insert is log32(n) where n is the size of the map.

.. code:: python

    >>> from pyrsistent import m, pmap, v

    # No mutation of maps once created, instead they are
    # "evolved" leaving the original untouched
    >>> m1 = m(a=1, b=2)
    >>> m2 = m1.set('c', 3)
    >>> m3 = m2.set('a', 5)
    >>> m1
    pmap({'a': 1, 'b': 2})
    >>> m2
    pmap({'a': 1, 'c': 3, 'b': 2})
    >>> m3
    pmap({'a': 5, 'c': 3, 'b': 2})
    >>> m3['a']
    5

    # Evolution of nested persistent structures
    >>> m4 = m(a=5, b=6, c=v(1, 2))
    >>> m4.transform(('c', 1), 17)
    pmap({'a': 5, 'c': pvector([1, 17]), 'b': 6})
    >>> m5 = m(a=1, b=2)

    # Evolve by merging with other mappings
    >>> m5.update(m(a=2, c=3), {'a': 17, 'd': 35})
    pmap({'a': 17, 'c': 3, 'b': 2, 'd': 35})
    >>> pmap({'x': 1, 'y': 2}) + pmap({'y': 3, 'z': 4})
    pmap({'y': 3, 'x': 1, 'z': 4})

    # Dict-like methods to convert to list and iterate
    >>> m3.items()
    pvector([('a', 5), ('c', 3), ('b', 2)])
    >>> list(m3)
    ['a', 'c', 'b']

.. _PSet:

PSet
~~~~
With full support for the Set_ protocol PSet is meant as a drop in replacement to the built in set from a readers point
of view. Support for the Hashable_ protocol also means that it can be used as key in Mappings_.

Random access and insert is log32(n) where n is the size of the set.

.. code:: python

    >>> from pyrsistent import s

    # No mutation of sets once created, you know the story...
    >>> s1 = s(1, 2, 3, 2)
    >>> s2 = s1.add(4)
    >>> s3 = s1.remove(1)
    >>> s1
    pset([1, 2, 3])
    >>> s2
    pset([1, 2, 3, 4])
    >>> s3
    pset([2, 3])

    # Full support for set operations
    >>> s1 | s(3, 4, 5)
    pset([1, 2, 3, 4, 5])
    >>> s1 & s(3, 4, 5)
    pset([3])
    >>> s1 < s2
    True
    >>> s1 < s(3, 4, 5)
    False

.. _PRecord:

PRecord
~~~~~~~
A PRecord is a PMap with a fixed set of specified fields. Records are declared as python classes inheriting
from PRecord. Because it is a PMap it has full support for all Mapping methods such as iteration and element
access using subscript notation.

.. code:: python

    >>> from pyrsistent import PRecord, field
    >>> class ARecord(PRecord):
    ...     x = field()
    ...
    >>> r = ARecord(x=3)
    >>> r
    ARecord(x=3)
    >>> r.x
    3
    >>> r.set(x=2)
    ARecord(x=2)
    >>> r.set(y=2)
    Traceback (most recent call last):
    AttributeError: 'y' is not among the specified fields for ARecord

Type information
****************
It is possible to add type information to the record to enforce type checks. Multiple allowed types can be specified
by providing an iterable of types.

.. code:: python

    >>> class BRecord(PRecord):
    ...     x = field(type=int)
    ...     y = field(type=(int, type(None)))
    ...
    >>> BRecord(x=3, y=None)
    BRecord(y=None, x=3)
    >>> BRecord(x=3.0)
    Traceback (most recent call last):
    PTypeError: Invalid type for field BRecord.x, was float


Custom types (classes) that are iterable should be wrapped in a tuple to prevent their
members being added to the set of valid types.  Although Enums in particular are now 
supported without wrapping, see #83 for more information.

Mandatory fields
****************
Fields are not mandatory by default but can be specified as such. If fields are missing an
*InvariantException* will be thrown which contains information about the missing fields.

.. code:: python

    >>> from pyrsistent import InvariantException
    >>> class CRecord(PRecord):
    ...     x = field(mandatory=True)
    ...
    >>> r = CRecord(x=3)
    >>> try:
    ...    r.discard('x')
    ... except InvariantException as e:
    ...    print(e.missing_fields)
    ...
    ('CRecord.x',)

Invariants
**********
It is possible to add invariants that must hold when evolving the record. Invariants can be
specified on both field and record level. If invariants fail an *InvariantException* will be
thrown which contains information about the failing invariants. An invariant function should
return a tuple consisting of a boolean that tells if the invariant holds or not and an object
describing the invariant. This object can later be used to identify which invariant that failed.

The global invariant function is only executed if all field invariants hold.

Global invariants are inherited to subclasses.

.. code:: python

    >>> class RestrictedVector(PRecord):
    ...     __invariant__ = lambda r: (r.y >= r.x, 'x larger than y')
    ...     x = field(invariant=lambda x: (x > 0, 'x negative'))
    ...     y = field(invariant=lambda y: (y > 0, 'y negative'))
    ...
    >>> r = RestrictedVector(y=3, x=2)
    >>> try:
    ...    r.set(x=-1, y=-2)
    ... except InvariantException as e:
    ...    print(e.invariant_errors)
    ...
    ('y negative', 'x negative')
    >>> try:
    ...    r.set(x=2, y=1)
    ... except InvariantException as e:
    ...    print(e.invariant_errors)
    ...
    ('x larger than y',)

Invariants may also contain multiple assertions. For those cases the invariant function should
return a tuple of invariant tuples as described above. This structure is reflected in the
invariant_errors attribute of the exception which will contain tuples with data from all failed
invariants. Eg:

.. code:: python

    >>> class EvenX(PRecord):
    ...     x = field(invariant=lambda x: ((x > 0, 'x negative'), (x % 2 == 0, 'x odd')))
    ...
    >>> try:
    ...    EvenX(x=-1)
    ... except InvariantException as e:
    ...    print(e.invariant_errors)
    ...
    (('x negative', 'x odd'),)


Factories
*********
It's possible to specify factory functions for fields. The factory function receives whatever
is supplied as field value and the actual returned by the factory is assigned to the field
given that any type and invariant checks hold.
PRecords have a default factory specified as a static function on the class, create(). It takes
a *Mapping* as argument and returns an instance of the specific record.
If a record has fields of type PRecord the create() method of that record will
be called to create the "sub record" if no factory has explicitly been specified to override
this behaviour.

.. code:: python

    >>> class DRecord(PRecord):
    ...     x = field(factory=int)
    ...
    >>> class ERecord(PRecord):
    ...     d = field(type=DRecord)
    ...
    >>> ERecord.create({'d': {'x': '1'}})
    ERecord(d=DRecord(x=1))

Collection fields
*****************
It is also possible to have fields with ``pyrsistent`` collections.

.. code:: python

   >>> from pyrsistent import pset_field, pmap_field, pvector_field
   >>> class MultiRecord(PRecord):
   ...     set_of_ints = pset_field(int)
   ...     map_int_to_str = pmap_field(int, str)
   ...     vector_of_strs = pvector_field(str)
   ...
	
Serialization
*************
PRecords support serialization back to dicts. Default serialization will take keys and values
"as is" and output them into a dict. It is possible to specify custom serialization functions
to take care of fields that require special treatment.

.. code:: python

    >>> from datetime import date
    >>> class Person(PRecord):
    ...     name = field(type=unicode)
    ...     birth_date = field(type=date,
    ...                        serializer=lambda format, d: d.strftime(format['date']))
    ...
    >>> john = Person(name=u'John', birth_date=date(1985, 10, 21))
    >>> john.serialize({'date': '%Y-%m-%d'})
    {'birth_date': '1985-10-21', 'name': u'John'}


.. _instar: https://github.com/boxed/instar/

.. _PClass:

PClass
~~~~~~
A PClass is a python class with a fixed set of specified fields. PClasses are declared as python classes inheriting
from PClass. It is defined the same way that PRecords are and behaves like a PRecord in all aspects except that it
is not a PMap and hence not a collection but rather a plain Python object.

.. code:: python

    >>> from pyrsistent import PClass, field
    >>> class AClass(PClass):
    ...     x = field()
    ...
    >>> a = AClass(x=3)
    >>> a
    AClass(x=3)
    >>> a.x
    3


Checked collections
~~~~~~~~~~~~~~~~~~~
Checked collections currently come in three flavors: CheckedPVector, CheckedPMap and CheckedPSet.

.. code:: python

    >>> from pyrsistent import CheckedPVector, CheckedPMap, CheckedPSet, thaw
    >>> class Positives(CheckedPSet):
    ...     __type__ = (long, int)
    ...     __invariant__ = lambda n: (n >= 0, 'Negative')
    ...
    >>> class Lottery(PRecord):
    ...     name = field(type=str)
    ...     numbers = field(type=Positives, invariant=lambda p: (len(p) > 0, 'No numbers'))
    ...
    >>> class Lotteries(CheckedPVector):
    ...     __type__ = Lottery
    ...
    >>> class LotteriesByDate(CheckedPMap):
    ...     __key_type__ = date
    ...     __value_type__ = Lotteries
    ...
    >>> lotteries = LotteriesByDate.create({date(2015, 2, 15): [{'name': 'SuperLotto', 'numbers': {1, 2, 3}},
    ...                                                         {'name': 'MegaLotto',  'numbers': {4, 5, 6}}],
    ...                                     date(2015, 2, 16): [{'name': 'SuperLotto', 'numbers': {3, 2, 1}},
    ...                                                         {'name': 'MegaLotto',  'numbers': {6, 5, 4}}]})
    >>> lotteries
    LotteriesByDate({datetime.date(2015, 2, 15): Lotteries([Lottery(numbers=Positives([1, 2, 3]), name='SuperLotto'), Lottery(numbers=Positives([4, 5, 6]), name='MegaLotto')]), datetime.date(2015, 2, 16): Lotteries([Lottery(numbers=Positives([1, 2, 3]), name='SuperLotto'), Lottery(numbers=Positives([4, 5, 6]), name='MegaLotto')])})

    # The checked versions support all operations that the corresponding
    # unchecked types do
    >>> lottery_0215 = lotteries[date(2015, 2, 15)]
    >>> lottery_0215.transform([0, 'name'], 'SuperDuperLotto')
    Lotteries([Lottery(numbers=Positives([1, 2, 3]), name='SuperDuperLotto'), Lottery(numbers=Positives([4, 5, 6]), name='MegaLotto')])

    # But also makes asserts that types and invariants hold
    >>> lottery_0215.transform([0, 'name'], 999)
    Traceback (most recent call last):
    PTypeError: Invalid type for field Lottery.name, was int

    >>> lottery_0215.transform([0, 'numbers'], set())
    Traceback (most recent call last):
    InvariantException: Field invariant failed

    # They can be converted back to python built ins with either thaw()
    # or serialize() (which provides possibilities to customize serialization)
    >>> thaw(lottery_0215)
    [{'numbers': set([1, 2, 3]), 'name': 'SuperLotto'}, {'numbers': set([4, 5, 6]), 'name': 'MegaLotto'}]
    >>> lottery_0215.serialize()
    [{'numbers': set([1, 2, 3]), 'name': 'SuperLotto'}, {'numbers': set([4, 5, 6]), 'name': 'MegaLotto'}]

.. _transformations:

Transformations
~~~~~~~~~~~~~~~
Transformations are inspired by the cool library instar_ for Clojure. They let you evolve PMaps and PVectors
with arbitrarily deep/complex nesting using simple syntax and flexible matching syntax.

The first argument to transformation is the path that points out the value to transform. The
second is the transformation to perform. If the transformation is callable it will be applied
to the value(s) matching the path. The path may also contain callables. In that case they are
treated as matchers. If the matcher returns True for a specific key it is considered for transformation.

.. code:: python

    # Basic examples
    >>> from pyrsistent import inc, freeze, thaw, rex, ny, discard
    >>> v1 = freeze([1, 2, 3, 4, 5])
    >>> v1.transform([2], inc)
    pvector([1, 2, 4, 4, 5])
    >>> v1.transform([lambda ix: 0 < ix < 4], 8)
    pvector([1, 8, 8, 8, 5])
    >>> v1.transform([lambda ix, v: ix == 0 or v == 5], 0)
    pvector([0, 2, 3, 4, 0])

    # The (a)ny matcher can be used to match anything
    >>> v1.transform([ny], 8)
    pvector([8, 8, 8, 8, 8])

    # Regular expressions can be used for matching
    >>> scores = freeze({'John': 12, 'Joseph': 34, 'Sara': 23})
    >>> scores.transform([rex('^Jo')], 0)
    pmap({'Joseph': 0, 'Sara': 23, 'John': 0})

    # Transformations can be done on arbitrarily deep structures
    >>> news_paper = freeze({'articles': [{'author': 'Sara', 'content': 'A short article'},
    ...                                   {'author': 'Steve', 'content': 'A slightly longer article'}],
    ...                      'weather': {'temperature': '11C', 'wind': '5m/s'}})
    >>> short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:25] + '...' if len(c) > 25 else c)
    >>> very_short_news = news_paper.transform(['articles', ny, 'content'], lambda c: c[:15] + '...' if len(c) > 15 else c)
    >>> very_short_news.articles[0].content
    'A short article'
    >>> very_short_news.articles[1].content
    'A slightly long...'

    # When nothing has been transformed the original data structure is kept
    >>> short_news is news_paper
    True
    >>> very_short_news is news_paper
    False
    >>> very_short_news.articles[0] is news_paper.articles[0]
    True

    # There is a special transformation that can be used to discard elements. Also
    # multiple transformations can be applied in one call
    >>> thaw(news_paper.transform(['weather'], discard, ['articles', ny, 'content'], discard))
    {'articles': [{'author': 'Sara'}, {'author': 'Steve'}]}

Evolvers
~~~~~~~~
PVector, PMap and PSet all have support for a concept dubbed *evolvers*. An evolver acts like a mutable
view of the underlying persistent data structure with "transaction like" semantics. No updates of the original
data structure is ever performed, it is still fully immutable.

The evolvers have a very limited API by design to discourage excessive, and inappropriate, usage as that would
take us down the mutable road. In principle only basic mutation and element access functions are supported.
Check out the documentation_ of each data structure for specific examples.

Examples of when you may want to use an evolver instead of working directly with the data structure include:

* Multiple updates are done to the same data structure and the intermediate results are of no
  interest. In this case using an evolver may be a more efficient and easier to work with.
* You need to pass a vector into a legacy function or a function that you have no control
  over which performs in place mutations. In this case pass an evolver instance
  instead and then create a new pvector from the evolver once the function returns.

.. code:: python

    >>> from pyrsistent import v

    # In place mutation as when working with the built in counterpart
    >>> v1 = v(1, 2, 3)
    >>> e = v1.evolver()
    >>> e[1] = 22
    >>> e = e.append(4)
    >>> e = e.extend([5, 6])
    >>> e[5] += 1
    >>> len(e)
    6

    # The evolver is considered *dirty* when it contains changes compared to the underlying vector
    >>> e.is_dirty()
    True

    # But the underlying pvector still remains untouched
    >>> v1
    pvector([1, 2, 3])

    # Once satisfied with the updates you can produce a new pvector containing the updates.
    # The new pvector will share data with the original pvector in the same way that would have
    # been done if only using operations on the pvector.
    >>> v2 = e.persistent()
    >>> v2
    pvector([1, 22, 3, 4, 5, 7])

    # The evolver is now no longer considered *dirty* as it contains no differences compared to the
    # pvector just produced.
    >>> e.is_dirty()
    False

    # You may continue to work with the same evolver without affecting the content of v2
    >>> e[0] = 11

    # Or create a new evolver from v2. The two evolvers can be updated independently but will both
    # share data with v2 where possible.
    >>> e2 = v2.evolver()
    >>> e2[0] = 1111
    >>> e.persistent()
    pvector([11, 22, 3, 4, 5, 7])
    >>> e2.persistent()
    pvector([1111, 22, 3, 4, 5, 7])

.. _freeze:
.. _thaw:

freeze and thaw
~~~~~~~~~~~~~~~
These functions are great when your cozy immutable world has to interact with the evil mutable world outside.

.. code:: python

    >>> from pyrsistent import freeze, thaw, v, m
    >>> freeze([1, {'a': 3}])
    pvector([1, pmap({'a': 3})])
    >>> thaw(v(1, m(a=3)))
    [1, {'a': 3}]

Compatibility
-------------

Pyrsistent is developed and tested on Python 2.7, 3.5, 3.6, 3.7 and PyPy (Python 2 and 3 compatible). It will most
likely work on all other versions >= 3.4 but no guarantees are given. :)

Compatibility issues
~~~~~~~~~~~~~~~~~~~~

.. _27: https://github.com/tobgu/pyrsistent/issues/27

There is currently one known compatibility issue when comparing built in sets and frozensets to PSets as discussed in 27_.
It affects python 2 versions < 2.7.8 and python 3 versions < 3.4.0 and is due to a bug described in
http://bugs.python.org/issue8743.

Comparisons will fail or be incorrect when using the set/frozenset as left hand side of the comparison. As a workaround
you need to either upgrade Python to a more recent version, avoid comparing sets/frozensets with PSets or always make
sure to convert both sides of the comparison to the same type before performing the comparison.

Performance
-----------

Pyrsistent is developed with performance in mind. Still, while some operations are nearly on par with their built in, 
mutable, counterparts in terms of speed, other operations are slower. In the cases where attempts at
optimizations have been done, speed has generally been valued over space.

Pyrsistent comes with two API compatible flavors of PVector (on which PMap and PSet are based), one pure Python 
implementation and one implemented as a C extension. The latter generally being 2 - 20 times faster than the former.
The C extension will be used automatically when possible.

The pure python implementation is fully PyPy compatible. Running it under PyPy speeds operations up considerably if 
the structures are used heavily (if JITed), for some cases the performance is almost on par with the built in counterparts.

Type hints
----------

PEP 561 style type hints for use with mypy and various editors are available for most types and functions in pyrsistent.

Type classes for annotating your own code with pyrsistent types are also available under pyrsistent.typing.

Installation
------------

pip install pyrsistent

Documentation
-------------

Available at http://pyrsistent.readthedocs.org/

Brief presentation available at http://slides.com/tobiasgustafsson/immutability-and-python/

Contributors
------------

Tobias Gustafsson https://github.com/tobgu

Christopher Armstrong https://github.com/radix

Anders Hovmöller https://github.com/boxed

Itamar Turner-Trauring https://github.com/itamarst

Jonathan Lange https://github.com/jml

Richard Futrell https://github.com/Futrell

Jakob Hollenstein https://github.com/jkbjh

David Honour https://github.com/foolswood

David R. MacIver https://github.com/DRMacIver

Marcus Ewert https://github.com/sarum90

Jean-Paul Calderone https://github.com/exarkun

Douglas Treadwell https://github.com/douglas-treadwell

Travis Parker https://github.com/teepark

Julian Berman https://github.com/Julian

Dennis Tomas https://github.com/dtomas

Neil Vyas https://github.com/neilvyas

doozr https://github.com/doozr

Kamil Galuszka https://github.com/galuszkak

Tsuyoshi Hombashi https://github.com/thombashi

nattofriends https://github.com/nattofriends

agberk https://github.com/agberk

Waleed Khan https://github.com/arxanas

Jean-Louis Fuchs https://github.com/ganwell

Carlos Corbacho https://github.com/ccorbacho

Felix Yan https://github.com/felixonmars

benrg https://github.com/benrg

Jere Lahelma https://github.com/je-l

Max Taggart https://github.com/MaxTaggart

Vincent Philippon https://github.com/vphilippon

Semen Zhydenko https://github.com/ss18

Till Varoquaux  https://github.com/till-varoquaux

Contributing
------------

Want to contribute? That's great! If you experience problems please log them on GitHub. If you want to contribute code,
please fork the repository and submit a pull request.

Run tests
~~~~~~~~~
.. _tox: https://tox.readthedocs.io/en/latest/

Tests can be executed using tox_.

Install tox: ``pip install tox``

Run test for Python 2.7: ``tox -epy27``

Release
~~~~~~~
* Update CHANGES.txt
* Update README with any new contributors and potential info needed.
* Update _pyrsistent_version.py
* python setup.py sdist upload
* Commit and tag with new version: git add -u . && git commit -m 'Prepare version vX.Y.Z' && git tag -a vX.Y.Z -m 'vX.Y.Z'
* Push commit and tags: git push && git push --tags

Project status
--------------
Pyrsistent can be considered stable and mature (who knows, there may even be a 1.0 some day :-)). The project is
maintained, bugs fixed, PRs reviewed and merged and new releases made. I currently do not have time for development
of new features or functionality which I don't have use for myself. I'm more than happy to take PRs for new
functionality though!

There are a bunch of issues marked with ``enhancement`` and ``help wanted`` that contain requests for new functionality
that would be nice to include. The level of difficulty and extend of the issues varies, please reach out to me if you're
interested in working on any of them.

If you feel that you have a grand master plan for where you would like Pyrsistent to go and have the time to put into
it please don't hesitate to discuss this with me and submit PRs for it. If all goes well I'd be more than happy to add
additional maintainers to the project!

  * [pyserial-3.4](https://github.com/pyserial/pyserial) Python Serial Port Extension for Win32, OSX, Linux, BSD, Jython, IronPython

Stable:

- Documentation: http://pythonhosted.org/pyserial/
- Download Page: https://pypi.python.org/pypi/pyserial

Latest:

- Documentation: http://pyserial.readthedocs.io/en/latest/
- Project Homepage: https://github.com/pyserial/pyserial



  * pytables 3.4.4
  * [pytest-5.0.1](https://docs.pytest.org/en/latest/) .. image:: https://docs.pytest.org/en/latest/_static/pytest1.png
   :target: https://docs.pytest.org/en/latest/
   :align: center
   :alt: pytest


------

.. image:: https://img.shields.io/pypi/v/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg
    :target: https://anaconda.org/conda-forge/pytest

.. image:: https://img.shields.io/pypi/pyversions/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://codecov.io/gh/pytest-dev/pytest/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/pytest-dev/pytest
    :alt: Code coverage Status

.. image:: https://travis-ci.org/pytest-dev/pytest.svg?branch=master
    :target: https://travis-ci.org/pytest-dev/pytest

.. image:: https://dev.azure.com/pytest-dev/pytest/_apis/build/status/pytest-CI?branchName=master
    :target: https://dev.azure.com/pytest-dev/pytest

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black

.. image:: https://www.codetriage.com/pytest-dev/pytest/badges/users.svg
    :target: https://www.codetriage.com/pytest-dev/pytest

The ``pytest`` framework makes it easy to write small tests, yet
scales to support complex functional testing for applications and libraries.

An example of a simple test:

.. code-block:: python

    # content of test_sample.py
    def inc(x):
        return x + 1


    def test_answer():
        assert inc(3) == 5


To execute it::

    $ pytest
    ============================= test session starts =============================
    collected 1 items

    test_sample.py F

    ================================== FAILURES ===================================
    _________________________________ test_answer _________________________________

        def test_answer():
    >       assert inc(3) == 5
    E       assert 4 == 5
    E        +  where 4 = inc(3)

    test_sample.py:5: AssertionError
    ========================== 1 failed in 0.04 seconds ===========================


Due to ``pytest``'s detailed assertion introspection, only plain ``assert`` statements are used. See `getting-started <https://docs.pytest.org/en/latest/getting-started.html#our-first-test-run>`_ for more examples.


Features
--------

- Detailed info on failing `assert statements <https://docs.pytest.org/en/latest/assert.html>`_ (no need to remember ``self.assert*`` names);

- `Auto-discovery
  <https://docs.pytest.org/en/latest/goodpractices.html#python-test-discovery>`_
  of test modules and functions;

- `Modular fixtures <https://docs.pytest.org/en/latest/fixture.html>`_ for
  managing small or parametrized long-lived test resources;

- Can run `unittest <https://docs.pytest.org/en/latest/unittest.html>`_ (or trial),
  `nose <https://docs.pytest.org/en/latest/nose.html>`_ test suites out of the box;

- Python 3.5+ and PyPy3;

- Rich plugin architecture, with over 315+ `external plugins <http://plugincompat.herokuapp.com>`_ and thriving community;


Documentation
-------------

For full documentation, including installation, tutorials and PDF documents, please see https://docs.pytest.org/en/latest/.


Bugs/Requests
-------------

Please use the `GitHub issue tracker <https://github.com/pytest-dev/pytest/issues>`_ to submit bugs or request features.


Changelog
---------

Consult the `Changelog <https://docs.pytest.org/en/latest/changelog.html>`__ page for fixes and enhancements of each version.


Support pytest
--------------

You can support pytest by obtaining a `Tidelift subscription`_.

Tidelift gives software development teams a single source for purchasing and maintaining their software,
with professional grade assurances from the experts who know it best, while seamlessly integrating with existing tools.


.. _`Tidelift subscription`: https://tidelift.com/subscription/pkg/pypi-pytest?utm_source=pypi-pytest&utm_medium=referral&utm_campaign=readme


Security
^^^^^^^^

pytest has never been associated with a security vunerability, but in any case, to report a
security vulnerability please use the `Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.


License
-------

Copyright Holger Krekel and others, 2004-2019.

Distributed under the terms of the `MIT`_ license, pytest is free and open source software.

.. _`MIT`: https://github.com/pytest-dev/pytest/blob/master/LICENSE



  * [pytest-checkdocs-1.2.0](https://github.com/jaraco/pytest-checkdocs) .. image:: https://img.shields.io/pypi/v/pytest-checkdocs.svg
   :target: https://pypi.org/project/pytest-checkdocs

.. image:: https://img.shields.io/pypi/pyversions/pytest-checkdocs.svg

.. image:: https://img.shields.io/travis/jaraco/pytest-checkdocs/master.svg
   :target: https://travis-ci.org/jaraco/pytest-checkdocs

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
   :alt: Code style: Black

.. .. image:: https://img.shields.io/appveyor/ci/jaraco/pytest-checkdocs/master.svg
..    :target: https://ci.appveyor.com/project/jaraco/pytest-checkdocs/branch/master

.. .. image:: https://readthedocs.org/projects/pytest-checkdocs/badge/?version=latest
..    :target: https://pytest-checkdocs.readthedocs.io/en/latest/?badge=latest


A pytest plugin that checks the long description of the project to ensure it
renders properly.



  * [pytest-cov-2.7.1](https://github.com/pytest-dev/pytest-cov) ========
Overview
========

.. start-badges

.. list-table::
    :stub-columns: 1

    * - docs
      - |docs|
    * - tests
      - | |travis| |appveyor| |requires|
    * - package
      - | |version| |conda-forge| |wheel| |supported-versions| |supported-implementations|
        | |commits-since|

.. |docs| image:: https://readthedocs.org/projects/pytest-cov/badge/?style=flat
    :target: https://readthedocs.org/projects/pytest-cov
    :alt: Documentation Status

.. |travis| image:: https://travis-ci.org/pytest-dev/pytest-cov.svg?branch=master
    :alt: Travis-CI Build Status
    :target: https://travis-ci.org/pytest-dev/pytest-cov

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/pytest-dev/pytest-cov?branch=master&svg=true
    :alt: AppVeyor Build Status
    :target: https://ci.appveyor.com/project/pytestbot/pytest-cov

.. |requires| image:: https://requires.io/github/pytest-dev/pytest-cov/requirements.svg?branch=master
    :alt: Requirements Status
    :target: https://requires.io/github/pytest-dev/pytest-cov/requirements/?branch=master

.. |version| image:: https://img.shields.io/pypi/v/pytest-cov.svg
    :alt: PyPI Package latest release
    :target: https://pypi.python.org/pypi/pytest-cov

.. |conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/pytest-cov.svg
    :target: https://anaconda.org/conda-forge/pytest-cov

.. |commits-since| image:: https://img.shields.io/github/commits-since/pytest-dev/pytest-cov/v2.8.1.svg
    :alt: Commits since latest release
    :target: https://github.com/pytest-dev/pytest-cov/compare/v2.8.1...master

.. |wheel| image:: https://img.shields.io/pypi/wheel/pytest-cov.svg
    :alt: PyPI Wheel
    :target: https://pypi.python.org/pypi/pytest-cov

.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/pytest-cov.svg
    :alt: Supported versions
    :target: https://pypi.python.org/pypi/pytest-cov

.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/pytest-cov.svg
    :alt: Supported implementations
    :target: https://pypi.python.org/pypi/pytest-cov

.. end-badges

This plugin produces coverage reports. Compared to just using ``coverage run`` this plugin does some extras:

* Subprocess support: you can fork or run stuff in a subprocess and will get covered without any fuss.
* Xdist support: you can use all of pytest-xdist's features and still get coverage.
* Consistent pytest behavior. If you run ``coverage run -m pytest`` you will have slightly different ``sys.path`` (CWD will be
  in it, unlike when running ``pytest``).

All features offered by the coverage package should work, either through pytest-cov's command line options or
through coverage's config file.

* Free software: MIT license

Installation
============

Install with pip::

    pip install pytest-cov

For distributed testing support install pytest-xdist::

    pip install pytest-xdist

Upgrading from ancient pytest-cov
---------------------------------

`pytest-cov 2.0` is using a new ``.pth`` file (``pytest-cov.pth``). You may want to manually remove the older
``init_cov_core.pth`` from site-packages as it's not automatically removed.

Uninstalling
------------

Uninstall with pip::

    pip uninstall pytest-cov

Under certain scenarios a stray ``.pth`` file may be left around in site-packages.

* `pytest-cov 2.0` may leave a ``pytest-cov.pth`` if you installed without wheels
  (``easy_install``, ``setup.py install`` etc).
* `pytest-cov 1.8 or older` will leave a ``init_cov_core.pth``.

Usage
=====

::

    pytest --cov=myproj tests/

Would produce a report like::

    -------------------- coverage: ... ---------------------
    Name                 Stmts   Miss  Cover
    ----------------------------------------
    myproj/__init__          2      0   100%
    myproj/myproj          257     13    94%
    myproj/feature4286      94      7    92%
    ----------------------------------------
    TOTAL                  353     20    94%

Documentation
=============

    http://pytest-cov.rtfd.org/






Coverage Data File
==================

The data file is erased at the beginning of testing to ensure clean data for each test run. If you
need to combine the coverage of several test runs you can use the ``--cov-append`` option to append
this coverage data to coverage data from previous test runs.

The data file is left at the end of testing so that it is possible to use normal coverage tools to
examine it.

Limitations
===========

For distributed testing the workers must have the pytest-cov package installed.  This is needed since
the plugin must be registered through setuptools for pytest to start the plugin on the
worker.

For subprocess measurement environment variables must make it from the main process to the
subprocess.  The python used by the subprocess must have pytest-cov installed.  The subprocess must
do normal site initialisation so that the environment variables can be detected and coverage
started.


Acknowledgements
================

Whilst this plugin has been built fresh from the ground up it has been influenced by the work done
on pytest-coverage (Ross Lawley, James Mills, Holger Krekel) and nose-cover (Jason Pellerin) which are
other coverage plugins.

Ned Batchelder for coverage and its ability to combine the coverage results of parallel runs.

Holger Krekel for pytest with its distributed testing support.

Jason Pellerin for nose.

Michael Foord for unittest2.

No doubt others have contributed to these tools as well.

Changelog
=========

2.8.1 (2019-10-05)
------------------

* Fixed `#348 <https://github.com/pytest-dev/pytest-cov/issues/348>`_ -
  regression when only certain reports (html or xml) are used then ``--cov-fail-under`` always fails.

2.8.0 (2019-10-04)
------------------

* Fixed ``RecursionError`` that can occur when using
  `cleanup_on_signal <https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html#if-you-got-custom-signal-handling>`__ or
  `cleanup_on_sigterm <https://pytest-cov.readthedocs.io/en/latest/subprocess-support.html#if-you-got-custom-signal-handling>`__.
  See: `#294 <https://github.com/pytest-dev/pytest-cov/issues/294>`_.
  The 2.7.x releases of pytest-cov should be considered broken regarding aforementioned cleanup API.
* Added compatibility with future xdist release that deprecates some internals
  (match pytest-xdist master/worker terminology).
  Contributed by Thomas Grainger in `#321 <https://github.com/pytest-dev/pytest-cov/pull/321>`_
* Fixed breakage that occurs when multiple reporting options are used.
  Contributed by Thomas Grainger in `#338 <https://github.com/pytest-dev/pytest-cov/pull/338>`_.
* Changed internals to use a stub instead of ``os.devnull``.
  Contributed by Thomas Grainger in `#332 <https://github.com/pytest-dev/pytest-cov/pull/332>`_.
* Added support for Coverage 5.0.
  Contributed by Ned Batchelder in `#319 <https://github.com/pytest-dev/pytest-cov/pull/319>`_.
* Added support for float values in ``--cov-fail-under``.
  Contributed by Martín Gaitán in `#311 <https://github.com/pytest-dev/pytest-cov/pull/311>`_.
* Various documentation fixes. Contributed by
  Juanjo Bazán,
  Andrew Murray and
  Albert Tugushev in
  `#298 <https://github.com/pytest-dev/pytest-cov/pull/298>`_,
  `#299 <https://github.com/pytest-dev/pytest-cov/pull/299>`_ and
  `#307 <https://github.com/pytest-dev/pytest-cov/pull/307>`_.
* Various testing improvements. Contributed by
  Ned Batchelder,
  Daniel Hahler,
  Ionel Cristian Mărieș and
  Hugo van Kemenade in
  `#313 <https://github.com/pytest-dev/pytest-cov/pull/313>`_,
  `#314 <https://github.com/pytest-dev/pytest-cov/pull/314>`_,
  `#315 <https://github.com/pytest-dev/pytest-cov/pull/315>`_,
  `#316 <https://github.com/pytest-dev/pytest-cov/pull/316>`_,
  `#325 <https://github.com/pytest-dev/pytest-cov/pull/325>`_,
  `#326 <https://github.com/pytest-dev/pytest-cov/pull/326>`_,
  `#334 <https://github.com/pytest-dev/pytest-cov/pull/334>`_ and
  `#335 <https://github.com/pytest-dev/pytest-cov/pull/335>`_.
* Added the ``--cov-context`` CLI options that enables coverage contexts. Only works with coverage 5.0+.
  Contributed by Ned Batchelder in `#345 <https://github.com/pytest-dev/pytest-cov/pull/345>`_.

2.7.1 (2019-05-03)
------------------

* Fixed source distribution manifest so that garbage ain't included in the tarball.

2.7.0 (2019-05-03)
------------------

* Fixed ``AttributeError: 'NoneType' object has no attribute 'configure_node'`` error when ``--no-cov`` is used.
  Contributed by Alexander Shadchin in `#263 <https://github.com/pytest-dev/pytest-cov/pull/263>`_.
* Various testing and CI improvements. Contributed by Daniel Hahler in
  `#255 <https://github.com/pytest-dev/pytest-cov/pull/255>`_,
  `#266 <https://github.com/pytest-dev/pytest-cov/pull/266>`_,
  `#272 <https://github.com/pytest-dev/pytest-cov/pull/272>`_,
  `#271 <https://github.com/pytest-dev/pytest-cov/pull/271>`_ and
  `#269 <https://github.com/pytest-dev/pytest-cov/pull/269>`_.
* Improved documentation regarding subprocess and multiprocessing.
  Contributed in `#265 <https://github.com/pytest-dev/pytest-cov/pull/265>`_.
* Improved ``pytest_cov.embed.cleanup_on_sigterm`` to be reentrant (signal deliveries while signal handling is
  running won't break stuff).
* Added ``pytest_cov.embed.cleanup_on_signal`` for customized cleanup.
* Improved cleanup code and fixed various issues with leftover data files. All contributed in
  `#265 <https://github.com/pytest-dev/pytest-cov/pull/265>`_ or
  `#262 <https://github.com/pytest-dev/pytest-cov/pull/262>`_.
* Improved examples. Now there are two examples for the common project layouts, complete with working coverage
  configuration. The examples have CI testing. Contributed in
  `#267 <https://github.com/pytest-dev/pytest-cov/pull/267>`_.
* Improved help text for CLI options.

2.6.1 (2019-01-07)
------------------

* Added support for Pytest 4.1. Contributed by Daniel Hahler and Семён Марьясин in
  `#253 <https://github.com/pytest-dev/pytest-cov/pull/253>`_ and
  `#230 <https://github.com/pytest-dev/pytest-cov/pull/230>`_.
* Various test and docs fixes. Contributed by Daniel Hahler in
  `#224 <https://github.com/pytest-dev/pytest-cov/pull/224>`_ and
  `#223 <https://github.com/pytest-dev/pytest-cov/pull/223>`_.
* Fixed the "Module already imported" issue (`#211 <https://github.com/pytest-dev/pytest-cov/issues/211>`_).
  Contributed by Daniel Hahler in `#228 <https://github.com/pytest-dev/pytest-cov/pull/228>`_.

2.6.0 (2018-09-03)
------------------

* Dropped support for Python < 3.4, Pytest < 3.5 and Coverage < 4.4.
* Fixed some documentation formatting. Contributed by Jean Jordaan and Julian.
* Added an example with ``addopts`` in documentation. Contributed by Samuel Giffard in
  `#195 <https://github.com/pytest-dev/pytest-cov/pull/195>`_.
* Fixed ``TypeError: 'NoneType' object is not iterable`` in certain xdist configurations. Contributed by Jeremy Bowman in
  `#213 <https://github.com/pytest-dev/pytest-cov/pull/213>`_.
* Added a ``no_cover`` marker and fixture. Fixes
  `#78 <https://github.com/pytest-dev/pytest-cov/issues/78>`_.
* Fixed broken ``no_cover`` check when running doctests. Contributed by Terence Honles in
  `#200 <https://github.com/pytest-dev/pytest-cov/pull/200>`_.
* Fixed various issues with path normalization in reports (when combining coverage data from parallel mode). Fixes
  `#130 <https://github.com/pytest-dev/pytest-cov/issues/161>`_.
  Contributed by Ryan Hiebert & Ionel Cristian Mărieș in
  `#178 <https://github.com/pytest-dev/pytest-cov/pull/178>`_.
* Report generation failures don't raise exceptions anymore. A warning will be logged instead. Fixes
  `#161 <https://github.com/pytest-dev/pytest-cov/issues/161>`_.
* Fixed multiprocessing issue on Windows (empty env vars are not passed). Fixes
  `#165 <https://github.com/pytest-dev/pytest-cov/issues/165>`_.

2.5.1 (2017-05-11)
------------------

* Fixed xdist breakage (regression in ``2.5.0``).
  Fixes `#157 <https://github.com/pytest-dev/pytest-cov/issues/157>`_.
* Allow setting custom ``data_file`` name in ``.coveragerc``.
  Fixes `#145 <https://github.com/pytest-dev/pytest-cov/issues/145>`_.
  Contributed by Jannis Leidel & Ionel Cristian Mărieș in
  `#156 <https://github.com/pytest-dev/pytest-cov/pull/156>`_.

2.5.0 (2017-05-09)
------------------

* Always show a summary when ``--cov-fail-under`` is used. Contributed by Francis Niu in `PR#141
  <https://github.com/pytest-dev/pytest-cov/pull/141>`_.
* Added ``--cov-branch`` option. Fixes `#85 <https://github.com/pytest-dev/pytest-cov/issues/85>`_.
* Improve exception handling in subprocess setup. Fixes `#144 <https://github.com/pytest-dev/pytest-cov/issues/144>`_.
* Fixed handling when ``--cov`` is used multiple times. Fixes `#151 <https://github.com/pytest-dev/pytest-cov/issues/151>`_.

2.4.0 (2016-10-10)
------------------

* Added a "disarm" option: ``--no-cov``. It will disable coverage measurements. Contributed by Zoltan Kozma in
  `PR#135 <https://github.com/pytest-dev/pytest-cov/pull/135>`_.

  **WARNING: Do not put this in your configuration files, it's meant to be an one-off for situations where you want to
  disable coverage from command line.**
* Fixed broken exception handling on ``.pth`` file. See `#136 <https://github.com/pytest-dev/pytest-cov/issues/136>`_.

2.3.1 (2016-08-07)
------------------

* Fixed regression causing spurious errors when xdist was used. See `#124
  <https://github.com/pytest-dev/pytest-cov/issues/124>`_.
* Fixed DeprecationWarning about incorrect `addoption` use. Contributed by Florian Bruhin in `PR#127
  <https://github.com/pytest-dev/pytest-cov/pull/127>`_.
* Fixed deprecated use of funcarg fixture API. Contributed by Daniel Hahler in `PR#125
  <https://github.com/pytest-dev/pytest-cov/pull/125>`_.

2.3.0 (2016-07-05)
------------------

* Add support for specifying output location for html, xml, and annotate report.
  Contributed by Patrick Lannigan in `PR#113 <https://github.com/pytest-dev/pytest-cov/pull/113>`_.
* Fix bug hiding test failure when cov-fail-under failed.
* For coverage >= 4.0, match the default behaviour of `coverage report` and
  error if coverage fails to find the source instead of just printing a warning.
  Contributed by David Szotten in `PR#116 <https://github.com/pytest-dev/pytest-cov/pull/116>`_.
* Fixed bug occurred when bare ``--cov`` parameter was used with xdist.
  Contributed by Michael Elovskikh in `PR#120 <https://github.com/pytest-dev/pytest-cov/pull/120>`_.
* Add support for ``skip_covered`` and added ``--cov-report=term-skip-covered`` command
  line options. Contributed by Saurabh Kumar in `PR#115 <https://github.com/pytest-dev/pytest-cov/pull/115>`_.

2.2.1 (2016-01-30)
------------------

* Fixed incorrect merging of coverage data when xdist was used and coverage was ``>= 4.0``.

2.2.0 (2015-10-04)
------------------

* Added support for changing working directory in tests. Previously changing working
  directory would disable coverage measurements in suprocesses.
* Fixed broken handling for ``--cov-report=annotate``.

2.1.0 (2015-08-23)
------------------

* Added support for `coverage 4.0b2`.
* Added the ``--cov-append`` command line options. Contributed by Christian Ledermann
  in `PR#80 <https://github.com/pytest-dev/pytest-cov/pull/80>`_.

2.0.0 (2015-07-28)
------------------

* Added ``--cov-fail-under``, akin to the new ``fail_under`` option in `coverage-4.0`
  (automatically activated if there's a ``[report] fail_under = ...`` in ``.coveragerc``).
* Changed ``--cov-report=term`` to automatically upgrade to ``--cov-report=term-missing``
  if there's ``[run] show_missing = True`` in ``.coveragerc``.
* Changed ``--cov`` so it can be used with no path argument (in which case the source
  settings from ``.coveragerc`` will be used instead).
* Fixed `.pth` installation to work in all cases (install, easy_install, wheels, develop etc).
* Fixed `.pth` uninstallation to work for wheel installs.
* Support for coverage 4.0.
* Data file suffixing changed to use coverage's ``data_suffix=True`` option (instead of the
  custom suffixing).
* Avoid warning about missing coverage data (just like ``coverage.control.process_startup``).
* Fixed a race condition when running with xdist (all the workers tried to combine the files).
  It's possible that this issue is not present in `pytest-cov 1.8.X`.

1.8.2 (2014-11-06)
------------------

* N/A



  * [pytest-flake8-1.0.4](https://github.com/tholo/pytest-flake8) pytest plugin for efficiently checking PEP8 compliance 
======================================================

.. image:: https://img.shields.io/pypi/v/pytest-flake8.svg
    :target: https://pypi.python.org/pypi/pytest-flake8

.. image:: https://img.shields.io/pypi/pyversions/pytest-flake8.svg
    :target: https://pypi.python.org/pypi/pytest-flake8

.. image:: https://img.shields.io/pypi/implementation/pytest-flake8.svg
    :target: https://pypi.python.org/pypi/pytest-flake8

.. image:: https://img.shields.io/pypi/status/pytest-flake8.svg
    :target: https://pypi.python.org/pypi/pytest-flake8

.. image:: https://travis-ci.org/tholo/pytest-flake8.svg?branch=master
    :target: https://travis-ci.org/tholo/pytest-flake8

.. image:: https://img.shields.io/github/issues/tholo/pytest-flake8.svg
    :target: https://github.com/tholo/pytest-flake8/issues

.. image:: https://img.shields.io/github/issues-pr/tholo/pytest-flake8.svg
    :target: https://github.com/tholo/pytest-flake8/pulls

Usage
-----

Install by running the command::

    pip install pytest-flake8

After installing it, when you run tests with the option::

    pytest --flake8

every file ending in ``.py`` will be discovered and checked with
flake8.

.. note::

    If optional flake8 plugins are installed, those will
    be used automatically. No provisions have been made for
    configuring these via `pytest`_.

.. warning::

    Running flake8 tests on your project is likely to cause a number 
    of issues. The plugin allows one to configure on a per-project and
    per-file basis which errors or warnings to ignore, see
    flake8-ignore_.

.. _flake8-ignore:

Configuring FLAKE8 options per project and file
-----------------------------------------------

Maximum line length can be configured for the whole project
by adding a ``flake8-max-line-length`` option to your ``setup.cfg``
or ``tox.ini`` file like this::

    # content of setup.cfg
    [pytest]
    flake8-max-line-length = 99

Note that the default will be what naturally comes with `flake8`_
(which it turn gets its default from `pycodestyle`_).

You may configure flake8-checking options for your project
by adding an ``flake8-ignore`` entry to your ``setup.cfg``
or ``tox.ini`` file like this::

    # content of setup.cfg
    [pytest]
    flake8-ignore = E201 E231

This would globally prevent complaints about two whitespace issues.
Rerunning with the above example will now look better::

    $ pytest -q  --flake8
    collecting ... collected 1 items
    .
    1 passed in 0.01 seconds

If you have some files where you want to specifically ignore 
some errors or warnings you can start a flake8-ignore line with 
a glob-pattern and a space-separated list of codes::

    # content of setup.cfg
    [pytest]
    flake8-ignore = 
        *.py E201
        doc/conf.py ALL

So if you have a conf.py like this::

    # content of doc/conf.py

    func (  [1,2,3]) #this line lots PEP8 errors :)

then running again with the previous example will show a single
failure and it will ignore doc/conf.py alltogether::

    $ pytest --flake8 -v # verbose shows what is ignored
    ======================================= test session starts ========================================
    platform darwin -- Python 2.7.6 -- py-1.4.26 -- pytest-2.7.0 -- /Users/tholo/Source/pytest/bin/python
    cachedir: /Users/tholo/Source/pytest/src/verify/.cache
    rootdir: /Users/tholo/Source/angular/src/verify, inifile: setup.cfg
    plugins: flake8, cache
    collected 1 items

    myfile.py PASSED

    ========================================= 1 passed in 0.00 seconds =========================================

Note that doc/conf.py was not considered or imported.

Notes
-----

The repository of this plugin is at https://github.com/tholo/pytest-flake8

For more info on `pytest`_ see http://pytest.org

The code is partially based on Ronny Pfannschmidt's `pytest-codecheckers`_ plugin.

.. _`pytest`: http://pytest.org
.. _`flake8`: https://pypi.python.org/pypi/flake8
.. _`pycodestyle`: https://pypi.python.org/pypi/pycodestyle
.. _`pytest-codecheckers`: https://pypi.python.org/pypi/pytest-codecheckers



  * [pytest-mock-1.10.4](https://github.com/pytest-dev/pytest-mock/) ===========
pytest-mock
===========

This plugin installs a ``mocker`` fixture which is a thin-wrapper around the patching API
provided by the `mock package <http://pypi.python.org/pypi/mock>`_,
but with the benefit of not having to worry about undoing patches at the end
of a test:

.. code-block:: python

    import os

    class UnixFS:

        @staticmethod
        def rm(filename):
            os.remove(filename)

    def test_unix_fs(mocker):
        mocker.patch('os.remove')
        UnixFS.rm('file')
        os.remove.assert_called_once_with('file')



|python| |version| |anaconda| |ci| |coverage| |black|

.. |version| image:: http://img.shields.io/pypi/v/pytest-mock.svg
  :target: https://pypi.python.org/pypi/pytest-mock

.. |anaconda| image:: https://img.shields.io/conda/vn/conda-forge/pytest-mock.svg
    :target: https://anaconda.org/conda-forge/pytest-mock

.. |ci| image:: https://github.com/pytest-dev/pytest-mock/workflows/build/badge.svg
  :target: https://github.com/pytest-dev/pytest-mock/actions

.. |coverage| image:: http://img.shields.io/coveralls/pytest-dev/pytest-mock.svg
  :target: https://coveralls.io/r/pytest-dev/pytest-mock

.. |python| image:: https://img.shields.io/pypi/pyversions/pytest-mock.svg
  :target: https://pypi.python.org/pypi/pytest-mock/

.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
  :target: https://github.com/ambv/black

`Professionally supported pytest-mock is now available <https://tidelift.com/subscription/pkg/pypi-pytest_mock?utm_source=pypi-pytest-mock&utm_medium=referral&utm_campaign=readme>`_

Usage
=====

The ``mocker`` fixture has the same API as
`mock.patch <https://docs.python.org/3/library/unittest.mock.html#patch>`_,
supporting the same arguments:

.. code-block:: python

    def test_foo(mocker):
        # all valid calls
        mocker.patch('os.remove')
        mocker.patch.object(os, 'listdir', autospec=True)
        mocked_isfile = mocker.patch('os.path.isfile')

The supported methods are:

* `mocker.patch <https://docs.python.org/3/library/unittest.mock.html#patch>`_
* `mocker.patch.object <https://docs.python.org/3/library/unittest.mock.html#patch-object>`_
* `mocker.patch.multiple <https://docs.python.org/3/library/unittest.mock.html#patch-multiple>`_
* `mocker.patch.dict <https://docs.python.org/3/library/unittest.mock.html#patch-dict>`_
* `mocker.stopall <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.patch.stopall>`_
* ``mocker.resetall()``: calls `reset_mock() <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.reset_mock>`_ in all mocked objects up to this point.

These objects from the ``mock`` module are accessible directly from ``mocker`` for convenience:

* `Mock <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock>`_
* `MagicMock <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.MagicMock>`_
* `PropertyMock <https://docs.python.org/3/library/unittest.mock.html#unittest.mock.PropertyMock>`_
* `ANY <https://docs.python.org/3/library/unittest.mock.html#any>`_
* `DEFAULT <https://docs.python.org/3/library/unittest.mock.html#default>`_ *(Version 1.4)*
* `call <https://docs.python.org/3/library/unittest.mock.html#call>`_ *(Version 1.1)*
* `sentinel <https://docs.python.org/3/library/unittest.mock.html#sentinel>`_ *(Version 1.2)*
* `mock_open <https://docs.python.org/3/library/unittest.mock.html#mock-open>`_


Spy
---

The spy acts exactly like the original method in all cases, except it allows use of ``mock``
features with it, like retrieving call count. It also works for class and static methods.

.. code-block:: python

    def test_spy(mocker):
        class Foo(object):
            def bar(self):
                return 42

        foo = Foo()
        mocker.spy(foo, 'bar')
        assert foo.bar() == 42
        assert foo.bar.call_count == 1

Since version ``1.11``, it is also possible to query the ``return_value`` attribute
to observe what the spied function/method returned.

Stub
----


The stub is a mock object that accepts any arguments and is useful to test callbacks, for instance.
May be passed a name to be used by the constructed stub object in its repr (useful for debugging).

.. code-block:: python

    def test_stub(mocker):
        def foo(on_something):
            on_something('foo', 'bar')

        stub = mocker.stub(name='on_something_stub')

        foo(stub)
        stub.assert_called_once_with('foo', 'bar')


Improved reporting of mock call assertion errors
------------------------------------------------


This plugin monkeypatches the mock library to improve pytest output for failures
of mock call assertions like ``Mock.assert_called_with()`` by hiding internal traceback
entries from the ``mock`` module.

It also adds introspection information on differing call arguments when
calling the helper methods. This features catches `AssertionError` raised in
the method, and uses py.test's own `advanced assertions`_ to return a better
diff::


    mocker = <pytest_mock.MockFixture object at 0x0381E2D0>

        def test(mocker):
            m = mocker.Mock()
            m('fo')
    >       m.assert_called_once_with('', bar=4)
    E       AssertionError: Expected call: mock('', bar=4)
    E       Actual call: mock('fo')
    E
    E       pytest introspection follows:
    E
    E       Args:
    E       assert ('fo',) == ('',)
    E         At index 0 diff: 'fo' != ''
    E         Use -v to get the full diff
    E       Kwargs:
    E       assert {} == {'bar': 4}
    E         Right contains more items:
    E         {'bar': 4}
    E         Use -v to get the full diff


    test_foo.py:6: AssertionError
    ========================== 1 failed in 0.03 seconds ===========================


This is useful when asserting mock calls with many/nested arguments and trying
to quickly see the difference.

This feature is probably safe, but if you encounter any problems it can be disabled in
your ``pytest.ini`` file:

.. code-block:: ini

    [pytest]
    mock_traceback_monkeypatch = false

Note that this feature is automatically disabled with the ``--tb=native`` option. The underlying
mechanism used to suppress traceback entries from ``mock`` module does not work with that option
anyway plus it generates confusing messages on Python 3.5 due to exception chaining

.. _advanced assertions: http://pytest.org/latest/assert.html


Use standalone "mock" package
-----------------------------

*New in version 1.4.0.*

Python 3 users might want to use a newest version of the ``mock`` package as published on PyPI
than the one that comes with the Python distribution.

.. code-block:: ini

    [pytest]
    mock_use_standalone_module = true

This will force the plugin to import ``mock`` instead of the ``unittest.mock`` module bundled with
Python 3.4+. Note that this option is only used in Python 3+, as Python 2 users only have the option
to use the ``mock`` package from PyPI anyway.

Note about usage as context manager
-----------------------------------

Although mocker's API is intentionally the same as ``mock.patch``'s, its use
as context manager and function decorator is **not** supported through the
fixture:

.. code-block:: python

    def test_context_manager(mocker):
        a = A()
        with mocker.patch.object(a, 'doIt', return_value=True, autospec=True):  # DO NOT DO THIS
            assert a.doIt() == True

The purpose of this plugin is to make the use of context managers and
function decorators for mocking unnecessary.


Requirements
============

* Python 2.7, Python 3.4+
* pytest
* mock (for Python 2)


Install
=======

Install using `pip <http://pip-installer.org/>`_:

.. code-block:: console

    $ pip install pytest-mock

Changelog
=========

Please consult the `changelog page`_.

.. _changelog page: https://github.com/pytest-dev/pytest-mock/blob/master/CHANGELOG.rst

Why bother with a plugin?
=========================

There are a number of different ``patch`` usages in the standard ``mock`` API,
but IMHO they don't scale very well when you have more than one or two
patches to apply.

It may lead to an excessive nesting of ``with`` statements, breaking the flow
of the test:

.. code-block:: python

    import mock

    def test_unix_fs():
        with mock.patch('os.remove'):
            UnixFS.rm('file')
            os.remove.assert_called_once_with('file')

            with mock.patch('os.listdir'):
                assert UnixFS.ls('dir') == expected
                # ...

        with mock.patch('shutil.copy'):
            UnixFS.cp('src', 'dst')
            # ...


One can use ``patch`` as a decorator to improve the flow of the test:

.. code-block:: python

    @mock.patch('os.remove')
    @mock.patch('os.listdir')
    @mock.patch('shutil.copy')
    def test_unix_fs(mocked_copy, mocked_listdir, mocked_remove):
        UnixFS.rm('file')
        os.remove.assert_called_once_with('file')

        assert UnixFS.ls('dir') == expected
        # ...

        UnixFS.cp('src', 'dst')
        # ...

But this poses a few disadvantages:

- test functions must receive the mock objects as parameter, even if you don't plan to
  access them directly; also, order depends on the order of the decorated ``patch``
  functions;
- receiving the mocks as parameters doesn't mix nicely with pytest's approach of
  naming fixtures as parameters, or ``pytest.mark.parametrize``;
- you can't easily undo the mocking during the test execution;

An alternative is to use ``contextlib.ExitStack`` to stack the context managers in a single level of indentation
to improve the flow of the test:

.. code-block:: python

    import contextlib
    import mock

    def test_unix_fs():
        with contextlib.ExitStack() as stack:
            stack.enter_context(mock.patch('os.remove'))
            UnixFS.rm('file')
            os.remove.assert_called_once_with('file')

            stack.enter_context(mock.patch('os.listdir'))
            assert UnixFS.ls('dir') == expected
            # ...

            stack.enter_context(mock.patch('shutil.copy'))
            UnixFS.cp('src', 'dst')
            # ...

But this is arguably a little more complex than using ``pytest-mock``.

Contributing
============

Contributions are welcome! After cloning the repository, create a virtual env
and install ``pytest-mock`` in editable mode with ``dev`` extras:

.. code-block:: console

    $ pip install --editable .[dev]
    $ pre-commit install

Tests are run with ``tox``, you can run the baseline environments before submitting a PR:

.. code-block:: console

    $ tox -e py27,py36,linting

Style checks and formatting are done automatically during commit courtesy of
`pre-commit <https://pre-commit.com>`_.

License
=======

Distributed under the terms of the `MIT`_ license.

Security contact information
============================

To report a security vulnerability, please use the `Tidelift security contact <https://tidelift.com/security>`__. Tidelift will coordinate the fix and disclosure.

.. _MIT: https://github.com/pytest-dev/pytest-mock/blob/master/LICENSE



  * [python-Levenshtein-0.12.0](http://github.com/ztane/python-Levenshtein) .. contents ::

Introduction
------------

The Levenshtein Python C extension module contains functions for fast
computation of

* Levenshtein (edit) distance, and edit operations

* string similarity

* approximate median strings, and generally string averaging

* string sequence and set similarity

It supports both normal and Unicode strings.

Python 2.2 or newer is required; Python 3 is supported.

StringMatcher.py is an example SequenceMatcher-like class built on the top of
Levenshtein.  It misses some SequenceMatcher's functionality, and has some
extra OTOH.

Levenshtein.c can be used as a pure C library, too.  You only have to define
NO_PYTHON preprocessor symbol (-DNO_PYTHON) when compiling it.  The
functionality is similar to that of the Python extension.  No separate docs
are provided yet, RTFS.  But they are not interchangeable:

* C functions exported when compiling with -DNO_PYTHON (see Levenshtein.h)
  are not exported when compiling as a Python extension (and vice versa)

* Unicode character type used with -DNO_PYTHON is wchar_t, Python extension
  uses Py_UNICODE, they may be the same but don't count on it

Documentation
--------------

gendoc.sh generates HTML API documentation,
you probably want a selfcontained instead of includable version, so run
in ``./gendoc.sh --selfcontained``.  It needs Levenshtein already installed
and genextdoc.py.

License
-----------

Levenshtein can be copied and/or modified under the terms of GNU General
Public License, see the file COPYING for full license text.

History
-------

This package was long missing from PyPi and available as source checkout only.
We needed to restore this package for `Go Mobile for Plone <http://webandmobile.mfabrik.com>`_
and `Pywurfl <http://celljam.net/>`_ projects which depend on this.

Source code
-----------

* http://github.com/ztane/python-Levenshtein/

Documentation
-------------

* `Documentation for the current version <https://rawgit.com/ztane/python-Levenshtein/master/docs/Levenshtein.html>`_

Authors
-------

* Maintainer: `Antti Haapala <antti@haapala.name>`

* Python 3 compatibility: Esa Määttä

* Jonatas CD: Fixed documentation generation

* Previous maintainer: `Mikko Ohtamaa <http://opensourcehacker.com>`_

* Original code: David Necas (Yeti) <yeti at physics.muni.cz>

============
 Changelog
============

0.12.0
------

* Fixed a bug in StringMatcher.StringMatcher.get_matching_blocks /
  extract_editops for Python 3; now allow only `str` editops on
  both Python 2 and Python 3, for simpler and working code.

* Added documentation in the source distribution and in GIT

* Fixed the package layout: renamed the .so/.dll to _levenshtein,
  and made it reside inside a package, along with the StringMatcher
  class.

* Fixed spelling errors.

0.11.2
------

* Fixed a bug in setup.py: installation would fail on Python 3 if the locale
  did not specify UTF-8 charset (Felix Yan).

* Added COPYING, StringMatcher.py, gendoc.sh and NEWS in MANIFEST.in, as they
  were missing from source distributions.

0.11.1
------

* Added Levenshtein.h to MANIFEST.in

0.11.0
------

* Python 3 support, maintainership passed to Antti Haapala

0.10.1 - 0.10.2
---------------

* Made python-Lehvenstein Git compatible and use setuptools for PyPi upload

* Created HISTORY.txt and made README reST compatible
  * [python-cinderclient-4.2.1](https://docs.openstack.org/python-cinderclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-cinderclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

Python bindings to the OpenStack Cinder API
===========================================

.. image:: https://img.shields.io/pypi/v/python-cinderclient.svg
    :target: https://pypi.org/project/python-cinderclient/
    :alt: Latest Version

This is a client for the OpenStack Cinder API. There's a Python API (the
``cinderclient`` module), and a command-line script (``cinder``). Each
implements 100% of the OpenStack Cinder API.

See the `OpenStack CLI Reference`_ for information on how to use the ``cinder``
command-line tool. You may also want to look at the
`OpenStack API documentation`_.

.. _OpenStack CLI Reference: https://docs.openstack.org/python-openstackclient/latest/cli/
.. _OpenStack API documentation: https://docs.openstack.org/api-quick-start/

The project is hosted on `Launchpad`_, where bugs can be filed. The code is
hosted on `OpenStack`_. Patches must be submitted using `Gerrit`_.

.. _OpenStack: https://opendev.org/openstack/python-cinderclient
.. _Launchpad: https://launchpad.net/python-cinderclient
.. _Gerrit: https://docs.openstack.org/infra/manual/developers.html#development-workflow

* License: Apache License, Version 2.0
* `PyPi`_ - package installation
* `Online Documentation`_
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Specs`_
* `How to Contribute`_

.. _PyPi: https://pypi.org/project/python-cinderclient
.. _Online Documentation: https://docs.openstack.org/python-cinderclient/latest/
.. _Blueprints: https://blueprints.launchpad.net/python-cinderclient
.. _Bugs: https://bugs.launchpad.net/python-cinderclient
.. _Source: https://opendev.org/openstack/python-cinderclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Specs: https://specs.openstack.org/openstack/cinder-specs/


.. contents:: Contents:
   :local:

Command-line API
----------------

Installing this package gets you a shell command, ``cinder``, that you
can use to interact with any Rackspace compatible API (including OpenStack).

You'll need to provide your OpenStack username and password. You can do this
with the ``--os-username``, ``--os-password`` and  ``--os-tenant-name``
params, but it's easier to just set them as environment variables::

    export OS_USERNAME=openstack
    export OS_PASSWORD=yadayada
    export OS_TENANT_NAME=myproject

You will also need to define the authentication url with ``--os-auth-url``
and the version of the API with ``--os-volume-api-version``. Or set them as
environment variables as well. Since Block Storage API V2 is officially
deprecated, you are encouraged to set ``OS_VOLUME_API_VERSION=3``. If you
are using Keystone, you need to set the ``OS_AUTH_URL`` to the keystone
endpoint::

    export OS_AUTH_URL=http://controller:5000/v3
    export OS_VOLUME_API_VERSION=3

Since Keystone can return multiple regions in the Service Catalog, you
can specify the one you want with ``--os-region-name`` (or
``export OS_REGION_NAME``). It defaults to the first in the list returned.

You'll find complete documentation on the shell by running
``cinder help``::

    usage: cinder [--version] [-d] [--os-auth-system <auth-system>]
                  [--service-type <service-type>] [--service-name <service-name>]
                  [--volume-service-name <volume-service-name>]
                  [--os-endpoint-type <os-endpoint-type>]
                  [--endpoint-type <endpoint-type>]
                  [--os-volume-api-version <volume-api-ver>]
                  [--bypass-url <bypass-url>] [--retries <retries>]
                  [--profile HMAC_KEY] [--os-auth-strategy <auth-strategy>]
                  [--os-username <auth-user-name>] [--os-password <auth-password>]
                  [--os-tenant-name <auth-tenant-name>]
                  [--os-tenant-id <auth-tenant-id>] [--os-auth-url <auth-url>]
                  [--os-user-id <auth-user-id>]
                  [--os-user-domain-id <auth-user-domain-id>]
                  [--os-user-domain-name <auth-user-domain-name>]
                  [--os-project-id <auth-project-id>]
                  [--os-project-name <auth-project-name>]
                  [--os-project-domain-id <auth-project-domain-id>]
                  [--os-project-domain-name <auth-project-domain-name>]
                  [--os-region-name <region-name>] [--os-token <token>]
                  [--os-url <url>] [--insecure] [--os-cacert <ca-certificate>]
                  [--os-cert <certificate>] [--os-key <key>] [--timeout <seconds>]
                  <subcommand> ...

    Command-line interface to the OpenStack Cinder API.

    Positional arguments:
      <subcommand>
        absolute-limits     Lists absolute limits for a user.
        api-version         Display the server API version information. (Supported
                            by API versions 3.0 - 3.latest)
        availability-zone-list
                            Lists all availability zones.
        backup-create       Creates a volume backup.
        backup-delete       Removes one or more backups.
        backup-export       Export backup metadata record.
        backup-import       Import backup metadata record.
        backup-list         Lists all backups.
        backup-reset-state  Explicitly updates the backup state.
        backup-restore      Restores a backup.
        backup-show         Shows backup details.
        cgsnapshot-create   Creates a cgsnapshot.
        cgsnapshot-delete   Removes one or more cgsnapshots.
        cgsnapshot-list     Lists all cgsnapshots.
        cgsnapshot-show     Shows cgsnapshot details.
        consisgroup-create  Creates a consistency group.
        consisgroup-create-from-src
                            Creates a consistency group from a cgsnapshot or a
                            source CG.
        consisgroup-delete  Removes one or more consistency groups.
        consisgroup-list    Lists all consistency groups.
        consisgroup-show    Shows details of a consistency group.
        consisgroup-update  Updates a consistency group.
        create              Creates a volume.
        credentials         Shows user credentials returned from auth.
        delete              Removes one or more volumes.
        encryption-type-create
                            Creates encryption type for a volume type. Admin only.
        encryption-type-delete
                            Deletes encryption type for a volume type. Admin only.
        encryption-type-list
                            Shows encryption type details for volume types. Admin
                            only.
        encryption-type-show
                            Shows encryption type details for a volume type. Admin
                            only.
        encryption-type-update
                            Update encryption type information for a volume type
                            (Admin Only).
        endpoints           Discovers endpoints registered by authentication
                            service.
        extend              Attempts to extend size of an existing volume.
        extra-specs-list    Lists current volume types and extra specs.
        failover-host       Failover a replicating cinder-volume host.
        force-delete        Attempts force-delete of volume, regardless of state.
        freeze-host         Freeze and disable the specified cinder-volume host.
        get-capabilities    Show backend volume stats and properties. Admin only.
        get-pools           Show pool information for backends. Admin only.
        image-metadata      Sets or deletes volume image metadata.
        image-metadata-show
                            Shows volume image metadata.
        list                Lists all volumes.
        manage              Manage an existing volume.
        metadata            Sets or deletes volume metadata.
        metadata-show       Shows volume metadata.
        metadata-update-all
                            Updates volume metadata.
        migrate             Migrates volume to a new host.
        qos-associate       Associates qos specs with specified volume type.
        qos-create          Creates a qos specs.
        qos-delete          Deletes a specified qos specs.
        qos-disassociate    Disassociates qos specs from specified volume type.
        qos-disassociate-all
                            Disassociates qos specs from all its associations.
        qos-get-association
                            Lists all associations for specified qos specs.
        qos-key             Sets or unsets specifications for a qos spec.
        qos-list            Lists qos specs.
        qos-show            Shows qos specs details.
        quota-class-show    Lists quotas for a quota class.
        quota-class-update  Updates quotas for a quota class.
        quota-defaults      Lists default quotas for a tenant.
        quota-delete        Delete the quotas for a tenant.
        quota-show          Lists quotas for a tenant.
        quota-update        Updates quotas for a tenant.
        quota-usage         Lists quota usage for a tenant.
        rate-limits         Lists rate limits for a user.
        readonly-mode-update
                            Updates volume read-only access-mode flag.
        rename              Renames a volume.
        reset-state         Explicitly updates the volume state in the Cinder
                            database.
        retype              Changes the volume type for a volume.
        service-disable     Disables the service.
        service-enable      Enables the service.
        service-list        Lists all services. Filter by host and service binary.
                            (Supported by API versions 3.0 - 3.latest)
        set-bootable        Update bootable status of a volume.
        show                Shows volume details.
        snapshot-create     Creates a snapshot.
        snapshot-delete     Removes one or more snapshots.
        snapshot-list       Lists all snapshots.
        snapshot-manage     Manage an existing snapshot.
        snapshot-metadata   Sets or deletes snapshot metadata.
        snapshot-metadata-show
                            Shows snapshot metadata.
        snapshot-metadata-update-all
                            Updates snapshot metadata.
        snapshot-rename     Renames a snapshot.
        snapshot-reset-state
                            Explicitly updates the snapshot state.
        snapshot-show       Shows snapshot details.
        snapshot-unmanage   Stop managing a snapshot.
        thaw-host           Thaw and enable the specified cinder-volume host.
        transfer-accept     Accepts a volume transfer.
        transfer-create     Creates a volume transfer.
        transfer-delete     Undoes a transfer.
        transfer-list       Lists all transfers.
        transfer-show       Shows transfer details.
        type-access-add     Adds volume type access for the given project.
        type-access-list    Print access information about the given volume type.
        type-access-remove  Removes volume type access for the given project.
        type-create         Creates a volume type.
        type-default        List the default volume type.
        type-delete         Deletes volume type or types.
        type-key            Sets or unsets extra_spec for a volume type.
        type-list           Lists available 'volume types'.
        type-show           Show volume type details.
        type-update         Updates volume type name, description, and/or
                            is_public.
        unmanage            Stop managing a volume.
        upload-to-image     Uploads volume to Image Service as an image.
        version-list        List all API versions. (Supported by API versions 3.0
                            - 3.latest)
        bash-completion     Prints arguments for bash_completion.
        help                Shows help about this program or one of its
                            subcommands.
        list-extensions

    Optional arguments:
      --version             show program's version number and exit
      -d, --debug           Shows debugging output.
      --os-auth-system <auth-system>
                            Defaults to env[OS_AUTH_SYSTEM].
      --service-type <service-type>
                            Service type. For most actions, default is volume.
      --service-name <service-name>
                            Service name. Default=env[CINDER_SERVICE_NAME].
      --volume-service-name <volume-service-name>
                            Volume service name.
                            Default=env[CINDER_VOLUME_SERVICE_NAME].
      --os-endpoint-type <os-endpoint-type>
                            Endpoint type, which is publicURL or internalURL.
                            Default=env[OS_ENDPOINT_TYPE] or nova
                            env[CINDER_ENDPOINT_TYPE] or publicURL.
      --endpoint-type <endpoint-type>
                            DEPRECATED! Use --os-endpoint-type.
      --os-volume-api-version <volume-api-ver>
                            Block Storage API version. Accepts X, X.Y (where X is
                            major and Y is minor
                            part).Default=env[OS_VOLUME_API_VERSION].
      --bypass-url <bypass-url>
                            Use this API endpoint instead of the Service Catalog.
                            Defaults to env[CINDERCLIENT_BYPASS_URL].
      --retries <retries>   Number of retries.
      --profile HMAC_KEY    HMAC key to use for encrypting context data for
                            performance profiling of operation. This key needs to
                            match the one configured on the cinder api server.
                            Without key the profiling will not be triggered even
                            if osprofiler is enabled on server side.
                            Defaults to env[OS_PROFILE].
      --os-auth-strategy <auth-strategy>
                            Authentication strategy (Env: OS_AUTH_STRATEGY,
                            default keystone). For now, any other value will
                            disable the authentication.
      --os-username <auth-user-name>
                            OpenStack user name. Default=env[OS_USERNAME].
      --os-password <auth-password>
                            Password for OpenStack user. Default=env[OS_PASSWORD].
      --os-tenant-name <auth-tenant-name>
                            Tenant name. Default=env[OS_TENANT_NAME].
      --os-tenant-id <auth-tenant-id>
                            ID for the tenant. Default=env[OS_TENANT_ID].
      --os-auth-url <auth-url>
                            URL for the authentication service.
                            Default=env[OS_AUTH_URL].
      --os-user-id <auth-user-id>
                            Authentication user ID (Env: OS_USER_ID).
      --os-user-domain-id <auth-user-domain-id>
                            OpenStack user domain ID. Defaults to
                            env[OS_USER_DOMAIN_ID].
      --os-user-domain-name <auth-user-domain-name>
                            OpenStack user domain name. Defaults to
                            env[OS_USER_DOMAIN_NAME].
      --os-project-id <auth-project-id>
                            Another way to specify tenant ID. This option is
                            mutually exclusive with --os-tenant-id. Defaults to
                            env[OS_PROJECT_ID].
      --os-project-name <auth-project-name>
                            Another way to specify tenant name. This option is
                            mutually exclusive with --os-tenant-name. Defaults to
                            env[OS_PROJECT_NAME].
      --os-project-domain-id <auth-project-domain-id>
                            Defaults to env[OS_PROJECT_DOMAIN_ID].
      --os-project-domain-name <auth-project-domain-name>
                            Defaults to env[OS_PROJECT_DOMAIN_NAME].
      --os-region-name <region-name>
                            Region name. Default=env[OS_REGION_NAME].
      --os-token <token>    Defaults to env[OS_TOKEN].
      --os-url <url>        Defaults to env[OS_URL].

    API Connection Options:
      Options controlling the HTTP API Connections

      --insecure            Explicitly allow client to perform "insecure" TLS
                            (https) requests. The server's certificate will not be
                            verified against any certificate authorities. This
                            option should be used with caution.
      --os-cacert <ca-certificate>
                            Specify a CA bundle file to use in verifying a TLS
                            (https) server certificate. Defaults to
                            env[OS_CACERT].
      --os-cert <certificate>
                            Defaults to env[OS_CERT].
      --os-key <key>        Defaults to env[OS_KEY].
      --timeout <seconds>   Set request timeout (in seconds).

    Run "cinder help SUBCOMMAND" for help on a subcommand.

If you want to get a particular version API help message, you can add
``--os-volume-api-version <volume-api-ver>`` in help command, like
this::

    cinder --os-volume-api-version 3.28 help

Python API
----------

There's also a complete Python API, but it has not yet been documented.

Quick-start using keystone::

    # use v3 auth with http://controller:5000/v3
    >>> from cinderclient.v3 import client
    >>> nt = client.Client(USERNAME, PASSWORD, PROJECT_ID, AUTH_URL)
    >>> nt.volumes.list()
    [...]

See release notes and more at `<https://docs.openstack.org/python-cinderclient/latest/>`_.




  * [python-consul-1.1.0](https://github.com/cablehead/python-consul) Python client for `Consul.io <http://www.consul.io/>`_
======================================================

Documentation
-------------

`Read the Docs`_

Status
------

|Build Status|

Example
-------

.. code:: python

    import consul

    c = consul.Consul()

    # poll a key for updates
    index = None
    while True:
        index, data = c.kv.get('foo', index=index)
        print data['Value']

    # in another process
    c.kv.put('foo', 'bar')

Installation
------------

::

    pip install python-consul

**Note:** When using python-consul library in environment with proxy server, setting of ``http_proxy``, ``https_proxy`` and ``no_proxy`` environment variables can be required for proper functionality.

.. |Build Status|
   image:: https://img.shields.io/travis/cablehead/python-consul.svg?style=flat-square
   :target: https://travis-ci.org/cablehead/python-consul
.. |Coverage Status|
   image:: https://img.shields.io/coveralls/cablehead/python-consul.svg?style=flat-square
   :target: https://coveralls.io/r/cablehead/python-consul?branch=master
.. _Read the Docs: https://python-consul.readthedocs.io/

Status
------

There's a few API endpoints still to go to expose all features available in
Consul v0.6.0. If you need an endpoint that's not in the documentation, just
open an issue and I'll try and add it straight away.

Mailing List
------------

- https://groups.google.com/forum/#!forum/python-consul

Contributing
------------

python-consul is currently maintained by:

- @matusvalo
- @abn
- @cablehead

Please reach out if you're interested in being a maintainer as well. Otherwise,
open a PR or Issue we'll try and respond as quickly as we're able.

Issue Labels
~~~~~~~~~~~~

:today!: Some triaging is in progress and this issue should be taken care of in
         a couple of hours!

:priority: There's a clear need to address this issue and it's likely a core
           contributor will take it on. Opening a PR for these is greatly
           appreciated!

:help wanted: This issue makes sense and would be useful. It's unlikely a core
              contributor will get to this though, so if you'd like to see it
              addressed please open a PR.

:question: The need for the issue isn't clear or needs clarification, so please
           follow up.  Issues in this state for a few months, without
           responses will likely will be closed.

PRs
~~~

Pull requests are very much appreciated! When you create a PR please ensure:

#. All current tests pass, including flake8
#. To add tests for your new features, if reasonable
#. To add docstrings for new api features you add and if needed link to these
   docstrings from the sphinx documentation

Releases
~~~~~~~~

.. code:: bash

    # release the current version, eg: 0.6.1-dev -> 0.6.1
    bumpversion release

    # prepare the next patch (z-stream) version, eg: 0.6.1 -> 0.6.2-dev
    bumpversion --no-tag patch

    # else, prepare the next minor (y-stream) version, eg: 0.6.1 -> 0.7.0-dev
    bumpversion --no-tag minor


Change log
==========

1.1.0
-----

* add support for nodemeta to catalog and health end points (thanks
  @ibrahimmenem)
* update Check.script to use args, as Consul 1.1 has dropped the script
  parameter

1.0.1
-----

* Support for Python 3.4 dropped (sorry)
* Add support for Consul 1.0.0 (thanks @matusvalo!)
* Expose all 400 errors and add tests for common callback handler (thanks @bagerard)

0.7.2
-----

* Add header parameter to http check (thanks @matusvalo)
* Add basic Transaction HTTP API support (thanks @iandyh)
* Fix invalid url error when specifying credentials via host (Issue #167)

0.7.1
-----

* Add a common base client for shared functionality between different HTTP clients (thanks @abn!)
* Fix request quoting issue (thanks @abn)
* Fix installation issue due to aiohttp only being available for Python>=3.4.2 (thanks @abn)
* Added support for current release of aiohttp (thanks @eaterek)
* Improved Tornado example (thanks @chriswue)
* Add and use ACL token in Event.fire (thanks @illenseer)
* Add client side cert support (thanks @brocade-craig)
* Add token params to catalog register (thanks @gregdurham)
* Add support for DeregisterCriticalServiceAfter (thanks @daroot)
* Improve reliability of test suite (thanks @daroot!)
* Update CI: Add py35 and py36 to tests (thanks @Poogles)

0.7.0
-----

Features
~~~~~~~~

* Add Operator endpoint (thanks @bantonj!)

0.6.2
-----

Bug Fix
~~~~~~~

* Tornado backend encoding bug related to None values (thanks @plredmond)
* python-consul doesn't support python 2.6 (thanks @lowzj)

Maintenance
~~~~~~~~~~~

* update max ttl to 86400 to conform to consul (thanks @philloooo)
* Correct error message in ACL create/update (thanks @Crypto89)

Features
~~~~~~~~

* Catalog API should support tokens (thanks @racktear!)
* Allow enable tag override (thanks @shalev67!)

0.6.1
------

Features
~~~~~~~~

* Add the coordinate endpoint and near support on Catalog and Health Checks
  (thanks @shalev67!)
* Rework all endpoints to use a common callback handler to help ensure
  consistent handling of responses (thanks @shalev67)
* Add Query api support (thanks @shalev67)
* Add token support for the Health endpoints (thanks @morpheu!)
* Force to use UTF-8 encoding for the response with the request's client
  (thanks @maxnasonov)

Maintenance
~~~~~~~~~~~

* Migrate readthedocs links from .org to .io (thanks @adamchainz)

0.6.0
------

Features
~~~~~~~~

* Add support for the new TCP and Docker health checks (thanks @abn)
* Add support for join and force-leave (thanks @abn)
* Use standard consul environment variables to override configuration (thanks
  @amayausky)

Maintenance
~~~~~~~~~~~

* Test binaries updated to Consul 0.6.4
* Tweaks to fix small updates to Consul's API

0.4.7
------

Features
~~~~~~~~

* Add ACL token support to agent.service.register and agent.check.register

0.4.6
------

Features
~~~~~~~~

* Add health.checks endpoint, update health TODOs (thanks @cruatta!)
* Improve error when a HTTP 503 status code is returned (thanks @raboof!)
* Added index and wait parameter to event.list (thanks @max0d41!)


0.4.5
------

Features
~~~~~~~~

* Allow SSL certificate verification to be disabled (thanks @jgadling!)
* Use requests.session for performance (thanks @msabramo!)
* Support 'wait' param for all blocking queries (thanks @rmt!)
* deduplicate query string when doing deletes with the std (requests) library
  (thanks @sduthil!)

0.4.4
------

Features
~~~~~~~~

* Support creation of ALCs with explicit ID. (thanks @KyleJamesWalker)

0.4.3
------

Features
~~~~~~~~

* Support 'dc' argument to health endpoints (thanks @etuttle!)

0.4.2
------

Features
~~~~~~~~

* Add status endpoints (thanks @cruatta!)

0.4.1
------

Features
~~~~~~~~

* Add health.node (thanks @davidbirdsong!)

0.4.0
-----

API changes (backwards incompatible)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Deprecated old health.check.ttl_pass call has been removed

* Deprecate loose parameters *script*, *interval*, *ttl*, *http* and *timeout*,
  to configure checks via agent.service.register and agent.check.register. Both
  methods now take a single argument to specify checks. A convenience
  consul.Check has been added to create checks.

0.3.20
------

Features
~~~~~~~~

* Add Node and Service Maintenance (thanks @cruatta!)

Bug Fix
~~~~~~~

* Unclosed connector Exception in consul.aio (thanks @jettify!)

0.3.19
------

Bug Fix
~~~~~~~

* Fix six dependency (thanks @pawlowskimichal!)

0.3.18
------

Features
~~~~~~~~

* Adding ability to register checks with services (thanks @cruatta!)

Bug Fix
~~~~~~~
* Fix distribution for consul.aio for python3 (thanks @mbachry!)

0.3.17
------

Features
~~~~~~~~

* Add address param to agent.service.register

0.3.16
------

Features
~~~~~~~~

* Add cas param for kv.delete (thanks @qix)

0.3.15
------

Features
~~~~~~~~

* Add tag parameter to health.service() (thanks @reversefold)

0.3.14
------

Features
~~~~~~~~

* add the keys and separator params to kv.get (thanks @Heuriskein)
* add support for the events api (thanks @Heuriskein!)

0.3.13
------

Features
~~~~~~~~

* add HTTP check support (thanks @JoeHazzers)
* raise ConsulException on kv.get 500 response code (thanks @jjpersch)
* add the wait argument to kv.get

0.3.12
------

Features
~~~~~~~~

* add behavior and ttl to session.create
* add session.renew

0.3.11
------

Features
~~~~~~~~

* add the health.state endpoint (thanks @pete0emerson!)
* bump test binaries to 0.5.0

0.3.9
-----

Bug Fix
~~~~~~~

* Exclude consul.aio if asyncio isn't available, avoids an error message on
  install, trying to byte compile that module

0.3.8
-----

API changes (backwards incompatible)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Reorder named arguments to be more consistent. index is always the first
  named argument, if available, and dc is now always the last named argument.

0.3.7
-----

Features
~~~~~~~~

* Add dc support for kv calls; add ability to set the default dc for an entire
  client session (thanks @angad)
* Add asyncio client (thanks @jettify)

0.3.6
-----

Features
~~~~~~~~

* Add https support (thanks @pete0emerson)
* Add wan param to agent.members (thanks @sgargan)

0.3.5
-----

Bug Fix
~~~~~~~

* Fix typo setting notes on a check (thanks @ShaheedHaque!)

0.3.4
-----

Features
~~~~~~~~

* Add support for the Agent.Check (thanks @sgargan and @ShaheedHaque)

Deprecated
~~~~~~~~~~

* health.check.ttl_pass has been moved to agent.check.ttl_pass

0.3.3
-----

Features
~~~~~~~~

* Add support for the Session API (Consul.Session)

Bug Fixes
~~~~~~~~~

* Fix a bug retrieving folder nodes from the KV store
  https://github.com/cablehead/python-consul/pull/6#issue-48589128
  Thanks @zacman85

0.3.2
-----

Features
~~~~~~~~

* Add support for Python 3.4

0.3.1
-----

Features
~~~~~~~~

* Add support for the Catalog API (Consul.Catalog)
* Add ability to set a default consistency mode for an entire client session
* Add the ability to pass the consistency mode with kv.get

0.3.0
-----

Features
~~~~~~~~

* Add support for ACLs (Consul.ACL)


API changes (backwards incompatible)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* For Consul.Agent.Service.register, rename *check* argument to *script*



  * [python-dateutil-2.7.3](https://dateutil.readthedocs.io) dateutil - powerful extensions to datetime
==========================================

|pypi| |support| |licence| 

|gitter| |readthedocs|

|travis| |appveyor| |coverage|

.. |pypi| image:: https://img.shields.io/pypi/v/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: pypi version

.. |support| image:: https://img.shields.io/pypi/pyversions/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: supported Python version

.. |travis| image:: https://img.shields.io/travis/dateutil/dateutil/master.svg?style=flat-square&label=Travis%20Build
    :target: https://travis-ci.org/dateutil/dateutil
    :alt: travis build status

.. |appveyor| image:: https://img.shields.io/appveyor/ci/dateutil/dateutil/master.svg?style=flat-square&logo=appveyor
    :target: https://ci.appveyor.com/project/dateutil/dateutil
    :alt: appveyor build status

.. |coverage| image:: https://codecov.io/github/dateutil/dateutil/coverage.svg?branch=master
    :target: https://codecov.io/github/dateutil/dateutil?branch=master
    :alt: Code coverage

.. |gitter| image:: https://badges.gitter.im/dateutil/dateutil.svg
   :alt: Join the chat at https://gitter.im/dateutil/dateutil
   :target: https://gitter.im/dateutil/dateutil

.. |licence| image:: https://img.shields.io/pypi/l/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: licence

.. |readthedocs| image:: https://img.shields.io/readthedocs/dateutil/latest.svg?style=flat-square&label=Read%20the%20Docs
   :alt: Read the documentation at https://dateutil.readthedocs.io/en/latest/
   :target: https://dateutil.readthedocs.io/en/latest/

The `dateutil` module provides powerful extensions to
the standard `datetime` module, available in Python.

Installation
============
`dateutil` can be installed from PyPI using `pip` (note that the package name is
different from the importable name)::

	pip install python-dateutil

Download
========
dateutil is available on PyPI
https://pypi.org/project/python-dateutil/

The documentation is hosted at:
https://dateutil.readthedocs.io/en/stable/

Code
====
The code and issue tracker are hosted on Github:
https://github.com/dateutil/dateutil/

Features
========

* Computing of relative deltas (next month, next year,
  next monday, last week of month, etc);
* Computing of relative deltas between two given
  date and/or datetime objects;
* Computing of dates based on very flexible recurrence rules,
  using a superset of the `iCalendar <https://www.ietf.org/rfc/rfc2445.txt>`_
  specification. Parsing of RFC strings is supported as well.
* Generic parsing of dates in almost any string format;
* Timezone (tzinfo) implementations for tzfile(5) format
  files (/etc/localtime, /usr/share/zoneinfo, etc), TZ
  environment string (in all known formats), iCalendar
  format files, given ranges (with help from relative deltas),
  local machine timezone, fixed offset timezone, UTC timezone,
  and Windows registry-based time zones.
* Internal up-to-date world timezone information based on
  Olson's database.
* Computing of Easter Sunday dates for any given year,
  using Western, Orthodox or Julian algorithms;
* A comprehensive test suite.

Quick example
=============
Here's a snapshot, just to give an idea about the power of the
package. For more examples, look at the documentation.

Suppose you want to know how much time is left, in
years/months/days/etc, before the next easter happening on a
year with a Friday 13th in August, and you want to get today's
date out of the "date" unix system command. Here is the code:

.. code-block:: python3

    >>> from dateutil.relativedelta import *
    >>> from dateutil.easter import *
    >>> from dateutil.rrule import *
    >>> from dateutil.parser import *
    >>> from datetime import *
    >>> now = parse("Sat Oct 11 17:13:46 UTC 2003")
    >>> today = now.date()
    >>> year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year
    >>> rdelta = relativedelta(easter(year), today)
    >>> print("Today is: %s" % today)
    Today is: 2003-10-11
    >>> print("Year with next Aug 13th on a Friday is: %s" % year)
    Year with next Aug 13th on a Friday is: 2004
    >>> print("How far is the Easter of that year: %s" % rdelta)
    How far is the Easter of that year: relativedelta(months=+6)
    >>> print("And the Easter of that year is: %s" % (today+rdelta))
    And the Easter of that year is: 2004-04-11

Being exactly 6 months ahead was **really** a coincidence :)

Contributing
============

We welcome many types of contributions - bug reports, pull requests (code, infrastructure or documentation fixes). For more information about how to contribute to the project, see the ``CONTRIBUTING.md`` file in the repository.


Author
======
The dateutil module was written by Gustavo Niemeyer <gustavo@niemeyer.net>
in 2003.

It is maintained by:

* Gustavo Niemeyer <gustavo@niemeyer.net> 2003-2011
* Tomi Pieviläinen <tomi.pievilainen@iki.fi> 2012-2014
* Yaron de Leeuw <me@jarondl.net> 2014-2016
* Paul Ganssle <paul@ganssle.io> 2015-

Starting with version 2.4.1, all source and binary distributions will be signed
by a PGP key that has, at the very least, been signed by the key which made the
previous release. A table of release signing keys can be found below:

===========  ============================
Releases     Signing key fingerprint
===========  ============================
2.4.1-       `6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB`_ (|pgp_mirror|_)
===========  ============================


Contact
=======
Our mailing list is available at `dateutil@python.org <https://mail.python.org/mailman/listinfo/dateutil>`_. As it is hosted by the PSF, it is subject to the `PSF code of
conduct <https://www.python.org/psf/codeofconduct/>`_.

License
=======

All contributions after December 1, 2017 released under dual license - either `Apache 2.0 License <https://www.apache.org/licenses/LICENSE-2.0>`_ or the `BSD 3-Clause License <https://opensource.org/licenses/BSD-3-Clause>`_. Contributions before December 1, 2017 - except those those explicitly relicensed - are released only under the BSD 3-Clause License.


.. _6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB:
   https://pgp.mit.edu/pks/lookup?op=vindex&search=0xCD54FCE3D964BEFB

.. |pgp_mirror| replace:: mirror
.. _pgp_mirror: https://sks-keyservers.net/pks/lookup?op=vindex&search=0xCD54FCE3D964BEFB



  * [python-dotenv-0.10.3](http://github.com/theskumar/python-dotenv) ```
        _______ .__   __. ____    ____
       |   ____||  \ |  | \   \  /   /
       |  |__   |   \|  |  \   \/   /
       |   __|  |  . `  |   \      /
    __ |  |____ |  |\   |    \    /
   (__)|_______||__| \__|     \__/
```
python-dotenv | [![Build Status](https://travis-ci.org/theskumar/python-dotenv.svg?branch=master)](https://travis-ci.org/theskumar/python-dotenv) [![Coverage Status](https://coveralls.io/repos/theskumar/python-dotenv/badge.svg?branch=master)](https://coveralls.io/r/theskumar/python-dotenv?branch=master) [![PyPI version](https://badge.fury.io/py/python-dotenv.svg)](http://badge.fury.io/py/python-dotenv) [![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/theskumar)
===============================================================================

Reads the key,value pair from `.env` file and adds them to environment
variable. It is great for managing app settings during development and
in production using [12-factor](http://12factor.net/) principles.

> Do one thing, do it well!

- [Usages](#usages)
- [Installation](#installation)
- [Command-line interface](#command-line-interface)
- [iPython Support](#ipython-support)
- [Setting config on remote servers](#setting-config-on-remote-servers)
- [Related Projects](#related-projects)
- [Contributing](#contributing)
- [Changelog](#changelog)

> Hey just wanted to let you know that since I've started writing 12-factor apps I've found python-dotenv to be invaluable for all my projects. It's super useful and “just works.” --Daniel Fridkin

Usages
======

The easiest and most common usage consists on calling `load_dotenv` when
the application starts, which will load environment variables from a
file named `.env` in the current directory or any of its parents or from
the path specificied; after that, you can just call the
environment-related method you need as provided by `os.getenv`.

`.env` looks like this:

```shell
# a comment and that will be ignored.
REDIS_ADDRESS=localhost:6379
MEANING_OF_LIFE=42
MULTILINE_VAR="hello\nworld"
```

You can optionally prefix each line with the word `export`, which is totally ignored by this library, but might allow you to [`source`](https://bash.cyberciti.biz/guide/Source_command) the file in bash.

```
export S3_BUCKET=YOURS3BUCKET
export SECRET_KEY=YOURSECRETKEYGOESHERE
```

`.env` can interpolate variables using POSIX variable expansion,
variables are replaced from the environment first or from other values
in the `.env` file if the variable is not present in the environment.
(`Note`: Default Value Expansion is not supported as of yet, see
[\#30](https://github.com/theskumar/python-dotenv/pull/30#issuecomment-244036604).)

```shell
CONFIG_PATH=${HOME}/.config/foo
DOMAIN=example.org
EMAIL=admin@${DOMAIN}
```

Getting started
===============

Assuming you have created the `.env` file along-side your settings
module.

    .
    ├── .env
    └── settings.py

Add the following code to your `settings.py`

```python
# settings.py
from dotenv import load_dotenv
load_dotenv()

# OR, the same with increased verbosity:
load_dotenv(verbose=True)

# OR, explicitly providing path to '.env'
from pathlib import Path  # python3 only
env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path)
```

At this point, parsed key/value from the .env file is now present as
system environment variable and they can be conveniently accessed via
`os.getenv()`

```python
# settings.py
import os
SECRET_KEY = os.getenv("EMAIL")
DATABASE_PASSWORD = os.getenv("DATABASE_PASSWORD")
```

`load_dotenv` do not override existing System environment variables. To
override, pass `override=True` to `load_dotenv()`.

`load_dotenv` also accepts `encoding` parameter to open the `.env` file. The default encoding is platform dependent (whatever `locale.getpreferredencoding()` returns), but any encoding supported by Python can be used. See the [codecs](https://docs.python.org/3/library/codecs.html#standard-encodings) module for the list of supported encodings.

You can use `find_dotenv()` method that will try to find a `.env` file
by (a) guessing where to start using `__file__` or the working directory
-- allowing this to work in non-file contexts such as IPython notebooks
and the REPL, and then (b) walking up the directory tree looking for the
specified file -- called `.env` by default.

```python
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
```

In-memory filelikes
-------------------

It is possible to not rely on the filesystem to parse filelikes from
other sources (e.g. from a network storage). `load_dotenv` and
`dotenv_values` accepts a filelike `stream`. Just be sure to rewind it
before passing.

```python
>>> from io import StringIO     # Python2: from StringIO import StringIO
>>> from dotenv import dotenv_values
>>> filelike = StringIO('SPAM=EGGS\n')
>>> filelike.seek(0)
>>> parsed = dotenv_values(stream=filelike)
>>> parsed['SPAM']
'EGGS'
```

The returned value is dictionary with key value pair.

`dotenv_values` could be useful if you need to *consume* the envfile but
not *apply* it directly into the system environment.

Django
------

If you are using django you should add the above loader script at the
top of `wsgi.py` and `manage.py`.

Installation
============

    pip install -U python-dotenv

iPython Support
---------------

You can use dotenv with iPython. You can either let the dotenv search
for .env with %dotenv or provide the path to .env file explicitly, see
below for usages.

    %load_ext dotenv

    # Use find_dotenv to locate the file
    %dotenv

    # Specify a particular file
    %dotenv relative/or/absolute/path/to/.env

    # Use '-o' to indicate override of existing variables
    %dotenv -o

    # Use '-v' to turn verbose mode on
    %dotenv -v

Command-line interface
======================

For commandline support, use the cli option during installation:

    pip install -U "python-dotenv[cli]"

A cli interface `dotenv` is also included, which helps you manipulate
the `.env` file without manually opening it. The same cli installed on
remote machine combined with fabric (discussed later) will enable you to
update your settings on remote server, handy isn't it!

```
Usage: dotenv [OPTIONS] COMMAND [ARGS]...

  This script is used to set, get or unset values from a .env file.

Options:
  -f, --file PATH                 Location of the .env file, defaults to .env
                                  file in current working directory.
  -q, --quote [always|never|auto]
                                  Whether to quote or not the variable values.
                                  Default mode is always. This does not affect
                                  parsing.
  --help                          Show this message and exit.

Commands:
  get    Retrive the value for the given key.
  list   Display all the stored key/value.
  run    Run command with environment variables from .env file present
  set    Store the given key/value.
  unset  Removes the given key.
```

Setting config on remote servers
--------------------------------

We make use of excellent [Fabric](http://www.fabfile.org/) to acomplish
this. Add a config task to your local fabfile, `dotenv_path` is the
location of the absolute path of `.env` file on the remote server.

```python
# fabfile.py

import dotenv
from fabric.api import task, run, env

# absolute path to the location of .env on remote server.
env.dotenv_path = '/opt/myapp/.env'

@task
def config(action=None, key=None, value=None):
    '''Manage project configuration via .env

    e.g: fab config:set,<key>,<value>
         fab config:get,<key>
         fab config:unset,<key>
         fab config:list
    '''
    run('touch %(dotenv_path)s' % env)
    command = dotenv.get_cli_string(env.dotenv_path, action, key, value)
    run(command)
```

Usage is designed to mirror the heroku config api very closely.

Get all your remote config info with `fab config`

    $ fab config
    foo="bar"

Set remote config variables with `fab config:set,<key>,<value>`

    $ fab config:set,hello,world

Get a single remote config variables with `fab config:get,<key>`

    $ fab config:get,hello

Delete a remote config variables with `fab config:unset,<key>`

    $ fab config:unset,hello

Thanks entirely to fabric and not one bit to this project, you can chain
commands like so
`fab config:set,<key1>,<value1> config:set,<key2>,<value2>`

    $ fab config:set,hello,world config:set,foo,bar config:set,fizz=buzz

Related Projects
================

-   [Honcho](https://github.com/nickstenning/honcho) - For managing
    Procfile-based applications.
-   [django-dotenv](https://github.com/jpadilla/django-dotenv)
-   [django-environ](https://github.com/joke2k/django-environ)
-   [django-configuration](https://github.com/jezdez/django-configurations)
-   [dump-env](https://github.com/sobolevn/dump-env)
-   [environs](https://github.com/sloria/environs)

Contributing
============

All the contributions are welcome! Please open [an
issue](https://github.com/theskumar/python-dotenv/issues/new) or send us
a pull request.

This project is currently maintained by [Saurabh Kumar](https://saurabh-kumar.com) and [Bertrand Bonnefoy-Claudet](https://github.com/bbc2) and would not
have been possible without the support of these [awesome
people](https://github.com/theskumar/python-dotenv/graphs/contributors).

Executing the tests:

    $ pip install -r requirements.txt
    $ pip install -e .
    $ flake8
    $ pytest

or with [tox](https://pypi.org/project/tox/) installed:

    $ tox

Changelog
=========

Unreleased
-----

- Improve interactive mode detection ([@andrewsmith])([#183]).
- Refactor parser to fix parsing inconsistencies ([@bbc2])([#170]).
  - Interpret escapes as control characters only in double-quoted strings.
  - Interpret `#` as start of comment only if preceded by whitespace.

0.10.2
-----

- Add type hints and expose them to users ([@qnighy])([#172])
- `load_dotenv` and `dotenv_values` now accept an `encoding` parameter, defaults to `None`
  ([@theskumar])([@earlbread])([#161])
- Fix `str`/`unicode` inconsistency in Python 2: values are always `str` now. ([@bbc2])([#121])
- Fix Unicode error in Python 2, introduced in 0.10.0. ([@bbc2])([#176])

0.10.1
-----
- Fix parsing of variable without a value ([@asyncee])([@bbc2])([#158])

0.10.0
-----

- Add support for UTF-8 in unquoted values ([@bbc2])([#148])
- Add support for trailing comments ([@bbc2])([#148])
- Add backslashes support in values ([@bbc2])([#148])
- Add support for newlines in values ([@bbc2])([#148])
- Force environment variables to str with Python2 on Windows ([@greyli])
- Drop Python 3.3 support ([@greyli])
- Fix stderr/-out/-in redirection ([@venthur])


0.9.0
-----
- Add `--version` parameter to cli ([@venthur])
- Enable loading from current directory ([@cjauvin])
- Add 'dotenv run' command for calling arbitrary shell script with .env ([@venthur])

0.8.1
-----

-   Add tests for docs ([@Flimm])
-   Make 'cli' support optional. Use `pip install python-dotenv[cli]`. ([@theskumar])

0.8.0
-----

-   `set_key` and `unset_key` only modified the affected file instead of
    parsing and re-writing file, this causes comments and other file
    entact as it is.
-   Add support for `export` prefix in the line.
-   Internal refractoring ([@theskumar])
-   Allow `load_dotenv` and `dotenv_values` to work with `StringIO())` ([@alanjds])([@theskumar])([#78])

0.7.1
-----

-   Remove hard dependency on iPython ([@theskumar])

0.7.0
-----

-   Add support to override system environment variable via .env.
    ([@milonimrod](https://github.com/milonimrod))
    ([\#63](https://github.com/theskumar/python-dotenv/issues/63))
-   Disable ".env not found" warning by default
    ([@maxkoryukov](https://github.com/maxkoryukov))
    ([\#57](https://github.com/theskumar/python-dotenv/issues/57))

0.6.5
-----

-   Add support for special characters `\`.
    ([@pjona](https://github.com/pjona))
    ([\#60](https://github.com/theskumar/python-dotenv/issues/60))

0.6.4
-----

-   Fix issue with single quotes ([@Flimm])
    ([\#52](https://github.com/theskumar/python-dotenv/issues/52))

0.6.3
-----

-   Handle unicode exception in setup.py
    ([\#46](https://github.com/theskumar/python-dotenv/issues/46))

0.6.2
-----

-   Fix dotenv list command ([@ticosax](https://github.com/ticosax))
-   Add iPython Suport
    ([@tillahoffmann](https://github.com/tillahoffmann))

0.6.0
-----

-   Drop support for Python 2.6
-   Handle escaped charaters and newlines in quoted values. (Thanks
    [@iameugenejo](https://github.com/iameugenejo))
-   Remove any spaces around unquoted key/value. (Thanks
    [@paulochf](https://github.com/paulochf))
-   Added POSIX variable expansion. (Thanks
    [@hugochinchilla](https://github.com/hugochinchilla))

0.5.1
-----

-   Fix find\_dotenv - it now start search from the file where this
    function is called from.

0.5.0
-----

-   Add `find_dotenv` method that will try to find a `.env` file.
    (Thanks [@isms](https://github.com/isms))

0.4.0
-----

-   cli: Added `-q/--quote` option to control the behaviour of quotes
    around values in `.env`. (Thanks
    [@hugochinchilla](https://github.com/hugochinchilla)).
-   Improved test coverage.

[#161]: https://github.com/theskumar/python-dotenv/issues/161
[#78]: https://github.com/theskumar/python-dotenv/issues/78
[#148]: https://github.com/theskumar/python-dotenv/issues/148
[#158]: https://github.com/theskumar/python-dotenv/issues/158
[#172]: https://github.com/theskumar/python-dotenv/issues/172
[#121]: https://github.com/theskumar/python-dotenv/issues/121
[#176]: https://github.com/theskumar/python-dotenv/issues/176
[#170]: https://github.com/theskumar/python-dotenv/issues/170
[#183]: https://github.com/theskumar/python-dotenv/issues/183

[@andrewsmith]: https://github.com/andrewsmith
[@asyncee]: https://github.com/asyncee
[@greyli]: https://github.com/greyli
[@venthur]: https://github.com/venthur
[@Flimm]: https://github.com/Flimm
[@theskumar]: https://github.com/theskumar
[@alanjds]: https://github.com/alanjds
[@cjauvin]: https://github.com/cjauvin
[@bbc2]: https://github.com/bbc2
[@qnighy]: https://github.com/qnighy
[@earlbread]: https://github.com/earlbread



  * [python-editor-1.0.4](https://github.com/fmoo/python-editor) `python-editor` is a library that provides the `editor` module for programmatically
interfacing with your system's $EDITOR.

Examples
--------

```python
import editor
commit_msg = editor.edit(contents=b"# Enter commit message here")
```

Opens an editor, prefilled with the contents, `# Enter commit message here`.
When the editor is closed, returns the contents (bytes) in variable `commit_msg`.
Note that the argument to `contents` needs to be a bytes object on Python 3.


```python
editor.edit(file="README.txt")
```

Opens README.txt in an editor.  Changes are saved in place.  If there is
a `contents` argument then the file contents will be overwritten.

```python
editor.edit(..., use_tty=True)
```

Opens the editor in a TTY.  This is usually done in programs which output is
piped to other programs.  In this case the TTY is used as the editor's stdout,
allowing interactive usage.


How it Works
------------

`editor` first looks for the ${EDITOR} environment variable.  If set, it uses
the value as-is, without fallbacks.

If no $EDITOR is set, editor will search through a list of known editors, and
use the first one that exists on the system.

For example, on Linux, `editor` will look for the following editors in order:

* vim
* emacs
* nano

When calling `editor.edit`, an editor will be opened in a subprocess, inheriting
the parent process's stdin, stdout.



  * [python-freezerclient-2.1.0](https://docs.openstack.org/python-freezerclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-freezerclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

=============================================================
Python bindings to the OpenStack Backup/Restore API (Freezer)
=============================================================

.. image:: https://img.shields.io/pypi/v/python-freezerclient.svg
    :target: https://pypi.org/project/python-freezerclient/
    :alt: Latest Version

This is a client library for Freezer built on the OpenStack Disaster Recovery API. It provides a Python API (the freezerclient module) and a command-line tool (freezer). This library fully supports the v1 Disaster Recovery API.

Development takes place via the usual OpenStack processes as outlined in the `developer guide <https://docs.openstack.org/infra/manual/developers.html>`_.  The master repository is in `Git <https://opendev.org/openstack/python-freezerclient>`_.


* License: Apache License, Version 2.0
* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `How to Contribute`_

.. _PyPi: https://pypi.org/project/python-freezerclient
.. _Online Documentation: https://wiki.openstack.org/wiki/Python-freezerclient
.. _Launchpad project: https://launchpad.net/python-freezerclient
.. _Blueprints: https://blueprints.launchpad.net/python-freezerclient
.. _Bugs: https://storyboard.openstack.org/#!/project/openstack/python-freezerclient
.. _Source: https://opendev.org/openstack/python-freezerclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Release notes: https://docs.openstack.org/releasenotes/python-freezerclient/




  * [python-glanceclient-2.16.0](https://docs.openstack.org/python-glanceclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-glanceclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html
    :alt: The following tags have been asserted for Python bindings to the
          OpenStack Images API:
          "project:official",
          "stable:follows-policy",
          "vulnerability:managed".
          Follow the link for an explanation of these tags.
.. NOTE(rosmaita): the alt text above will have to be updated when
   additional tags are asserted for python-glanceclient.  (The SVG in the
   governance repo is updated automatically.)

.. Change things from this point on

===========================================
Python bindings to the OpenStack Images API
===========================================

.. image:: https://img.shields.io/pypi/v/python-glanceclient.svg
    :target: https://pypi.org/project/python-glanceclient/
    :alt: Latest Version

This is a client library for Glance built on the OpenStack Images API. It provides a Python API (the ``glanceclient`` module) and a command-line tool (``glance``). This library fully supports the v1 Images API, while support for the v2 API is in progress.

Development takes place via the usual OpenStack processes as outlined in the `developer guide <https://docs.openstack.org/infra/manual/developers.html>`_.  The master repository is in `Git <https://git.openstack.org/cgit/openstack/python-glanceclient>`_.

See release notes and more at `<https://docs.openstack.org/python-glanceclient/latest/>`_.

* License: Apache License, Version 2.0
* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Specs`_
* `How to Contribute`_

.. _PyPi: https://pypi.org/project/python-glanceclient
.. _Online Documentation: https://docs.openstack.org/python-glanceclient/latest/
.. _Launchpad project: https://launchpad.net/python-glanceclient
.. _Blueprints: https://blueprints.launchpad.net/python-glanceclient
.. _Bugs: https://bugs.launchpad.net/python-glanceclient
.. _Source: https://git.openstack.org/cgit/openstack/python-glanceclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Specs: https://specs.openstack.org/openstack/glance-specs/
.. _Release notes: https://docs.openstack.org/releasenotes/python-glanceclient




  * [python-hostlist-1.18](http://www.nsc.liu.se/~kent/python-hostlist/) The hostlist.py module knows how to expand and collect hostlist expressions.
  * [python-igraph-0.7.1.post6](http://pypi.python.org/pypi/python-igraph) Python interface to the igraph high performance graph
library, primarily aimed at complex network research and analysis.

Graph plotting functionality is provided by the Cairo library, so make
sure you install the Python bindings of Cairo if you want to generate
publication-quality graph plots.

See the `Cairo homepage <http://cairographics.org/pycairo>`_ for details.

From release 0.5, the C core of the igraph library is **not** included
in the Python distribution - you must compile and install the C core
separately. Windows installers already contain a compiled igraph DLL,
so they should work out of the box. Linux users should refer to the
`igraph homepage <http://igraph.org>`_ for
compilation instructions (but check your distribution first, maybe
there are pre-compiled packages available). OS X users may
benefit from the disk images in the Python Package Index.

Unofficial installers for 64-bit Windows machines and/or different Python
versions can also be found `here <http://www.lfd.uci.edu/~gohlke/pythonlibs>`_.
Many thanks to the maintainers of this page!
  * [python-json-logger-0.1.11](http://github.com/madzak/python-json-logger) 
  * [python-keystoneclient-3.20.0](https://docs.openstack.org/python-keystoneclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-keystoneclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

Python bindings to the OpenStack Identity API (Keystone)
========================================================

.. image:: https://img.shields.io/pypi/v/python-keystoneclient.svg
    :target: https://pypi.org/project/python-keystoneclient/
    :alt: Latest Version

This is a client for the OpenStack Identity API, implemented by the Keystone
team; it contains a Python API (the ``keystoneclient`` module) for
OpenStack's Identity Service. For command line interface support, use
`OpenStackClient`_.

* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Specs`_
* `How to Contribute`_
* `Release Notes`_

.. _PyPi: https://pypi.org/project/python-keystoneclient
.. _Online Documentation: https://docs.openstack.org/python-keystoneclient/latest/
.. _Launchpad project: https://launchpad.net/python-keystoneclient
.. _Blueprints: https://blueprints.launchpad.net/python-keystoneclient
.. _Bugs: https://bugs.launchpad.net/python-keystoneclient
.. _Source: https://opendev.org/openstack/python-keystoneclient
.. _OpenStackClient: https://pypi.org/project/python-openstackclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Specs: https://specs.openstack.org/openstack/keystone-specs/
.. _Release Notes: https://docs.openstack.org/releasenotes/python-keystoneclient

.. contents:: Contents:
   :local:

Python API
----------

By way of a quick-start::

    >>> from keystoneauth1.identity import v3
    >>> from keystoneauth1 import session
    >>> from keystoneclient.v3 import client
    >>> auth = v3.Password(auth_url="http://example.com:5000/v3", username="admin",
    ...                     password="password", project_name="admin",
    ...                     user_domain_id="default", project_domain_id="default")
    >>> sess = session.Session(auth=auth)
    >>> keystone = client.Client(session=sess)
    >>> keystone.projects.list()
        [...]
    >>> project = keystone.projects.create(name="test", description="My new Project!", domain="default", enabled=True)
    >>> project.delete()




  * [python-neutronclient-6.12.0](https://docs.openstack.org/python-neutronclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-neutronclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

Python bindings to the Neutron API
==================================

.. image:: https://img.shields.io/pypi/v/python-neutronclient.svg
    :target: https://pypi.org/project/python-neutronclient/
    :alt: Latest Version

This is a client library for Neutron built on the Neutron API. It
provides a Python API (the ``neutronclient`` module) and a command-line tool
(``neutron``).

* License: Apache License, Version 2.0
* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Developer's Guide`_

.. _PyPi: https://pypi.org/project/python-neutronclient
.. _Online Documentation: https://docs.openstack.org/python-neutronclient/latest/
.. _Launchpad project: https://launchpad.net/python-neutronclient
.. _Blueprints: https://blueprints.launchpad.net/python-neutronclient
.. _Bugs: https://bugs.launchpad.net/python-neutronclient
.. _Source: https://opendev.org/openstack/python-neutronclient
.. _Developer's Guide: http://docs.openstack.org/infra/manual/developers.html
.. _Release Notes: https://docs.openstack.org/releasenotes/python-neutronclient




  * [python-novaclient-14.2.0](https://docs.openstack.org/python-novaclient/latest) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-novaclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

============================================
Python bindings to the OpenStack Compute API
============================================

.. image:: https://img.shields.io/pypi/v/python-novaclient.svg
    :target: https://pypi.org/project/python-novaclient/
    :alt: Latest Version

This is a client for the OpenStack Compute API. It provides a Python API (the
``novaclient`` module) and a command-line script (``nova``). Each implements
100% of the OpenStack Compute API.

* License: Apache License, Version 2.0
* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Specs`_
* `How to Contribute`_
* `Release Notes`_

.. _PyPi: https://pypi.org/project/python-novaclient
.. _Online Documentation: https://docs.openstack.org/python-novaclient/latest
.. _Launchpad project: https://launchpad.net/python-novaclient
.. _Blueprints: https://blueprints.launchpad.net/python-novaclient
.. _Bugs: https://bugs.launchpad.net/python-novaclient
.. _Source: https://opendev.org/openstack/python-novaclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Specs: http://specs.openstack.org/openstack/nova-specs/
.. _Release Notes: https://docs.openstack.org/releasenotes/python-novaclient




  * [python-openstackclient-3.19.0](https://docs.openstack.org/python-openstackclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-openstackclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

===============
OpenStackClient
===============

.. image:: https://img.shields.io/pypi/v/python-openstackclient.svg
    :target: https://pypi.org/project/python-openstackclient/
    :alt: Latest Version

OpenStackClient (aka OSC) is a command-line client for OpenStack that brings
the command set for Compute, Identity, Image, Network, Object Store and Block
Storage APIs together in a single shell with a uniform command structure.

The primary goal is to provide a unified shell command structure and a common
language to describe operations in OpenStack.

* `PyPi`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Developer`_ - getting started as a developer
* `Contributing`_ - contributing code
* `Testing`_ - testing code
* IRC: #openstack-sdks on Freenode (irc.freenode.net)
* License: Apache 2.0

.. _PyPi: https://pypi.org/project/python-openstackclient
.. _Online Documentation: https://docs.openstack.org/python-openstackclient/latest/
.. _Launchpad project: https://launchpad.net/python-openstackclient
.. _Blueprints: https://blueprints.launchpad.net/python-openstackclient
.. _Bugs: https://storyboard.openstack.org/#!/project/975
.. _Source: https://opendev.org/openstack/python-openstackclient
.. _Developer: https://docs.openstack.org/project-team-guide/project-setup/python.html
.. _Contributing: https://docs.openstack.org/infra/manual/developers.html
.. _Testing: https://docs.openstack.org/python-openstackclient/latest/contributor/developing.html#testing
.. _Release Notes: https://docs.openstack.org/releasenotes/python-openstackclient

Getting Started
===============

OpenStack Client can be installed from PyPI using pip::

    pip install python-openstackclient

There are a few variants on getting help.  A list of global options and supported
commands is shown with ``--help``::

   openstack --help

There is also a ``help`` command that can be used to get help text for a specific
command::

    openstack help
    openstack help server create

If you want to make changes to the OpenStackClient for testing and contribution,
make any changes and then run::

    python setup.py develop

or::

    pip install -e .

Configuration
=============

The CLI is configured via environment variables and command-line
options as listed in  https://docs.openstack.org/python-openstackclient/latest/cli/authentication.html.

Authentication using username/password is most commonly used::

   export OS_AUTH_URL=<url-to-openstack-identity>
   export OS_IDENTITY_API_VERSION=3
   export OS_PROJECT_NAME=<project-name>
   export OS_PROJECT_DOMAIN_NAME=<project-domain-name>
   export OS_USERNAME=<username>
   export OS_USER_DOMAIN_NAME=<user-domain-name>
   export OS_PASSWORD=<password>  # (optional)

The corresponding command-line options look very similar::

   --os-auth-url <url>
   --os-identity-api-version 3
   --os-project-name <project-name>
   --os-project-domain-name <project-domain-name>
   --os-username <username>
   --os-user-domain-name <user-domain-name>
   [--os-password <password>]

If a password is not provided above (in plaintext), you will be interactively
prompted to provide one securely.




  * [python-pam-1.8.4](https://github.com/FirefighterBlu3/python-pam) python-pam
==========

Python pam module supporting py3 (and py2)

Commandline example:

```
[david@Scott python-pam]$ python pam.py
Username: david
Password: 
0 Success

[david@Scott python-pam]$ python2 pam.py
Username: david
Password: 
0 Success
```

Inline examples:
```
[david@Scott python-pam]$ python
Python 3.4.1 (default, May 19 2014, 17:23:49)
[GCC 4.9.0 20140507 (prerelease)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import pam
>>> p = pam.pam()
>>> p.authenticate('david', 'correctpassword')
True
>>> p.authenticate('david', 'badpassword')
False
>>> p.authenticate('david', 'correctpassword', service='login')
True
>>> p.authenticate('david', 'correctpassword', service='unknownservice')
False
>>> p.authenticate('david', 'correctpassword', service='login', resetcreds=True)
True
>>> p.authenticate('david', 'correctpassword', encoding='latin-1')
True
>>> print('{} {}'.format(p.code, p.reason))
0 Success
>>> p.authenticate('david', 'badpassword')
False
>>> print('{} {}'.format(p.code, p.reason))
7 Authentication failure
>>>
```



  * [python-snappy-0.5.4](http://github.com/andrix/python-snappy) 
Python bindings for the snappy compression library from Google.

More details about Snappy library: http://google.github.io/snappy



  * [python-subunit-1.3.0](http://launchpad.net/subunit) 
  subunit: A streaming protocol for test results
  Copyright (C) 2005-2013 Robert Collins <robertc@robertcollins.net>

  Licensed under either the Apache License, Version 2.0 or the BSD 3-clause
  license at the users choice. A copy of both licenses are available in the
  project source as Apache-2.0 and BSD. You may not use this file except in
  compliance with one of these two licences.

  Unless required by applicable law or agreed to in writing, software
  distributed under these licenses is distributed on an "AS IS" BASIS, WITHOUT
  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
  license you chose for the specific language governing permissions and
  limitations under that license.

  See the COPYING file for full details on the licensing of Subunit.

  subunit reuses iso8601 by Michael Twomey, distributed under an MIT style
  licence - see python/iso8601/LICENSE for details.

Subunit
-------

Subunit is a streaming protocol for test results.

There are two major revisions of the protocol. Version 1 was trivially human
readable but had significant defects as far as highly parallel testing was
concerned - it had no room for doing discovery and execution in parallel,
required substantial buffering when multiplexing and was fragile - a corrupt
byte could cause an entire stream to be misparsed. Version 1.1 added
encapsulation of binary streams which mitigated some of the issues but the
core remained.

Version 2 shares many of the good characteristics of Version 1 - it can be
embedded into a regular text stream (e.g. from a build system) and it still
models xUnit style test execution. It also fixes many of the issues with
Version 1 - Version 2 can be multiplexed without excessive buffering (in
time or space), it has a well defined recovery mechanism for dealing with
corrupted streams (e.g. where two processes write to the same stream
concurrently, or where the stream generator suffers a bug).

More details on both protocol version s can be found in the 'Protocol' section
of this document.

Subunit comes with command line filters to process a subunit stream and
language bindings for python, C, C++ and shell. Bindings are easy to write
for other languages.

A number of useful things can be done easily with subunit:
 * Test aggregation: Tests run separately can be combined and then
   reported/displayed together. For instance, tests from different languages
   can be shown as a seamless whole, and tests running on multiple machines
   can be aggregated into a single stream through a multiplexer.
 * Test archiving: A test run may be recorded and replayed later.
 * Test isolation: Tests that may crash or otherwise interact badly with each
   other can be run seperately and then aggregated, rather than interfering
   with each other or requiring an adhoc test->runner reporting protocol.
 * Grid testing: subunit can act as the necessary serialisation and
   deserialiation to get test runs on distributed machines to be reported in
   real time.

Subunit supplies the following filters:
 * tap2subunit - convert perl's TestAnythingProtocol to subunit.
 * subunit2csv - convert a subunit stream to csv.
 * subunit2disk - export a subunit stream to files on disk.
 * subunit2pyunit - convert a subunit stream to pyunit test results.
 * subunit2gtk - show a subunit stream in GTK.
 * subunit2junitxml - convert a subunit stream to JUnit's XML format.
 * subunit-diff - compare two subunit streams.
 * subunit-filter - filter out tests from a subunit stream.
 * subunit-ls - list info about tests present in a subunit stream.
 * subunit-stats - generate a summary of a subunit stream.
 * subunit-tags - add or remove tags from a stream.

Integration with other tools
----------------------------

Subunit's language bindings act as integration with various test runners like
'check', 'cppunit', Python's 'unittest'. Beyond that a small amount of glue
(typically a few lines) will allow Subunit to be used in more sophisticated
ways.

Python
======

Subunit has excellent Python support: most of the filters and tools are written
in python and there are facilities for using Subunit to increase test isolation
seamlessly within a test suite.

The most common way is to run an existing python test suite and have it output
subunit via the ``subunit.run`` module::

  $ python -m subunit.run mypackage.tests.test_suite

For more information on the Python support Subunit offers , please see
``pydoc subunit``, or the source in ``python/subunit/``

C
=

Subunit has C bindings to emit the protocol. The 'check' C unit testing project
has included subunit support in their project for some years now. See
'c/README' for more details.

C++
===

The C library is includable and usable directly from C++. A TestListener for
CPPUnit is included in the Subunit distribution. See 'c++/README' for details.

shell
=====

There are two sets of shell tools. There are filters, which accept a subunit
stream on stdin and output processed data (or a transformed stream) on stdout.

Then there are unittest facilities similar to those for C : shell bindings
consisting of simple functions to output protocol elements, and a patch for
adding subunit output to the 'ShUnit' shell test runner. See 'shell/README' for
details.

Filter recipes
--------------

To ignore some failing tests whose root cause is already known::

  subunit-filter --without 'AttributeError.*flavor'


The xUnit test model
--------------------

Subunit implements a slightly modified xUnit test model. The stock standard
model is that there are tests, which have an id(), can be run, and when run
start, emit an outcome (like success or failure) and then finish.

Subunit extends this with the idea of test enumeration (find out about tests
a runner has without running them), tags (allow users to describe tests in
ways the test framework doesn't apply any semantic value to), file attachments
(allow arbitrary data to make analysing a failure easy) and timestamps.

The protocol
------------

Version 2, or v2 is new and still under development, but is intended to
supercede version 1 in the very near future. Subunit's bundled tools accept
only version 2 and only emit version 2, but the new filters subunit-1to2 and
subunit-2to1 can be used to interoperate with older third party libraries.

Version 2
=========

Version 2 is a binary protocol consisting of independent packets that can be
embedded in the output from tools like make - as long as each packet has no
other bytes mixed in with it (which 'make -j N>1' has a tendency of doing).
Version 2 is currently in draft form, and early adopters should be willing
to either discard stored results (if protocol changes are made), or bulk
convert them back to v1 and then to a newer edition of v2.

The protocol synchronises at the start of the stream, after a packet, or
after any 0x0A byte. That is, a subunit v2 packet starts after a newline or
directly after the end of the prior packet.

Subunit is intended to be transported over a reliable streaming protocol such
as TCP. As such it does not concern itself with out of order delivery of
packets. However, because of the possibility of corruption due to either
bugs in the sender, or due to mixed up data from concurrent writes to the same
fd when being embedded, subunit strives to recover reasonably gracefully from
damaged data.

A key design goal for Subunit version 2 is to allow processing and multiplexing
without forcing buffering for semantic correctness, as buffering tends to hide
hung or otherwise misbehaving tests. That said, limited time based buffering
for network efficiency is a good idea - this is ultimately implementator
choice. Line buffering is also discouraged for subunit streams, as dropping
into a debugger or other tool may require interactive traffic even if line
buffering would not otherwise be a problem.

In version two there are two conceptual events - a test status event and a file
attachment event. Events may have timestamps, and the path of multiplexers that
an event is routed through is recorded to permit sending actions back to the
source (such as new tests to run or stdin for driving debuggers and other
interactive input). Test status events are used to enumerate tests, to report
tests and test helpers as they run. Tests may have tags, used to allow
tunnelling extra meanings through subunit without requiring parsing of
arbitrary file attachments. Things that are not standalone tests get marked
as such by setting the 'Runnable' flag to false. (For instance, individual
assertions in TAP are not runnable tests, only the top level TAP test script
is runnable).

File attachments are used to provide rich detail about the nature of a failure.
File attachments can also be used to encapsulate stdout and stderr both during
and outside tests.

Most numbers are stored in network byte order - Most Significant Byte first
encoded using a variation of http://www.dlugosz.com/ZIP2/VLI.html. The first
byte's top 2 high order bits encode the total number of octets in the number.
This encoding can encode values from 0 to 2**30-1, enough to encode a
nanosecond. Numbers that are not variable length encoded are still stored in
MSB order.

+--------+--------+---------+------------+
| prefix | octets | max     | max        |
+========+========+=========+============+
| 00     |      1 |  2**6-1 |         63 |
+--------+--------+---------+------------+
| 01     |      2 | 2**14-1 |      16383 |
+--------+--------+---------+------------+
| 10     |      3 | 2**22-1 |    4194303 |
+--------+--------+---------+------------+
| 11     |      4 | 2**30-1 | 1073741823 |
+--------+--------+---------+------------+

All variable length elements of the packet are stored with a length prefix
number allowing them to be skipped over for consumers that don't need to
interpret them.

UTF-8 strings are with no terminating NUL and should not have any embedded NULs
(implementations SHOULD validate any such strings that they process and take
some remedial action (such as discarding the packet as corrupt).

In short the structure of a packet is:

  PACKET := SIGNATURE FLAGS PACKET_LENGTH TIMESTAMP? TESTID? TAGS? MIME?
            FILECONTENT? ROUTING_CODE? CRC32

In more detail...

Packets are identified by a single byte signature - 0xB3, which is never legal
in a UTF-8 stream as the first byte of a character. 0xB3 starts with the first
bit set and the second not, which is the UTF-8 signature for a continuation
byte. 0xB3 was chosen as 0x73 ('s' in ASCII') with the top two bits replaced by
the 1 and 0 for a continuation byte.

If subunit packets are being embedded in a non-UTF-8 text stream, where 0x73 is
a legal character, consider either recoding the text to UTF-8, or using
subunit's 'file' packets to embed the text stream in subunit, rather than the
other way around.

Following the signature byte comes a 16-bit flags field, which includes a
4-bit version field - if the version is not 0x2 then the packet cannot be
read. It is recommended to signal an error at this point (e.g. by emitting
a synthetic error packet and returning to the top level loop to look for
new packets, or exiting with an error). If recovery is desired, treat the
packet signature as an opaque byte and scan for a new synchronisation point.
NB: Subunit V1 and V2 packets may legitimately included 0xB3 internally,
as they are an 8-bit safe container format, so recovery from this situation
may involve an arbitrary number of false positives until an actual packet
is encountered : and even then it may still be false, failing after passing
the version check due to coincidence.

Flags are stored in network byte order too.

+------------+------------+------------------------+
| High byte               | Low byte               |
+------------+------------+------------------------+
| 15 14 13 12 11 10  9  8 | 7  6  5  4  3  2  1  0 |
+------------+------------+------------------------+
| VERSION    |      feature bits                   |
+------------+-------------------------------------+

Valid version values are:
0x2 - version 2

Feature bits:
Bit 11 - mask 0x0800 - Test id present.
Bit 10 - mask 0x0400 - Routing code present.
Bit  9 - mask 0x0200 - Timestamp present.
Bit  8 - mask 0x0100 - Test is 'runnable'.
Bit  7 - mask 0x0080 - Tags are present.
Bit  6 - mask 0x0040 - File content is present.
Bit  5 - mask 0x0020 - File MIME type is present.
Bit  4 - mask 0x0010 - EOF marker.
Bit  3 - mask 0x0008 - Must be zero in version 2.

Test status gets three bits:
Bit 2 | Bit 1 | Bit 0 - mask 0x0007 - A test status enum lookup:
000 - undefined / no test
001 - Enumeration / existence
002 - In progress
003 - Success
004 - Unexpected Success
005 - Skipped
006 - Failed
007 - Expected failure

After the flags field is a number field giving the length in bytes for the
entire packet including the signature and the checksum. This length must
be less than 4MiB - 4194303 bytes. The encoding can obviously record a larger
number but one of the goals is to avoid requiring large buffers, or causing
large latency in the packet forward/processing pipeline. Larger file
attachments can be communicated in multiple packets, and the overhead in such a
4MiB packet is approximately 0.2%.

The rest of the packet is a series of optional features as specified by the set
feature bits in the flags field. When absent they are entirely absent.

Forwarding and multiplexing of packets can be done without interpreting the
remainder of the packet until the routing code and checksum (which are both at
the end of the packet). Additionally, routers can often avoid copying or moving
the bulk of the packet, as long as the routing code size increase doesn't force
the length encoding to take up a new byte (which will only happen to packets
less than or equal to 16KiB in length) - large packets are very efficient to
route.

Timestamp when present is a 32 bit unsigned integer for seconds, and a variable
length number for nanoseconds, representing UTC time since Unix Epoch in
seconds and nanoseconds.

Test id when present is a UTF-8 string. The test id should uniquely identify
runnable tests such that they can be selected individually. For tests and other
actions which cannot be individually run (such as test
fixtures/layers/subtests) uniqueness is not required (though being human
meaningful is highly recommended).

Tags when present is a length prefixed vector of UTF-8 strings, one per tag.
There are no restrictions on tag content (other than the restrictions on UTF-8
strings in subunit in general). Tags have no ordering.

When a MIME type is present, it defines the MIME type for the file across all
packets same file (routing code + testid + name uniquely identifies a file,
reset when EOF is flagged). If a file never has a MIME type set, it should be
treated as application/octet-stream.

File content when present is a UTF-8 string for the name followed by the length
in bytes of the content, and then the content octets.

If present routing code is a UTF-8 string. The routing code is used to
determine which test backend a test was running on when doing data analysis,
and to route stdin to the test process if interaction is required.

Multiplexers SHOULD add a routing code if none is present, and prefix any
existing routing code with a routing code ('/' separated) if one is already
present. For example, a multiplexer might label each stream it is multiplexing
with a simple ordinal ('0', '1' etc), and given an incoming packet with route
code '3' from stream '0' would adjust the route code when forwarding the packet
to be '0/3'.

Following the end of the packet is a CRC-32 checksum of the contents of the
packet including the signature.

Example packets
~~~~~~~~~~~~~~~

Trivial test "foo" enumeration packet, with test id, runnable set,
status=enumeration. Spaces below are to visually break up signature / flags /
length / testid / crc32

b3 2901 0c 03666f6f 08555f1b


Version 1 (and 1.1)
===================

Version 1 (and 1.1) are mostly human readable protocols.

Sample subunit wire contents
----------------------------

The following::

  test: test foo works
  success: test foo works
  test: tar a file.
  failure: tar a file. [
  ..
   ]..  space is eaten.
  foo.c:34 WARNING foo is not defined.
  ]
  a writeln to stdout

When run through subunit2pyunit::

  .F
  a writeln to stdout

  ========================
  FAILURE: tar a file.
  -------------------
  ..
  ]..  space is eaten.
  foo.c:34 WARNING foo is not defined.


Subunit v1 protocol description
===============================

This description is being ported to an EBNF style. Currently its only partly in
that style, but should be fairly clear all the same. When in doubt, refer the
source (and ideally help fix up the description!). Generally the protocol is
line orientated and consists of either directives and their parameters, or
when outside a DETAILS region unexpected lines which are not interpreted by
the parser - they should be forwarded unaltered::

    test|testing|test:|testing: test LABEL
    success|success:|successful|successful: test LABEL
    success|success:|successful|successful: test LABEL DETAILS
    failure: test LABEL
    failure: test LABEL DETAILS
    error: test LABEL
    error: test LABEL DETAILS
    skip[:] test LABEL
    skip[:] test LABEL DETAILS
    xfail[:] test LABEL
    xfail[:] test LABEL DETAILS
    uxsuccess[:] test LABEL
    uxsuccess[:] test LABEL DETAILS
    progress: [+|-]X
    progress: push
    progress: pop
    tags: [-]TAG ...
    time: YYYY-MM-DD HH:MM:SSZ

    LABEL: UTF8*
    NAME: UTF8*
    DETAILS ::= BRACKETED | MULTIPART
    BRACKETED ::= '[' CR UTF8-lines ']' CR
    MULTIPART ::= '[ multipart' CR PART* ']' CR
    PART ::= PART_TYPE CR NAME CR PART_BYTES CR
    PART_TYPE ::= Content-Type: type/sub-type(;parameter=value,parameter=value)
    PART_BYTES ::= (DIGITS CR LF BYTE{DIGITS})* '0' CR LF

unexpected output on stdout -> stdout.
exit w/0 or last test completing -> error

Tags given outside a test are applied to all following tests
Tags given after a test: line and before the result line for the same test
apply only to that test, and inherit the current global tags.
A '-' before a tag is used to remove tags - e.g. to prevent a global tag
applying to a single test, or to cancel a global tag.

The progress directive is used to provide progress information about a stream
so that stream consumer can provide completion estimates, progress bars and so
on. Stream generators that know how many tests will be present in the stream
should output "progress: COUNT". Stream filters that add tests should output
"progress: +COUNT", and those that remove tests should output
"progress: -COUNT". An absolute count should reset the progress indicators in
use - it indicates that two separate streams from different generators have
been trivially concatenated together, and there is no knowledge of how many
more complete streams are incoming. Smart concatenation could scan each stream
for their count and sum them, or alternatively translate absolute counts into
relative counts inline. It is recommended that outputters avoid absolute counts
unless necessary. The push and pop directives are used to provide local regions
for progress reporting. This fits with hierarchically operating test
environments - such as those that organise tests into suites - the top-most
runner can report on the number of suites, and each suite surround its output
with a (push, pop) pair. Interpreters should interpret a pop as also advancing
the progress of the restored level by one step. Encountering progress
directives between the start and end of a test pair indicates that a previous
test was interrupted and did not cleanly terminate: it should be implicitly
closed with an error (the same as when a stream ends with no closing test
directive for the most recently started test).

The time directive acts as a clock event - it sets the time for all future
events. The value should be a valid ISO8601 time.

The skip, xfail and uxsuccess outcomes are not supported by all testing
environments. In Python the testttools (https://launchpad.net/testtools)
library is used to translate these automatically if an older Python version
that does not support them is in use. See the testtools documentation for the
translation policy.

skip is used to indicate a test was discovered but not executed. xfail is used
to indicate a test that errored in some expected fashion (also know as "TODO"
tests in some frameworks). uxsuccess is used to indicate and unexpected success
where a test though to be failing actually passes. It is complementary to
xfail.

Hacking on subunit
------------------

Releases
========

* Update versions in configure.ac and python/subunit/__init__.py.
* Update NEWS.
* Do a make distcheck, which will update Makefile etc.
* Do a PyPI release: PYTHONPATH=../../python python ../../setup.py sdist bdist_wheel upload -s
* Upload the regular one to LP.
* Push a tagged commit.
  git push -t origin master:master

  * [python-swiftclient-3.8.0](https://docs.openstack.org/python-swiftclient/latest/) ========================
Team and repository tags
========================

.. image:: https://governance.openstack.org/tc/badges/python-swiftclient.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

.. Change things from this point on

Python bindings to the OpenStack Object Storage API
===================================================

.. image:: https://img.shields.io/pypi/v/python-swiftclient.svg
    :target: https://pypi.org/project/python-swiftclient/
    :alt: Latest Version

This is a python client for the Swift API. There's a Python API (the
``swiftclient`` module), and a command-line script (``swift``).

Development takes place via the usual OpenStack processes as outlined
in the `OpenStack wiki`__.

__ https://docs.openstack.org/infra/manual/developers.html

This code is based on the original client previously included with
`OpenStack's Swift`__ The python-swiftclient is licensed under the
Apache License like the rest of OpenStack.

__ https://github.com/openstack/swift

* Free software: Apache license
* `PyPI`_ - package installation
* `Online Documentation`_
* `Launchpad project`_ - release management
* `Blueprints`_ - feature specifications
* `Bugs`_ - issue tracking
* `Source`_
* `Specs`_
* `How to Contribute`_
* `Release Notes`_

.. _PyPI: https://pypi.org/project/python-swiftclient
.. _Online Documentation: https://docs.openstack.org/python-swiftclient/latest/
.. _Launchpad project: https://launchpad.net/python-swiftclient
.. _Blueprints: https://blueprints.launchpad.net/python-swiftclient
.. _Bugs: https://bugs.launchpad.net/python-swiftclient
.. _Source: https://opendev.org/openstack/python-swiftclient
.. _How to Contribute: https://docs.openstack.org/infra/manual/developers.html
.. _Specs: https://specs.openstack.org/openstack/swift-specs/
.. _Release Notes: https://docs.openstack.org/releasenotes/python-swiftclient

.. contents:: Contents:
   :local:




  * [pytoml-0.1.21](https://github.com/avakar/pytoml) [![PyPI](https://img.shields.io/pypi/v/pytoml.svg)](https://pypi.python.org/pypi/pytoml)
[![Build Status](https://travis-ci.org/avakar/pytoml.svg?branch=master)](https://travis-ci.org/avakar/pytoml)

# Deprecated

The pytoml project is no longer being actively maintained. Consider using the
[toml](https://github.com/uiri/toml) package instead.

# pytoml

This project aims at being a specs-conforming and strict parser and writer for [TOML][1] files.
The library currently supports [version 0.4.0][2] of the specs and runs with Python 2.7+ and 3.5+.

Install:

    pip install pytoml

The interface is the same as for the standard `json` package.

    >>> import pytoml as toml
    >>> toml.loads('a = 1')
    {'a': 1}
    >>> with open('file.toml', 'rb') as fin:
    ...     obj = toml.load(fin)
    >>> obj
    {'a': 1}

The `loads` function accepts either a bytes object
(that gets decoded as UTF-8 with no BOM allowed),
or a unicode object.

Use `dump` or `dumps` to serialize a dict into TOML.

    >>> print toml.dumps(obj)
    a = 1

## tests

To run the tests update the `toml-test` submodule:

    git submodule update --init --recursive

Then run the tests:

    python test/test.py

  [1]: https://github.com/toml-lang/toml
  [2]: https://github.com/toml-lang/toml/blob/master/versions/en/toml-v0.4.0.md



  * [pytype-2019.7.30](https://google.github.io/pytype) [![Build Status](https://travis-ci.org/google/pytype.svg?branch=master)](https://travis-ci.org/google/pytype)

# pytype - 🦆✔

Pytype checks and infers types for your Python code - without requiring type
annotations. Pytype can:

* Lint plain Python code, flagging common mistakes such as misspelled attribute
names, incorrect function calls, and [much more][error-classes], even across
file boundaries.
* Enforce user-provided [type annotations][pep-484]. While annotations are
optional for pytype, it will check and apply them where present.
* Generate type annotations in standalone files ("[pyi files][pyi-stub-files]"),
which can be merged back into the Python source with a provided
[merge-pyi][merge-pyi] tool.

Pytype is a static analyzer, meaning it does not execute the code it runs on.

Thousands of projects at Google rely on pytype to keep their Python code
well-typed and error-free.

For more information, check out the [user guide][user-guide] or [FAQ][faq].

## How is pytype different from other type checkers?

1. Pytype uses **inference** instead of gradual typing. This means it will
infer types on code even when the code has no type hints on it. So it can
detect issues with code like this, which other type checkers would miss:

    ```python
    def f():
        return "PyCon"
    def g():
        return f() + 2019

    # pytype: line 4, in g: unsupported operand type(s) for +: 'str'
    # and 'int' [unsupported-operands]
    ```

1. Pytype is **lenient** instead of strict. That means it allows all
operations that succeed at runtime and don't contradict annotations. For
instance, this code will pass as safe in pytype, but fail in other type
checkers, which assign types to variables as soon as they are initialized:

    ```python
    from typing import List
    def get_list() -> List[str]:
        lst = ["PyCon"]
        lst.append(2019)
        return [str(x) for x in lst]

    # mypy: line 4: error: Argument 1 to "append" of "list" has
    # incompatible type "int"; expected "str"
    ```

Also see the corresponding [FAQ entry][faq-diff].

## Quickstart

To quickly get started with type-checking a file or directory, run the
following, replacing `file_or_directory` with your input:

```
pip install pytype
pytype file_or_directory
```

To set up pytype on an entire package, add the following to a `setup.cfg` file
in the directory immediately above the package, replacing `package_name` with
the package name:

```
[pytype]
inputs = package_name
```

Now you can run the no-argument command `pytype` to type-check the package. It's
also easy to add pytype to your automated testing; see this
[example][importlab-travis] of a GitHub project that runs pytype on Travis.

Finally, pytype generates files of inferred type information, located by default
in `.pytype/pyi`. You can use this information to type-annotate the
corresponding source file, replacing `module.py` with the file's import path:

```
merge-pyi -i module.py .pytype/pyi/module.pyi
```

## Requirements

You need a Python 2.7 or 3.5+ interpreter to run pytype, as well as an
interpreter in `$PATH` for the Python version of the code you're analyzing.

Platform support:

* Pytype is currently developed and tested on Linux, which is the main supported
  platform.
* Installation on MacOSX requires OSX 10.7 or higher and Xcode v8 or higher.
* Windows is currently not supported unless you use [WSL][wsl].

## Installing

Pytype can be installed via pip. Note that the installation requires `wheel`
and `setuptools`. (If you're working in a virtualenv, these two packages should
already be present.)

```
pip install pytype
```

Or from the source code [on GitHub][github].

```
git clone --recurse-submodules https://github.com/google/pytype.git
cd pytype
pip install -U .
```

Instead of using `--recurse-submodules`, you could also have run

```
git submodule init
git submodule update
```

in the `pytype` directory.

### Installing on WSL

Follow the steps above, but make sure you have the correct libraries first:

```shell
sudo apt install build-essential python3-dev libpython3-dev
```

## Usage

```
usage: pytype [options] input [input ...]

positional arguments:
  input                 file or directory to process
```

Common options:

* `-V, --python-version`: Python version (major.minor) of the target code.
  Defaults to `3.6`.
* `-o, --output`: The directory into which all pytype output goes, including
  generated .pyi files. Defaults to `.pytype`.
* `-d, --disable`. Comma separated list of error names to ignore. Detailed
  explanations of pytype's error names are in [this doc][error-classes].
  Defaults to empty.

For a full list of options, run `pytype --help`.

In addition to the above, you can direct pytype to use a custom typeshed
installation instead of its own bundled copy by setting `$TYPESHED_HOME`.

### Config File

For convenience, you can save your pytype configuration in a file. The config
file is an INI-style file with a `[pytype]` section; if an explicit config file
is not supplied, pytype will look for a `[pytype]` section in the first
`setup.cfg` file found by walking upwards from the current working directory.

Start off by generating a sample config file:

```
$ pytype --generate-config pytype.cfg
```

Now customize the file based on your local setup, keeping only the sections you
need. Directories may be relative to the location of the config file, which is
useful if you want to check in the config file as part of your project.

For example, suppose you have the following directory structure and want to
analyze package `~/repo1/foo`, which depends on package `~/repo2/bar`:

```
~/
├── repo1
│   └── foo
│       ├── __init__.py
│       └── file_to_check.py
└── repo2
    └── bar
        ├── __init__.py
        └── dependency.py
```

Here is the filled-in config file, which instructs pytype to type-check
`~/repo1/foo` as Python 3.6 code, look for packages in `~/repo1` and `~/repo2`,
and ignore attribute errors. Notice that the path to a package does not include
the package itself.

```
$ cat ~/repo1/pytype.cfg

# NOTE: All relative paths are relative to the location of this file.

[pytype]

# Space-separated list of files or directories to process.
inputs =
    foo

# Python version (major.minor) of the target code.
python_version = 3.6

# Paths to source code directories, separated by ':'.
pythonpath =
    .:
    ~/repo2

# Comma separated list of error names to ignore.
disable =
    attribute-error
```

We could've discovered that `~/repo2` needed to be added to the pythonpath by
running pytype's broken dependency checker:

```
$ pytype --config=~/repo1/pytype.cfg ~/repo1/foo/*.py --unresolved

Unresolved dependencies:
  bar.dependency
```

### Subtools

Pytype ships with a few scripts in addition to `pytype` itself:

* `annotate-ast`, an in-progress type annotator for ASTs.
* [`merge-pyi`][merge-pyi], for merging type information from a .pyi file into a
Python file.
* `pytd`, a parser for .pyi files.
* `pytype-single`, a debugging tool for pytype developers, which analyzes a
single Python file assuming that .pyi files have already been generated for all
of its dependencies.
* `pyxref`, a cross references generator.

## Roadmap

* Windows support

## License
Apache 2.0

## Disclaimer
This is not an official Google product.

[error-classes]: docs/errors.md
[faq]: docs/faq.md
[faq-diff]: docs/faq.md#how-is-pytype-different-from-other-type-checkers
[github]: https://github.com/google/pytype/
[importlab-travis]: https://github.com/google/importlab/blob/master/.travis.yml
[merge-pyi]: https://github.com/google/pytype/tree/master/pytype/tools/merge_pyi
[pep-484]: https://www.python.org/dev/peps/pep-0484
[pyi-stub-files]: docs/user_guide.md#pyi-stub-files
[user-guide]: docs/user_guide.md
[wsl]: https://docs.microsoft.com/en-us/windows/wsl/faq
  * [pytz-2018.5](http://pythonhosted.org/pytz) pytz - World Timezone Definitions for Python
============================================

:Author: Stuart Bishop <stuart@stuartbishop.net>

Introduction
~~~~~~~~~~~~

pytz brings the Olson tz database into Python. This library allows
accurate and cross platform timezone calculations using Python 2.4
or higher. It also solves the issue of ambiguous times at the end
of daylight saving time, which you can read more about in the Python
Library Reference (``datetime.tzinfo``).

Almost all of the Olson timezones are supported.

.. note::

    This library differs from the documented Python API for
    tzinfo implementations; if you want to create local wallclock
    times you need to use the ``localize()`` method documented in this
    document. In addition, if you perform date arithmetic on local
    times that cross DST boundaries, the result may be in an incorrect
    timezone (ie. subtract 1 minute from 2002-10-27 1:00 EST and you get
    2002-10-27 0:59 EST instead of the correct 2002-10-27 1:59 EDT). A
    ``normalize()`` method is provided to correct this. Unfortunately these
    issues cannot be resolved without modifying the Python datetime
    implementation (see PEP-431).


Installation
~~~~~~~~~~~~

This package can either be installed using ``pip`` or from a tarball using the
standard Python distutils.

If you are installing using ``pip``, you don't need to download anything as the
latest version will be downloaded for you from PyPI::

    pip install pytz

If you are installing from a tarball, run the following command as an
administrative user::

    python setup.py install


Example & Usage
~~~~~~~~~~~~~~~

Localized times and date arithmetic
-----------------------------------

>>> from datetime import datetime, timedelta
>>> from pytz import timezone
>>> import pytz
>>> utc = pytz.utc
>>> utc.zone
'UTC'
>>> eastern = timezone('US/Eastern')
>>> eastern.zone
'US/Eastern'
>>> amsterdam = timezone('Europe/Amsterdam')
>>> fmt = '%Y-%m-%d %H:%M:%S %Z%z'

This library only supports two ways of building a localized time. The
first is to use the ``localize()`` method provided by the pytz library.
This is used to localize a naive datetime (datetime with no timezone
information):

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 6, 0, 0))
>>> print(loc_dt.strftime(fmt))
2002-10-27 06:00:00 EST-0500

The second way of building a localized time is by converting an existing
localized time using the standard ``astimezone()`` method:

>>> ams_dt = loc_dt.astimezone(amsterdam)
>>> ams_dt.strftime(fmt)
'2002-10-27 12:00:00 CET+0100'

Unfortunately using the tzinfo argument of the standard datetime
constructors ''does not work'' with pytz for many timezones.

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=amsterdam).strftime(fmt)  # /!\ Does not work this way!
'2002-10-27 12:00:00 LMT+0020'

It is safe for timezones without daylight saving transitions though, such
as UTC:

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=pytz.utc).strftime(fmt)  # /!\ Not recommended except for UTC
'2002-10-27 12:00:00 UTC+0000'

The preferred way of dealing with times is to always work in UTC,
converting to localtime only when generating output to be read
by humans.

>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)
>>> loc_dt = utc_dt.astimezone(eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:00:00 EST-0500'

This library also allows you to do date arithmetic using local
times, although it is more complicated than working in UTC as you
need to use the ``normalize()`` method to handle daylight saving time
and other timezone transitions. In this example, ``loc_dt`` is set
to the instant when daylight saving time ends in the US/Eastern
timezone.

>>> before = loc_dt - timedelta(minutes=10)
>>> before.strftime(fmt)
'2002-10-27 00:50:00 EST-0500'
>>> eastern.normalize(before).strftime(fmt)
'2002-10-27 01:50:00 EDT-0400'
>>> after = eastern.normalize(before + timedelta(minutes=20))
>>> after.strftime(fmt)
'2002-10-27 01:10:00 EST-0500'

Creating local times is also tricky, and the reason why working with
local times is not recommended. Unfortunately, you cannot just pass
a ``tzinfo`` argument when constructing a datetime (see the next
section for more details)

>>> dt = datetime(2002, 10, 27, 1, 30, 0)
>>> dt1 = eastern.localize(dt, is_dst=True)
>>> dt1.strftime(fmt)
'2002-10-27 01:30:00 EDT-0400'
>>> dt2 = eastern.localize(dt, is_dst=False)
>>> dt2.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

Converting between timezones is more easily done, using the
standard astimezone method.

>>> utc_dt = utc.localize(datetime.utcfromtimestamp(1143408899))
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = utc_dt.astimezone(au_tz)
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 AEDT+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> utc_dt == utc_dt2
True

You can take shortcuts when dealing with the UTC side of timezone
conversions. ``normalize()`` and ``localize()`` are not really
necessary when there are no daylight saving time transitions to
deal with.

>>> utc_dt = datetime.utcfromtimestamp(1143408899).replace(tzinfo=utc)
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 AEDT+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'


``tzinfo`` API
--------------

The ``tzinfo`` instances returned by the ``timezone()`` function have
been extended to cope with ambiguous times by adding an ``is_dst``
parameter to the ``utcoffset()``, ``dst()`` && ``tzname()`` methods.

>>> tz = timezone('America/St_Johns')

>>> normal = datetime(2009, 9, 1)
>>> ambiguous = datetime(2009, 10, 31, 23, 30)

The ``is_dst`` parameter is ignored for most timestamps. It is only used
during DST transition ambiguous periods to resolve that ambiguity.

>>> print(tz.utcoffset(normal, is_dst=True))
-1 day, 21:30:00
>>> print(tz.dst(normal, is_dst=True))
1:00:00
>>> tz.tzname(normal, is_dst=True)
'NDT'

>>> print(tz.utcoffset(ambiguous, is_dst=True))
-1 day, 21:30:00
>>> print(tz.dst(ambiguous, is_dst=True))
1:00:00
>>> tz.tzname(ambiguous, is_dst=True)
'NDT'

>>> print(tz.utcoffset(normal, is_dst=False))
-1 day, 21:30:00
>>> tz.dst(normal, is_dst=False)
datetime.timedelta(0, 3600)
>>> tz.tzname(normal, is_dst=False)
'NDT'

>>> print(tz.utcoffset(ambiguous, is_dst=False))
-1 day, 20:30:00
>>> tz.dst(ambiguous, is_dst=False)
datetime.timedelta(0)
>>> tz.tzname(ambiguous, is_dst=False)
'NST'

If ``is_dst`` is not specified, ambiguous timestamps will raise
an ``pytz.exceptions.AmbiguousTimeError`` exception.

>>> print(tz.utcoffset(normal))
-1 day, 21:30:00
>>> print(tz.dst(normal))
1:00:00
>>> tz.tzname(normal)
'NDT'

>>> import pytz.exceptions
>>> try:
...     tz.utcoffset(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.dst(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.tzname(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00


Problems with Localtime
~~~~~~~~~~~~~~~~~~~~~~~

The major problem we have to deal with is that certain datetimes
may occur twice in a year. For example, in the US/Eastern timezone
on the last Sunday morning in October, the following sequence
happens:

    - 01:00 EDT occurs
    - 1 hour later, instead of 2:00am the clock is turned back 1 hour
      and 01:00 happens again (this time 01:00 EST)

In fact, every instant between 01:00 and 02:00 occurs twice. This means
that if you try and create a time in the 'US/Eastern' timezone
the standard datetime syntax, there is no way to specify if you meant
before of after the end-of-daylight-saving-time transition. Using the
pytz custom syntax, the best you can do is make an educated guess:

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 1, 30, 00))
>>> loc_dt.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

As you can see, the system has chosen one for you and there is a 50%
chance of it being out by one hour. For some applications, this does
not matter. However, if you are trying to schedule meetings with people
in different timezones or analyze log files it is not acceptable.

The best and simplest solution is to stick with using UTC.  The pytz
package encourages using UTC for internal timezone representation by
including a special UTC implementation based on the standard Python
reference implementation in the Python documentation.

The UTC timezone unpickles to be the same instance, and pickles to a
smaller size than other pytz tzinfo instances.  The UTC implementation
can be obtained as pytz.utc, pytz.UTC, or pytz.timezone('UTC').

>>> import pickle, pytz
>>> dt = datetime(2005, 3, 1, 14, 13, 21, tzinfo=utc)
>>> naive = dt.replace(tzinfo=None)
>>> p = pickle.dumps(dt, 1)
>>> naive_p = pickle.dumps(naive, 1)
>>> len(p) - len(naive_p)
17
>>> new = pickle.loads(p)
>>> new == dt
True
>>> new is dt
False
>>> new.tzinfo is dt.tzinfo
True
>>> pytz.utc is pytz.UTC is pytz.timezone('UTC')
True

Note that some other timezones are commonly thought of as the same (GMT,
Greenwich, Universal, etc.). The definition of UTC is distinct from these
other timezones, and they are not equivalent. For this reason, they will
not compare the same in Python.

>>> utc == pytz.timezone('GMT')
False

See the section `What is UTC`_, below.

If you insist on working with local times, this library provides a
facility for constructing them unambiguously:

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00)
>>> est_dt = eastern.localize(loc_dt, is_dst=True)
>>> edt_dt = eastern.localize(loc_dt, is_dst=False)
>>> print(est_dt.strftime(fmt) + ' / ' + edt_dt.strftime(fmt))
2002-10-27 01:30:00 EDT-0400 / 2002-10-27 01:30:00 EST-0500

If you pass None as the is_dst flag to localize(), pytz will refuse to
guess and raise exceptions if you try to build ambiguous or non-existent
times.

For example, 1:30am on 27th Oct 2002 happened twice in the US/Eastern
timezone when the clocks where put back at the end of Daylight Saving
Time:

>>> dt = datetime(2002, 10, 27, 1, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % dt)
pytz.exceptions.AmbiguousTimeError: 2002-10-27 01:30:00

Similarly, 2:30am on 7th April 2002 never happened at all in the
US/Eastern timezone, as the clocks where put forward at 2:00am skipping
the entire hour:

>>> dt = datetime(2002, 4, 7, 2, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.NonExistentTimeError:
...     print('pytz.exceptions.NonExistentTimeError: %s' % dt)
pytz.exceptions.NonExistentTimeError: 2002-04-07 02:30:00

Both of these exceptions share a common base class to make error handling
easier:

>>> isinstance(pytz.AmbiguousTimeError(), pytz.InvalidTimeError)
True
>>> isinstance(pytz.NonExistentTimeError(), pytz.InvalidTimeError)
True


A special case is where countries change their timezone definitions
with no daylight savings time switch. For example, in 1915 Warsaw
switched from Warsaw time to Central European time with no daylight savings
transition. So at the stroke of midnight on August 5th 1915 the clocks
were wound back 24 minutes creating an ambiguous time period that cannot
be specified without referring to the timezone abbreviation or the
actual UTC offset. In this case midnight happened twice, neither time
during a daylight saving time period. pytz handles this transition by
treating the ambiguous period before the switch as daylight savings
time, and the ambiguous period after as standard time.


>>> warsaw = pytz.timezone('Europe/Warsaw')
>>> amb_dt1 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=True)
>>> amb_dt1.strftime(fmt)
'1915-08-04 23:59:59 WMT+0124'
>>> amb_dt2 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=False)
>>> amb_dt2.strftime(fmt)
'1915-08-04 23:59:59 CET+0100'
>>> switch_dt = warsaw.localize(datetime(1915, 8, 5, 00, 00, 00), is_dst=False)
>>> switch_dt.strftime(fmt)
'1915-08-05 00:00:00 CET+0100'
>>> str(switch_dt - amb_dt1)
'0:24:01'
>>> str(switch_dt - amb_dt2)
'0:00:01'

The best way of creating a time during an ambiguous time period is
by converting from another timezone such as UTC:

>>> utc_dt = datetime(1915, 8, 4, 22, 36, tzinfo=pytz.utc)
>>> utc_dt.astimezone(warsaw).strftime(fmt)
'1915-08-04 23:36:00 CET+0100'

The standard Python way of handling all these ambiguities is not to
handle them, such as demonstrated in this example using the US/Eastern
timezone definition from the Python documentation (Note that this
implementation only works for dates between 1987 and 2006 - it is
included for tests only!):

>>> from pytz.reference import Eastern # pytz.reference only for tests
>>> dt = datetime(2002, 10, 27, 0, 30, tzinfo=Eastern)
>>> str(dt)
'2002-10-27 00:30:00-04:00'
>>> str(dt + timedelta(hours=1))
'2002-10-27 01:30:00-05:00'
>>> str(dt + timedelta(hours=2))
'2002-10-27 02:30:00-05:00'
>>> str(dt + timedelta(hours=3))
'2002-10-27 03:30:00-05:00'

Notice the first two results? At first glance you might think they are
correct, but taking the UTC offset into account you find that they are
actually two hours appart instead of the 1 hour we asked for.

>>> from pytz.reference import UTC # pytz.reference only for tests
>>> str(dt.astimezone(UTC))
'2002-10-27 04:30:00+00:00'
>>> str((dt + timedelta(hours=1)).astimezone(UTC))
'2002-10-27 06:30:00+00:00'


Country Information
~~~~~~~~~~~~~~~~~~~

A mechanism is provided to access the timezones commonly in use
for a particular country, looked up using the ISO 3166 country code.
It returns a list of strings that can be used to retrieve the relevant
tzinfo instance using ``pytz.timezone()``:

>>> print(' '.join(pytz.country_timezones['nz']))
Pacific/Auckland Pacific/Chatham

The Olson database comes with a ISO 3166 country code to English country
name mapping that pytz exposes as a dictionary:

>>> print(pytz.country_names['nz'])
New Zealand


What is UTC
~~~~~~~~~~~

'UTC' is `Coordinated Universal Time`_. It is a successor to, but distinct
from, Greenwich Mean Time (GMT) and the various definitions of Universal
Time. UTC is now the worldwide standard for regulating clocks and time
measurement.

All other timezones are defined relative to UTC, and include offsets like
UTC+0800 - hours to add or subtract from UTC to derive the local time. No
daylight saving time occurs in UTC, making it a useful timezone to perform
date arithmetic without worrying about the confusion and ambiguities caused
by daylight saving time transitions, your country changing its timezone, or
mobile computers that roam through multiple timezones.

..  _Coordinated Universal Time: https://en.wikipedia.org/wiki/Coordinated_Universal_Time


Helpers
~~~~~~~

There are two lists of timezones provided.

``all_timezones`` is the exhaustive list of the timezone names that can
be used.

>>> from pytz import all_timezones
>>> len(all_timezones) >= 500
True
>>> 'Etc/Greenwich' in all_timezones
True

``common_timezones`` is a list of useful, current timezones. It doesn't
contain deprecated zones or historical zones, except for a few I've
deemed in common usage, such as US/Eastern (open a bug report if you
think other timezones are deserving of being included here). It is also
a sequence of strings.

>>> from pytz import common_timezones
>>> len(common_timezones) < len(all_timezones)
True
>>> 'Etc/Greenwich' in common_timezones
False
>>> 'Australia/Melbourne' in common_timezones
True
>>> 'US/Eastern' in common_timezones
True
>>> 'Canada/Eastern' in common_timezones
True
>>> 'Australia/Yancowinna' in all_timezones
True
>>> 'Australia/Yancowinna' in common_timezones
False

Both ``common_timezones`` and ``all_timezones`` are alphabetically
sorted:

>>> common_timezones_dupe = common_timezones[:]
>>> common_timezones_dupe.sort()
>>> common_timezones == common_timezones_dupe
True
>>> all_timezones_dupe = all_timezones[:]
>>> all_timezones_dupe.sort()
>>> all_timezones == all_timezones_dupe
True

``all_timezones`` and ``common_timezones`` are also available as sets.

>>> from pytz import all_timezones_set, common_timezones_set
>>> 'US/Eastern' in all_timezones_set
True
>>> 'US/Eastern' in common_timezones_set
True
>>> 'Australia/Victoria' in common_timezones_set
False

You can also retrieve lists of timezones used by particular countries
using the ``country_timezones()`` function. It requires an ISO-3166
two letter country code.

>>> from pytz import country_timezones
>>> print(' '.join(country_timezones('ch')))
Europe/Zurich
>>> print(' '.join(country_timezones('CH')))
Europe/Zurich


Internationalization - i18n/l10n
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pytz is an interface to the IANA database, which uses ASCII names. The `Unicode  Consortium's Unicode Locales (CLDR) <http://cldr.unicode.org>`_
project provides translations. Thomas Khyn's
`l18n <https://pypi.org/project/l18n/>`_ package can be used to access
these translations from Python.


License
~~~~~~~

MIT license.

This code is also available as part of Zope 3 under the Zope Public
License,  Version 2.1 (ZPL).

I'm happy to relicense this code if necessary for inclusion in other
open source projects.


Latest Versions
~~~~~~~~~~~~~~~

This package will be updated after releases of the Olson timezone
database.  The latest version can be downloaded from the `Python Package
Index <https://pypi.org/project/pytz/>`_.  The code that is used
to generate this distribution is hosted on launchpad.net and available
using git::

    git clone https://git.launchpad.net/pytz

A mirror on github is also available at https://github.com/stub42/pytz

Announcements of new releases are made on
`Launchpad <https://launchpad.net/pytz>`_, and the
`Atom feed <http://feeds.launchpad.net/pytz/announcements.atom>`_
hosted there.


Bugs, Feature Requests & Patches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Bugs can be reported using `Launchpad <https://bugs.launchpad.net/pytz>`__.


Issues & Limitations
~~~~~~~~~~~~~~~~~~~~

- Offsets from UTC are rounded to the nearest whole minute, so timezones
  such as Europe/Amsterdam pre 1937 will be up to 30 seconds out. This
  is a limitation of the Python datetime library.

- If you think a timezone definition is incorrect, I probably can't fix
  it. pytz is a direct translation of the Olson timezone database, and
  changes to the timezone definitions need to be made to this source.
  If you find errors they should be reported to the time zone mailing
  list, linked from http://www.iana.org/time-zones.


Further Reading
~~~~~~~~~~~~~~~

More info than you want to know about timezones:
http://www.twinsun.com/tz/tz-link.htm


Contact
~~~~~~~

Stuart Bishop <stuart@stuartbishop.net>





  * [pyxdg-0.26](http://freedesktop.org/wiki/Software/pyxdg) UNKNOWN



  * [pyzmq-18.0.2](https://pyzmq.readthedocs.org) # PyZMQ: Python bindings for ØMQ

[![Build Status](https://travis-ci.org/zeromq/pyzmq.svg?branch=master)](https://travis-ci.org/zeromq/pyzmq)

[![Windows Build status](https://ci.appveyor.com/api/projects/status/ugoid0r2fnq8sr56/branch/master?svg=true)](https://ci.appveyor.com/project/minrk/pyzmq/branch/master)

This package contains Python bindings for [ØMQ](http://www.zeromq.org).
ØMQ is a lightweight and fast messaging implementation.

PyZMQ should work with any reasonable version of Python (≥ 3.4),
as well as Python 2.7 and 3.3, as well as PyPy.
The Cython backend used by CPython supports libzmq ≥ 2.1.4 (including 3.2.x and 4.x),
but the CFFI backend used by PyPy only supports libzmq ≥ 3.2.2 (including 4.x).

For a summary of changes to pyzmq, see our
[changelog](https://pyzmq.readthedocs.org/en/latest/changelog.html).

### ØMQ 3.x, 4.x

PyZMQ fully supports the 3.x and 4.x APIs of libzmq,
developed at [zeromq/libzmq](https://github.com/zeromq/libzmq).
No code to change, no flags to pass,
just build pyzmq against the latest and it should work.

PyZMQ does not support the old libzmq 2 API on PyPy.

## Documentation

See PyZMQ's Sphinx-generated
[documentation](https://zeromq.github.io/pyzmq) on GitHub for API
details, and some notes on Python and Cython development. If you want to
learn about using ØMQ in general, the excellent [ØMQ
Guide](http://zguide.zeromq.org/py:all) is the place to start, which has a
Python version of every example. We also have some information on our
[wiki](https://github.com/zeromq/pyzmq/wiki).

## Downloading

Unless you specifically want to develop PyZMQ, we recommend downloading
the PyZMQ source code or wheels from
[PyPI](https://pypi.io/project/pyzmq),
or install with conda.

You can also get the latest source code from our GitHub repository, but
building from the repository will require that you install recent Cython.

## Building and installation

For more detail on building pyzmq, see [our Wiki](https://github.com/zeromq/pyzmq/wiki/Building-and-Installing-PyZMQ).

We build wheels for OS X, Windows, and Linux, so you can get a binary on those platforms with:

    pip install pyzmq

but compiling from source with `pip install pyzmq` should work in most environments.
Especially on OS X, make sure you are using the latest pip (≥ 8), or it may not find the right wheels.

If the wheel doesn't work for some reason, or you want to force pyzmq to be compiled
(this is often preferable if you already have libzmq installed and configured the way you want it),
you can force installation with:

    pip install --no-use-wheel pyzmq

When compiling pyzmq (e.g. installing with pip on Linux),
it is generally recommended that zeromq be installed separately,
via homebrew, apt, yum, etc:

    # Debian-based
    sudo apt-get install libzmq3-dev

    # RHEL-based
    sudo yum install libzmq3-devel

If this is not available, pyzmq will *try* to build libzmq as a Python Extension,
though this is not guaranteed to work.

Building pyzmq from the git repo (including release tags on GitHub) requires Cython.

## Old versions


pyzmq 16 drops support Python 2.6 and 3.2.
If you need to use one of those Python versions, you can pin your pyzmq version to before 16:

    pip install 'pyzmq<16'

For libzmq 2.0.x, use 'pyzmq<2.1'

pyzmq-2.1.11 was the last version of pyzmq to support Python 2.5,
and pyzmq ≥ 2.2.0 requires Python ≥ 2.6.
pyzmq-13.0.0 introduces PyPy support via CFFI, which only supports libzmq-3.2.2 and newer.

PyZMQ releases ≤ 2.2.0 matched libzmq versioning, but this is no longer the case,
starting with PyZMQ 13.0.0 (it was the thirteenth release, so why not?).
PyZMQ ≥ 13.0 follows semantic versioning conventions accounting only for PyZMQ itself.
  * [qrcode-6.1](https://github.com/lincolnloop/python-qrcode) =============================
Pure python QR Code generator
=============================

Generate QR codes.

For a standard install (which will include pillow_ for generating images),
run::

    pip install qrcode[pil]

.. _pillow: https://pypi.python.org/pypi/Pillow


What is a QR Code?
==================

A Quick Response code is a two-dimensional pictographic code used for its fast
readability and comparatively large storage capacity. The code consists of
black modules arranged in a square pattern on a white background. The
information encoded can be made up of any kind of data (e.g., binary,
alphanumeric, or Kanji symbols)

Usage
=====

From the command line, use the installed ``qr`` script::

    qr "Some text" > test.png

Or in Python, use the ``make`` shortcut function:

.. code:: python

    import qrcode
    img = qrcode.make('Some data here')

Advanced Usage
--------------

For more control, use the ``QRCode`` class. For example:

.. code:: python

    import qrcode
    qr = qrcode.QRCode(
        version=1,
        error_correction=qrcode.constants.ERROR_CORRECT_L,
        box_size=10,
        border=4,
    )
    qr.add_data('Some data')
    qr.make(fit=True)

    img = qr.make_image(fill_color="black", back_color="white")

The ``version`` parameter is an integer from 1 to 40 that controls the size of
the QR Code (the smallest, version 1, is a 21x21 matrix).
Set to ``None`` and use the ``fit`` parameter when making the code to determine
this automatically.

``fill_color`` and ``back_color`` can change the background and the painting
color of the QR, when using the default image factory.

The ``error_correction`` parameter controls the error correction used for the
QR Code. The following four constants are made available on the ``qrcode``
package:

``ERROR_CORRECT_L``
    About 7% or less errors can be corrected.
``ERROR_CORRECT_M`` (default)
    About 15% or less errors can be corrected.
``ERROR_CORRECT_Q``
    About 25% or less errors can be corrected.
``ERROR_CORRECT_H``.
    About 30% or less errors can be corrected.

The ``box_size`` parameter controls how many pixels each "box" of the QR code
is.

The ``border`` parameter controls how many boxes thick the border should be
(the default is 4, which is the minimum according to the specs).

Other image factories
=====================

You can encode as SVG, or use a new pure Python image processor to encode to
PNG images.

The Python examples below use the ``make`` shortcut. The same ``image_factory``
keyword argument is a valid option for the ``QRCode`` class for more advanced
usage.

SVG
---

You can create the entire SVG or an SVG fragment. When building an entire SVG
image, you can use the factory that combines as a path (recommended, and
default for the script) or a factory that creates a simple set of rectangles.

From your command line::

    qr --factory=svg-path "Some text" > test.svg
    qr --factory=svg "Some text" > test.svg
    qr --factory=svg-fragment "Some text" > test.svg

Or in Python:

.. code:: python

    import qrcode
    import qrcode.image.svg

    if method == 'basic':
        # Simple factory, just a set of rects.
        factory = qrcode.image.svg.SvgImage
    elif method == 'fragment':
        # Fragment factory (also just a set of rects)
        factory = qrcode.image.svg.SvgFragmentImage
    else:
        # Combined path factory, fixes white space that may occur when zooming
        factory = qrcode.image.svg.SvgPathImage

    img = qrcode.make('Some data here', image_factory=factory)

Two other related factories are available that work the same, but also fill the
background of the SVG with white::

    qrcode.image.svg.SvgFillImage
    qrcode.image.svg.SvgPathFillImage


Pure Python PNG
---------------

Install the following two packages::

    pip install git+git://github.com/ojii/pymaging.git#egg=pymaging
    pip install git+git://github.com/ojii/pymaging-png.git#egg=pymaging-png

From your command line::

    qr --factory=pymaging "Some text" > test.png

Or in Python:

.. code:: python

    import qrcode
    from qrcode.image.pure import PymagingImage
    img = qrcode.make('Some data here', image_factory=PymagingImage)

==========
Change log
==========

6.1 (14 January 2019)
=====================

- Fix short chunks of data not being optimized to the correct mode.

- Tests fixed for Python 3


6.0 (23 March 2018)
===================

- Fix optimize length being ignored in ``QRCode.add_data``.

- Better calculation of the best mask pattern and related optimizations. Big
  thanks to cryptogun!


5.3 (18 May 2016)
=================

* Fix incomplete block table for QR version 15. Thanks Rodrigo Queiro for the
  report and Jacob Welsh for the investigation and fix.

* Avoid unnecessary dependency for non MS platforms, thanks to Noah Vesely.

* Make ``BaseImage.get_image()`` actually work.


5.2 (25 Jan 2016)
=================

* Add ``--error-correction`` option to qr script.

* Fix script piping to stdout in Python 3 and reading non-UTF-8 characters in
  Python 3.

* Fix script piping in Windows.

* Add some useful behind-the-curtain methods for tinkerers.

* Fix terminal output when using Python 2.6

* Fix terminal output to display correctly on MS command line.

5.2.1
-----

* Small fix to terminal output in Python 3 (and fix tests)

5.2.2
-----

* Revert some terminal changes from 5.2 that broke Python 3's real life tty
  code generation and introduce a better way from Jacob Welsh.


5.1 (22 Oct 2014)
=================

* Make ``qr`` script work in Windows. Thanks Ionel Cristian Mărieș

* Fixed print_ascii function in Python 3.

* Out-of-bounds code version numbers are handled more consistently with a
  ValueError.

* Much better test coverage (now only officially supporting Python 2.6+)


5.0 (17 Jun 2014)
=================

* Speed optimizations.

* Change the output when using the ``qr`` script to use ASCII rather than
  just colors, better using the terminal real estate.

* Fix a bug in passing bytecode data directly when in Python 3.

* Substation speed optimizations to best-fit algorithm (thanks Jacob Welsh!).

* Introduce a ``print_ascii`` method and use it as the default for the ``qr``
  script rather than ``print_tty``.

5.0.1
-----

* Update version numbers correctly.


4.0 (4 Sep 2013)
================

* Made qrcode work on Python 2.4 - Thanks tcely.
  Note: officially, qrcode only supports 2.5+.

* Support pure-python PNG generation (via pymaging) for Python 2.6+ -- thanks
  Adam Wisniewski!

* SVG image generation now supports alternate sizing (the default box size of
  10 == 1mm per rectangle).

* SVG path image generation allows cleaner SVG output by combining all QR rects
  into a single path. Thank you, Viktor Stískala.

* Added some extra simple SVG factories that fill the background white.

4.0.1
-----

* Fix the pymaging backend not able to save the image to a buffer. Thanks ilj!

4.0.2
-----

* Fix incorrect regex causing a comma to be considered part of the alphanumeric
  set.

* Switch to using setuptools for setup.py.

4.0.3
-----

* Fix bad QR code generation due to the regex comma fix in version 4.0.2.

4.0.4
-----

* Bad version number for previous hotfix release.


3.1 (12 Aug 2013)
=================

* Important fixes for incorrect matches of the alpha-numeric encoding mode.
  Previously, the pattern would match if a single line was alpha-numeric only
  (even if others wern't). Also, the two characters ``{`` and ``}`` had snuck
  in as valid characters. Thanks to Eran Tromer for the report and fix.

* Optimized chunking -- if the parts of the data stream can be encoded more
  efficiently, the data will be split into chunks of the most efficient modes.

3.1.1
-----

* Update change log to contain version 3.1 changes. :P

* Give the ``qr`` script an ``--optimize`` argument to control the chunk
  optimization setting.


3.0 (25 Jun 2013)
=================

* Python 3 support.

* Add QRCode.get_matrix, an easy way to get the matrix array of a QR code
  including the border. Thanks Hugh Rawlinson.

* Add in a workaround so that Python 2.6 users can use SVG generation (they
  must install ``lxml``).

* Some initial tests! And tox support (``pip install tox``) for testing across
  Python platforms.


2.7 (5 Mar 2013)
================

* Fix incorrect termination padding.


2.6 (2 Apr 2013)
================

* Fix the first four columns incorrectly shifted by one. Thanks to Josep
  Gómez-Suay for the report and fix.

* Fix strings within 4 bits of the QR version limit being incorrectly
  terminated. Thanks to zhjie231 for the report.


2.5 (12 Mar 2013)
=================

* The PilImage wrapper is more transparent - you can use any methods or
  attributes available to the underlying PIL Image instance.

* Fixed the first column of the QR Code coming up empty! Thanks to BecoKo.

2.5.1
-----

* Fix installation error on Windows.


2.4 (23 Apr 2012)
=================

* Use a pluggable backend system for generating images, thanks to Branko Čibej!
  Comes with PIL and SVG backends built in.

2.4.1
-----

* Fix a packaging issue

2.4.2
-----

* Added a ``show`` method to the PIL image wrapper so the ``run_example``
  function actually works.


2.3 (29 Jan 2012)
=================

* When adding data, auto-select the more efficient encoding methods for numbers
  and alphanumeric data (KANJI still not supported).

2.3.1
-----

* Encode unicode to utf-8 bytestrings when adding data to a QRCode.


2.2 (18 Jan 2012)
=================

* Fixed tty output to work on both white and black backgrounds.

* Added `border` parameter to allow customizing of the number of boxes used to
  create the border of the QR code


2.1 (17 Jan 2012)
=================

* Added a ``qr`` script which can be used to output a qr code to the tty using
  background colors, or to a file via a pipe.



  * [qtconsole-4.5.2](http://jupyter.org) # Jupyter QtConsole

[![Build Status](https://travis-ci.org/jupyter/qtconsole.svg?branch=master)](https://travis-ci.org/jupyter/qtconsole)
[![Coverage Status](https://coveralls.io/repos/github/jupyter/qtconsole/badge.svg?branch=master)](https://coveralls.io/github/jupyter/qtconsole?branch=master)
[![Documentation Status](https://readthedocs.org/projects/qtconsole/badge/?version=stable)](https://qtconsole.readthedocs.io/en/stable/)
[![Google Group](https://img.shields.io/badge/-Google%20Group-lightgrey.svg)](https://groups.google.com/forum/#!forum/jupyter)

A rich Qt-based console for working with Jupyter kernels,
supporting rich media output, session export, and more.

The Qtconsole is a very lightweight application that largely feels like a terminal, but
provides a number of enhancements only possible in a GUI, such as inline
figures, proper multiline editing with syntax highlighting, graphical calltips,
and more.

![qtconsole](https://raw.githubusercontent.com/jupyter/qtconsole/master/docs/source/_images/qtconsole.png)

## Install Qtconsole
The Qtconsole requires Python bindings for Qt, such as [PyQt5](http://www.riverbankcomputing.com/software/pyqt/intro),
[PyQt4](https://www.riverbankcomputing.com/software/pyqt/download),
or [PySide](http://pyside.github.io/docs/pyside).

Although [pip](https://pypi.python.org/pypi/pip) and
[conda](http://conda.pydata.org/docs) may be used to install the Qtconsole, conda
is simpler to use since it automatically installs PyQt5. Alternatively,
the Qtconsole installation with pip needs additional steps since pip doesn't install
the Qt requirement.

### Install using conda
To install:

    conda install qtconsole

**Note:** If the Qtconsole is installed using conda, it will **automatically**
install the Qt requirement as well.

### Install using pip
To install:

    pip install qtconsole

**Note:** Make sure that Qt is installed. Unfortunately, Qt is not
installed when using pip. The next section gives instructions on doing it.

### Installing Qt (if needed)
You can install PyQt5 with pip using the following command:

    pip install pyqt5

or with a system package manager on Linux. For Windows, PyQt binary packages may be
used.

**Note:** Additional information about using a system package manager may be
found in the [qtconsole documentation](https://qtconsole.readthedocs.io).

More installation instructions for PyQt can be found in the [PyQt5 documentation](http://pyqt.sourceforge.net/Docs/PyQt5/installation.html) and [PyQt4 documentation](http://pyqt.sourceforge.net/Docs/PyQt4/installation.html)

Source packages for Windows/Linux/MacOS can be found here: [PyQt5](https://www.riverbankcomputing.com/software/pyqt/download5) and [PyQt4](https://riverbankcomputing.com/software/pyqt/download).


## Usage
To run the Qtconsole:

    jupyter qtconsole

## Resources
- [Project Jupyter website](https://jupyter.org)
- Documentation for the Qtconsole
  * [latest version](https://qtconsole.readthedocs.io/en/latest/) [[PDF](https://media.readthedocs.org/pdf/qtconsole/latest/qtconsole.pdf)]
  * [stable version](https://qtconsole.readthedocs.io/en/stable/) [[PDF](https://media.readthedocs.org/pdf/qtconsole/stable/qtconsole.pdf)]
- [Documentation for Project Jupyter](https://jupyter.readthedocs.io/en/latest/index.html) [[PDF](https://media.readthedocs.org/pdf/jupyter/latest/jupyter.pdf)]
- [Issues](https://github.com/jupyter/qtconsole/issues)
- [Technical support - Jupyter Google Group](https://groups.google.com/forum/#!forum/jupyter)



  * [rackspace-auth-openstack-1.3](https://github.com/rackerlabs/rackspace-auth-openstack) Rackspace Auth Plugin for OpenStack Clients
===========================================

.. image:: https://badge.fury.io/py/rackspace-auth-openstack.png
   :target: http://badge.fury.io/py/rackspace-auth-openstack

.. image:: https://travis-ci.org/rackerlabs/rackspace-auth-openstack.png?branch=master
   :target: https://travis-ci.org/rackerlabs/rackspace-auth-openstack

This is a plugin for OpenStack Clients which provides client support for
Rackspace authentication extensions to OpenStack.
  * [rackspace-novaclient-2.1](https://github.com/rackerlabs/rackspace-novaclient) ====================
rackspace-novaclient
====================


Metapackage to install python-novaclient and Rackspace extensions


Install
=======

::

  pip install rackspace-novaclient


Usage
=====

This metapackage will ensure that python-novaclient and these extensions
are installed that are compatible with the Rackspace cloud:

- rackspace-auth-openstack
- os_diskconfig_python_novaclient_ext
- rax_scheduled_images_python_novaclient_ext
- os_networksv2_python_novaclient_ext
- os_virtual_interfacesv2_python_novaclient_ext
- rax_default_network_flags_python_novaclient_ext
- ip_associations_python_novaclient_ext


  * [rax_default_network_flags_python_novaclient_ext-0.4.0](https://github.com/rackerlabs/rax_default_network_flags_python_novaclient_ext) =========================================
rax_default_network_flags_python_novaclient_ext
=========================================

Adds instance default networks extension support to python-novaclient.

This extension is autodiscovered once installed. To use::

    pip install rax_default_network_flags_python_novaclient_ext
    nova boot <usual args> --no-public --no-service-net

  * [rax_scheduled_images_python_novaclient_ext-0.3.1](https://github.com/rackspace-titan/rax_scheduled_images_python_novaclient_ext) UNKNOWN
  * [rdflib-4.2.2](https://github.com/RDFLib/rdflib) RDFLib is a Python library for working with
RDF, a simple yet powerful language for representing information.

The library contains parsers and serializers for RDF/XML, N3,
NTriples, Turtle, TriX, RDFa and Microdata . The library presents
a Graph interface which can be backed by any one of a number of
Store implementations. The core rdflib includes store
implementations for in memory storage, persistent storage on top
of the Berkeley DB, and a wrapper for remote SPARQL endpoints.

A SPARQL 1.1 engine is also included.

If you have recently reported a bug marked as fixed, or have a craving for
the very latest, you may want the development version instead:

   easy_install https://github.com/RDFLib/rdflib/tarball/master

Read the docs at:

   http://rdflib.readthedocs.org




  * [rdflib-jsonld-0.4.0](https://github.com/RDFLib/rdflib-jsonld) This parser/serialiser will

    * read in an JSON-LD formatted document and create an RDF graph
    * serialize an RDF graph to JSON-LD formatted output

    See:

        http://json-ld.org/
  * [readme_renderer-24.0](https://github.com/pypa/readme_renderer) Readme Renderer
===============

Readme Renderer is a library that will safely render arbitrary
``README`` files into HTML. It is designed to be used in Warehouse_ to
render the ``long_description`` for packages. It can handle Markdown,
reStructuredText (``.rst``), and plain text.

.. _Warehouse: https://github.com/pypa/warehouse


Check Description Locally
-------------------------

To locally check whether your long descriptions will render on PyPI, first
build your distributions, and then use the |twine check|_ command.


Code of Conduct
---------------

Everyone interacting in the readme_renderer project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.


.. |twine check| replace:: ``twine check``
.. _twine check: https://packaging.python.org/guides/making-a-pypi-friendly-readme#validating-restructuredtext-markup
.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/



  * [redis-3.3.5](https://github.com/andymccurdy/redis-py) redis-py
========

The Python interface to the Redis key-value store.

.. image:: https://secure.travis-ci.org/andymccurdy/redis-py.svg?branch=master
        :target: https://travis-ci.org/andymccurdy/redis-py
.. image:: https://readthedocs.org/projects/redis-py/badge/?version=latest&style=flat
        :target: https://redis-py.readthedocs.io/en/latest/
.. image:: https://badge.fury.io/py/redis.svg
        :target: https://pypi.org/project/redis/
.. image:: https://codecov.io/gh/andymccurdy/redis-py/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/andymccurdy/redis-py

Installation
------------

redis-py requires a running Redis server. See `Redis's quickstart
<https://redis.io/topics/quickstart>`_ for installation instructions.

redis-py can be installed using `pip` similar to other Python packages. Do not use `sudo`
with `pip`. It is usually good to work in a
`virtualenv <https://virtualenv.pypa.io/en/latest/>`_ or
`venv <https://docs.python.org/3/library/venv.html>`_ to avoid conflicts with other package
managers and Python projects. For a quick introduction see
`Python Virtual Environments in Five Minutes <https://bit.ly/py-env>`_.

To install redis-py, simply:

.. code-block:: bash

    $ pip install redis

or from source:

.. code-block:: bash

    $ python setup.py install


Getting Started
---------------

.. code-block:: pycon

    >>> import redis
    >>> r = redis.Redis(host='localhost', port=6379, db=0)
    >>> r.set('foo', 'bar')
    True
    >>> r.get('foo')
    'bar'

By default, all responses are returned as `bytes` in Python 3 and `str` in
Python 2. The user is responsible for decoding to Python 3 strings or Python 2
unicode objects.

If **all** string responses from a client should be decoded, the user can
specify `decode_responses=True` to `Redis.__init__`. In this case, any
Redis command that returns a string type will be decoded with the `encoding`
specified.


Upgrading from redis-py 2.X to 3.0
----------------------------------

redis-py 3.0 introduces many new features but required a number of backwards
incompatible changes to be made in the process. This section attempts to
provide an upgrade path for users migrating from 2.X to 3.0.


Python Version Support
^^^^^^^^^^^^^^^^^^^^^^

redis-py 3.0 now supports Python 2.7 and Python 3.4+. Python 2.6 and 3.3
support has been dropped.


Client Classes: Redis and StrictRedis
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

redis-py 3.0 drops support for the legacy "Redis" client class. "StrictRedis"
has been renamed to "Redis" and an alias named "StrictRedis" is provided so
that users previously using "StrictRedis" can continue to run unchanged.

The 2.X "Redis" class provided alternative implementations of a few commands.
This confused users (rightfully so) and caused a number of support issues. To
make things easier going forward, it was decided to drop support for these
alternate implementations and instead focus on a single client class.

2.X users that are already using StrictRedis don't have to change the class
name. StrictRedis will continue to work for the forseeable future.

2.X users that are using the Redis class will have to make changes if they
use any of the following commands:

* SETEX: The argument order has changed. The new order is (name, time, value).
* LREM: The argument order has changed. The new order is (name, num, value).
* TTL and PTTL: The return value is now always an int and matches the
  official Redis command (>0 indicates the timeout, -1 indicates that the key
  exists but that it has no expire time set, -2 indicates that the key does
  not exist)


SSL Connections
^^^^^^^^^^^^^^^

redis-py 3.0 changes the default value of the `ssl_cert_reqs` option from
`None` to `'required'`. See
`Issue 1016 <https://github.com/andymccurdy/redis-py/issues/1016>`_. This
change enforces hostname validation when accepting a cert from a remote SSL
terminator. If the terminator doesn't properly set the hostname on the cert
this will cause redis-py 3.0 to raise a ConnectionError.

This check can be disabled by setting `ssl_cert_reqs` to `None`. Note that
doing so removes the security check. Do so at your own risk.

It has been reported that SSL certs received from AWS ElastiCache do not have
proper hostnames and turning off hostname verification is currently required.


MSET, MSETNX and ZADD
^^^^^^^^^^^^^^^^^^^^^

These commands all accept a mapping of key/value pairs. In redis-py 2.X
this mapping could be specified as ``*args`` or as ``**kwargs``. Both of these
styles caused issues when Redis introduced optional flags to ZADD. Relying on
``*args`` caused issues with the optional argument order, especially in Python
2.7. Relying on ``**kwargs`` caused potential collision issues of user keys with
the argument names in the method signature.

To resolve this, redis-py 3.0 has changed these three commands to all accept
a single positional argument named mapping that is expected to be a dict. For
MSET and MSETNX, the dict is a mapping of key-names -> values. For ZADD, the
dict is a mapping of element-names -> score.

MSET, MSETNX and ZADD now look like:

.. code-block:: pycon

    def mset(self, mapping):
    def msetnx(self, mapping):
    def zadd(self, name, mapping, nx=False, xx=False, ch=False, incr=False):

All 2.X users that use these commands must modify their code to supply
keys and values as a dict to these commands.


ZINCRBY
^^^^^^^

redis-py 2.X accidentally modified the argument order of ZINCRBY, swapping the
order of value and amount. ZINCRBY now looks like:

.. code-block:: pycon

    def zincrby(self, name, amount, value):

All 2.X users that rely on ZINCRBY must swap the order of amount and value
for the command to continue to work as intended.


Encoding of User Input
^^^^^^^^^^^^^^^^^^^^^^

redis-py 3.0 only accepts user data as bytes, strings or numbers (ints, longs
and floats). Attempting to specify a key or a value as any other type will
raise a DataError exception.

redis-py 2.X attempted to coerce any type of input into a string. While
occasionally convenient, this caused all sorts of hidden errors when users
passed boolean values (which were coerced to 'True' or 'False'), a None
value (which was coerced to 'None') or other values, such as user defined
types.

All 2.X users should make sure that the keys and values they pass into
redis-py are either bytes, strings or numbers.


Locks
^^^^^

redis-py 3.0 drops support for the pipeline-based Lock and now only supports
the Lua-based lock. In doing so, LuaLock has been renamed to Lock. This also
means that redis-py Lock objects require Redis server 2.6 or greater.

2.X users that were explicitly referring to "LuaLock" will have to now refer
to "Lock" instead.


Locks as Context Managers
^^^^^^^^^^^^^^^^^^^^^^^^^

redis-py 3.0 now raises a LockError when using a lock as a context manager and
the lock cannot be acquired within the specified timeout. This is more of a
bug fix than a backwards incompatible change. However, given an error is now
raised where none was before, this might alarm some users.

2.X users should make sure they're wrapping their lock code in a try/catch
like this:

.. code-block:: pycon

    try:
        with r.lock('my-lock-key', blocking_timeout=5) as lock:
            # code you want executed only after the lock has been acquired
    except LockError:
        # the lock wasn't acquired


API Reference
-------------

The `official Redis command documentation <https://redis.io/commands>`_ does a
great job of explaining each command in detail. redis-py attempts to adhere
to the official command syntax. There are a few exceptions:

* **SELECT**: Not implemented. See the explanation in the Thread Safety section
  below.
* **DEL**: 'del' is a reserved keyword in the Python syntax. Therefore redis-py
  uses 'delete' instead.
* **MULTI/EXEC**: These are implemented as part of the Pipeline class. The
  pipeline is wrapped with the MULTI and EXEC statements by default when it
  is executed, which can be disabled by specifying transaction=False.
  See more about Pipelines below.
* **SUBSCRIBE/LISTEN**: Similar to pipelines, PubSub is implemented as a separate
  class as it places the underlying connection in a state where it can't
  execute non-pubsub commands. Calling the pubsub method from the Redis client
  will return a PubSub instance where you can subscribe to channels and listen
  for messages. You can only call PUBLISH from the Redis client (see
  `this comment on issue #151
  <https://github.com/andymccurdy/redis-py/issues/151#issuecomment-1545015>`_
  for details).
* **SCAN/SSCAN/HSCAN/ZSCAN**: The \*SCAN commands are implemented as they
  exist in the Redis documentation. In addition, each command has an equivalent
  iterator method. These are purely for convenience so the user doesn't have
  to keep track of the cursor while iterating. Use the
  scan_iter/sscan_iter/hscan_iter/zscan_iter methods for this behavior.


More Detail
-----------

Connection Pools
^^^^^^^^^^^^^^^^

Behind the scenes, redis-py uses a connection pool to manage connections to
a Redis server. By default, each Redis instance you create will in turn create
its own connection pool. You can override this behavior and use an existing
connection pool by passing an already created connection pool instance to the
connection_pool argument of the Redis class. You may choose to do this in order
to implement client side sharding or have finer grain control of how
connections are managed.

.. code-block:: pycon

    >>> pool = redis.ConnectionPool(host='localhost', port=6379, db=0)
    >>> r = redis.Redis(connection_pool=pool)

Connections
^^^^^^^^^^^

ConnectionPools manage a set of Connection instances. redis-py ships with two
types of Connections. The default, Connection, is a normal TCP socket based
connection. The UnixDomainSocketConnection allows for clients running on the
same device as the server to connect via a unix domain socket. To use a
UnixDomainSocketConnection connection, simply pass the unix_socket_path
argument, which is a string to the unix domain socket file. Additionally, make
sure the unixsocket parameter is defined in your redis.conf file. It's
commented out by default.

.. code-block:: pycon

    >>> r = redis.Redis(unix_socket_path='/tmp/redis.sock')

You can create your own Connection subclasses as well. This may be useful if
you want to control the socket behavior within an async framework. To
instantiate a client class using your own connection, you need to create
a connection pool, passing your class to the connection_class argument.
Other keyword parameters you pass to the pool will be passed to the class
specified during initialization.

.. code-block:: pycon

    >>> pool = redis.ConnectionPool(connection_class=YourConnectionClass,
                                    your_arg='...', ...)

Connections maintain an open socket to the Redis server. Sometimes these
sockets are interrupted or disconnected for a variety of reasons. For example,
network appliances, load balancers and other services that sit between clients
and servers are often configured to kill connections that remain idle for a
given threshold.

When a connection becomes disconnected, the next command issued on that
connection will fail and redis-py will raise a ConnectionError to the caller.
This allows each application that uses redis-py to handle errors in a way
that's fitting for that specific application. However, constant error
handling can be verbose and cumbersome, especially when socket disconnections
happen frequently in many production environments.

To combat this, redis-py can issue regular health checks to assess the
liveliness of a connection just before issuing a command. Users can pass
``health_check_interval=N`` to the Redis or ConnectionPool classes or
as a query argument within a Redis URL. The value of ``health_check_interval``
must be an integer. A value of ``0``, the default, disables health checks.
Any positive integer will enable health checks. Health checks are performed
just before a command is executed if the underlying connection has been idle
for more than ``health_check_interval`` seconds. For example,
``health_check_interval=30`` will ensure that a health check is run on any
connection that has been idle for 30 or more seconds just before a command
is executed on that connection.

If your application is running in an environment that disconnects idle
connections after 30 seconds you should set the ``health_check_interval``
option to a value less than 30.

This option also works on any PubSub connection that is created from a
client with ``health_check_interval`` enabled. PubSub users need to ensure
that ``get_message()`` or ``listen()`` are called more frequently than
``health_check_interval`` seconds. It is assumed that most workloads already
do this.

If your PubSub use case doesn't call ``get_message()`` or ``listen()``
frequently, you should call ``pubsub.check_health()`` explicitly on a
regularly basis.

Parsers
^^^^^^^

Parser classes provide a way to control how responses from the Redis server
are parsed. redis-py ships with two parser classes, the PythonParser and the
HiredisParser. By default, redis-py will attempt to use the HiredisParser if
you have the hiredis module installed and will fallback to the PythonParser
otherwise.

Hiredis is a C library maintained by the core Redis team. Pieter Noordhuis was
kind enough to create Python bindings. Using Hiredis can provide up to a
10x speed improvement in parsing responses from the Redis server. The
performance increase is most noticeable when retrieving many pieces of data,
such as from LRANGE or SMEMBERS operations.

Hiredis is available on PyPI, and can be installed via pip just like redis-py.

.. code-block:: bash

    $ pip install hiredis

Response Callbacks
^^^^^^^^^^^^^^^^^^

The client class uses a set of callbacks to cast Redis responses to the
appropriate Python type. There are a number of these callbacks defined on
the Redis client class in a dictionary called RESPONSE_CALLBACKS.

Custom callbacks can be added on a per-instance basis using the
set_response_callback method. This method accepts two arguments: a command
name and the callback. Callbacks added in this manner are only valid on the
instance the callback is added to. If you want to define or override a callback
globally, you should make a subclass of the Redis client and add your callback
to its RESPONSE_CALLBACKS class dictionary.

Response callbacks take at least one parameter: the response from the Redis
server. Keyword arguments may also be accepted in order to further control
how to interpret the response. These keyword arguments are specified during the
command's call to execute_command. The ZRANGE implementation demonstrates the
use of response callback keyword arguments with its "withscores" argument.

Thread Safety
^^^^^^^^^^^^^

Redis client instances can safely be shared between threads. Internally,
connection instances are only retrieved from the connection pool during
command execution, and returned to the pool directly after. Command execution
never modifies state on the client instance.

However, there is one caveat: the Redis SELECT command. The SELECT command
allows you to switch the database currently in use by the connection. That
database remains selected until another is selected or until the connection is
closed. This creates an issue in that connections could be returned to the pool
that are connected to a different database.

As a result, redis-py does not implement the SELECT command on client
instances. If you use multiple Redis databases within the same application, you
should create a separate client instance (and possibly a separate connection
pool) for each database.

It is not safe to pass PubSub or Pipeline objects between threads.

Pipelines
^^^^^^^^^

Pipelines are a subclass of the base Redis class that provide support for
buffering multiple commands to the server in a single request. They can be used
to dramatically increase the performance of groups of commands by reducing the
number of back-and-forth TCP packets between the client and server.

Pipelines are quite simple to use:

.. code-block:: pycon

    >>> r = redis.Redis(...)
    >>> r.set('bing', 'baz')
    >>> # Use the pipeline() method to create a pipeline instance
    >>> pipe = r.pipeline()
    >>> # The following SET commands are buffered
    >>> pipe.set('foo', 'bar')
    >>> pipe.get('bing')
    >>> # the EXECUTE call sends all buffered commands to the server, returning
    >>> # a list of responses, one for each command.
    >>> pipe.execute()
    [True, 'baz']

For ease of use, all commands being buffered into the pipeline return the
pipeline object itself. Therefore calls can be chained like:

.. code-block:: pycon

    >>> pipe.set('foo', 'bar').sadd('faz', 'baz').incr('auto_number').execute()
    [True, True, 6]

In addition, pipelines can also ensure the buffered commands are executed
atomically as a group. This happens by default. If you want to disable the
atomic nature of a pipeline but still want to buffer commands, you can turn
off transactions.

.. code-block:: pycon

    >>> pipe = r.pipeline(transaction=False)

A common issue occurs when requiring atomic transactions but needing to
retrieve values in Redis prior for use within the transaction. For instance,
let's assume that the INCR command didn't exist and we need to build an atomic
version of INCR in Python.

The completely naive implementation could GET the value, increment it in
Python, and SET the new value back. However, this is not atomic because
multiple clients could be doing this at the same time, each getting the same
value from GET.

Enter the WATCH command. WATCH provides the ability to monitor one or more keys
prior to starting a transaction. If any of those keys change prior the
execution of that transaction, the entire transaction will be canceled and a
WatchError will be raised. To implement our own client-side INCR command, we
could do something like this:

.. code-block:: pycon

    >>> with r.pipeline() as pipe:
    ...     while True:
    ...         try:
    ...             # put a WATCH on the key that holds our sequence value
    ...             pipe.watch('OUR-SEQUENCE-KEY')
    ...             # after WATCHing, the pipeline is put into immediate execution
    ...             # mode until we tell it to start buffering commands again.
    ...             # this allows us to get the current value of our sequence
    ...             current_value = pipe.get('OUR-SEQUENCE-KEY')
    ...             next_value = int(current_value) + 1
    ...             # now we can put the pipeline back into buffered mode with MULTI
    ...             pipe.multi()
    ...             pipe.set('OUR-SEQUENCE-KEY', next_value)
    ...             # and finally, execute the pipeline (the set command)
    ...             pipe.execute()
    ...             # if a WatchError wasn't raised during execution, everything
    ...             # we just did happened atomically.
    ...             break
    ...        except WatchError:
    ...             # another client must have changed 'OUR-SEQUENCE-KEY' between
    ...             # the time we started WATCHing it and the pipeline's execution.
    ...             # our best bet is to just retry.
    ...             continue

Note that, because the Pipeline must bind to a single connection for the
duration of a WATCH, care must be taken to ensure that the connection is
returned to the connection pool by calling the reset() method. If the
Pipeline is used as a context manager (as in the example above) reset()
will be called automatically. Of course you can do this the manual way by
explicitly calling reset():

.. code-block:: pycon

    >>> pipe = r.pipeline()
    >>> while True:
    ...     try:
    ...         pipe.watch('OUR-SEQUENCE-KEY')
    ...         ...
    ...         pipe.execute()
    ...         break
    ...     except WatchError:
    ...         continue
    ...     finally:
    ...         pipe.reset()

A convenience method named "transaction" exists for handling all the
boilerplate of handling and retrying watch errors. It takes a callable that
should expect a single parameter, a pipeline object, and any number of keys to
be WATCHed. Our client-side INCR command above can be written like this,
which is much easier to read:

.. code-block:: pycon

    >>> def client_side_incr(pipe):
    ...     current_value = pipe.get('OUR-SEQUENCE-KEY')
    ...     next_value = int(current_value) + 1
    ...     pipe.multi()
    ...     pipe.set('OUR-SEQUENCE-KEY', next_value)
    >>>
    >>> r.transaction(client_side_incr, 'OUR-SEQUENCE-KEY')
    [True]

Publish / Subscribe
^^^^^^^^^^^^^^^^^^^

redis-py includes a `PubSub` object that subscribes to channels and listens
for new messages. Creating a `PubSub` object is easy.

.. code-block:: pycon

    >>> r = redis.Redis(...)
    >>> p = r.pubsub()

Once a `PubSub` instance is created, channels and patterns can be subscribed
to.

.. code-block:: pycon

    >>> p.subscribe('my-first-channel', 'my-second-channel', ...)
    >>> p.psubscribe('my-*', ...)

The `PubSub` instance is now subscribed to those channels/patterns. The
subscription confirmations can be seen by reading messages from the `PubSub`
instance.

.. code-block:: pycon

    >>> p.get_message()
    {'pattern': None, 'type': 'subscribe', 'channel': 'my-second-channel', 'data': 1L}
    >>> p.get_message()
    {'pattern': None, 'type': 'subscribe', 'channel': 'my-first-channel', 'data': 2L}
    >>> p.get_message()
    {'pattern': None, 'type': 'psubscribe', 'channel': 'my-*', 'data': 3L}

Every message read from a `PubSub` instance will be a dictionary with the
following keys.

* **type**: One of the following: 'subscribe', 'unsubscribe', 'psubscribe',
  'punsubscribe', 'message', 'pmessage'
* **channel**: The channel [un]subscribed to or the channel a message was
  published to
* **pattern**: The pattern that matched a published message's channel. Will be
  `None` in all cases except for 'pmessage' types.
* **data**: The message data. With [un]subscribe messages, this value will be
  the number of channels and patterns the connection is currently subscribed
  to. With [p]message messages, this value will be the actual published
  message.

Let's send a message now.

.. code-block:: pycon

    # the publish method returns the number matching channel and pattern
    # subscriptions. 'my-first-channel' matches both the 'my-first-channel'
    # subscription and the 'my-*' pattern subscription, so this message will
    # be delivered to 2 channels/patterns
    >>> r.publish('my-first-channel', 'some data')
    2
    >>> p.get_message()
    {'channel': 'my-first-channel', 'data': 'some data', 'pattern': None, 'type': 'message'}
    >>> p.get_message()
    {'channel': 'my-first-channel', 'data': 'some data', 'pattern': 'my-*', 'type': 'pmessage'}

Unsubscribing works just like subscribing. If no arguments are passed to
[p]unsubscribe, all channels or patterns will be unsubscribed from.

.. code-block:: pycon

    >>> p.unsubscribe()
    >>> p.punsubscribe('my-*')
    >>> p.get_message()
    {'channel': 'my-second-channel', 'data': 2L, 'pattern': None, 'type': 'unsubscribe'}
    >>> p.get_message()
    {'channel': 'my-first-channel', 'data': 1L, 'pattern': None, 'type': 'unsubscribe'}
    >>> p.get_message()
    {'channel': 'my-*', 'data': 0L, 'pattern': None, 'type': 'punsubscribe'}

redis-py also allows you to register callback functions to handle published
messages. Message handlers take a single argument, the message, which is a
dictionary just like the examples above. To subscribe to a channel or pattern
with a message handler, pass the channel or pattern name as a keyword argument
with its value being the callback function.

When a message is read on a channel or pattern with a message handler, the
message dictionary is created and passed to the message handler. In this case,
a `None` value is returned from get_message() since the message was already
handled.

.. code-block:: pycon

    >>> def my_handler(message):
    ...     print 'MY HANDLER: ', message['data']
    >>> p.subscribe(**{'my-channel': my_handler})
    # read the subscribe confirmation message
    >>> p.get_message()
    {'pattern': None, 'type': 'subscribe', 'channel': 'my-channel', 'data': 1L}
    >>> r.publish('my-channel', 'awesome data')
    1
    # for the message handler to work, we need tell the instance to read data.
    # this can be done in several ways (read more below). we'll just use
    # the familiar get_message() function for now
    >>> message = p.get_message()
    MY HANDLER:  awesome data
    # note here that the my_handler callback printed the string above.
    # `message` is None because the message was handled by our handler.
    >>> print message
    None

If your application is not interested in the (sometimes noisy)
subscribe/unsubscribe confirmation messages, you can ignore them by passing
`ignore_subscribe_messages=True` to `r.pubsub()`. This will cause all
subscribe/unsubscribe messages to be read, but they won't bubble up to your
application.

.. code-block:: pycon

    >>> p = r.pubsub(ignore_subscribe_messages=True)
    >>> p.subscribe('my-channel')
    >>> p.get_message()  # hides the subscribe message and returns None
    >>> r.publish('my-channel', 'my data')
    1
    >>> p.get_message()
    {'channel': 'my-channel', 'data': 'my data', 'pattern': None, 'type': 'message'}

There are three different strategies for reading messages.

The examples above have been using `pubsub.get_message()`. Behind the scenes,
`get_message()` uses the system's 'select' module to quickly poll the
connection's socket. If there's data available to be read, `get_message()` will
read it, format the message and return it or pass it to a message handler. If
there's no data to be read, `get_message()` will immediately return None. This
makes it trivial to integrate into an existing event loop inside your
application.

.. code-block:: pycon

    >>> while True:
    >>>     message = p.get_message()
    >>>     if message:
    >>>         # do something with the message
    >>>     time.sleep(0.001)  # be nice to the system :)

Older versions of redis-py only read messages with `pubsub.listen()`. listen()
is a generator that blocks until a message is available. If your application
doesn't need to do anything else but receive and act on messages received from
redis, listen() is an easy way to get up an running.

.. code-block:: pycon

    >>> for message in p.listen():
    ...     # do something with the message

The third option runs an event loop in a separate thread.
`pubsub.run_in_thread()` creates a new thread and starts the event loop. The
thread object is returned to the caller of `run_in_thread()`. The caller can
use the `thread.stop()` method to shut down the event loop and thread. Behind
the scenes, this is simply a wrapper around `get_message()` that runs in a
separate thread, essentially creating a tiny non-blocking event loop for you.
`run_in_thread()` takes an optional `sleep_time` argument. If specified, the
event loop will call `time.sleep()` with the value in each iteration of the
loop.

Note: Since we're running in a separate thread, there's no way to handle
messages that aren't automatically handled with registered message handlers.
Therefore, redis-py prevents you from calling `run_in_thread()` if you're
subscribed to patterns or channels that don't have message handlers attached.

.. code-block:: pycon

    >>> p.subscribe(**{'my-channel': my_handler})
    >>> thread = p.run_in_thread(sleep_time=0.001)
    # the event loop is now running in the background processing messages
    # when it's time to shut it down...
    >>> thread.stop()

A PubSub object adheres to the same encoding semantics as the client instance
it was created from. Any channel or pattern that's unicode will be encoded
using the `charset` specified on the client before being sent to Redis. If the
client's `decode_responses` flag is set the False (the default), the
'channel', 'pattern' and 'data' values in message dictionaries will be byte
strings (str on Python 2, bytes on Python 3). If the client's
`decode_responses` is True, then the 'channel', 'pattern' and 'data' values
will be automatically decoded to unicode strings using the client's `charset`.

PubSub objects remember what channels and patterns they are subscribed to. In
the event of a disconnection such as a network error or timeout, the
PubSub object will re-subscribe to all prior channels and patterns when
reconnecting. Messages that were published while the client was disconnected
cannot be delivered. When you're finished with a PubSub object, call its
`.close()` method to shutdown the connection.

.. code-block:: pycon

    >>> p = r.pubsub()
    >>> ...
    >>> p.close()


The PUBSUB set of subcommands CHANNELS, NUMSUB and NUMPAT are also
supported:

.. code-block:: pycon

    >>> r.pubsub_channels()
    ['foo', 'bar']
    >>> r.pubsub_numsub('foo', 'bar')
    [('foo', 9001), ('bar', 42)]
    >>> r.pubsub_numsub('baz')
    [('baz', 0)]
    >>> r.pubsub_numpat()
    1204

Monitor
^^^^^^^
redis-py includes a `Monitor` object that streams every command processed
by the Redis server. Use `listen()` on the `Monitor` object to block
until a command is received.

.. code-block:: pycon

    >>> r = redis.Redis(...)
    >>> with r.monitor() as m:
    >>>     for command in m.listen():
    >>>         print(command)

Lua Scripting
^^^^^^^^^^^^^

redis-py supports the EVAL, EVALSHA, and SCRIPT commands. However, there are
a number of edge cases that make these commands tedious to use in real world
scenarios. Therefore, redis-py exposes a Script object that makes scripting
much easier to use.

To create a Script instance, use the `register_script` function on a client
instance passing the Lua code as the first argument. `register_script` returns
a Script instance that you can use throughout your code.

The following trivial Lua script accepts two parameters: the name of a key and
a multiplier value. The script fetches the value stored in the key, multiplies
it with the multiplier value and returns the result.

.. code-block:: pycon

    >>> r = redis.Redis()
    >>> lua = """
    ... local value = redis.call('GET', KEYS[1])
    ... value = tonumber(value)
    ... return value * ARGV[1]"""
    >>> multiply = r.register_script(lua)

`multiply` is now a Script instance that is invoked by calling it like a
function. Script instances accept the following optional arguments:

* **keys**: A list of key names that the script will access. This becomes the
  KEYS list in Lua.
* **args**: A list of argument values. This becomes the ARGV list in Lua.
* **client**: A redis-py Client or Pipeline instance that will invoke the
  script. If client isn't specified, the client that initially
  created the Script instance (the one that `register_script` was
  invoked from) will be used.

Continuing the example from above:

.. code-block:: pycon

    >>> r.set('foo', 2)
    >>> multiply(keys=['foo'], args=[5])
    10

The value of key 'foo' is set to 2. When multiply is invoked, the 'foo' key is
passed to the script along with the multiplier value of 5. Lua executes the
script and returns the result, 10.

Script instances can be executed using a different client instance, even one
that points to a completely different Redis server.

.. code-block:: pycon

    >>> r2 = redis.Redis('redis2.example.com')
    >>> r2.set('foo', 3)
    >>> multiply(keys=['foo'], args=[5], client=r2)
    15

The Script object ensures that the Lua script is loaded into Redis's script
cache. In the event of a NOSCRIPT error, it will load the script and retry
executing it.

Script objects can also be used in pipelines. The pipeline instance should be
passed as the client argument when calling the script. Care is taken to ensure
that the script is registered in Redis's script cache just prior to pipeline
execution.

.. code-block:: pycon

    >>> pipe = r.pipeline()
    >>> pipe.set('foo', 5)
    >>> multiply(keys=['foo'], args=[5], client=pipe)
    >>> pipe.execute()
    [True, 25]

Sentinel support
^^^^^^^^^^^^^^^^

redis-py can be used together with `Redis Sentinel <https://redis.io/topics/sentinel>`_
to discover Redis nodes. You need to have at least one Sentinel daemon running
in order to use redis-py's Sentinel support.

Connecting redis-py to the Sentinel instance(s) is easy. You can use a
Sentinel connection to discover the master and slaves network addresses:

.. code-block:: pycon

    >>> from redis.sentinel import Sentinel
    >>> sentinel = Sentinel([('localhost', 26379)], socket_timeout=0.1)
    >>> sentinel.discover_master('mymaster')
    ('127.0.0.1', 6379)
    >>> sentinel.discover_slaves('mymaster')
    [('127.0.0.1', 6380)]

You can also create Redis client connections from a Sentinel instance. You can
connect to either the master (for write operations) or a slave (for read-only
operations).

.. code-block:: pycon

    >>> master = sentinel.master_for('mymaster', socket_timeout=0.1)
    >>> slave = sentinel.slave_for('mymaster', socket_timeout=0.1)
    >>> master.set('foo', 'bar')
    >>> slave.get('foo')
    'bar'

The master and slave objects are normal Redis instances with their
connection pool bound to the Sentinel instance. When a Sentinel backed client
attempts to establish a connection, it first queries the Sentinel servers to
determine an appropriate host to connect to. If no server is found,
a MasterNotFoundError or SlaveNotFoundError is raised. Both exceptions are
subclasses of ConnectionError.

When trying to connect to a slave client, the Sentinel connection pool will
iterate over the list of slaves until it finds one that can be connected to.
If no slaves can be connected to, a connection will be established with the
master.

See `Guidelines for Redis clients with support for Redis Sentinel
<https://redis.io/topics/sentinel-clients>`_ to learn more about Redis Sentinel.

Scan Iterators
^^^^^^^^^^^^^^

The \*SCAN commands introduced in Redis 2.8 can be cumbersome to use. While
these commands are fully supported, redis-py also exposes the following methods
that return Python iterators for convenience: `scan_iter`, `hscan_iter`,
`sscan_iter` and `zscan_iter`.

.. code-block:: pycon

    >>> for key, value in (('A', '1'), ('B', '2'), ('C', '3')):
    ...     r.set(key, value)
    >>> for key in r.scan_iter():
    ...     print key, r.get(key)
    A 1
    B 2
    C 3

Author
^^^^^^

redis-py is developed and maintained by Andy McCurdy (sedrik@gmail.com).
It can be found here: https://github.com/andymccurdy/redis-py

Special thanks to:

* Ludovico Magnocavallo, author of the original Python Redis client, from
  which some of the socket code is still used.
* Alexander Solovyov for ideas on the generic response callback system.
* Paul Hubbard for initial packaging support.




  * [regex-2019.06.08](https://bitbucket.org/mrabarnett/mrab-regex) Introduction
------------

This regex implementation is backwards-compatible with the standard 're' module, but offers additional functionality.

Note
----

The re module's behaviour with zero-width matches changed in Python 3.7, and this module will follow that behaviour when compiled for Python 3.7.

Old vs new behaviour
--------------------

In order to be compatible with the re module, this module has 2 behaviours:

* **Version 0** behaviour (old behaviour, compatible with the re module):

  Please note that the re module's behaviour may change over time, and I'll endeavour to match that behaviour in version 0.

  * Indicated by the ``VERSION0`` or ``V0`` flag, or ``(?V0)`` in the pattern.

  * Zero-width matches are not handled correctly in the re module before Python 3.7. The behaviour in those earlier versions is:

    * ``.split`` won't split a string at a zero-width match.

    * ``.sub`` will advance by one character after a zero-width match.

  * Inline flags apply to the entire pattern, and they can't be turned off.

  * Only simple sets are supported.

  * Case-insensitive matches in Unicode use simple case-folding by default.

* **Version 1** behaviour (new behaviour, possibly different from the re module):

  * Indicated by the ``VERSION1`` or ``V1`` flag, or ``(?V1)`` in the pattern.

  * Zero-width matches are handled correctly.

  * Inline flags apply to the end of the group or pattern, and they can be turned off.

  * Nested sets and set operations are supported.

  * Case-insensitive matches in Unicode use full case-folding by default.

If no version is specified, the regex module will default to ``regex.DEFAULT_VERSION``.

Case-insensitive matches in Unicode
-----------------------------------

The regex module supports both simple and full case-folding for case-insensitive matches in Unicode. Use of full case-folding can be turned on using the ``FULLCASE`` or ``F`` flag, or ``(?f)`` in the pattern. Please note that this flag affects how the ``IGNORECASE`` flag works; the ``FULLCASE`` flag itself does not turn on case-insensitive matching.

In the version 0 behaviour, the flag is off by default.

In the version 1 behaviour, the flag is on by default.

Nested sets and set operations
------------------------------

It's not possible to support both simple sets, as used in the re module, and nested sets at the same time because of a difference in the meaning of an unescaped ``"["`` in a set.

For example, the pattern ``[[a-z]--[aeiou]]`` is treated in the version 0 behaviour (simple sets, compatible with the re module) as:

* Set containing "[" and the letters "a" to "z"

* Literal "--"

* Set containing letters "a", "e", "i", "o", "u"

* Literal "]"

but in the version 1 behaviour (nested sets, enhanced behaviour) as:

* Set which is:

  * Set containing the letters "a" to "z"

* but excluding:

  * Set containing the letters "a", "e", "i", "o", "u"

Version 0 behaviour: only simple sets are supported.

Version 1 behaviour: nested sets and set operations are supported.

Flags
-----

There are 2 kinds of flag: scoped and global. Scoped flags can apply to only part of a pattern and can be turned on or off; global flags apply to the entire pattern and can only be turned on.

The scoped flags are: ``FULLCASE``, ``IGNORECASE``, ``MULTILINE``, ``DOTALL``, ``VERBOSE``, ``WORD``.

The global flags are: ``ASCII``, ``BESTMATCH``, ``ENHANCEMATCH``, ``LOCALE``, ``POSIX``, ``REVERSE``, ``UNICODE``, ``VERSION0``, ``VERSION1``.

If neither the ``ASCII``, ``LOCALE`` nor ``UNICODE`` flag is specified, it will default to ``UNICODE`` if the regex pattern is a Unicode string and ``ASCII`` if it's a bytestring.

The ``ENHANCEMATCH`` flag makes fuzzy matching attempt to improve the fit of the next match that it finds.

The ``BESTMATCH`` flag makes fuzzy matching search for the best match instead of the next match.

Notes on named capture groups
-----------------------------

All capture groups have a group number, starting from 1.

Groups with the same group name will have the same group number, and groups with a different group name will have a different group number.

The same name can be used by more than one group, with later captures 'overwriting' earlier captures. All of the captures of the group will be available from the ``captures`` method of the match object.

Group numbers will be reused across different branches of a branch reset, eg. ``(?|(first)|(second))`` has only group 1. If capture groups have different group names then they will, of course, have different group numbers, eg. ``(?|(?P<foo>first)|(?P<bar>second))`` has group 1 ("foo") and group 2 ("bar").

In the regex ``(\s+)(?|(?P<foo>[A-Z]+)|(\w+) (?P<foo>[0-9]+)`` there are 2 groups:

* ``(\s+)`` is group 1.

* ``(?P<foo>[A-Z]+)`` is group 2, also called "foo".

* ``(\w+)`` is group 2 because of the branch reset.

* ``(?P<foo>[0-9]+)`` is group 2 because it's called "foo".

If you want to prevent ``(\w+)`` from being group 2, you need to name it (different name, different group number).

Multithreading
--------------

The regex module releases the GIL during matching on instances of the built-in (immutable) string classes, enabling other Python threads to run concurrently. It is also possible to force the regex module to release the GIL during matching by calling the matching methods with the keyword argument ``concurrent=True``. The behaviour is undefined if the string changes during matching, so use it *only* when it is guaranteed that that won't happen.

Unicode
-------

This module supports Unicode 12.1.0.

Full Unicode case-folding is supported.

Additional features
-------------------

The issue numbers relate to the Python bug tracker, except where listed as "Hg issue".

* Added support for lookaround in conditional pattern (`Hg issue 163 <https://bitbucket.org/mrabarnett/mrab-regex/issues/163>`_)

  The test of a conditional pattern can now be a lookaround.

  Examples:

  .. sourcecode:: python

    >>> regex.match(r'(?(?=\d)\d+|\w+)', '123abc')
    <regex.Match object; span=(0, 3), match='123'>
    >>> regex.match(r'(?(?=\d)\d+|\w+)', 'abc123')
    <regex.Match object; span=(0, 6), match='abc123'>

  This is not quite the same as putting a lookaround in the first branch of a pair of alternatives.

  Examples:

  .. sourcecode:: python

    >>> print(regex.match(r'(?:(?=\d)\d+\b|\w+)', '123abc'))
    <regex.Match object; span=(0, 6), match='123abc'>
    >>> print(regex.match(r'(?(?=\d)\d+\b|\w+)', '123abc'))
    None

  In the first example, the lookaround matched, but the remainder of the first branch failed to match, and so the second branch was attempted, whereas in the second example, the lookaround matched, and the first branch failed to match, but the second branch was **not** attempted.

* Added POSIX matching (leftmost longest) (`Hg issue 150 <https://bitbucket.org/mrabarnett/mrab-regex/issues/150>`_)

  The POSIX standard for regex is to return the leftmost longest match. This can be turned on using the ``POSIX`` flag (``(?p)``).

  Examples:

  .. sourcecode:: python

    >>> # Normal matching.
    >>> regex.search(r'Mr|Mrs', 'Mrs')
    <regex.Match object; span=(0, 2), match='Mr'>
    >>> regex.search(r'one(self)?(selfsufficient)?', 'oneselfsufficient')
    <regex.Match object; span=(0, 7), match='oneself'>
    >>> # POSIX matching.
    >>> regex.search(r'(?p)Mr|Mrs', 'Mrs')
    <regex.Match object; span=(0, 3), match='Mrs'>
    >>> regex.search(r'(?p)one(self)?(selfsufficient)?', 'oneselfsufficient')
    <regex.Match object; span=(0, 17), match='oneselfsufficient'>

  Note that it will take longer to find matches because when it finds a match at a certain position, it won't return that immediately, but will keep looking to see if there's another longer match there.

* Added ``(?(DEFINE)...)`` (`Hg issue 152 <https://bitbucket.org/mrabarnett/mrab-regex/issues/152>`_)

  If there's no group called "DEFINE", then ... will be ignored, but any group definitions within it will be available.

  Examples:

  .. sourcecode:: python

    >>> regex.search(r'(?(DEFINE)(?P<quant>\d+)(?P<item>\w+))(?&quant) (?&item)', '5 elephants')
    <regex.Match object; span=(0, 11), match='5 elephants'>

* Added ``(*PRUNE)``, ``(*SKIP)`` and ``(*FAIL)`` (`Hg issue 153 <https://bitbucket.org/mrabarnett/mrab-regex/issues/153>`_)

  ``(*PRUNE)`` discards the backtracking info up to that point. When used in an atomic group or a lookaround, it won't affect the enclosing pattern.

  ``(*SKIP)`` is similar to ``(*PRUNE)``, except that it also sets where in the text the next attempt to match will start. When used in an atomic group or a lookaround, it won't affect the enclosing pattern.

  ``(*FAIL)`` causes immediate backtracking. ``(*F)`` is a permitted abbreviation.

* Added ``\K`` (`Hg issue 151 <https://bitbucket.org/mrabarnett/mrab-regex/issues/151>`_)

  Keeps the part of the entire match after the position where ``\K`` occurred; the part before it is discarded.

  It does not affect what capture groups return.

  Examples:

  .. sourcecode:: python

    >>> m = regex.search(r'(\w\w\K\w\w\w)', 'abcdef')
    >>> m[0]
    'cde'
    >>> m[1]
    'abcde'
    >>>
    >>> m = regex.search(r'(?r)(\w\w\K\w\w\w)', 'abcdef')
    >>> m[0]
    'bc'
    >>> m[1]
    'bcdef'

* Added capture subscripting for ``expandf`` and ``subf``/``subfn`` (`Hg issue 133 <https://bitbucket.org/mrabarnett/mrab-regex/issues/133>`_)

  You can now use subscripting to get the captures of a repeated capture group.

  Examples:

  .. sourcecode:: python

    >>> m = regex.match(r"(\w)+", "abc")
    >>> m.expandf("{1}")
    'c'
    >>> m.expandf("{1[0]} {1[1]} {1[2]}")
    'a b c'
    >>> m.expandf("{1[-1]} {1[-2]} {1[-3]}")
    'c b a'
    >>>
    >>> m = regex.match(r"(?P<letter>\w)+", "abc")
    >>> m.expandf("{letter}")
    'c'
    >>> m.expandf("{letter[0]} {letter[1]} {letter[2]}")
    'a b c'
    >>> m.expandf("{letter[-1]} {letter[-2]} {letter[-3]}")
    'c b a'

* Added support for referring to a group by number using ``(?P=...)``.

  This is in addition to the existing ``\g<...>``.

* Fixed the handling of locale-sensitive regexes.

  The ``LOCALE`` flag is intended for legacy code and has limited support. You're still recommended to use Unicode instead.

* Added partial matches (`Hg issue 102 <https://bitbucket.org/mrabarnett/mrab-regex/issues/102>`_)

  A partial match is one that matches up to the end of string, but that string has been truncated and you want to know whether a complete match could be possible if the string had not been truncated.

  Partial matches are supported by ``match``, ``search``, ``fullmatch`` and ``finditer`` with the ``partial`` keyword argument.

  Match objects have a ``partial`` attribute, which is ``True`` if it's a partial match.

  For example, if you wanted a user to enter a 4-digit number and check it character by character as it was being entered:

  .. sourcecode:: python

    >>> pattern = regex.compile(r'\d{4}')

    >>> # Initially, nothing has been entered:
    >>> print(pattern.fullmatch('', partial=True))
    <regex.Match object; span=(0, 0), match='', partial=True>

    >>> # An empty string is OK, but it's only a partial match.
    >>> # The user enters a letter:
    >>> print(pattern.fullmatch('a', partial=True))
    None
    >>> # It'll never match.

    >>> # The user deletes that and enters a digit:
    >>> print(pattern.fullmatch('1', partial=True))
    <regex.Match object; span=(0, 1), match='1', partial=True>
    >>> # It matches this far, but it's only a partial match.

    >>> # The user enters 2 more digits:
    >>> print(pattern.fullmatch('123', partial=True))
    <regex.Match object; span=(0, 3), match='123', partial=True>
    >>> # It matches this far, but it's only a partial match.

    >>> # The user enters another digit:
    >>> print(pattern.fullmatch('1234', partial=True))
    <regex.Match object; span=(0, 4), match='1234'>
    >>> # It's a complete match.

    >>> # If the user enters another digit:
    >>> print(pattern.fullmatch('12345', partial=True))
    None
    >>> # It's no longer a match.

    >>> # This is a partial match:
    >>> pattern.match('123', partial=True).partial
    True

    >>> # This is a complete match:
    >>> pattern.match('1233', partial=True).partial
    False

* ``*`` operator not working correctly with sub() (`Hg issue 106 <https://bitbucket.org/mrabarnett/mrab-regex/issues/106>`_)

  Sometimes it's not clear how zero-width matches should be handled. For example, should ``.*`` match 0 characters directly after matching >0 characters?

  Examples:

  .. sourcecode:: python

    # Python 3.7 and later
    >>> regex.sub('.*', 'x', 'test')
    'xx'
    >>> regex.sub('.*?', '|', 'test')
    '|||||||||'

    # Python 3.6 and earlier
    >>> regex.sub('(?V0).*', 'x', 'test')
    'x'
    >>> regex.sub('(?V1).*', 'x', 'test')
    'xx'
    >>> regex.sub('(?V0).*?', '|', 'test')
    '|t|e|s|t|'
    >>> regex.sub('(?V1).*?', '|', 'test')
    '|||||||||'

* Added ``capturesdict`` (`Hg issue 86 <https://bitbucket.org/mrabarnett/mrab-regex/issues/86>`_)

  ``capturesdict`` is a combination of ``groupdict`` and ``captures``:

  ``groupdict`` returns a dict of the named groups and the last capture of those groups.

  ``captures`` returns a list of all the captures of a group

  ``capturesdict`` returns a dict of the named groups and lists of all the captures of those groups.

  Examples:

  .. sourcecode:: python

    >>> m = regex.match(r"(?:(?P<word>\w+) (?P<digits>\d+)\n)+", "one 1\ntwo 2\nthree 3\n")
    >>> m.groupdict()
    {'word': 'three', 'digits': '3'}
    >>> m.captures("word")
    ['one', 'two', 'three']
    >>> m.captures("digits")
    ['1', '2', '3']
    >>> m.capturesdict()
    {'word': ['one', 'two', 'three'], 'digits': ['1', '2', '3']}

* Allow duplicate names of groups (`Hg issue 87 <https://bitbucket.org/mrabarnett/mrab-regex/issues/87>`_)

  Group names can now be duplicated.

  Examples:

  .. sourcecode:: python

    >>> # With optional groups:
    >>>
    >>> # Both groups capture, the second capture 'overwriting' the first.
    >>> m = regex.match(r"(?P<item>\w+)? or (?P<item>\w+)?", "first or second")
    >>> m.group("item")
    'second'
    >>> m.captures("item")
    ['first', 'second']
    >>> # Only the second group captures.
    >>> m = regex.match(r"(?P<item>\w+)? or (?P<item>\w+)?", " or second")
    >>> m.group("item")
    'second'
    >>> m.captures("item")
    ['second']
    >>> # Only the first group captures.
    >>> m = regex.match(r"(?P<item>\w+)? or (?P<item>\w+)?", "first or ")
    >>> m.group("item")
    'first'
    >>> m.captures("item")
    ['first']
    >>>
    >>> # With mandatory groups:
    >>>
    >>> # Both groups capture, the second capture 'overwriting' the first.
    >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)?", "first or second")
    >>> m.group("item")
    'second'
    >>> m.captures("item")
    ['first', 'second']
    >>> # Again, both groups capture, the second capture 'overwriting' the first.
    >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)", " or second")
    >>> m.group("item")
    'second'
    >>> m.captures("item")
    ['', 'second']
    >>> # And yet again, both groups capture, the second capture 'overwriting' the first.
    >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)", "first or ")
    >>> m.group("item")
    ''
    >>> m.captures("item")
    ['first', '']

* Added ``fullmatch`` (`issue #16203 <https://bugs.python.org/issue16203>`_)

  ``fullmatch`` behaves like ``match``, except that it must match all of the string.

  Examples:

  .. sourcecode:: python

    >>> print(regex.fullmatch(r"abc", "abc").span())
    (0, 3)
    >>> print(regex.fullmatch(r"abc", "abcx"))
    None
    >>> print(regex.fullmatch(r"abc", "abcx", endpos=3).span())
    (0, 3)
    >>> print(regex.fullmatch(r"abc", "xabcy", pos=1, endpos=4).span())
    (1, 4)
    >>>
    >>> regex.match(r"a.*?", "abcd").group(0)
    'a'
    >>> regex.fullmatch(r"a.*?", "abcd").group(0)
    'abcd'

* Added ``subf`` and ``subfn``

  ``subf`` and ``subfn`` are alternatives to ``sub`` and ``subn`` respectively. When passed a replacement string, they treat it as a format string.

  Examples:

  .. sourcecode:: python

    >>> regex.subf(r"(\w+) (\w+)", "{0} => {2} {1}", "foo bar")
    'foo bar => bar foo'
    >>> regex.subf(r"(?P<word1>\w+) (?P<word2>\w+)", "{word2} {word1}", "foo bar")
    'bar foo'

* Added ``expandf`` to match object

  ``expandf`` is an alternative to ``expand``. When passed a replacement string, it treats it as a format string.

  Examples:

  .. sourcecode:: python

    >>> m = regex.match(r"(\w+) (\w+)", "foo bar")
    >>> m.expandf("{0} => {2} {1}")
    'foo bar => bar foo'
    >>>
    >>> m = regex.match(r"(?P<word1>\w+) (?P<word2>\w+)", "foo bar")
    >>> m.expandf("{word2} {word1}")
    'bar foo'

* Detach searched string

  A match object contains a reference to the string that was searched, via its ``string`` attribute. The ``detach_string`` method will 'detach' that string, making it available for garbage collection, which might save valuable memory if that string is very large.

  Example:

  .. sourcecode:: python

    >>> m = regex.search(r"\w+", "Hello world")
    >>> print(m.group())
    Hello
    >>> print(m.string)
    Hello world
    >>> m.detach_string()
    >>> print(m.group())
    Hello
    >>> print(m.string)
    None

* Recursive patterns (`Hg issue 27 <https://bitbucket.org/mrabarnett/mrab-regex/issues/27>`_)

  Recursive and repeated patterns are supported.

  ``(?R)`` or ``(?0)`` tries to match the entire regex recursively. ``(?1)``, ``(?2)``, etc, try to match the relevant capture group.

  ``(?&name)`` tries to match the named capture group.

  Examples:

  .. sourcecode:: python

    >>> regex.match(r"(Tarzan|Jane) loves (?1)", "Tarzan loves Jane").groups()
    ('Tarzan',)
    >>> regex.match(r"(Tarzan|Jane) loves (?1)", "Jane loves Tarzan").groups()
    ('Jane',)

    >>> m = regex.search(r"(\w)(?:(?R)|(\w?))\1", "kayak")
    >>> m.group(0, 1, 2)
    ('kayak', 'k', None)

  The first two examples show how the subpattern within the capture group is reused, but is _not_ itself a capture group. In other words, ``"(Tarzan|Jane) loves (?1)"`` is equivalent to ``"(Tarzan|Jane) loves (?:Tarzan|Jane)"``.

  It's possible to backtrack into a recursed or repeated group.

  You can't call a group if there is more than one group with that group name or group number (``"ambiguous group reference"``).

  The alternative forms ``(?P>name)`` and ``(?P&name)`` are also supported.

* Full Unicode case-folding is supported.

  In version 1 behaviour, the regex module uses full case-folding when performing case-insensitive matches in Unicode.

  Examples (in Python 3):

  .. sourcecode:: python

    >>> regex.match(r"(?iV1)strasse", "stra\N{LATIN SMALL LETTER SHARP S}e").span()
    (0, 6)
    >>> regex.match(r"(?iV1)stra\N{LATIN SMALL LETTER SHARP S}e", "STRASSE").span()
    (0, 7)

  In version 0 behaviour, it uses simple case-folding for backward compatibility with the re module.

* Approximate "fuzzy" matching (`Hg issue 12 <https://bitbucket.org/mrabarnett/mrab-regex/issues/12>`_, `Hg issue 41 <https://bitbucket.org/mrabarnett/mrab-regex/issues/41>`_, `Hg issue 109 <https://bitbucket.org/mrabarnett/mrab-regex/issues/109>`_)

  Regex usually attempts an exact match, but sometimes an approximate, or "fuzzy", match is needed, for those cases where the text being searched may contain errors in the form of inserted, deleted or substituted characters.

  A fuzzy regex specifies which types of errors are permitted, and, optionally, either the minimum and maximum or only the maximum permitted number of each type. (You cannot specify only a minimum.)

  The 3 types of error are:

  * Insertion, indicated by "i"

  * Deletion, indicated by "d"

  * Substitution, indicated by "s"

  In addition, "e" indicates any type of error.

  The fuzziness of a regex item is specified between "{" and "}" after the item.

  Examples:

  * ``foo`` match "foo" exactly

  * ``(?:foo){i}`` match "foo", permitting insertions

  * ``(?:foo){d}`` match "foo", permitting deletions

  * ``(?:foo){s}`` match "foo", permitting substitutions

  * ``(?:foo){i,s}`` match "foo", permitting insertions and substitutions

  * ``(?:foo){e}`` match "foo", permitting errors

  If a certain type of error is specified, then any type not specified will **not** be permitted.

  In the following examples I'll omit the item and write only the fuzziness:

  * ``{d<=3}`` permit at most 3 deletions, but no other types

  * ``{i<=1,s<=2}`` permit at most 1 insertion and at most 2 substitutions, but no deletions

  * ``{1<=e<=3}`` permit at least 1 and at most 3 errors

  * ``{i<=2,d<=2,e<=3}`` permit at most 2 insertions, at most 2 deletions, at most 3 errors in total, but no substitutions

  It's also possible to state the costs of each type of error and the maximum permitted total cost.

  Examples:

  * ``{2i+2d+1s<=4}`` each insertion costs 2, each deletion costs 2, each substitution costs 1, the total cost must not exceed 4

  * ``{i<=1,d<=1,s<=1,2i+2d+1s<=4}`` at most 1 insertion, at most 1 deletion, at most 1 substitution; each insertion costs 2, each deletion costs 2, each substitution costs 1, the total cost must not exceed 4

  You can also use "<" instead of "<=" if you want an exclusive minimum or maximum.

  You can add a test to perform on a character that's substituted or inserted.

  Examples:

  * ``{s<=2:[a-z]}`` at most 2 substitutions, which must be in the character set ``[a-z]``.

  * ``{s<=2,i<=3:\d}`` at most 2 substitutions, at most 3 insertions, which must be digits.

  By default, fuzzy matching searches for the first match that meets the given constraints. The ``ENHANCEMATCH`` flag will cause it to attempt to improve the fit (i.e. reduce the number of errors) of the match that it has found.

  The ``BESTMATCH`` flag will make it search for the best match instead.

  Further examples to note:

  * ``regex.search("(dog){e}", "cat and dog")[1]`` returns ``"cat"`` because that matches ``"dog"`` with 3 errors (an unlimited number of errors is permitted).

  * ``regex.search("(dog){e<=1}", "cat and dog")[1]`` returns ``" dog"`` (with a leading space) because that matches ``"dog"`` with 1 error, which is within the limit.

  * ``regex.search("(?e)(dog){e<=1}", "cat and dog")[1]`` returns ``"dog"`` (without a leading space) because the fuzzy search matches ``" dog"`` with 1 error, which is within the limit, and the ``(?e)`` then it attempts a better fit.

  In the first two examples there are perfect matches later in the string, but in neither case is it the first possible match.

  The match object has an attribute ``fuzzy_counts`` which gives the total number of substitutions, insertions and deletions.

  .. sourcecode:: python

    >>> # A 'raw' fuzzy match:
    >>> regex.fullmatch(r"(?:cats|cat){e<=1}", "cat").fuzzy_counts
    (0, 0, 1)
    >>> # 0 substitutions, 0 insertions, 1 deletion.

    >>> # A better match might be possible if the ENHANCEMATCH flag used:
    >>> regex.fullmatch(r"(?e)(?:cats|cat){e<=1}", "cat").fuzzy_counts
    (0, 0, 0)
    >>> # 0 substitutions, 0 insertions, 0 deletions.

  The match object also has an attribute ``fuzzy_changes`` which gives a tuple of the positions of the substitutions, insertions and deletions.

  .. sourcecode:: python

    >>> m = regex.search('(fuu){i<=2,d<=2,e<=5}', 'anaconda foo bar')
    >>> m
    <regex.Match object; span=(7, 10), match='a f', fuzzy_counts=(0, 2, 2)>
    >>> m.fuzzy_changes
    ([], [7, 8], [10, 11])

  What this means is that if the matched part of the string had been:

  .. sourcecode:: python

    'anacondfuuoo bar'

  it would've been an exact match.

  However, there were insertions at positions 7 and 8:

  .. sourcecode:: python

    'anaconda fuuoo bar'
            ^^

  and deletions at positions 10 and 11:

  .. sourcecode:: python

    'anaconda f~~oo bar'
               ^^

  So the actual string was:

  .. sourcecode:: python

    'anaconda foo bar'

* Named lists (`Hg issue 11 <https://bitbucket.org/mrabarnett/mrab-regex/issues/11>`_)

  ``\L<name>``

  There are occasions where you may want to include a list (actually, a set) of options in a regex.

  One way is to build the pattern like this:

  .. sourcecode:: python

    >>> p = regex.compile(r"first|second|third|fourth|fifth")

  but if the list is large, parsing the resulting regex can take considerable time, and care must also be taken that the strings are properly escaped and properly ordered, for example, "cats" before "cat".

  The new alternative is to use a named list:

  .. sourcecode:: python

    >>> option_set = ["first", "second", "third", "fourth", "fifth"]
    >>> p = regex.compile(r"\L<options>", options=option_set)

  The order of the items is irrelevant, they are treated as a set. The named lists are available as the ``.named_lists`` attribute of the pattern object :

  .. sourcecode:: python

    >>> print(p.named_lists)
    # Python 3
    {'options': frozenset({'fifth', 'first', 'fourth', 'second', 'third'})}
    # Python 2
    {'options': frozenset(['fifth', 'fourth', 'second', 'third', 'first'])}

* Start and end of word

  ``\m`` matches at the start of a word.

  ``\M`` matches at the end of a word.

  Compare with ``\b``, which matches at the start or end of a word.

* Unicode line separators

  Normally the only line separator is ``\n`` (``\x0A``), but if the ``WORD`` flag is turned on then the line separators are ``\x0D\x0A``, ``\x0A``, ``\x0B``, ``\x0C`` and ``\x0D``, plus ``\x85``, ``\u2028`` and ``\u2029`` when working with Unicode.

  This affects the regex dot ``"."``, which, with the ``DOTALL`` flag turned off, matches any character except a line separator. It also affects the line anchors ``^`` and ``$`` (in multiline mode).

* Set operators

  **Version 1 behaviour only**

  Set operators have been added, and a set ``[...]`` can include nested sets.

  The operators, in order of increasing precedence, are:

  * ``||`` for union ("x||y" means "x or y")

  * ``~~`` (double tilde) for symmetric difference ("x~~y" means "x or y, but not both")

  * ``&&`` for intersection ("x&&y" means "x and y")

  * ``--`` (double dash) for difference ("x--y" means "x but not y")

  Implicit union, ie, simple juxtaposition like in ``[ab]``, has the highest precedence. Thus, ``[ab&&cd]`` is the same as ``[[a||b]&&[c||d]]``.

  Examples:

  * ``[ab]`` # Set containing 'a' and 'b'

  * ``[a-z]`` # Set containing 'a' .. 'z'

  * ``[[a-z]--[qw]]`` # Set containing 'a' .. 'z', but not 'q' or 'w'

  * ``[a-z--qw]`` # Same as above

  * ``[\p{L}--QW]`` # Set containing all letters except 'Q' and 'W'

  * ``[\p{N}--[0-9]]`` # Set containing all numbers except '0' .. '9'

  * ``[\p{ASCII}&&\p{Letter}]`` # Set containing all characters which are ASCII and letter

* regex.escape (`issue #2650 <https://bugs.python.org/issue2650>`_)

  regex.escape has an additional keyword parameter ``special_only``. When True, only 'special' regex characters, such as '?', are escaped.

  Examples:

  .. sourcecode:: python

    >>> regex.escape("foo!?", special_only=False)
    'foo\\!\\?'
    >>> regex.escape("foo!?", special_only=True)
    'foo!\\?'

* regex.escape (`Hg issue 249 <https://bitbucket.org/mrabarnett/mrab-regex/issues/249>`_)

  regex.escape has an additional keyword parameter ``literal_spaces``. When True, spaces are not escaped.

  Examples:

  .. sourcecode:: python

    >>> regex.escape("foo bar!?", literal_spaces=False)
    'foo\\ bar!\\?'
    >>> regex.escape("foo bar!?", literal_spaces=True)
    'foo bar!\\?'

* Repeated captures (`issue #7132 <https://bugs.python.org/issue7132>`_)

  A match object has additional methods which return information on all the successful matches of a repeated capture group. These methods are:

  * ``matchobject.captures([group1, ...])``

    * Returns a list of the strings matched in a group or groups. Compare with ``matchobject.group([group1, ...])``.

  * ``matchobject.starts([group])``

    * Returns a list of the start positions. Compare with ``matchobject.start([group])``.

  * ``matchobject.ends([group])``

    * Returns a list of the end positions. Compare with ``matchobject.end([group])``.

  * ``matchobject.spans([group])``

    * Returns a list of the spans. Compare with ``matchobject.span([group])``.

  Examples:

  .. sourcecode:: python

    >>> m = regex.search(r"(\w{3})+", "123456789")
    >>> m.group(1)
    '789'
    >>> m.captures(1)
    ['123', '456', '789']
    >>> m.start(1)
    6
    >>> m.starts(1)
    [0, 3, 6]
    >>> m.end(1)
    9
    >>> m.ends(1)
    [3, 6, 9]
    >>> m.span(1)
    (6, 9)
    >>> m.spans(1)
    [(0, 3), (3, 6), (6, 9)]

* Atomic grouping (`issue #433030 <https://bugs.python.org/issue433030>`_)

  ``(?>...)``

  If the following pattern subsequently fails, then the subpattern as a whole will fail.

* Possessive quantifiers.

  ``(?:...)?+`` ; ``(?:...)*+`` ; ``(?:...)++`` ; ``(?:...){min,max}+``

  The subpattern is matched up to 'max' times. If the following pattern subsequently fails, then all of the repeated subpatterns will fail as a whole. For example, ``(?:...)++`` is equivalent to ``(?>(?:...)+)``.

* Scoped flags (`issue #433028 <https://bugs.python.org/issue433028>`_)

  ``(?flags-flags:...)``

  The flags will apply only to the subpattern. Flags can be turned on or off.

* Definition of 'word' character (`issue #1693050 <https://bugs.python.org/issue1693050>`_)

  The definition of a 'word' character has been expanded for Unicode. It now conforms to the Unicode specification at ``http://www.unicode.org/reports/tr29/``.

* Variable-length lookbehind

  A lookbehind can match a variable-length string.

* Flags argument for regex.split, regex.sub and regex.subn (`issue #3482 <https://bugs.python.org/issue3482>`_)

  ``regex.split``, ``regex.sub`` and ``regex.subn`` support a 'flags' argument.

* Pos and endpos arguments for regex.sub and regex.subn

  ``regex.sub`` and ``regex.subn`` support 'pos' and 'endpos' arguments.

* 'Overlapped' argument for regex.findall and regex.finditer

  ``regex.findall`` and ``regex.finditer`` support an 'overlapped' flag which permits overlapped matches.

* Splititer

  ``regex.splititer`` has been added. It's a generator equivalent of ``regex.split``.

* Subscripting for groups

  A match object accepts access to the captured groups via subscripting and slicing:

  .. sourcecode:: python

    >>> m = regex.search(r"(?P<before>.*?)(?P<num>\d+)(?P<after>.*)", "pqr123stu")
    >>> print(m["before"])
    pqr
    >>> print(len(m))
    4
    >>> print(m[:])
    ('pqr123stu', 'pqr', '123', 'stu')

* Named groups

  Groups can be named with ``(?<name>...)`` as well as the current ``(?P<name>...)``.

* Group references

  Groups can be referenced within a pattern with ``\g<name>``. This also allows there to be more than 99 groups.

* Named characters

  ``\N{name}``

  Named characters are supported. (Note: only those known by Python's Unicode database are supported.)

* Unicode codepoint properties, including scripts and blocks

  ``\p{property=value}``; ``\P{property=value}``; ``\p{value}`` ; ``\P{value}``

  Many Unicode properties are supported, including blocks and scripts. ``\p{property=value}`` or ``\p{property:value}`` matches a character whose property ``property`` has value ``value``. The inverse of ``\p{property=value}`` is ``\P{property=value}`` or ``\p{^property=value}``.

  If the short form ``\p{value}`` is used, the properties are checked in the order: ``General_Category``, ``Script``, ``Block``, binary property:

  * ``Latin``, the 'Latin' script (``Script=Latin``).

  * ``BasicLatin``, the 'BasicLatin' block (``Block=BasicLatin``).

  * ``Alphabetic``, the 'Alphabetic' binary property (``Alphabetic=Yes``).

  A short form starting with ``Is`` indicates a script or binary property:

  * ``IsLatin``, the 'Latin' script (``Script=Latin``).

  * ``IsAlphabetic``, the 'Alphabetic' binary property (``Alphabetic=Yes``).

  A short form starting with ``In`` indicates a block property:

  * ``InBasicLatin``, the 'BasicLatin' block (``Block=BasicLatin``).

* POSIX character classes

  ``[[:alpha:]]``; ``[[:^alpha:]]``

  POSIX character classes are supported. These are normally treated as an alternative form of ``\p{...}``.

  The exceptions are ``alnum``, ``digit``, ``punct`` and ``xdigit``, whose definitions are different from those of Unicode.

  ``[[:alnum:]]`` is equivalent to ``\p{posix_alnum}``.

  ``[[:digit:]]`` is equivalent to ``\p{posix_digit}``.

  ``[[:punct:]]`` is equivalent to ``\p{posix_punct}``.

  ``[[:xdigit:]]`` is equivalent to ``\p{posix_xdigit}``.

* Search anchor

  ``\G``

  A search anchor has been added. It matches at the position where each search started/continued and can be used for contiguous matches or in negative variable-length lookbehinds to limit how far back the lookbehind goes:

  .. sourcecode:: python

    >>> regex.findall(r"\w{2}", "abcd ef")
    ['ab', 'cd', 'ef']
    >>> regex.findall(r"\G\w{2}", "abcd ef")
    ['ab', 'cd']

  * The search starts at position 0 and matches 2 letters 'ab'.

  * The search continues at position 2 and matches 2 letters 'cd'.

  * The search continues at position 4 and fails to match any letters.

  * The anchor stops the search start position from being advanced, so there are no more results.

* Reverse searching

  Searches can now work backwards:

  .. sourcecode:: python

    >>> regex.findall(r".", "abc")
    ['a', 'b', 'c']
    >>> regex.findall(r"(?r).", "abc")
    ['c', 'b', 'a']

  Note: the result of a reverse search is not necessarily the reverse of a forward search:

  .. sourcecode:: python

    >>> regex.findall(r"..", "abcde")
    ['ab', 'cd']
    >>> regex.findall(r"(?r)..", "abcde")
    ['de', 'bc']

* Matching a single grapheme

  ``\X``

  The grapheme matcher is supported. It now conforms to the Unicode specification at ``http://www.unicode.org/reports/tr29/``.

* Branch reset

  ``(?|...|...)``

  Capture group numbers will be reused across the alternatives, but groups with different names will have different group numbers.

  Examples:

  .. sourcecode:: python

    >>> regex.match(r"(?|(first)|(second))", "first").groups()
    ('first',)
    >>> regex.match(r"(?|(first)|(second))", "second").groups()
    ('second',)

  Note that there is only one group.

* Default Unicode word boundary

  The ``WORD`` flag changes the definition of a 'word boundary' to that of a default Unicode word boundary. This applies to ``\b`` and ``\B``.

* Timeout (Python 3)

  The matching methods and functions support timeouts. The timeout (in seconds) applies to the entire operation:

  .. sourcecode:: python

    >>> from time import sleep
    >>>
    >>> def fast_replace(m):
    ...     return 'X'
    ...
    >>> def slow_replace(m):
    ...     sleep(0.5)
    ...     return 'X'
    ...
    >>> regex.sub(r'[a-z]', fast_replace, 'abcde', timeout=2)
    'XXXXX'
    >>> regex.sub(r'[a-z]', slow_replace, 'abcde', timeout=2)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "C:\Python37\lib\site-packages\regex\regex.py", line 276, in sub
        endpos, concurrent, timeout)
    TimeoutError: regex timed out

  * [reno-2.11.3](https://docs.openstack.org/reno/latest/) =========================================
 reno: A New Way to Manage Release Notes
=========================================

Reno is a release notes manager designed with high throughput in mind,
supporting fast distributed development teams without introducing
additional development processes.  Our goal is to encourage detailed
and accurate release notes for every release.

Reno uses git to store its data, along side the code being
described. This means release notes can be written when the code
changes are fresh, so no details are forgotten. It also means that
release notes can go through the same review process used for managing
code and other documentation changes.

Reno stores each release note in a separate file to enable a large
number of developers to work on multiple patches simultaneously, all
targeting the same branch, without worrying about merge
conflicts. This cuts down on the need to rebase or otherwise manually
resolve conflicts, and keeps a development team moving quickly.

Reno also supports multiple branches, allowing release notes to be
back-ported from master to maintenance branches together with the
code for bug fixes.

Reno organizes notes into logical groups based on whether they
describe new features, bug fixes, known issues, or other topics of
interest to the user. Contributors categorize individual notes as they
are added, and reno combines them before publishing.

Notes can be styled using reStructuredText directives, and reno's
Sphinx integration makes it easy to incorporate release notes into
automated documentation builds.

Notes are automatically associated with the release version based on
the git tags applied to the repository, so it is not necessary to
track changes manually using a bug tracker or other tool, or to worry
that an important change will be missed when the release notes are
written by hand all at one time, just before a release.

Modifications to notes are incorporated when the notes are shown in
their original location in the history. This feature makes it possible
to correct typos or otherwise fix a published release note after a
release is made, but have the new note content associated with the
original version number. Notes also can be deleted, eliminating them
from future documentation builds.

Project Meta-data
=================

.. .. image:: https://governance.openstack.org/tc/badges/reno.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

* Free software: Apache license
* Documentation: https://docs.openstack.org/reno/latest/
* Source: https://git.openstack.org/cgit/openstack/reno
* Bugs: https://storyboard.openstack.org/#!/project/933
* IRC: #openstack-release on freenode




  * [reportlab-3.5.23](http://www.reportlab.com/) The ReportLab Toolkit. An Open Source Python library for generating PDFs and graphics.



  * [repoze.lru-0.7](http://www.repoze.org) 




  * [repoze.sphinx.autointerface-0.8](http://www.repoze.org) repoze.sphinx.autointerface README
==================================

Overview
--------

Thie package defines an extension for the
`Sphinx <http://sphinx.pocool.org>`_ documentation system.  The extension
allows generation of API documentation by introspection of
`zope.interface <http://pypi.python.org/pypi/zope.interface>`_ instances in 
code.


Installation
------------

Install via `easy_install
<http://peak.telecommunity.com/DevCenter/EasyInstall>`_::

 $ bin/easy_install repoze.sphinx.autointerface

or any other means which gets the package on your ``PYTHONPATH``.


Registering the Extension
-------------------------

Add ``repoze.sphinx.autointerface`` to the ``extensions`` list in the
``conf.py`` of the Sphinx documentation for your product.  E.g.::

 extensions = ['sphinx.ext.autodoc',
               'sphinx.ext.doctest',
               'repoze.sphinx.autointerface',
              ]


Using the Extension
-------------------

At appropriate points in your document, call out the interface
autodocs via::

  .. autointerface:: yourpackage.interfaces.IFoo

Output from the directive includes

- the fully-qualified interface name
- any base interfaces
- the doctstring from the interface, rendered as reSTX.
- the members of the interface (methods and attributes).

  * For each attribute, the output includes the attribute name
    and its description.
  * For each method, the output includes the method name, its signature,
    and its docstring (also rendered as reSTX).


repoze.sphinx.autointerface Changelog
=====================================

0.8 (2016-03-28)
----------------

- Add support for Python 3.3, 3.4, and 3.5.

- Drop support for Python 2.6 and 3.2.

- Allow cross references using the ``:class:`` directive to use the
  ``.`` for "fuzzy" searching.  Thanks to Jason Madden for the patch.

0.7.1 (2012-09-15)
------------------

- Remove ``setup.py`` dependency on ``ez_setup.py``.

0.7.0 (2012-06-20)
------------------

- PyPy compatibility.

- Python 3.2+ compatibility.  Thanks to Arfrever for the patch.

- Include interface docs under the ``automodule`` directive.  Thanks to
  Krys Lawrence for the patch.


0.6.2 (2011-02-13)
------------------

- Fix ``TypeError: 'NoneType' object is not iterable`` error when generating
  a rendering of an interface under Python 2.7.


0.6.1 (2011-01-28)
------------------

- Fix ':member-order: bysource' handling.


0.6 (2011-01-28)
----------------

- Correctly handle ':members:' values explicitly set in the directive.


0.5 (2011-01-18)
----------------

- Added support for the ':member-order:' flag, which can take one of the
  three stock values, "alphabetical", "groupwise", or "bysource".  By
  default, members are documented in "hash" order.


0.4 (2010-07-26)
----------------

- Fixed compatibility with Sphinx 1.0

- Un-break PyPI ReST/HTML-rendering again.


0.3 (2009-10-25)
----------------

- Refactor sphinx integration. There are now separate ``autointerface``
  and ``interface`` directives.


0.2.1 (2009-08-20)
------------------

- Fix add_directive arguments to work with Sphinx 0.6.1, now required.


0.1.3 (2009-01-14)
------------------

- Coerce unicode path elements to str in ``_resolve_dotted_name``.
  Note that non-ASCII path elements won't work:  this fix just deals
  with the case where the path was of type unicode.

- Fixed spelling of directive in README.txt.

- Added dependency on ``zope.interface``.


0.1.2 (2008-10-03)
------------------

- Packaging change:  improved description in README.txt.


0.1.1 (2008-10-03)
------------------

- Packaging bug:  the ``long_description`` was not rendering properly to
  HTML on PyPI.


0.1 (2008-10-02)
----------------

- Initial release.
  * [requests-2.19.1](http://python-requests.org) Requests: HTTP for Humans™
==========================

[![image](https://img.shields.io/pypi/v/requests.svg)](https://pypi.org/project/requests/)
[![image](https://img.shields.io/pypi/l/requests.svg)](https://pypi.org/project/requests/)
[![image](https://img.shields.io/pypi/pyversions/requests.svg)](https://pypi.org/project/requests/)
[![codecov.io](https://codecov.io/github/requests/requests/coverage.svg?branch=master)](https://codecov.io/github/requests/requests)
[![image](https://img.shields.io/github/contributors/requests/requests.svg)](https://github.com/requests/requests/graphs/contributors)
[![image](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/kennethreitz)

Requests is the only *Non-GMO* HTTP library for Python, safe for human
consumption.

![image](https://farm5.staticflickr.com/4317/35198386374_1939af3de6_k_d.jpg)

Behold, the power of Requests:

``` {.sourceCode .python}
>>> import requests
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{"type":"User"...'
>>> r.json()
{u'disk_usage': 368627, u'private_gists': 484, ...}
```

See [the similar code, sans Requests](https://gist.github.com/973705).

[![image](https://raw.githubusercontent.com/requests/requests/master/docs/_static/requests-logo-small.png)](http://docs.python-requests.org/)

Requests allows you to send *organic, grass-fed* HTTP/1.1 requests,
without the need for manual labor. There's no need to manually add query
strings to your URLs, or to form-encode your POST data. Keep-alive and
HTTP connection pooling are 100% automatic, thanks to
[urllib3](https://github.com/shazow/urllib3).

Besides, all the cool kids are doing it. Requests is one of the most
downloaded Python packages of all time, pulling in over 11,000,000
downloads every month. You don't want to be left out!

Feature Support
---------------

Requests is ready for today's web.

-   International Domains and URLs
-   Keep-Alive & Connection Pooling
-   Sessions with Cookie Persistence
-   Browser-style SSL Verification
-   Basic/Digest Authentication
-   Elegant Key/Value Cookies
-   Automatic Decompression
-   Automatic Content Decoding
-   Unicode Response Bodies
-   Multipart File Uploads
-   HTTP(S) Proxy Support
-   Connection Timeouts
-   Streaming Downloads
-   `.netrc` Support
-   Chunked Requests

Requests officially supports Python 2.7 & 3.4–3.7, and runs great on
PyPy.

Installation
------------

To install Requests, simply use [pipenv](http://pipenv.org/) (or pip, of
course):

``` {.sourceCode .bash}
$ pipenv install requests
✨🍰✨
```

Satisfaction guaranteed.

Documentation
-------------

Fantastic documentation is available at
<http://docs.python-requests.org/>, for a limited time only.

How to Contribute
-----------------

1.  Become more familiar with the project by reading our [Contributor's Guide](http://docs.python-requests.org/en/latest/dev/contributing/) and our [development philosophy](http://docs.python-requests.org/en/latest/dev/philosophy/).
2.  Check for open issues or open a fresh issue to start a discussion
    around a feature idea or a bug. There is a [Contributor
    Friendly](https://github.com/requests/requests/issues?direction=desc&labels=Contributor+Friendly&page=1&sort=updated&state=open)
    tag for issues that should be ideal for people who are not very
    familiar with the codebase yet.
3.  Fork [the repository](https://github.com/requests/requests) on
    GitHub to start making your changes to the **master** branch (or
    branch off of it).
4.  Write a test which shows that the bug was fixed or that the feature
    works as expected.
5.  Send a pull request and bug the maintainer until it gets merged and
    published. :) Make sure to add yourself to
    [AUTHORS](https://github.com/requests/requests/blob/master/AUTHORS.rst).




  * [requests-kerberos-0.12.0](https://github.com/requests/requests-kerberos) requests Kerberos/GSSAPI authentication library
===============================================

.. image:: https://travis-ci.org/requests/requests-kerberos.svg?branch=master
    :target: https://travis-ci.org/requests/requests-kerberos

.. image:: https://coveralls.io/repos/github/requests/requests-kerberos/badge.svg?branch=master
    :target: https://coveralls.io/github/requests/requests-kerberos?branch=master

Requests is an HTTP library, written in Python, for human beings. This library
adds optional Kerberos/GSSAPI authentication support and supports mutual
authentication. Basic GET usage:


.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth
    >>> r = requests.get("http://example.org", auth=HTTPKerberosAuth())
    ...

The entire ``requests.api`` should be supported.

Authentication Failures
-----------------------

Client authentication failures will be communicated to the caller by returning
the 401 response.

Mutual Authentication
---------------------

REQUIRED
^^^^^^^^

By default, ``HTTPKerberosAuth`` will require mutual authentication from the
server, and if a server emits a non-error response which cannot be
authenticated, a ``requests_kerberos.errors.MutualAuthenticationError`` will
be raised. If a server emits an error which cannot be authenticated, it will
be returned to the user but with its contents and headers stripped. If the
response content is more important than the need for mutual auth on errors,
(eg, for certain WinRM calls) the stripping behavior can be suppressed by
setting ``sanitize_mutual_error_response=False``:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, REQUIRED
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=REQUIRED, sanitize_mutual_error_response=False)
    >>> r = requests.get("https://windows.example.org/wsman", auth=kerberos_auth)
    ...


OPTIONAL
^^^^^^^^

If you'd prefer to not require mutual authentication, you can set your
preference when constructing your ``HTTPKerberosAuth`` object:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, OPTIONAL
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=OPTIONAL)
    >>> r = requests.get("http://example.org", auth=kerberos_auth)
    ...

This will cause ``requests_kerberos`` to attempt mutual authentication if the
server advertises that it supports it, and cause a failure if authentication
fails, but not if the server does not support it at all.

DISABLED
^^^^^^^^

While we don't recommend it, if you'd prefer to never attempt mutual
authentication, you can do that as well:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, DISABLED
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=DISABLED)
    >>> r = requests.get("http://example.org", auth=kerberos_auth)
    ...

Preemptive Authentication
-------------------------

``HTTPKerberosAuth`` can be forced to preemptively initiate the Kerberos
GSS exchange and present a Kerberos ticket on the initial request (and all
subsequent). By default, authentication only occurs after a
``401 Unauthorized`` response containing a Kerberos or Negotiate challenge
is received from the origin server. This can cause mutual authentication
failures for hosts that use a persistent connection (eg, Windows/WinRM), as
no Kerberos challenges are sent after the initial auth handshake. This
behavior can be altered by setting  ``force_preemptive=True``:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, REQUIRED
    >>> kerberos_auth = HTTPKerberosAuth(mutual_authentication=REQUIRED, force_preemptive=True)
    >>> r = requests.get("https://windows.example.org/wsman", auth=kerberos_auth)
    ...

Hostname Override
-----------------

If communicating with a host whose DNS name doesn't match its
kerberos hostname (eg, behind a content switch or load balancer),
the hostname used for the Kerberos GSS exchange can be overridden by
setting the ``hostname_override`` arg:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, REQUIRED
    >>> kerberos_auth = HTTPKerberosAuth(hostname_override="internalhost.local")
    >>> r = requests.get("https://externalhost.example.org/", auth=kerberos_auth)
    ...

Explicit Principal
------------------

``HTTPKerberosAuth`` normally uses the default principal (ie, the user for
whom you last ran ``kinit`` or ``kswitch``, or an SSO credential if
applicable). However, an explicit principal can be specified, which will
cause Kerberos to look for a matching credential cache for the named user.
This feature depends on OS support for collection-type credential caches,
as well as working principal support in PyKerberos (it is broken in many
builds). An explicit principal can be specified with the ``principal`` arg:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth, REQUIRED
    >>> kerberos_auth = HTTPKerberosAuth(principal="user@REALM")
    >>> r = requests.get("http://example.org", auth=kerberos_auth)
    ...

On Windows, WinKerberos is used instead of PyKerberos. WinKerberos allows the
use of arbitrary principals instead of a credential cache. Passwords can be
specified by following the form ``user@realm:password`` for ``principal``.

Delegation
----------

``requests_kerberos`` supports credential delegation (``GSS_C_DELEG_FLAG``).
To enable delegation of credentials to a server that requests delegation, pass
``delegate=True`` to ``HTTPKerberosAuth``:

.. code-block:: python

    >>> import requests
    >>> from requests_kerberos import HTTPKerberosAuth
    >>> r = requests.get("http://example.org", auth=HTTPKerberosAuth(delegate=True))
    ...

Be careful to only allow delegation to servers you trust as they will be able
to impersonate you using the delegated credentials.

Logging
-------

This library makes extensive use of Python's logging facilities.

Log messages are logged to the ``requests_kerberos`` and
``requests_kerberos.kerberos_`` named loggers.

If you are having difficulty we suggest you configure logging. Issues with the
underlying kerberos libraries will be made apparent. Additionally, copious debug
information is made available which may assist in troubleshooting if you
increase your log level all the way up to debug.


History
=======

0.12.0: 2017-12-20
------------------------

- Add support for channel binding tokens (assumes pykerberos support >= 1.2.1)
- Add support for kerberos message encryption (assumes pykerberos support >= 1.2.1)
- Misc CI/test fixes

0.11.0: 2016-11-02
------------------

- Switch dependency on Windows from kerberos-sspi/pywin32 to WinKerberos.
  This brings Custom Principal support to Windows users.

0.10.0: 2016-05-18
------------------

- Make it possible to receive errors without having their contents and headers
  stripped.
- Resolve a bug caused by passing the ``principal`` keyword argument to
  kerberos-sspi on Windows.

0.9.0: 2016-05-06
-----------------

- Support for principal, hostname, and realm override.

- Added support for mutual auth.

0.8.0: 2016-01-07
-----------------

- Support for Kerberos delegation.

- Fixed problems declaring kerberos-sspi on Windows installs.

0.7.0: 2015-05-04
-----------------

- Added Windows native authentication support by adding kerberos-sspi as an
  alternative backend.

- Prevent infinite recursion when a server returns 401 to an authorization
  attempt.

- Reduce the logging during successful responses.

0.6.1: 2014-11-14
-----------------

- Fix HTTPKerberosAuth not to treat non-file as a file

- Prevent infinite recursion when GSSErrors occurs

0.6: 2014-11-04
---------------

- Handle mutual authentication (see pull request 36_)

  All users should upgrade immediately. This has been reported to
  oss-security_ and we are awaiting a proper CVE identifier.

  **Update**: We were issued CVE-2014-8650

- Distribute as a wheel.

.. _36: https://github.com/requests/requests-kerberos/pull/36
.. _oss-security: http://www.openwall.com/lists/oss-security/

0.5: 2014-05-14
---------------

- Allow non-HTTP service principals with HTTPKerberosAuth using a new optional
  argument ``service``.

- Fix bug in ``setup.py`` on distributions where the ``compiler`` module is
  not available.

- Add test dependencies to ``setup.py`` so ``python setup.py test`` will work.

0.4: 2013-10-26
---------------

- Minor updates in the README
- Change requirements to depend on requests above 1.1.0

0.3: 2013-06-02
---------------

- Work with servers operating on non-standard ports

0.2: 2013-03-26
---------------

- Not documented

0.1: Never released
-------------------

- Initial Release



  * [requests-mock-1.6.0](https://requests-mock.readthedocs.io/) ===============================
requests-mock
===============================

.. image:: https://badge.fury.io/py/requests-mock.png
    :target: https://pypi.org/project/requests-mock/

.. image:: https://circleci.com/gh/jamielennox/requests-mock.svg?style=svg
    :target: https://circleci.com/gh/jamielennox/requests-mock

Intro
=====

`requests-mock` provides a building block to stub out the HTTP `requests`_ portions of your testing code.
You should checkout the `docs`_ for more information.

The Basics
==========

Everything in `requests`_ eventually goes through an adapter to do the transport work.
`requests-mock` creates a custom `adapter` that allows you to predefine responses when certain URIs are called.

There are then a number of methods provided to get the adapter used.

A simple example:

.. code:: python

    >>> import requests
    >>> import requests_mock

    >>> session = requests.Session()
    >>> adapter = requests_mock.Adapter()
    >>> session.mount('mock', adapter)

    >>> adapter.register_uri('GET', 'mock://test.com', text='data')
    >>> resp = session.get('mock://test.com')
    >>> resp.status_code, resp.text
    (200, 'data')

Obviously having all URLs be `mock://` prefixed isn't going to useful, so you can use `requests_mock.mock` to get the adapter into place.

As a context manager:

.. code:: python

    >>> with requests_mock.mock() as m:
    ...     m.get('http://test.com', text='data')
    ...     requests.get('http://test.com').text
    ...
    'data'

Or as a decorator:

.. code:: python

    >>> @requests_mock.mock()
    ... def test_func(m):
    ...     m.get('http://test.com', text='data')
    ...     return requests.get('http://test.com').text
    ...
    >>> test_func()
    'data'

For more information checkout the `docs`_.

Reporting Bugs
==============

Development and bug tracking is performed on `GitHub`_.

License
=======

Licensed under the Apache License, Version 2.0 (the "License"); you may
not use this file except in compliance with the License. You may obtain
a copy of the License at

     https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.

.. _requests: http://python-requests.org
.. _docs: https://requests-mock.readthedocs.io/
.. _GitHub: https://github.com/jamielennox/requests-mock




  * [requests-oauthlib-1.2.0](https://github.com/requests/requests-oauthlib) Requests-OAuthlib |build-status| |coverage-status| |docs|
=========================================================

This project provides first-class OAuth library support for `Requests <http://python-requests.org>`_.

The OAuth 1 workflow
--------------------

OAuth 1 can seem overly complicated and it sure has its quirks. Luckily,
requests_oauthlib hides most of these and let you focus at the task at hand.

Accessing protected resources using requests_oauthlib is as simple as:

.. code-block:: pycon

    >>> from requests_oauthlib import OAuth1Session
    >>> twitter = OAuth1Session('client_key',
                                client_secret='client_secret',
                                resource_owner_key='resource_owner_key',
                                resource_owner_secret='resource_owner_secret')
    >>> url = 'https://api.twitter.com/1/account/settings.json'
    >>> r = twitter.get(url)

Before accessing resources you will need to obtain a few credentials from your
provider (e.g. Twitter) and authorization from the user for whom you wish to
retrieve resources for. You can read all about this in the full
`OAuth 1 workflow guide on RTD <https://requests-oauthlib.readthedocs.io/en/latest/oauth1_workflow.html>`_.

The OAuth 2 workflow
--------------------

OAuth 2 is generally simpler than OAuth 1 but comes in more flavours. The most
common being the Authorization Code Grant, also known as the WebApplication
flow.

Fetching a protected resource after obtaining an access token can be extremely
simple. However, before accessing resources you will need to obtain a few
credentials from your provider (e.g. Google) and authorization from the user
for whom you wish to retrieve resources for. You can read all about this in the
full `OAuth 2 workflow guide on RTD <https://requests-oauthlib.readthedocs.io/en/latest/oauth2_workflow.html>`_.

Installation
-------------

To install requests and requests_oauthlib you can use pip:

.. code-block:: bash

    $ pip install requests requests_oauthlib

.. |build-status| image:: https://travis-ci.org/requests/requests-oauthlib.svg?branch=master
   :target: https://travis-ci.org/requests/requests-oauthlib
.. |coverage-status| image:: https://img.shields.io/coveralls/requests/requests-oauthlib.svg
   :target: https://coveralls.io/r/requests/requests-oauthlib
.. |docs| image:: https://readthedocs.org/projects/requests-oauthlib/badge/
   :alt: Documentation Status
   :scale: 100%
   :target: https://requests-oauthlib.readthedocs.io/


History
-------

UNRELEASED
++++++++++

nothing yet

v1.2.0 (14 January 2019)
++++++++++++++++++++++++

- This project now depends on OAuthlib 3.0.0 and above. It does **not** support
  versions of OAuthlib before 3.0.0.
- Updated oauth2 tests to use 'sess' for an OAuth2Session instance instead of `auth`
  because OAuth2Session objects and methods acceept an `auth` paramether which is
  typically an instance of `requests.auth.HTTPBasicAuth`
- `OAuth2Session.fetch_token` previously tried to guess how and where to provide
  "client" and "user" credentials incorrectly. This was incompatible with some
  OAuth servers and incompatible with breaking changes in oauthlib that seek to
  correctly provide the `client_id`. The older implementation also did not raise
  the correct exceptions when username and password are not present on Legacy
  clients.
- Avoid automatic netrc authentication for OAuth2Session.

v1.1.0 (9 January 2019)
+++++++++++++++++++++++

- Adjusted version specifier for ``oauthlib`` dependency: this project is
  not yet compatible with ``oauthlib`` 3.0.0.
- Dropped dependency on ``nose``.
- Minor changes to clean up the code and make it more readable/maintainable.

v1.0.0 (4 June 2018)
++++++++++++++++++++

- **Removed support for Python 2.6 and Python 3.3.**
  This project now supports Python 2.7, and Python 3.4 and above.
- Added several examples to the documentation.
- Added plentymarkets compliance fix.
- Added a ``token`` property to OAuth1Session, to match the corresponding
  ``token`` property on OAuth2Session.

v0.8.0 (14 February 2017)
+++++++++++++++++++++++++

- Added Fitbit compliance fix.
- Fixed an issue where newlines in the response body for the access token
  request would cause errors when trying to extract the token.
- Fixed an issue introduced in v0.7.0 where users passing ``auth`` to several
  methods would encounter conflicts with the ``client_id`` and
  ``client_secret``-derived auth. The user-supplied ``auth`` argument is now
  used in preference to those options.

v0.7.0 (22 September 2016)
++++++++++++++++++++++++++

- Allowed ``OAuth2Session.request`` to take the ``client_id`` and
  ``client_secret`` parameters for the purposes of automatic token refresh,
  which may need them.

v0.6.2 (12 July 2016)
+++++++++++++++++++++

- Use ``client_id`` and ``client_secret`` for the Authorization header if
  provided.
- Allow explicit bypass of the Authorization header by setting ``auth=False``.
- Pass through the ``proxies`` kwarg when refreshing tokens.
- Miscellaneous cleanups.

v0.6.1 (19 February 2016)
+++++++++++++++++++++++++

- Fixed a bug when sending authorization in headers with no username and
  password present.
- Make sure we clear the session token before obtaining a new one.
- Some improvements to the Slack compliance fix.
- Avoid timing problems around token refresh.
- Allow passing arbitrary arguments to requests when calling
  ``fetch_request_token`` and ``fetch_access_token``.

v0.6.0 (14 December 2015)
+++++++++++++++++++++++++

- Add compliance fix for Slack.
- Add compliance fix for Mailchimp.
- ``TokenRequestDenied`` exceptions now carry the entire response, not just the
  status code.
- Pass through keyword arguments when refreshing tokens automatically.
- Send authorization in headers, not just body, to maximize compatibility.
- More getters/setters available for OAuth2 session client values.
- Allow sending custom headers when refreshing tokens, and set some defaults.


v0.5.0 (4 May 2015)
+++++++++++++++++++
- Fix ``TypeError`` being raised instead of ``TokenMissing`` error.
- Raise requests exceptions on 4XX and 5XX responses in the OAuth2 flow.
- Avoid ``AttributeError`` when initializing the ``OAuth2Session`` class
  without complete client information.

v0.4.2 (16 October 2014)
++++++++++++++++++++++++
- New ``authorized`` property on OAuth1Session and OAuth2Session, which allows
  you to easily determine if the session is already authorized with OAuth tokens
  or not.
- New ``TokenMissing`` and ``VerifierMissing`` exception classes for OAuth1Session:
  this will make it easier to catch and identify these exceptions.

v0.4.1 (6 June 2014)
++++++++++++++++++++
- New install target ``[rsa]`` for people using OAuth1 RSA-SHA1 signature
  method.
- Fixed bug in OAuth2 where supplied state param was not used in auth url.
- OAuth2 HTTPS checking can be disabled by setting environment variable
  ``OAUTHLIB_INSECURE_TRANSPORT``.
- OAuth1 now re-authorize upon redirects.
- OAuth1 token fetching now raise a detailed error message when the
  response body is incorrectly encoded or the request was denied.
- Added support for custom OAuth1 clients.
- OAuth2 compliance fix for Sina Weibo.
- Multiple fixes to facebook compliance fix.
- Compliance fixes now re-encode body properly as bytes in Python 3.
- Logging now properly done under ``requests_oauthlib`` namespace instead
  of piggybacking on oauthlib namespace.
- Logging introduced for OAuth1 auth and session.

v0.4.0 (29 September 2013)
++++++++++++++++++++++++++
- OAuth1Session methods only return unicode strings. #55.
- Renamed requests_oauthlib.core to requests_oauthlib.oauth1_auth for consistency. #79.
- Added Facebook compliance fix and access_token_response hook to OAuth2Session. #63.
- Added LinkedIn compliance fix.
- Added refresh_token_response compliance hook, invoked before parsing the refresh token.
- Correctly limit compliance hooks to running only once!
- Content type guessing should only be done when no content type is given
- OAuth1 now updates r.headers instead of replacing it with non case insensitive dict
- Remove last use of Response.content (in OAuth1Session). #44.
- State param can now be supplied in OAuth2Session.authorize_url

  * [requests-toolbelt-0.9.1](https://toolbelt.readthedocs.org) The Requests Toolbelt
=====================

This is just a collection of utilities for `python-requests`_, but don't 
really belong in ``requests`` proper. The minimum tested requests version is 
``2.1.0``. In reality, the toolbelt should work with ``2.0.1`` as well, but 
some idiosyncracies prevent effective or sane testing on that version.

``pip install requests-toolbelt`` to get started!


multipart/form-data Encoder
---------------------------

The main attraction is a streaming multipart form-data object, ``MultipartEncoder``.
Its API looks like this:

.. code-block:: python

    from requests_toolbelt import MultipartEncoder
    import requests

    m = MultipartEncoder(
        fields={'field0': 'value', 'field1': 'value',
                'field2': ('filename', open('file.py', 'rb'), 'text/plain')}
        )

    r = requests.post('http://httpbin.org/post', data=m,
                      headers={'Content-Type': m.content_type})


You can also use ``multipart/form-data`` encoding for requests that don't
require files:

.. code-block:: python

    from requests_toolbelt import MultipartEncoder
    import requests

    m = MultipartEncoder(fields={'field0': 'value', 'field1': 'value'})

    r = requests.post('http://httpbin.org/post', data=m,
                      headers={'Content-Type': m.content_type})


Or, you can just create the string and examine the data:

.. code-block:: python

    # Assuming `m` is one of the above
    m.to_string()  # Always returns unicode


User-Agent constructor
----------------------

You can easily construct a requests-style ``User-Agent`` string::

    from requests_toolbelt import user_agent

    headers = {
        'User-Agent': user_agent('my_package', '0.0.1')
        }

    r = requests.get('https://api.github.com/users', headers=headers)


SSLAdapter
----------

The ``SSLAdapter`` was originally published on `Cory Benfield's blog`_. 
This adapter allows the user to choose one of the SSL protocols made available 
in Python's ``ssl`` module for outgoing HTTPS connections:

.. code-block:: python

    from requests_toolbelt import SSLAdapter
    import requests
    import ssl

    s = requests.Session()
    s.mount('https://', SSLAdapter(ssl.PROTOCOL_TLSv1))

cookies/ForgetfulCookieJar
--------------------------

The ``ForgetfulCookieJar`` prevents a particular requests session from storing 
cookies:

.. code-block:: python

    from requests_toolbelt.cookies.forgetful import ForgetfulCookieJar

    session = requests.Session()
    session.cookies = ForgetfulCookieJar()

Known Issues
------------

On Python 3.3.0 and 3.3.1, the standard library's ``http`` module will fail
when passing an instance of the ``MultipartEncoder``. This is fixed in later
minor releases of Python 3.3. Please consider upgrading to a later minor
version or Python 3.4. *There is absolutely nothing this library can do to
work around that bug.*

Contributing
------------

Please read the `suggested workflow
<https://toolbelt.readthedocs.org/en/latest/contributing.html>`_ for
contributing to this project.

Please report any bugs on the `issue tracker`_

.. _Cory Benfield's blog: https://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/
.. _python-requests: https://github.com/kennethreitz/requests
.. _issue tracker: https://github.com/requests/toolbelt/issues


History
=======

0.9.1 -- 2019-01-29
-------------------

Fixed Bugs
~~~~~~~~~~

- Fix import of pyOpenSSL shim from urllib3 for PKCS12 adapter

0.9.0 -- 2019-01-29
-------------------

New Features
~~~~~~~~~~~~

- Add X509 Adapter that can handle PKCS12 
- Add stateless solution for streaming files by MultipartEncoder from one host to another (in chunks)

Fixed Bugs
~~~~~~~~~~

- Update link to example
- Move import of ``ABCs`` from collections into version-specific part of
  _compat module
- Fix backwards incompatibility in ``get_encodings_from_content``
- Correct callback documentation for ``MultipartEncoderMonitor``
- Fix bug when ``MultipartEncoder`` is asked to encode zero parts
- Correct the type of non string request body dumps
- Removed content from being stored in MultipartDecoder
- Fix bug by enabling support for contenttype with capital letters. 
- Coerce proxy URL to bytes before dumping request
- Avoid bailing out with exception upon empty response reason
- Corrected Pool documentation
- Corrected parentheses match in example usage
- Fix "oject" to "object" in ``MultipartEncoder``
- Fix URL for the project after the move 
- Add fix for OSX TCPKeepAliveAdapter

Miscellaneous
~~~~~~~~~~~~~

- Remove py33 from testing and add Python 3.6 and nightly testing to the travis matrix.

0.8.0 -- 2017-05-20
-------------------

More information about this release can be found on the `0.8.0 milestone`_.

New Features
~~~~~~~~~~~~

- Add ``UserAgentBuilder`` to provide more control over generated User-Agent
  strings.

Fixed Bugs
~~~~~~~~~~

- Include ``_validate_certificate`` in the lits of picked attributes on the
  ``AppEngineAdapter``.
- Fix backwards incompatibility in ``get_encodings_from_content``

.. _0.8.0 milestone:
    https://github.com/requests/toolbelt/milestones/0.8.0

0.7.1 -- 2017-02-13
-------------------

More information about this release can be found on the `0.7.1 milestone`_.

Fixed Bugs
~~~~~~~~~~

- Fixed monkey-patching for the AppEngineAdapter.

- Make it easier to disable certificate verification when monkey-patching
  AppEngine.

- Handle ``multipart/form-data`` bodies without a trailing ``CRLF``.


.. links
.. _0.7.1 milestone:
    https://github.com/requests/toolbelt/milestone/9

0.7.0 -- 2016-07-21
-------------------

More information about this release can be found on the `0.7.0 milestone`_.

New Features
~~~~~~~~~~~~

- Add ``BaseUrlSession`` to allow developers to have a session that has a
  "Base" URL. See the documentation for more details and examples.

- Split the logic of ``stream_response_to_file`` into two separate functions:

  * ``get_download_file_path`` to generate the file name from the Response.

  * ``stream_response_to_file`` which will use ``get_download_file_path`` if
    necessary

Fixed Bugs
~~~~~~~~~~

- Fixed the issue for people using *very* old versions of Requests where they
  would see an ImportError from ``requests_toolbelt._compat`` when trying to
  import ``connection``.


.. _0.7.0 milestone:
    https://github.com/requests/toolbelt/milestones/0.7.0

0.6.2 -- 2016-05-10
-------------------

Fixed Bugs
~~~~~~~~~~

- When passing a timeout via Requests, it was not appropriately translated to
  the timeout that the urllib3 code was expecting.

0.6.1 -- 2016-05-05
-------------------

Fixed Bugs
~~~~~~~~~~

- Remove assertion about request URLs in the AppEngineAdapter.

- Prevent pip from installing requests 3.0.0 when that is released until we
  are ready to handle it.

0.6.0 -- 2016-01-27
-------------------

More information about this release can be found on the `0.6.0 milestone`_.

New Features
~~~~~~~~~~~~

- Add ``AppEngineAdapter`` to support developers using Google's AppEngine
  platform with Requests.

- Add ``GuessProxyAuth`` class to support guessing between Basic and Digest
  Authentication for proxies.

Fixed Bugs
~~~~~~~~~~

- Ensure that proxies use the correct TLS version when using the
  ``SSLAdapter``.

- Fix an ``AttributeError`` when using the ``HTTPProxyDigestAuth`` class.

Miscellaneous
~~~~~~~~~~~~~

- Drop testing support for Python 3.2. virtualenv and pip have stopped
  supporting it meaning that it is harder to test for this with our CI
  infrastructure. Moving forward we will make a best-effort attempt to
  support 3.2 but will not test for it.


.. _0.6.0 milestone:
    https://github.com/requests/toolbelt/milestones/0.6.0

0.5.1 -- 2015-12-16
-------------------

More information about this release can be found on the `0.5.1 milestone`_.

Fixed Bugs
~~~~~~~~~~

- Now papers over the differences in requests' ``super_len`` function from
  versions prior to 2.9.0 and versions 2.9.0 and later.


.. _0.5.1 milestone:
    https://github.com/requests/toolbelt/milestones/0.5.1

0.5.0 -- 2015-11-24
-------------------

More information about this release can be found on the `milestone
<https://github.com/requests/toolbelt/issues?utf8=%E2%9C%93&q=is%3Aall+milestone%3A0.5+>`_
for 0.5.0.

New Features
~~~~~~~~~~~~

- The ``tee`` submodule was added to ``requests_toolbelt.downloadutils``. It
  allows you to iterate over the bytes of a response while also writing them
  to a file. The ``tee.tee`` function, expects you to pass an open file
  object, while ``tee.tee_to_file`` will use the provided file name to open
  the file for you.

- Added a new parameter to ``requests_toolbelt.utils.user_agent`` that allows
  the user to specify additional items.

- Added nested form-data helper,
  ``requests_toolbelt.utils.formdata.urlencode``.

- Added the ``ForgetfulCookieJar`` to ``requests_toolbelt.cookies``.

- Added utilities for dumping the information about a request-response cycle
  in ``requests_toolbelt.utils.dump``.

- Implemented the API described in the ``requests_toolbelt.threaded`` module
  docstring, i.e., added ``requests_toolbelt.threaded.map`` as an available
  function.

Fixed Bugs
~~~~~~~~~~

- Now papers over the API differences in versions of requests installed from
  system packages versus versions of requests installed from PyPI.

- Allow string types for ``SourceAddressAdapter``.

0.4.0 -- 2015-04-03
-------------------

For more information about this release, please see `milestone 0.4.0
<https://github.com/requests/toolbelt/issues?q=milestone%3A0.4>`_
on the project's page.

New Features
~~~~~~~~~~~~

- A naive implemenation of a thread pool is now included in the toolbelt. See
  the docs in ``docs/threading.rst`` or on `Read The Docs
  <https://toolbelt.readthedocs.org>`_.

- The ``StreamingIterator`` now accepts files (such as ``sys.stdin``) without
  a specific length and will properly stream them.

- The ``MultipartEncoder`` now accepts exactly the same format of fields as
  requests' ``files`` parameter does. In other words, you can now also pass in
  extra headers to add to a part in the body. You can also now specify a
  custom ``Content-Type`` for a part.

- An implementation of HTTP Digest Authentication for Proxies is now included.

- A transport adapter that allows a user to specify a specific Certificate
  Fingerprint is now included in the toolbelt.

- A transport adapter that simplifies how users specify socket options is now
  included.

- A transport adapter that simplifies how users can specify TCP Keep-Alive
  options is now included in the toolbelt.

- Deprecated functions from ``requests.utils`` are now included and
  maintained.

- An authentication tool that allows users to specify how to authenticate to
  several different domains at once is now included.

- A function to save streamed responses to disk by analyzing the
  ``Content-Disposition`` header is now included in the toolbelt.

Fixed Bugs
~~~~~~~~~~

- The ``MultipartEncoder`` will now allow users to upload files larger than
  4GB on 32-bit systems.

- The ``MultipartEncoder`` will now accept empty unicode strings for form
  values.

0.3.1 -- 2014-06-23
-------------------

- Fix the fact that 0.3.0 bundle did not include the ``StreamingIterator``

0.3.0 -- 2014-05-21
-------------------

Bug Fixes
~~~~~~~~~

- Complete rewrite of ``MultipartEncoder`` fixes bug where bytes were lost in
  uploads

New Features
~~~~~~~~~~~~

- ``MultipartDecoder`` to accept ``multipart/form-data`` response bodies and
  parse them into an easy to use object.

- ``SourceAddressAdapter`` to allow users to choose a local address to bind
  connections to.

- ``GuessAuth`` which accepts a username and password and uses the
  ``WWW-Authenticate`` header to determine how to authenticate against a
  server.

- ``MultipartEncoderMonitor`` wraps an instance of the ``MultipartEncoder``
  and keeps track of how many bytes were read and will call the provided
  callback.

- ``StreamingIterator`` will wrap an iterator and stream the upload instead of
  chunk it, provided you also provide the length of the content you wish to
  upload.

0.2.0 -- 2014-02-24
-------------------

- Add ability to tell ``MultipartEncoder`` which encoding to use. By default
  it uses 'utf-8'.

- Fix #10 - allow users to install with pip

- Fix #9 - Fix ``MultipartEncoder#to_string`` so that it properly handles file
  objects as fields

0.1.2 -- 2014-01-19
-------------------

- At some point during development we broke how we handle normal file objects.
  Thanks to @konomae this is now fixed.

0.1.1 -- 2014-01-19
-------------------

- Handle ``io.BytesIO``-like objects better

0.1.0 -- 2014-01-18
-------------------

- Add initial implementation of the streaming ``MultipartEncoder``

- Add initial implementation of the ``user_agent`` function

- Add the ``SSLAdapter``



  * [requestsexceptions-1.4.0](http://www.openstack.org/) requestsexceptions
==================

The python requests library bundles the urllib3 library, however, some
software distributions modify requests to remove the bundled library.
This makes some operations, such as supressing the "insecure platform
warning" messages that urllib emits difficult.  This is a simple
library to find the correct path to exceptions in the requests library
regardless of whether they are bundled.




  * [requirementslib-1.5.3](https://github.com/sarugaku/requirementslib) RequirementsLib: Requirement Management Library for Pip and Pipenv

.. image:: https://img.shields.io/pypi/v/requirementslib.svg
    :target: https://pypi.org/project/requirementslib

.. image:: https://img.shields.io/pypi/l/requirementslib.svg
    :target: https://pypi.org/project/requirementslib

.. image:: https://travis-ci.org/sarugaku/requirementslib.svg?branch=master
    :target: https://travis-ci.org/sarugaku/requirementslib

.. image:: https://ci.appveyor.com/api/projects/status/hntt25rbnr5yiwmf/branch/master?svg=true
    :target: https://ci.appveyor.com/project/sarugaku/requirementslib

.. image:: https://img.shields.io/pypi/pyversions/requirementslib.svg
    :target: https://pypi.org/project/requirementslib

.. image:: https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg
    :target: https://saythanks.io/to/techalchemy

.. image:: https://readthedocs.org/projects/requirementslib/badge/?version=master
    :target: http://requirementslib.readthedocs.io/en/master/?badge=master
    :alt: Documentation Status

🐉 Installation
==================

Install from `PyPI`_:

.. code:: console

    $ pipenv install requirementslib

Install from `Github`_:

.. code:: console

    $ pipenv install -e git+https://github.com/sarugaku/requirementslib.git#egg=requirementslib


.. _PyPI: https://www.pypi.org/project/requirementslib
.. _Github: https://github.com/sarugaku/requirementslib


.. _`Summary`:

🐉 Summary
============

RequirementsLib provides a simple layer for building and interacting with
requirements in both the `Pipfile <https://github.com/pypa/pipfile/>`_ format
and the `requirements.txt <https://github.com/pypa/pip/>`_ format.  This library
was originally built for converting between these formats in `Pipenv <https://github.com/pypa/pipenv>`_.

.. _`Usage`:

🐉 Usage
=========

Importing a lockfile into your *setup.py* file
-----------------------------------------------------

You can use RequirementsLib to import your lockfile into your setup file for including your
**install_requires** dependencies:

.. code-block:: pycon

    from requirementslib import Lockfile
    lockfile = Lockfile.create('/path/to/project/dir')
    install_requires = lockfile.as_requirements(dev=False)


Interacting with a *Pipfile* directly
-------------------------------------------

You can also interact directly with a Pipfile:

.. code-block:: pycon

    >>> from requirementslib import Pipfile
    >>> pf = Pipfile.load('/home/hawk/git/pypa-pipenv')
    >>> pf.sections
    [Section(name='packages', requirements=[]), Section(name='dev-packages', requirements=[Requirement(name='pipenv', vcs=None, req=FileRequirement(setup_path=None, path='.', editable=True, uri='file:///home/hawk/git/pypa-pipenv', link=<Link file:///home/hawk/git/pypa-pipenv>, name='pipenv', req=<Requirement: "-e file:///home/hawk/git/pypa-pipenv">), markers='', specifiers=None, index=None, editable=True, hashes=[], extras=None),...]


And you can even write it back out into Pipfile's native format:

.. code-block:: pycon

    >>> print(pf.dump(to_dict=False))
    [packages]

    [dev-packages]
    pipenv = {path = ".", editable = true}
    flake8 = ">=3.3.0,<4"
    pytest = "*"
    mock = "*"

    [scripts]
    tests = "bash ./run-tests.sh"

    [pipenv]
    allow_prereleases = true


Create a requirement object from *requirements.txt* format
------------------------------------------------------------------

.. code-block:: pycon

    >>> from requirementslib import Requirement
    >>> r = Requirement.from_line('-e git+https://github.com/pypa/pipenv.git@master#egg=pipenv')
    >>> print(r)
    Requirement(name='pipenv', vcs='git', req=VCSRequirement(editable=True, uri='git+https://github.com/pypa/pipenv.git', path=None, vcs='git', ref='master', subdirectory=None, name='pipenv', link=<Link git+https://github.com/pypa/pipenv.git@master#egg=pipenv>, req=<Requirement: "-e git+https://github.com/pypa/pipenv.git@master#egg=pipenv">), markers=None, specifiers=None, index=None, editable=True, hashes=[], extras=[])

    >>> r.as_pipfile()
    {'pipenv': {'editable': True, 'ref': 'master', 'git': 'https://github.com/pypa/pipenv.git'}}


Or move from *Pipfile* format to *requirements.txt*:

.. code-block:: pycon

    >>> r = Requirement.from_pipfile(name='pythonfinder', indexes=[], pipfile={'path': '../pythonfinder', 'editable': True})
    >>> r.as_line()
    '-e ../pythonfinder'


Resolving Editable Package Dependencies
---------------------------------------------

Requirementslib also can resolve the dependencies of editable packages by calling the ``run_requires`` method.
This method returns a detailed dictionary containing metadata parsed from the package built in
a transient folder (unless it is already on the system or the call is run in a virtualenv).

The output of ``run_requires`` is very detailed and in most cases will be sufficient:

.. code-block:: pycon

    >>> from pprint import pprint
    >>> from requirementslib.models.requirements import Requirement
    >>> r = Requirement.from_line("-e git+git@github.com:sarugaku/vistir.git#egg=vistir[spinner]")
    >>> setup_info_dict = r.run_requires()
    >>> from pprint import pprint
    >>> pprint(setup_info_dict)
    {'base_dir': '/tmp/requirementslib-t_ftl6no-src/src/vistir',
    'build_backend': 'setuptools.build_meta',
    'build_requires': ['setuptools>=36.2.2', 'wheel>=0.28.0'],
    'extra_kwargs': {'build_dir': '/tmp/requirementslib-t_ftl6no-src/src',
                    'download_dir': '/home/hawk/.cache/pipenv/pkgs',
                    'src_dir': '/tmp/requirementslib-t_ftl6no-src/src',
                    'wheel_download_dir': '/home/hawk/.cache/pipenv/wheels'},
    'extras': {'spinner': [Requirement.parse('cursor'),
                            Requirement.parse('yaspin')],
                'tests': [Requirement.parse('pytest'),
                        Requirement.parse('pytest-xdist'),
                        Requirement.parse('pytest-cov'),
                        Requirement.parse('pytest-timeout'),
                        Requirement.parse('hypothesis-fspaths'),
                        Requirement.parse('hypothesis')]},
    'ireq': <InstallRequirement object: vistir[spinner] from git+ssh://git@github.com/sarugaku/vistir.git#egg=vistir editable=True>,
    'name': 'vistir',
    'pyproject': PosixPath('/tmp/requirementslib-t_ftl6no-src/src/vistir/pyproject.toml'),
    'python_requires': '>=2.6,!=3.0,!=3.1,!=3.2,!=3.3',
    'requires': {'backports.functools_lru_cache;python_version<="3.4"': Requirement.parse('backports.functools_lru_cache; python_version <= "3.4"'),
                'backports.shutil_get_terminal_size;python_version<"3.3"': Requirement.parse('backports.shutil_get_terminal_size; python_version < "3.3"'),
                'backports.weakref;python_version<"3.3"': Requirement.parse('backports.weakref; python_version < "3.3"'),
                'colorama': Requirement.parse('colorama'),
                'pathlib2;python_version<"3.5"': Requirement.parse('pathlib2; python_version < "3.5"'),
                'requests': Requirement.parse('requests'),
                'six': Requirement.parse('six'),
                'spinner': [Requirement.parse('cursor'),
                            Requirement.parse('yaspin')]},
    'setup_cfg': PosixPath('/tmp/requirementslib-t_ftl6no-src/src/vistir/setup.cfg'),
    'setup_py': PosixPath('/tmp/requirementslib-t_ftl6no-src/src/vistir/setup.py')}


As a side-effect of calls to ``run_requires``, new metadata is made available on the
requirement itself via the property ``requirement.req.dependencies``:


.. code-block:: pycon

    >>> pprint(r.req.dependencies)
    ({'backports.functools_lru_cache;python_version<="3.4"': Requirement.parse('backports.functools_lru_cache; python_version <= "3.4"'),
    'backports.shutil_get_terminal_size;python_version<"3.3"': Requirement.parse('backports.shutil_get_terminal_size; python_version < "3.3"'),
    'backports.weakref;python_version<"3.3"': Requirement.parse('backports.weakref; python_version < "3.3"'),
    'colorama': Requirement.parse('colorama'),
    'pathlib2;python_version<"3.5"': Requirement.parse('pathlib2; python_version < "3.5"'),
    'requests': Requirement.parse('requests'),
    'six': Requirement.parse('six'),
    'spinner': [Requirement.parse('cursor'), Requirement.parse('yaspin')]},
    [],
    ['setuptools>=36.2.2', 'wheel>=0.28.0'])


🐉 Integrations
==================

* `Pip <https://github.com/pypa/pip>`_
* `Pipenv <https://github.com/pypa/pipenv>`_
* `Pipfile`_



  * [restructuredtext-lint-1.3.0](https://github.com/twolfson/restructuredtext-lint) restructuredtext-lint
=====================

.. image:: https://travis-ci.org/twolfson/restructuredtext-lint.png?branch=master
   :target: https://travis-ci.org/twolfson/restructuredtext-lint
   :alt: Build Status

`reStructuredText`_ `linter`_

This was created out of frustration with `PyPI`_; it sucks finding out your `reST`_ is invalid **after** uploading it. It is being developed in junction with a `Sublime Text`_ linter.

.. _`reStructuredText`: http://docutils.sourceforge.net/rst.html
.. _`linter`: http://en.wikipedia.org/wiki/Lint_%28software%29
.. _`reST`: `reStructuredText`_
.. _`PyPI`: http://pypi.python.org/
.. _`Sublime Text`: http://sublimetext.com/

Getting Started
---------------
Install the module with: ``pip install restructuredtext_lint``

.. code:: python

    import restructuredtext_lint
    errors = restructuredtext_lint.lint("""
    Hello World
    =======
    """)

    # `errors` will be list of system messages
    # [<system_message: <paragraph...><literal_block...>>]
    errors[0].message  # Title underline too short.

CLI Utility
^^^^^^^^^^^
For your convenience, we present a CLI utility ``rst-lint`` (also available as ``restructuredtext-lint``).

.. code:: console

    $ rst-lint --help
    usage: rst-lint [-h] [--version] [--format {text,json}] [--encoding ENCODING]
                    [--level {debug,info,warning,error,severe}]
                    [--rst-prolog RST_PROLOG]
                    path [path ...]

    Lint reStructuredText files. Returns 0 if all files pass linting, 1 for an
    internal error, and 2 if linting failed.

    positional arguments:
      path                  File/folder to lint

    optional arguments:
      -h, --help            show this help message and exit
      --version             show program's version number and exit
      --format {text,json}  Format of the output (default: "text")
      --encoding ENCODING   Encoding of the input file (e.g. "utf-8")
      --level {debug,info,warning,error,severe}
                            Minimum error level to report (default: "warning")
      --rst-prolog RST_PROLOG
                            reStructuredText content to prepend to all files
                            (useful for substitutions)

    $ rst-lint README.rst
    WARNING README.rst:2 Title underline too short.

PyPI issues
^^^^^^^^^^^
While a document may lint cleanly locally, there can be issues when submitted it to `PyPI`_. Here are some common problems:

- Usage of non-builtin lexers (e.g. ``bibtex``) will pass locally but not be recognized/parsable on `PyPI`_

  - This is due to `PyPI`_ not having a non-builtin lexer installed
  - Please avoid non-builtin lexers to avoid complications
  - For more information, see `#27`_

- Relative hyperlinks will not work (e.g. ``./UNLICENSE``)

  - According to Stack Overflow, hyperlinks must use a scheme (e.g. ``http``, ``https``) and that scheme must be whitelisted

    - http://stackoverflow.com/a/16594755

  - Please use absolute hyperlinks (e.g. ``https://github.com/twolfson/restructuredtext-lint/blob/master/UNLICENSE``)

.. _`#27`: https://github.com/twolfson/restructuredtext-lint/issues/27

Documentation
-------------
``restructuredtext-lint`` exposes a ``lint`` and ``lint_file`` function

``restructuredtext_lint.lint(content, filepath=None, rst_prolog=None)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Lint `reStructuredText`_ and return errors

- content ``String`` - `reStructuredText`_ to be linted
- filepath ``String`` - Optional path to file, this will be returned as the source
- rst_prolog ``String`` - Optional content to prepend to content, line numbers will be offset to ignore this

Returns:

- errors ``List`` - List of errors

  - Each error is a class from `docutils`_ with the following attrs

    - line ``Integer|None`` - Line where the error occurred

      - On rare occasions, this will be ``None`` (e.g. anonymous link mismatch)

    - source ``String`` - ``filepath`` provided in parameters
    - level ``Integer`` - Level of the warning

      - Levels represent 'info': 1, 'warning': 2, 'error': 3, 'severe': 4

    - type ``String`` - Noun describing the error level

      - Levels can be 'INFO', 'WARNING', 'ERROR', or 'SEVERE'
    - message ``String`` - Error message
    - full_message ``String`` - Error message and source lines where the error occurred

  - It should be noted that ``level``, ``type``, ``message``, and ``full_message`` are custom attrs added onto the original ``system_message``

.. _`docutils`: http://docutils.sourceforge.net/

``restructuredtext_lint.lint_file(filepath, encoding=None, *args, **kwargs)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Lint a `reStructuredText`_ file and return errors

- filepath ``String`` - Path to file for linting
- encoding ``String`` - Encoding to read file in as

  - When ``None`` is provided, it will use OS default as provided by `locale.getpreferredencoding`_
  - The list of supported encodings can be found at http://docs.python.org/2/library/codecs.html#standard-encodings

- ``*args`` - Additional arguments to be passed to ``lint``
- ``**kwargs`` - Additional keyword arguments to be passed to ``lint``

.. _`locale.getpreferredencoding`: http://docs.python.org/2/library/locale.html#locale.getpreferredencoding

Returns: Same structure as ``restructuredtext_lint.lint``

Extension
---------
Under the hood, we leverage `docutils`_ for parsing reStructuredText documents. `docutils`_ supports adding new directives and roles via ``register_directive`` and ``register_role``.

Sphinx
^^^^^^
Unfortunately due to customizations in `Sphinx's parser`_ we cannot include all of its directives/roles (see `#29`_). However, we can include some of them as one-offs. Here is an example of adding a directive from `Sphinx`_.

.. _`Sphinx`: http://sphinx-doc.org/
.. _`Sphinx's parser`:  Sphinx_
.. _`#29`: https://github.com/twolfson/restructuredtext-lint/issues/29#issuecomment-243456787

https://github.com/sphinx-doc/sphinx/blob/1.3/sphinx/directives/code.py

**sphinx.rst**

.. code:: rst

    Hello
    =====
    World

    .. highlight:: python

        Hello World!

**sphinx.py**

.. code:: python

    # Load in our dependencies
    from docutils.parsers.rst.directives import register_directive
    from sphinx.directives.code import Highlight
    import restructuredtext_lint

    # Load our new directive
    register_directive('highlight', Highlight)

    # Lint our README
    errors = restructuredtext_lint.lint_file('docs/sphinx/README.rst')
    print errors[0].message # Error in "highlight" directive: no content permitted.

Examples
--------
Here is an example of all invalid properties

.. code:: python

    rst = """
    Some content.

    Hello World
    =======
    Some more content!
    """
    errors = restructuredtext_lint.lint(rst, 'myfile.py')
    errors[0].line  # 5
    errors[0].source  # myfile.py
    errors[0].level  # 2
    errors[0].type  # WARNING
    errors[0].message  # Title underline too short.
    errors[0].full_message  # Title underline too short.
                            #
                            # Hello World
                            # =======

Contributing
------------
In lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Test via ``nosetests``.

Donating
--------
Support this project and `others by twolfson`_ via `donations`_.

http://twolfson.com/support-me

.. _`others by twolfson`: http://twolfson.com/projects
.. _donations: http://twolfson.com/support-me

Unlicense
---------
As of Nov 22 2013, Todd Wolfson has released this repository and its contents to the public domain.

It has been released under the `UNLICENSE`_.

.. _UNLICENSE: https://github.com/twolfson/restructuredtext-lint/blob/master/UNLICENSE
  * [rethinkdb-2.4.2.post1](https://github.com/RethinkDB/rethinkdb-python) # RethinkDB Python driver
[![PyPI version](https://badge.fury.io/py/rethinkdb.svg)](https://badge.fury.io/py/rethinkdb) [![Build Status](https://travis-ci.org/rethinkdb/rethinkdb-python.svg?branch=master)](https://travis-ci.org/rethinkdb/rethinkdb-python) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/2b5231a6f90a4a1ba2fc795f8466bbe4)](https://www.codacy.com/app/rethinkdb/rethinkdb-python?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=rethinkdb/rethinkdb-python&amp;utm_campaign=Badge_Grade) [![Codacy Badge](https://api.codacy.com/project/badge/Coverage/2b5231a6f90a4a1ba2fc795f8466bbe4)](https://www.codacy.com/app/rethinkdb/rethinkdb-python?utm_source=github.com&utm_medium=referral&utm_content=rethinkdb/rethinkdb-python&utm_campaign=Badge_Coverage)

## Overview

### What is RethinkDB?
RethinkDB is the first open-source scalable database built for realtime applications. It exposes a new database access model -- instead of polling for changes, the developer can tell the database to continuously push updated query results to applications in realtime. RethinkDB allows developers to build scalable realtime apps in a fraction of the time with less effort.

## Installation
```bash
$ pip install rethinkdb
```
*Note: this package is the extracted driver of RethinkDB's original python driver.*

## Quickstart
The main difference with the previous driver (except the name of the package) is we are **not** importing RethinkDB as `r`. If you would like to use `RethinkDB`'s python driver as a drop in replacement, you should do the following:

```python
from rethinkdb import RethinkDB

r = RethinkDB()
connection = r.connect(db='test')
```

## Blocking and Non-blocking I/O
This driver supports blocking I/O (i.e. standard Python sockets) as well as
non-blocking I/O through multiple async frameworks:

* [Asyncio](https://docs.python.org/3/library/asyncio.html)
* [Gevent](http://www.gevent.org/)
* [Tornado](https://www.tornadoweb.org/en/stable/)
* [Trio](https://trio.readthedocs.io/en/latest/)
* [Twisted](https://twistedmatrix.com/trac/)

The following examples demonstrate how to use the driver in each mode.

### Default mode (blocking I/O)
The driver's default mode of operation is to use blocking I/O, i.e. standard Python
sockets. This example shows how to create a table, populate with data, and get every
document.

```python
from rethinkdb import RethinkDB

r = RethinkDB()
connection = r.connect(db='test')

r.table_create('marvel').run(connection)

marvel_heroes = r.table('marvel')
marvel_heroes.insert({
    'id': 1,
    'name': 'Iron Man',
    'first_appearance': 'Tales of Suspense #39'
}).run(connection)

for hero in marvel_heroes.run(connection):
    print(hero['name'])
```

### Asyncio mode
Asyncio mode is compatible with Python ≥ 3.4, which is when asyncio was
introduced into the standard library.

```python
import asyncio
from rethinkdb import RethinkDB

# Native coroutines are supported in Python ≥ 3.5. In Python 3.4, you should
# use the @asyncio.couroutine decorator instead of "async def", and "yield from"
# instead of "await".
async def main():
    r = RethinkDB()
    r.set_loop_type('asyncio')
    connection = await r.connect(db='test')

    await r.table_create('marvel').run(connection)

    marvel_heroes = r.table('marvel')
    await marvel_heroes.insert({
        'id': 1,
        'name': 'Iron Man',
        'first_appearance': 'Tales of Suspense #39'
    }).run(connection)

    # "async for" is supported in Python ≥ 3.6. In earlier versions, you should
    # call "await cursor.next()" in a loop.
    cursor = await marvel_heroes.run(connection)
    async for hero in cursor:
        print(hero['name'])

asyncio.get_event_loop().run_until_complete(main())
```

### Gevent mode

```python
import gevent
from rethinkdb import RethinkDB

def main():
    r = RethinkDB()
    r.set_loop_type('gevent')
    connection = r.connect(db='test')

    r.table_create('marvel').run(connection)

    marvel_heroes = r.table('marvel')
    marvel_heroes.insert({
        'id': 1,
        'name': 'Iron Man',
        'first_appearance': 'Tales of Suspense #39'
    }).run(connection)

    for hero in marvel_heroes.run(connection):
        print(hero['name'])

gevent.joinall([gevent.spawn(main)])
```

### Tornado mode
Tornado mode is compatible with Tornado < 5.0.0. Tornado 5 is not supported.

```python
from rethinkdb import RethinkDB
from tornado import gen
from tornado.ioloop import IOLoop

@gen.coroutine
def main():
    r = RethinkDB()
    r.set_loop_type('tornado')
    connection = yield r.connect(db='test')

    yield r.table_create('marvel').run(connection)

    marvel_heroes = r.table('marvel')
    yield marvel_heroes.insert({
        'id': 1,
        'name': 'Iron Man',
        'first_appearance': 'Tales of Suspense #39'
    }).run(connection)

    cursor = yield marvel_heroes.run(connection)
    while (yield cursor.fetch_next()):
        hero = yield cursor.next()
        print(hero['name'])

IOLoop.current().run_sync(main)
```

### Trio mode

```python
from rethinkdb import RethinkDB
import trio

async def main():
    r = RethinkDB()
    r.set_loop_type('trio')
    async with trio.open_nursery() as nursery:
        async with r.open(db='test', nursery=nursery) as conn:
            await r.table_create('marvel').run(conn)
            marvel_heroes = r.table('marvel')
            await marvel_heroes.insert({
                'id': 1,
                'name': 'Iron Man',
                'first_appearance': 'Tales of Suspense #39'
            }).run(conn)

            # "async for" is supported in Python ≥ 3.6. In earlier versions, you should
            # call "await cursor.next()" in a loop.
            cursor = await marvel_heroes.run(conn)
            async with cursor:
                async for hero in cursor:
                    print(hero['name'])

trio.run(main)
```

The Trio mode also supports a database connection pool. You can modify the example above
as follows:

```python
db_pool = r.ConnectionPool(db='test', nursery=nursery)
async with db_pool.connection() as conn:
    ...
await db_pool.close()
```

### Twisted mode

```python
from rethinkdb import RethinkDB
from twisted.internet import reactor, defer

@defer.inlineCallbacks
def main():
    r = RethinkDB()
    r.set_loop_type('twisted')
    connection = yield r.connect(db='test')

    yield r.table_create('marvel').run(connection)

    marvel_heroes = r.table('marvel')
    yield marvel_heroes.insert({
        'id': 1,
        'name': 'Iron Man',
        'first_appearance': 'Tales of Suspense #39'
    }).run(connection)

    cursor = yield marvel_heroes.run(connection)
    while (yield cursor.fetch_next()):
        hero = yield cursor.next()
        print(hero['name'])

main().addCallback(lambda d: print("stopping") or reactor.stop())
reactor.run()
```

## Misc
Although we recommend to use the import used in the examples, to help the migration from rethinkdb<2.4 we introduced a shortcut which can easily replace the old `import rethinkdb as r` import with `from rethinkdb import r`. 

## Run tests
In the `Makefile` you can find three different test commands: `test-unit`, `test-integration` and `test-remote`. As RethinkDB has dropped the support of Windows, we would like to ensure that those of us who are using Windows for development can still contribute. Because of this, we support running integration tests against Digital Ocean Droplets as well.

Before you run any test, make sure that you install the requirements.
```bash
$ pip install -r requirements.txt
$ make prepare
```

### Running unit tests
```bash
$ make test-unit
```

### Running integration tests
*To run integration tests locally, make sure you intstalled RethinkDB*
```bash
$ make test-integration
```

### Running remote integration tests
*To run the remote tests, you need to have a Digital Ocean account and an API key.*

Remote test will create a new temporary SSH key and a Droplet for you until the tests are finished.

**Available environment variables**

| Variable name | Default value |
|---------------|---------------|
| DO_TOKEN      | N/A           |
| DO_SIZE       | 512MB         |
| DO_REGION     | sfo2          |

```bash
$ pip install paramiko python-digitalocean
$ export DO_TOKEN=<YOUR_TOKEN>
$ make test-remote
```

## New features
Github's Issue tracker is **ONLY** used for reporting bugs. NO NEW FEATURE ACCEPTED! Use [spectrum](https://spectrum.chat/rethinkdb) for supporting features.

## Contributing
Hurray! You reached this section which means, that you would like to contribute. Please read our contributing guide lines and feel free to open a pull request.



  * [retrying-1.3.3](https://github.com/rholder/retrying) Retrying
=========================
.. image:: https://travis-ci.org/rholder/retrying.png?branch=master
    :target: https://travis-ci.org/rholder/retrying

.. image:: https://badge.fury.io/py/retrying.png
    :target: https://pypi.python.org/pypi/retrying

.. image:: https://pypip.in/d/retrying/badge.png
    :target: https://pypi.python.org/pypi/retrying

Retrying is an Apache 2.0 licensed general-purpose retrying library, written in
Python, to simplify the task of adding retry behavior to just about anything.


The simplest use case is retrying a flaky function whenever an Exception occurs
until a value is returned.

.. code-block:: python

    import random
    from retrying import retry

    @retry
    def do_something_unreliable():
        if random.randint(0, 10) > 1:
            raise IOError("Broken sauce, everything is hosed!!!111one")
        else:
            return "Awesome sauce!"

    print do_something_unreliable()


Features
--------

- Generic Decorator API
- Specify stop condition (i.e. limit by number of attempts)
- Specify wait condition (i.e. exponential backoff sleeping between attempts)
- Customize retrying on Exceptions
- Customize retrying on expected returned result


Installation
------------

To install retrying, simply:

.. code-block:: bash

    $ pip install retrying

Or, if you absolutely must:

.. code-block:: bash

    $ easy_install retrying

But, you might regret that later.


Examples
----------

As you saw above, the default behavior is to retry forever without waiting.

.. code-block:: python

    @retry
    def never_give_up_never_surrender():
        print "Retry forever ignoring Exceptions, don't wait between retries"


Let's be a little less persistent and set some boundaries, such as the number of attempts before giving up.

.. code-block:: python

    @retry(stop_max_attempt_number=7)
    def stop_after_7_attempts():
        print "Stopping after 7 attempts"

We don't have all day, so let's set a boundary for how long we should be retrying stuff.

.. code-block:: python

    @retry(stop_max_delay=10000)
    def stop_after_10_s():
        print "Stopping after 10 seconds"

Most things don't like to be polled as fast as possible, so let's just wait 2 seconds between retries.

.. code-block:: python

    @retry(wait_fixed=2000)
    def wait_2_s():
        print "Wait 2 second between retries"


Some things perform best with a bit of randomness injected.

.. code-block:: python

    @retry(wait_random_min=1000, wait_random_max=2000)
    def wait_random_1_to_2_s():
        print "Randomly wait 1 to 2 seconds between retries"

Then again, it's hard to beat exponential backoff when retrying distributed services and other remote endpoints.

.. code-block:: python

    @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)
    def wait_exponential_1000():
        print "Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards"


We have a few options for dealing with retries that raise specific or general exceptions, as in the cases here.

.. code-block:: python

    def retry_if_io_error(exception):
        """Return True if we should retry (in this case when it's an IOError), False otherwise"""
        return isinstance(exception, IOError)

    @retry(retry_on_exception=retry_if_io_error)
    def might_io_error():
        print "Retry forever with no wait if an IOError occurs, raise any other errors"

    @retry(retry_on_exception=retry_if_io_error, wrap_exception=True)
    def only_raise_retry_error_when_not_io_error():
        print "Retry forever with no wait if an IOError occurs, raise any other errors wrapped in RetryError"

We can also use the result of the function to alter the behavior of retrying.

.. code-block:: python

    def retry_if_result_none(result):
        """Return True if we should retry (in this case when result is None), False otherwise"""
        return result is None

    @retry(retry_on_result=retry_if_result_none)
    def might_return_none():
        print "Retry forever ignoring Exceptions with no wait if return value is None"


Any combination of stop, wait, etc. is also supported to give you the freedom to mix and match.

Contribute
----------

#. Check for open issues or open a fresh issue to start a discussion around a feature idea or a bug.
#. Fork `the repository`_ on GitHub to start making your changes to the **master** branch (or branch off of it).
#. Write a test which shows that the bug was fixed or that the feature works as expected.
#. Send a pull request and bug the maintainer until it gets merged and published. :) Make sure to add yourself to AUTHORS_.

.. _`the repository`: http://github.com/rholder/retrying
.. _AUTHORS: https://github.com/rholder/retrying/blob/master/AUTHORS.rst


.. :changelog:

History
-------
1.3.3 (2014-12-14)
++++++++++++++++++
- Add minimum six version of 1.7.0 since anything less will break things

1.3.2 (2014-11-09)
++++++++++++++++++
- Ensure we wrap the decorated functions to prevent information loss
- Allow a jitter value to be passed in

1.3.1 (2014-09-30)
++++++++++++++++++
- Add requirements.txt to MANIFEST.in to fix pip installs

1.3.0 (2014-09-30)
++++++++++++++++++
- Add upstream six dependency, remove embedded six functionality

1.2.3 (2014-08-25)
++++++++++++++++++
- Add support for custom wait and stop functions

1.2.2 (2014-06-20)
++++++++++++++++++
- Bug fix to not raise a RetryError on failure when exceptions aren't being wrapped

1.2.1 (2014-05-05)
++++++++++++++++++
- Bug fix for explicitly passing in a wait type

1.2.0 (2014-05-04)
++++++++++++++++++
- Remove the need for explicit specification of stop/wait types when they can be inferred
- Add a little checking for exception propagation

1.1.0 (2014-03-31)
++++++++++++++++++
- Added proper exception propagation through reraising with Python 2.6, 2.7, and 3.2 compatibility
- Update test suite for behavior changes

1.0.1 (2013-03-20)
++++++++++++++++++
- Fixed a bug where classes not extending from the Python exception hierarchy could slip through
- Update test suite for custom Python exceptions

1.0.0 (2013-01-21)
++++++++++++++++++
- First stable, tested version now exists
- Apache 2.0 license applied
- Sanitizing some setup.py and test suite running
- Added Travis CI support
  * [retype-17.12.0](https://github.com/ambv/retype) # retype

[![Build Status](https://dev.azure.com/ambv/retype/_apis/build/status/ambv.retype?branchName=master)](https://dev.azure.com/ambv/retype/_build/latest?definitionId=1&branchName=master)

Re-apply type annotations from .pyi stubs to your codebase.

## Usage

```
Usage: retype [OPTIONS] [SRC]...

  Re-apply type annotations from .pyi stubs to your codebase.

Options:
  -p, --pyi-dir DIRECTORY     Where to find .pyi stubs.  [default: types]
  -t, --target-dir DIRECTORY  Where to write annotated sources.  [default:
                              typed-src]
  -i, --incremental           Allow for missing type annotations in both stubs
                              and the source.
  -q, --quiet                 Don't emit warnings, just errors.
  -a, --replace-any           Allow replacing Any annotations.
  --hg                        Post-process files to preserve implicit byte
                              literals.
  --traceback                 Show a Python traceback on error.
  --version                   Show the version and exit.
  --help                      Show this message and exit.
```

When you run `retype`, it goes through all files you passed as SRC,
finds the corresponding .pyi files in the `types/` directory, and
re-applies typing annotations from .pyi to the sources, using the
Python 3 function and variable annotation syntax.  The resulting
combined sources are saved in `typed-src/`.

You can also pass directories as sources, in which case `retype` will
look for .py files in them recursively.

It's smart enough to do the following:

* reapply typing imports
* reapply function argument annotations
* reapply function return value annotations
* reapply method argument and return value annotations
* reapply function-level variable annotations
* reapply module-level name annotations
* reapply module-level type aliases
* reapply class-level field annotations
* reapply instance-level field annotations
* validate existing source annotations against the .pyi file
* validate source function signatures against the .pyi file
* read function signature type comments in .pyi files
* read variable type comments in .pyi files
* consider existing source type comments as annotations
* remove duplicate type comments from source when annotations are applied
* normalize remaining type comments in the source to annotations; this
  is done even if the corresponding .pyi file is missing


## List of things to be done

* [ ] add a --backward option to output type comments instead of annotations
* [ ] handle if sys.version_info and sys.platform checks in stubs


## Design principles

* it's okay for a given .pyi file to be incomplete (gradual typing,
  baby!)
* it's okay for functions and classes to be out of order in .pyi files
  and the source
* it's an **error** for a function or class to be missing in the source
* it's an **error** for a function's signature to be incompatible
  between the .pyi file and the source
* it's an **error** for an annotation in the source to be incompatible
  with the .pyi file


## Known limitations

* Line numbers in the annotated source will no longer match original
  source code; this is because re-application of types requires copying
  typing imports and alias definitions from the .pyi file.
* While formatting of the original source will be preserved, formatting
  of the applied annotations might differ from the formatting in .pyi
  files.
* The source where type annotations get re-applied cannot use the
  legacy `print` statement; that wouldn't work at runtime.
* Class attribute annotations in `__init__()` methods are moved verbatim
  to the respective `__init__()` method in the implementation.  They are
  never translated into class-level attribute annotations, so if that
  method is missing, the translation will fail.  Similarly, class-level
  attribute annotations are never applied to `__init__()` methods.
* Forward references in .pyi files will only be properly resolved for
  type aliases and type vars (by inserting them right before they're
  used in the source).  Other forms of forward references will not work
  in the source code due to out-of-order class and function definitions.
  Modify your .pyi files to use strings.  `retype` will not
  automatically discover failing forward references and stringify them.
* Local variable annotations present in the .pyi file are transferred to
  the body level of the given function in the source.  In other words,
  if the source defines a variable within a loop or a conditional
  statement branch, `retype` will create an value-less variable
  annotation at the beginning of the function.  Use a broad type and
  constrain types in relevant code paths using `assert isinstance()`
  checks.
* Because of the above, existing source variable annotations and type
  comments buried in conditionals and loops will not be deduplicated
  (and `mypy` will complain that a name was already defined).
* An async function in the stub will match a regular function of the
  same name in the same scope and vice versa.  This is to enable
  annotating async functions spelled with `@asyncio.coroutine`.


## Tests

Just run:

```
tox
```

## OMG, this is Python 3 only!

Relax, you can run *retype* **as a tool** perfectly fine under Python
3.6+ even if you want to analyze Python 2 code.  This way you'll be able
to parse all of the new syntax supported on Python 3 but also
*effectively all* the Python 2 syntax at the same time.

By making the code exclusively Python 3.6+, I'm able to focus on the
quality of the checks and re-use all the nice features of the new
releases (check out [pathlib](docs.python.org/3/library/pathlib.html)
or f-strings) instead of wasting cycles on Unicode compatibility, etc.

Note: to retype modules using f-strings you need to run on Python 3.6.2+
due to [bpo-23894](http://bugs.python.org/issue23894).

## License

MIT


## Change Log

### next

* add a module entry-point, now you can call it via ``python -m retype``
* automatically all files excluded by ``.gitignore`` on merge of folders
* support for ``ast3.num``
* fix a bug that meant the merge was not recursive in paths
* use `setup.cfg` based packaging configuration
* add PEP-517/8 declaration via `pyproject.toml`
* include license in both wheel and sdist
* this projects code base is now formatted with *black*, import ordered via
  *isort*, and uses Azure Pipelines instead of Travis (also testing on Windows
  and macOs)

### 17.12.0

* support --replace-any to allow replacing pre-existing `Any` annotations
  without raising errors

* bugfix: don't re-apply `# type: ignore` as an annotation if followed
  by another comment.  Original patch by Shannon Zhu.

### 17.6.3

* bugfix: don't try to re-apply `# type: ignore` as a function annotation

* bugfix: support arbitrary source file encodings, patch by Michael Overmeyer.

* bugfix: support missing newlines at the end of the file, patch by Michael
  Overmeyer.

* bugfix: in --incremental, format default values according to PEP 8
  (no spaces around the = sign if the type is missing)

### 17.6.2

* bugfix: --incremental didn't work with multiple arguments before

### 17.6.1

* support --incremental stub application (i.e. allow for both stubs and the
  source to be missing annotations for some arguments and/or return value)

### 17.6.0

* support async functions

* support --traceback for getting more information about internal errors

### 17.4.0

* first published version

* date-versioned


## Authors

Glued together by [Łukasz Langa](mailto:lukasz@langa.pl).  Multiple
improvements by [Michael Overmeyer](mailto:m.overmeyer@yahoo.ca) and
[Bernat Gabor](mailto:gaborjbernat@gmail.com).



  * [rfc3986-1.3.2](http://rfc3986.readthedocs.io) rfc3986
=======

A Python implementation of `RFC 3986`_ including validation and authority 
parsing.

Installation
------------

Use pip to install ``rfc3986`` like so::

    pip install rfc3986

License
-------

`Apache License Version 2.0`_

Example Usage
-------------

The following are the two most common use cases envisioned for ``rfc3986``.

Replacing ``urlparse``
``````````````````````

To parse a URI and receive something very similar to the standard library's
``urllib.parse.urlparse``

.. code-block:: python

    from rfc3986 import urlparse

    ssh = urlparse('ssh://user@git.openstack.org:29418/openstack/glance.git')
    print(ssh.scheme)  # => ssh
    print(ssh.userinfo)  # => user
    print(ssh.params)  # => None
    print(ssh.port)  # => 29418

To create a copy of it with new pieces you can use ``copy_with``:

.. code-block:: python

    new_ssh = ssh.copy_with(
        scheme='https'
        userinfo='',
        port=443,
        path='/openstack/glance'
    )
    print(new_ssh.scheme)  # => https
    print(new_ssh.userinfo)  # => None
    # etc.

Strictly Parsing a URI and Applying Validation
``````````````````````````````````````````````

To parse a URI into a convenient named tuple, you can simply:

.. code-block:: python

    from rfc3986 import uri_reference

    example = uri_reference('http://example.com')
    email = uri_reference('mailto:user@domain.com')
    ssh = uri_reference('ssh://user@git.openstack.org:29418/openstack/keystone.git')

With a parsed URI you can access data about the components:

.. code-block:: python

    print(example.scheme)  # => http
    print(email.path)  # => user@domain.com
    print(ssh.userinfo)  # => user
    print(ssh.host)  # => git.openstack.org
    print(ssh.port)  # => 29418

It can also parse URIs with unicode present:

.. code-block:: python

    uni = uri_reference(b'http://httpbin.org/get?utf8=\xe2\x98\x83')  # ☃
    print(uni.query)  # utf8=%E2%98%83

With a parsed URI you can also validate it:

.. code-block:: python

    if ssh.is_valid():
        subprocess.call(['git', 'clone', ssh.unsplit()])

You can also take a parsed URI and normalize it:

.. code-block:: python

    mangled = uri_reference('hTTp://exAMPLe.COM')
    print(mangled.scheme)  # => hTTp
    print(mangled.authority)  # => exAMPLe.COM

    normal = mangled.normalize()
    print(normal.scheme)  # => http
    print(mangled.authority)  # => example.com

But these two URIs are (functionally) equivalent:

.. code-block:: python

    if normal == mangled:
        webbrowser.open(normal.unsplit())

Your paths, queries, and fragments are safe with us though:

.. code-block:: python

    mangled = uri_reference('hTTp://exAMPLe.COM/Some/reallY/biZZare/pAth')
    normal = mangled.normalize()
    assert normal == 'hTTp://exAMPLe.COM/Some/reallY/biZZare/pAth'
    assert normal == 'http://example.com/Some/reallY/biZZare/pAth'
    assert normal != 'http://example.com/some/really/bizzare/path'

If you do not actually need a real reference object and just want to normalize
your URI:

.. code-block:: python

    from rfc3986 import normalize_uri

    assert (normalize_uri('hTTp://exAMPLe.COM/Some/reallY/biZZare/pAth') ==
            'http://example.com/Some/reallY/biZZare/pAth')

You can also very simply validate a URI:

.. code-block:: python

    from rfc3986 import is_valid_uri

    assert is_valid_uri('hTTp://exAMPLe.COM/Some/reallY/biZZare/pAth')

Requiring Components
~~~~~~~~~~~~~~~~~~~~

You can validate that a particular string is a valid URI and require
independent components:

.. code-block:: python

    from rfc3986 import is_valid_uri

    assert is_valid_uri('http://localhost:8774/v2/resource',
                        require_scheme=True,
                        require_authority=True,
                        require_path=True)

    # Assert that a mailto URI is invalid if you require an authority
    # component
    assert is_valid_uri('mailto:user@example.com', require_authority=True) is False

If you have an instance of a ``URIReference``, you can pass the same arguments
to ``URIReference#is_valid``, e.g.,

.. code-block:: python

    from rfc3986 import uri_reference

    http = uri_reference('http://localhost:8774/v2/resource')
    assert uri.is_valid(require_scheme=True,
                        require_authority=True,
                        require_path=True)

    # Assert that a mailto URI is invalid if you require an authority
    # component
    mailto = uri_reference('mailto:user@example.com')
    assert uri.is_valid(require_authority=True) is False

Alternatives
------------

- `rfc3987 <https://pypi.python.org/pypi/rfc3987/1.3.4>`_

  This is a direct competitor to this library, with extra features,
  licensed under the GPL.

- `uritools <https://pypi.python.org/pypi/uritools/0.5.1>`_

  This can parse URIs in the manner of RFC 3986 but provides no validation and
  only recently added Python 3 support.

- Standard library's `urlparse`/`urllib.parse`

  The functions in these libraries can only split a URI (valid or not) and
  provide no validation.

Contributing
------------

This project follows and enforces the Python Software Foundation's `Code of
Conduct <https://www.python.org/psf/codeofconduct/>`_.

If you would like to contribute but do not have a bug or feature in mind, feel
free to email Ian and find out how you can help.

The git repository for this project is maintained at
https://github.com/python-hyper/rfc3986

.. _RFC 3986: http://tools.ietf.org/html/rfc3986
.. _Apache License Version 2.0: https://www.apache.org/licenses/LICENSE-2.0



  * [rfc3987-1.3.8](http://pypi.python.org/pypi/rfc3987) This module provides regular expressions according to `RFC 3986 "Uniform
Resource Identifier (URI): Generic Syntax"
<http://tools.ietf.org/html/rfc3986>`_ and `RFC 3987 "Internationalized
Resource Identifiers (IRIs)" <http://tools.ietf.org/html/rfc3987>`_, and
utilities for composition and relative resolution of references.


API
---

**match** (string, rule='IRI_reference')
    Convenience function for checking if `string` matches a specific rule.

    Returns a match object or None::

        >>> assert match('%C7X', 'pct_encoded') is None
        >>> assert match('%C7', 'pct_encoded')
        >>> assert match('%c7', 'pct_encoded')



**parse** (string, rule='IRI_reference')
    Parses `string` according to `rule` into a dict of subcomponents.

    If `rule` is None, parse an IRI_reference `without validation
    <http://tools.ietf.org/html/rfc3986#appendix-B>`_.

    If regex_ is available, any rule is supported; with re_, `rule` must be
    'IRI_reference' or some special case thereof ('IRI', 'absolute_IRI',
    'irelative_ref', 'irelative_part', 'URI_reference', 'URI', 'absolute_URI',
    'relative_ref', 'relative_part'). ::

        >>> d = parse('http://tools.ietf.org/html/rfc3986#appendix-A',
        ...           rule='URI')
        >>> assert all([ d['scheme'] == 'http',
        ...              d['authority'] == 'tools.ietf.org',
        ...              d['path'] == '/html/rfc3986',
        ...              d['query'] == None,
        ...              d['fragment'] == 'appendix-A' ])



**compose** (\*\*parts)
    Returns an URI composed_ from named parts.

    .. _composed: http://tools.ietf.org/html/rfc3986#section-5.3


**resolve** (base, uriref, strict=True, return_parts=False)
    Resolves_ an `URI reference` relative to a `base` URI.

    `Test cases <http://tools.ietf.org/html/rfc3986#section-5.4>`_::

        >>> base = resolve.test_cases_base
        >>> for relative, resolved in resolve.test_cases.items():
        ...     assert resolve(base, relative) == resolved

    If `return_parts` is True, returns a dict of named parts instead of
    a string.

    Examples::

        >>> assert resolve('urn:rootless', '../../name') == 'urn:name'
        >>> assert resolve('urn:root/less', '../../name') == 'urn:/name'
        >>> assert resolve('http://a/b', 'http:g') == 'http:g'
        >>> assert resolve('http://a/b', 'http:g', strict=False) == 'http://a/g'

    .. _Resolves: http://tools.ietf.org/html/rfc3986#section-5.2



**patterns**
    A dict of regular expressions with useful group names.
    Compilable (with regex_ only) without need for any particular compilation
    flag.

**[bmp_][u]patterns[_no_names]**
    Alternative versions of `patterns`.
    [u]nicode strings without group names for the re_ module.
    BMP only for narrow builds.

**get_compiled_pattern** (rule, flags=0)
    Returns a compiled pattern object for a rule name or template string.

    Usage for validation::

        >>> uri = get_compiled_pattern('^%(URI)s$')
        >>> assert uri.match('http://tools.ietf.org/html/rfc3986#appendix-A')
        >>> assert not get_compiled_pattern('^%(relative_ref)s$').match('#f#g')
        >>> from unicodedata import lookup
        >>> smp = 'urn:' + lookup('OLD ITALIC LETTER A')  # U+00010300
        >>> assert not uri.match(smp)
        >>> m = get_compiled_pattern('^%(IRI)s$').match(smp)

    On narrow builds, non-BMP characters are (incorrectly) excluded::

        >>> assert NARROW_BUILD == (not m)

    For parsing, some subcomponents are captured in named groups (*only if*
    regex_ is available, otherwise see `parse`)::

        >>> match = uri.match('http://tools.ietf.org/html/rfc3986#appendix-A')
        >>> d = match.groupdict()
        >>> if REGEX:
        ...     assert all([ d['scheme'] == 'http',
        ...                  d['authority'] == 'tools.ietf.org',
        ...                  d['path'] == '/html/rfc3986',
        ...                  d['query'] == None,
        ...                  d['fragment'] == 'appendix-A' ])

        >>> for r in patterns.keys():
        ...     assert get_compiled_pattern(r)



**format_patterns** (\*\*names)
    Returns a dict of patterns (regular expressions) keyed by
    `rule names for URIs`_ and `rule names for IRIs`_.

    See also the module level dicts of patterns, and `get_compiled_pattern`.

    To wrap a rule in a named capture group, pass it as keyword argument:
    rule_name='group_name'. By default, the formatted patterns contain no
    named groups.

    Patterns are `str` instances (be it in python 2.x or 3.x) containing ASCII
    characters only.

    Caveats:

      - with re_, named capture groups cannot occur on multiple branches of an
        alternation

      - with re_ before python 3.3, ``\u`` and ``\U`` escapes must be
        preprocessed (see `issue3665 <http://bugs.python.org/issue3665>`_)

      - on narrow builds, character ranges beyond BMP are not supported

    .. _rule names for URIs: http://tools.ietf.org/html/rfc3986#appendix-A
    .. _rule names for IRIs: http://tools.ietf.org/html/rfc3987#section-2.2



Dependencies
------------

Some features require regex_.

This package's docstrings are tested on python 2.6, 2.7, and 3.2 to 3.6.
Note that in python<=3.2, characters beyond the Basic Multilingual Plane are
not supported on narrow builds (see `issue12729
<http://bugs.python.org/issue12729>`_).


Release notes
-------------

version 1.3.8:

- fixed deprecated escape sequence

version 1.3.6:

- fixed a bug in IPv6 pattern:

  >>> assert match('::0:0:0:0:0.0.0.0', 'IPv6address')

version 1.3.4:

- allowed for lower case percent encoding

version 1.3.3:

- fixed a bug in `resolve` which left "../" at the beginning of some paths

version 1.3.2:

- convenience function `match`
- patterns restricted to the BMP for narrow builds
- adapted doctests for python 3.3
- compatibility with python 2.6 (thanks to Thijs Janssen)

version 1.3.1:

- some re_ compatibility: get_compiled_pattern, parse
- dropped regex_ from setup.py requirements

version 1.3.0:

- python 3.x compatibility
- format_patterns

version 1.2.1:

- compose, resolve


.. _re: http://docs.python.org/library/re
.. _regex: http://pypi.python.org/pypi/regex


Support
-------
This is free software. You may show your appreciation with a `donation`_.

.. _donation: http://danielgerber.net/¤#Thanks-for-python-package-rfc3987




  * [roman-3.2](https://github.com/zopefoundation/roman) .. image:: https://travis-ci.org/zopefoundation/roman.svg?branch=master
   :target: https://travis-ci.org/zopefoundation/roman

.. image:: https://coveralls.io/repos/github/zopefoundation/roman/badge.svg?branch=master
   :target: https://coveralls.io/github/zopefoundation/roman?branch=master

.. image:: https://img.shields.io/pypi/v/roman.svg
   :target: https://pypi.org/project/roman/
   :alt: Current version on PyPI

.. image:: https://img.shields.io/pypi/pyversions/roman.svg
   :target: https://pypi.org/project/roman/
   :alt: Supported Python versions

roman
=====

Small helper library to convert arabic to roman numerals.
/n/nChange log
==========

3.2 (2019-04-14)
----------------

- expanded test coverage

- Added support for 0 -> N
  (see https://en.wikipedia.org/wiki/Roman_numerals#Zero)

- Added support for Python 3.8


3.1 (2018-10-24)
----------------

- Added support for Python 3.7.


3.0 (2018-05-28)
----------------

- Added support for Python 3.5, 3.6 and PyPy3.

- Dropped support for Python 2.6 and 3.3.


2.0.0 (2013-02-25)
------------------

- Added Python 3.3 and PyPy support.

- Added tests.


1.4.0 (2009-07-23)
------------------

- Initial PyPI release.



  * [rope-0.14.0](https://github.com/python-rope/rope) 
.. _GitHub python-rope / rope: https://github.com/python-rope/rope


========================================
 rope, a python refactoring library ...
========================================


Overview
========

`Rope`_ is a python refactoring library.

.. _`rope`: https://github.com/python-rope/rope


Notes
============

* Nick Smith <nicks@fastmail.fm> takes over maintaining rope. Many thanks to
  Matej Cepl for his work maintaining rope for the past few years!!
* Partial Python3 support, please file bugs and contribute patches if you
  encounter gaps.




  * [rsa-4.0](https://stuvel.eu/rsa) Pure Python RSA implementation
==============================

[![PyPI](https://img.shields.io/pypi/v/rsa.svg)](https://pypi.org/project/rsa/)
[![Build Status](https://travis-ci.org/sybrenstuvel/python-rsa.svg?branch=master)](https://travis-ci.org/sybrenstuvel/python-rsa)
[![Coverage Status](https://coveralls.io/repos/github/sybrenstuvel/python-rsa/badge.svg?branch=master)](https://coveralls.io/github/sybrenstuvel/python-rsa?branch=master)
[![Code Climate](https://img.shields.io/codeclimate/github/sybrenstuvel/python-rsa.svg)](https://codeclimate.com/github/sybrenstuvel/python-rsa)

[Python-RSA](https://stuvel.eu/rsa) is a pure-Python RSA implementation. It supports
encryption and decryption, signing and verifying signatures, and key
generation according to PKCS#1 version 1.5. It can be used as a Python
library as well as on the commandline. The code was mostly written by
Sybren A.  StÃ¼vel.

Documentation can be found at the [Python-RSA homepage](https://stuvel.eu/rsa).

Download and install using:

    pip install rsa

or download it from the [Python Package Index](https://pypi.org/project/rsa/).

The source code is maintained at [GitHub](https://github.com/sybrenstuvel/python-rsa/) and is
licensed under the [Apache License, version 2.0](https://www.apache.org/licenses/LICENSE-2.0)


Major changes in 4.0
--------------------

Version 3.4 was the last version in the 3.x range. Version 4.0 drops the following modules,
as they are insecure:

- `rsa._version133`
- `rsa._version200`
- `rsa.bigfile`
- `rsa.varblock`

Those modules were marked as deprecated in version 3.4.

Furthermore, in 4.0 the I/O functions is streamlined to always work with bytes on all
supported versions of Python.

Version 4.0 drops support for Python 2.6 and 3.3.

  * [ruamel.yaml-0.16.0](https://bitbucket.org/ruamel/yaml) ruamel.yaml
===========

``ruamel.yaml`` is a YAML 1.2 loader/dumper package for Python.

:version:       0.16.5
:updated:       2019-08-18
:documentation: http://yaml.readthedocs.io
:repository:    https://bitbucket.org/ruamel/yaml
:pypi:          https://pypi.org/project/ruamel.yaml/


Starting with version 0.15.0 the way YAML files are loaded and dumped
is changing. See the API doc for details.  Currently existing
functionality will throw a warning before being changed/removed.
**For production systems you should pin the version being used with
``ruamel.yaml<=0.15``**. There might be bug fixes in the 0.14 series,
but new functionality is likely only to be available via the new API.

If your package uses ``ruamel.yaml`` and is not listed on PyPI, drop
me an email, preferably with some information on how you use the
package (or a link to bitbucket/github) and I'll keep you informed
when the status of the API is stable enough to make the transition.

* `Overview <http://yaml.readthedocs.org/en/latest/overview.html>`_
* `Installing <http://yaml.readthedocs.org/en/latest/install.html>`_
* `Basic Usage <http://yaml.readthedocs.org/en/latest/basicuse.html>`_
* `Details <http://yaml.readthedocs.org/en/latest/detail.html>`_
* `Examples <http://yaml.readthedocs.org/en/latest/example.html>`_
* `API <http://yaml.readthedocs.org/en/latest/api.html>`_
* `Differences with PyYAML <http://yaml.readthedocs.org/en/latest/pyyaml.html>`_

.. image:: https://readthedocs.org/projects/yaml/badge/?version=stable
   :target: https://yaml.readthedocs.org/en/stable

.. image:: https://bestpractices.coreinfrastructure.org/projects/1128/badge
   :target: https://bestpractices.coreinfrastructure.org/projects/1128

.. image:: https://bitbucket.org/ruamel/yaml/raw/default/_doc/_static/license.svg
   :target: https://opensource.org/licenses/MIT

.. image:: https://bitbucket.org/ruamel/yaml/raw/default/_doc/_static/pypi.svg
   :target: https://pypi.org/project/ruamel.yaml/

.. image:: https://bitbucket.org/ruamel/oitnb/raw/default/_doc/_static/oitnb.svg
   :target: https://pypi.org/project/oitnb/

.. image:: http://www.mypy-lang.org/static/mypy_badge.svg
   :target: http://mypy-lang.org/

ChangeLog
=========

.. should insert NEXT: at the beginning of line for next key (with empty line)

0.16.5 (2019-08-18):
  - allow for ``YAML(typ=['unsafe', 'pytypes'])``

0.16.4 (2019-08-16):
  - fix output of TAG directives with # (reported by `Thomas Smith
    <https://bitbucket.org/%7Bd4c57a72-f041-4843-8217-b4d48b6ece2f%7D/>`__)


0.16.3 (2019-08-15):
  - split construct_object
  - change stuff back to keep mypy happy
  - move setting of version based on YAML directive to scanner, allowing to
    check for file version during TAG directive scanning

0.16.2 (2019-08-15):
  - preserve YAML and TAG directives on roundtrip, correctly output #
    in URL for YAML 1.2 (both reported by `Thomas Smith
    <https://bitbucket.org/%7Bd4c57a72-f041-4843-8217-b4d48b6ece2f%7D/>`__)

0.16.1 (2019-08-08):
  - Force the use of new version of ruamel.yaml.clib (reported by `Alex Joz
    <https://bitbucket.org/%7B9af55900-2534-4212-976c-61339b6ffe14%7D/>`__)
  - Allow '#' in tag URI as these are allowed in YAML 1.2 (reported by
    `Thomas Smith
    <https://bitbucket.org/%7Bd4c57a72-f041-4843-8217-b4d48b6ece2f%7D/>`__)

0.16.0 (2019-07-25):
  - split of C source that generates .so file to ruamel.yaml.clib
  - duplicate keys are now an error when working with the old API as well

0.15.100 (2019-07-17):
  - fixing issue with dumping deep-copied data from commented YAML, by
    providing both the memo parameter to __deepcopy__, and by allowing
    startmarks to be compared on their content (reported by `Theofilos
    Petsios
    <https://bitbucket.org/%7Be550bc5d-403d-4fda-820b-bebbe71796d3%7D/>`__)

0.15.99 (2019-07-12):
  - add `py.typed` to distribution, based on a PR submitted by
    `Michael Crusoe
    <https://bitbucket.org/%7Bc9fbde69-e746-48f5-900d-34992b7860c8%7D/>`__
  - merge PR 40 (also by Michael Crusoe) to more accurately specify
    repository in the README (also reported in a misunderstood issue
    some time ago)

0.15.98 (2019-07-09):
  - regenerate ext/_ruamel_yaml.c with Cython version 0.29.12, needed
    for Python 3.8.0b2 (reported by `John Vandenberg
    <https://bitbucket.org/%7B6d4e8487-3c97-4dab-a060-088ec50c682c%7D/>`__)

0.15.97 (2019-06-06):
  - regenerate ext/_ruamel_yaml.c with Cython version 0.29.10, needed for
    Python 3.8.0b1
  - regenerate ext/_ruamel_yaml.c with Cython version 0.29.9, needed for
    Python 3.8.0a4 (reported by `Anthony Sottile
    <https://bitbucket.org/%7B569cc8ea-0d9e-41cb-94a4-19ea517324df%7D/>`__)

0.15.96 (2019-05-16):
  - fix failure to indent comments on round-trip anchored block style
    scalars in block sequence (reported by `William Kimball
    <https://bitbucket.org/%7Bba35ed20-4bb0-46f8-bb5d-c29871e86a22%7D/>`__)

0.15.95 (2019-05-16):
  - fix failure to round-trip anchored scalars in block sequence
    (reported by `William Kimball
    <https://bitbucket.org/%7Bba35ed20-4bb0-46f8-bb5d-c29871e86a22%7D/>`__)
  - wheel files for Python 3.4 no longer provided (`Python 3.4 EOL 2019-03-18
    <https://www.python.org/dev/peps/pep-0429/>`__)

0.15.94 (2019-04-23):
  - fix missing line-break after end-of-file comments not ending in
    line-break (reported by `Philip Thompson
    <https://bitbucket.org/%7Be42ba205-0876-4151-bcbe-ccaea5bd13ce%7D/>`__)

0.15.93 (2019-04-21):
  - fix failure to parse empty implicit flow mapping key
  - in YAML 1.1 plains scalars `y`, 'n', `Y`, and 'N' are now
    correctly recognised as booleans and such strings dumped quoted
    (reported by `Marcel Bollmann
    <https://bitbucket.org/%7Bd8850921-9145-4ad0-ac30-64c3bd9b036d%7D/>`__)

0.15.92 (2019-04-16):
  - fix failure to parse empty implicit block mapping key (reported by 
    `Nolan W <https://bitbucket.org/i2labs/>`__)

0.15.91 (2019-04-05):
  - allowing duplicate keys would not work for merge keys (reported by mamacdon on
    `StackOverflow <https://stackoverflow.com/questions/55540686/>`__ 

0.15.90 (2019-04-04):
  - fix issue with updating `CommentedMap` from list of tuples (reported by 
    `Peter Henry <https://bitbucket.org/mosbasik/>`__)

0.15.89 (2019-02-27):
  - fix for items with flow-mapping in block sequence output on single line
    (reported by `Zahari Dim <https://bitbucket.org/zahari_dim/>`__)
  - fix for safe dumping erroring in creation of representereror when dumping namedtuple
    (reported and solution by `Jaakko Kantojärvi <https://bitbucket.org/raphendyr/>`__)

0.15.88 (2019-02-12):
  - fix inclusing of python code from the subpackage data (containing extra tests,
    reported by `Florian Apolloner <https://bitbucket.org/apollo13/>`__)

0.15.87 (2019-01-22):
  - fix problem with empty lists and the code to reinsert merge keys (reported via email 
     by Zaloo)

0.15.86 (2019-01-16):
  - reinsert merge key in its old position (reported by grumbler on
    `StackOverflow <https://stackoverflow.com/a/54206512/1307905>`__)
  - fix for issue with non-ASCII anchor names (reported and fix
    provided by Dandaleon Flux via email)
  - fix for issue when parsing flow mapping value starting with colon (in pure Python only)
    (reported by `FichteFoll <https://bitbucket.org/FichteFoll/>`__)

0.15.85 (2019-01-08):
  - the types used by ``SafeConstructor`` for mappings and sequences can
    now by set by assigning to ``XXXConstructor.yaml_base_dict_type``
    (and ``..._list_type``), preventing the need to copy two methods
    with 50+ lines that had ``var = {}`` hardcoded.  (Implemented to
    help solve an feature request by `Anthony Sottile
    <https://bitbucket.org/asottile/>`__ in an easier way)

0.15.84 (2019-01-07):
  - fix for ``CommentedMap.copy()`` not returning ``CommentedMap``, let alone copying comments etc.
    (reported by `Anthony Sottile <https://bitbucket.org/asottile/>`__)

0.15.83 (2019-01-02):
  - fix for bug in roundtripping aliases used as key (reported via email by Zaloo)

0.15.82 (2018-12-28):
  - anchors and aliases on scalar int, float, string and bool are now preserved. Anchors
    do not need a referring alias for these (reported by 
    `Alex Harvey <https://bitbucket.org/alexharv074/>`__)
  - anchors no longer lost on tagged objects when roundtripping (reported by `Zaloo 
    <https://bitbucket.org/zaloo/>`__)

0.15.81 (2018-12-06):
  - fix issue dumping methods of metaclass derived classes (reported and fix provided
    by `Douglas Raillard <https://bitbucket.org/DouglasRaillard/>`__)

0.15.80 (2018-11-26):
  - fix issue emitting BEL character when round-tripping invalid folded input
    (reported by Isaac on `StackOverflow <https://stackoverflow.com/a/53471217/1307905>`__)
    
0.15.79 (2018-11-21):
  - fix issue with anchors nested deeper than alias (reported by gaFF on
    `StackOverflow <https://stackoverflow.com/a/53397781/1307905>`__)

0.15.78 (2018-11-15):
  - fix setup issue for 3.8 (reported by `Sidney Kuyateh 
    <https://bitbucket.org/autinerd/>`__)

0.15.77 (2018-11-09):
  - setting `yaml.sort_base_mapping_type_on_output = False`, will prevent
    explicit sorting by keys in the base representer of mappings. Roundtrip
    already did not do this. Usage only makes real sense for Python 3.6+
    (feature request by `Sebastian Gerber <https://bitbucket.org/spacemanspiff2007/>`__).
  - implement Python version check in YAML metadata in ``_test/test_z_data.py``

0.15.76 (2018-11-01):
  - fix issue with empty mapping and sequence loaded as flow-style
    (mapping reported by `Min RK <https://bitbucket.org/minrk/>`__, sequence
    by `Maged Ahmed <https://bitbucket.org/maged2/>`__)

0.15.75 (2018-10-27):
  - fix issue with single '?' scalar (reported by `Terrance 
    <https://bitbucket.org/OllieTerrance/>`__)
  - fix issue with duplicate merge keys (prompted by `answering 
    <https://stackoverflow.com/a/52852106/1307905>`__ a 
    `StackOverflow question <https://stackoverflow.com/q/52851168/1307905>`__
    by `math <https://stackoverflow.com/users/1355634/math>`__)

0.15.74 (2018-10-17):
  - fix dropping of comment on rt before sequence item that is sequence item
    (reported by `Thorsten Kampe <https://bitbucket.org/thorstenkampe/>`__)

0.15.73 (2018-10-16):
  - fix irregular output on pre-comment in sequence within sequence (reported
    by `Thorsten Kampe <https://bitbucket.org/thorstenkampe/>`__)
  - allow non-compact (i.e. next line) dumping sequence/mapping within sequence.

0.15.72 (2018-10-06):
  - fix regression on explicit 1.1 loading with the C based scanner/parser
    (reported by `Tomas Vavra <https://bitbucket.org/xtomik/>`__)

0.15.71 (2018-09-26):
  - some of the tests now live in YAML files in the 
    `yaml.data <https://bitbucket.org/ruamel/yaml.data>`__ repository. 
    ``_test/test_z_data.py`` processes these.
  - fix regression where handcrafted CommentedMaps could not be initiated (reported by 
    `Dan Helfman <https://bitbucket.org/dhelfman/>`__)
  - fix regression with non-root literal scalars that needed indent indicator
    (reported by `Clark Breyman <https://bitbucket.org/clarkbreyman/>`__)
  - tag:yaml.org,2002:python/object/apply now also uses __qualname__ on PY3
    (reported by `Douglas RAILLARD <https://bitbucket.org/DouglasRaillard/>`__)
  - issue with self-referring object creation
    (reported and fix by `Douglas RAILLARD <https://bitbucket.org/DouglasRaillard/>`__)

0.15.70 (2018-09-21):
  - reverted CommentedMap and CommentedSeq to subclass ordereddict resp. list,
    reimplemented merge maps so that both ``dict(**commented_map_instance)`` and JSON
    dumping works. This also allows checking with ``isinstance()`` on ``dict`` resp. ``list``.
    (Proposed by `Stuart Berg <https://bitbucket.org/stuarteberg/>`__, with feedback
    from `blhsing <https://stackoverflow.com/users/6890912/blhsing>`__ on
    `StackOverflow <https://stackoverflow.com/q/52314186/1307905>`__)

0.15.69 (2018-09-20):
  - fix issue with dump_all gobbling end-of-document comments on parsing
    (reported by `Pierre B. <https://bitbucket.org/octplane/>`__)

0.15.68 (2018-09-20):
  - fix issue with parsabel, but incorrect output with nested flow-style sequences
    (reported by `Dougal Seeley <https://bitbucket.org/dseeley/>`__)
  - fix issue with loading Python objects that have __setstate__ and recursion in parameters
    (reported by `Douglas RAILLARD <https://bitbucket.org/DouglasRaillard/>`__)

0.15.67 (2018-09-19):
  - fix issue with extra space inserted with non-root literal strings 
    (Issue reported and PR with fix provided by 
    `Naomi Seyfer <https://bitbucket.org/sixolet/>`__.)

0.15.66 (2018-09-07):
  - fix issue with fold indicating characters inserted in safe_load-ed folded strings
    (reported by `Maximilian Hils <https://bitbucket.org/mhils/>`__).

0.15.65 (2018-09-07):
  - fix issue #232 revert to throw ParserError for unexcpected ``]``
    and ``}`` instead of IndexError. (Issue reported and PR with fix
    provided by `Naomi Seyfer <https://bitbucket.org/sixolet/>`__.)
  - added ``key`` and ``reverse`` parameter (suggested by Jannik Klemm via email)
  - indent root level literal scalars that have directive or document end markers
    at the beginning of a line

0.15.64 (2018-08-30):
  - support round-trip of tagged sequences: ``!Arg [a, {b: 1}]``
  - single entry mappings in flow sequences now written by default without braces,
    set ``yaml.brace_single_entry_mapping_in_flow_sequence=True`` to force
    getting ``[a, {b: 1}, {c: {d: 2}}]`` instead of the default ``[a, b: 1, c: {d: 2}]``
  - fix issue when roundtripping floats starting with a dot such as ``.5``
    (reported by `Harrison Gregg <https://bitbucket.org/HarrisonGregg/>`__)

0.15.63 (2018-08-29):
  - small fix only necessary for Windows users that don't use wheels.

0.15.62 (2018-08-29):
  - C based reader/scanner & emitter now allow setting of 1.2 as YAML version.
    ** The loading/dumping is still YAML 1.1 code**, so use the common subset of
    YAML 1.2 and 1.1 (reported by `Ge Yang <https://bitbucket.org/yangge/>`__)

0.15.61 (2018-08-23):
  - support for round-tripping folded style scalars (initially requested 
    by `Johnathan Viduchinsky <https://bitbucket.org/johnathanvidu/>`__)
  - update of C code
  - speed up of scanning (~30% depending on the input)

0.15.60 (2018-08-18):
  - again allow single entry map in flow sequence context (reported by 
    `Lee Goolsbee <https://bitbucket.org/lgoolsbee/>`__)
  - cleanup for mypy 
  - spurious print in library (reported by 
    `Lele Gaifax <https://bitbucket.org/lele/>`__), now automatically checked 

0.15.59 (2018-08-17):
  - issue with C based loader and leading zeros (reported by 
    `Tom Hamilton Stubber <https://bitbucket.org/TomHamiltonStubber/>`__)

0.15.58 (2018-08-17):
  - simple mappings can now be used as keys when round-tripping::

      {a: 1, b: 2}: hello world
      
    although using the obvious operations (del, popitem) on the key will
    fail, you can mutilate it by going through its attributes. If you load the
    above YAML in `d`, then changing the value is cumbersome:

        d = {CommentedKeyMap([('a', 1), ('b', 2)]): "goodbye"}

    and changing the key even more so:

        d[CommentedKeyMap([('b', 1), ('a', 2)])] = d.pop(
                     CommentedKeyMap([('a', 1), ('b', 2)]))

    (you can use a `dict` instead of a list of tuples (or ordereddict), but that might result
    in a different order, of the keys of the key, in the output)
  - check integers to dump with 1.2 patterns instead of 1.1 (reported by 
    `Lele Gaifax <https://bitbucket.org/lele/>`__)
  

0.15.57 (2018-08-15):
  - Fix that CommentedSeq could no longer be used in adding or do a sort
    (reported by `Christopher Wright <https://bitbucket.org/CJ-Wright4242/>`__)

0.15.56 (2018-08-15):
  - fix issue with ``python -O`` optimizing away code (reported, and detailed cause
    pinpointed, by `Alex Grönholm <https://bitbucket.org/agronholm/>`__)

0.15.55 (2018-08-14):
  - unmade ``CommentedSeq`` a subclass of ``list``. It is now
    indirectly a subclass of the standard
    ``collections.abc.MutableSequence`` (without .abc if you are
    still on Python2.7). If you do ``isinstance(yaml.load('[1, 2]'),
    list)``) anywhere in your code replace ``list`` with
    ``MutableSequence``.  Directly, ``CommentedSeq`` is a subclass of
    the abstract baseclass ``ruamel.yaml.compat.MutableScliceableSequence``,
    with the result that *(extended) slicing is supported on 
    ``CommentedSeq``*.
    (reported by `Stuart Berg <https://bitbucket.org/stuarteberg/>`__)
  - duplicate keys (or their values) with non-ascii now correctly
    report in Python2, instead of raising a Unicode error.
    (Reported by `Jonathan Pyle <https://bitbucket.org/jonathan_pyle/>`__)

0.15.54 (2018-08-13):
  - fix issue where a comment could pop-up twice in the output (reported by 
    `Mike Kazantsev <https://bitbucket.org/mk_fg/>`__ and by 
    `Nate Peterson <https://bitbucket.org/ndpete21/>`__)
  - fix issue where JSON object (mapping) without spaces was not parsed
    properly (reported by `Marc Schmidt <https://bitbucket.org/marcj/>`__)
  - fix issue where comments after empty flow-style mappings were not emitted
    (reported by `Qinfench Chen <https://bitbucket.org/flyin5ish/>`__)

0.15.53 (2018-08-12):
  - fix issue with flow style mapping with comments gobbled newline (reported
    by `Christopher Lambert <https://bitbucket.org/XN137/>`__)
  - fix issue where single '+' under YAML 1.2 was interpreted as
    integer, erroring out (reported by `Jethro Yu
    <https://bitbucket.org/jcppkkk/>`__)

0.15.52 (2018-08-09):
  - added `.copy()` mapping representation for round-tripping
    (``CommentedMap``) to fix incomplete copies of merged mappings
    (reported by `Will Richards
    <https://bitbucket.org/will_richards/>`__) 
  - Also unmade that class a subclass of ordereddict to solve incorrect behaviour
    for ``{**merged-mapping}`` and ``dict(**merged-mapping)`` (reported independently by
    `Tim Olsson <https://bitbucket.org/tgolsson/>`__ and 
    `Filip Matzner <https://bitbucket.org/FloopCZ/>`__)

0.15.51 (2018-08-08):
  - Fix method name dumps (were not dotted) and loads (reported by `Douglas Raillard 
    <https://bitbucket.org/DouglasRaillard/>`__)
  - Fix spurious trailing white-space caused when the comment start
    column was no longer reached and there was no actual EOL comment
    (e.g. following empty line) and doing substitutions, or when
    quotes around scalars got dropped.  (reported by `Thomas Guillet
    <https://bitbucket.org/guillett/>`__)

0.15.50 (2018-08-05):
  - Allow ``YAML()`` as a context manager for output, thereby making it much easier
    to generate multi-documents in a stream. 
  - Fix issue with incorrect type information for `load()` and `dump()` (reported 
    by `Jimbo Jim <https://bitbucket.org/jimbo1qaz/>`__)

0.15.49 (2018-08-05):
  - fix preservation of leading newlines in root level literal style scalar,
    and preserve comment after literal style indicator (``|  # some comment``)
    Both needed for round-tripping multi-doc streams in 
    `ryd <https://pypi.org/project/ryd/>`__.

0.15.48 (2018-08-03):
  - housekeeping: ``oitnb`` for formatting, mypy 0.620 upgrade and conformity

0.15.47 (2018-07-31):
  - fix broken 3.6 manylinux1, the result of an unclean ``build`` (reported by 
    `Roman Sichnyi <https://bitbucket.org/rsichnyi-gl/>`__)


0.15.46 (2018-07-29):
  - fixed DeprecationWarning for importing from ``collections`` on 3.7
    (issue 210, reported by `Reinoud Elhorst
    <https://bitbucket.org/reinhrst/>`__). It was `difficult to find
    why tox/pytest did not report
    <https://stackoverflow.com/q/51573204/1307905>`__ and as time
    consuming to actually `fix
    <https://stackoverflow.com/a/51573205/1307905>`__ the tests.

0.15.45 (2018-07-26):
  - After adding failing test for ``YAML.load_all(Path())``, remove StopIteration 
    (PR provided by `Zachary Buhman <https://bitbucket.org/buhman/>`__,
    also reported by `Steven Hiscocks <https://bitbucket.org/sdhiscocks/>`__.

0.15.44 (2018-07-14):
  - Correct loading plain scalars consisting of numerals only and
    starting with `0`, when not explicitly specifying YAML version
    1.1. This also fixes the issue about dumping string `'019'` as
    plain scalars as reported by `Min RK
    <https://bitbucket.org/minrk/>`__, that prompted this chance.

0.15.43 (2018-07-12):
  - merge PR33: Python2.7 on Windows is narrow, but has no
    ``sysconfig.get_config_var('Py_UNICODE_SIZE')``. (merge provided by
    `Marcel Bargull <https://bitbucket.org/mbargull/>`__)
  - ``register_class()`` now returns class (proposed by
    `Mike Nerone <https://bitbucket.org/Manganeez/>`__}

0.15.42 (2018-07-01):
  - fix regression showing only on narrow Python 2.7 (py27mu) builds
    (with help from
    `Marcel Bargull <https://bitbucket.org/mbargull/>`__ and
    `Colm O'Connor <https://bitbucket.org/colmoconnorgithub/>`__).
  - run pre-commit ``tox`` on Python 2.7 wide and narrow, as well as
    3.4/3.5/3.6/3.7/pypy

0.15.41 (2018-06-27):
  - add detection of C-compile failure (investigation prompted by
    `StackOverlow <https://stackoverflow.com/a/51057399/1307905>`__ by
    `Emmanuel Blot <https://stackoverflow.com/users/8233409/emmanuel-blot>`__),
    which was removed while no longer dependent on ``libyaml``, C-extensions
    compilation still needs a compiler though.

0.15.40 (2018-06-18):
  - added links to landing places as suggested in issue 190 by
    `KostisA <https://bitbucket.org/ankostis/>`__
  - fixes issue #201: decoding unicode escaped tags on Python2, reported
    by `Dan Abolafia <https://bitbucket.org/danabo/>`__

0.15.39 (2018-06-17):
  - merge PR27 improving package startup time (and loading when regexp not
    actually used), provided by
    `Marcel Bargull <https://bitbucket.org/mbargull/>`__

0.15.38 (2018-06-13):
  - fix for losing precision when roundtripping floats by
    `Rolf Wojtech <https://bitbucket.org/asomov/>`__
  - fix for hardcoded dir separator not working for Windows by
    `Nuno André <https://bitbucket.org/nu_no/>`__
  - typo fix by `Andrey Somov <https://bitbucket.org/asomov/>`__

0.15.37 (2018-03-21):
  - again trying to create installable files for 187

0.15.36 (2018-02-07):
  - fix issue 187, incompatibility of C extension with 3.7 (reported by
    Daniel Blanchard)

0.15.35 (2017-12-03):
  - allow ``None`` as stream when specifying ``transform`` parameters to
    ``YAML.dump()``.
    This is useful if the transforming function doesn't return a meaningful value
    (inspired by `StackOverflow <https://stackoverflow.com/q/47614862/1307905>`__ by
    `rsaw <https://stackoverflow.com/users/406281/rsaw>`__).

0.15.34 (2017-09-17):
  - fix for issue 157: CDumper not dumping floats (reported by Jan Smitka)

0.15.33 (2017-08-31):
  - support for "undefined" round-tripping tagged scalar objects (in addition to
    tagged mapping object). Inspired by a use case presented by Matthew Patton
    on `StackOverflow <https://stackoverflow.com/a/45967047/1307905>`__.
  - fix issue 148: replace cryptic error message when using !!timestamp with an
    incorrectly formatted or non- scalar. Reported by FichteFoll.

0.15.32 (2017-08-21):
  - allow setting ``yaml.default_flow_style = None`` (default: ``False``) for
    for ``typ='rt'``.
  - fix for issue 149: multiplications on ``ScalarFloat`` now return ``float``
    (reported by jan.brezina@tul.cz)

0.15.31 (2017-08-15):
  - fix Comment dumping

0.15.30 (2017-08-14):
  - fix for issue with "compact JSON" not parsing: ``{"in":{},"out":{}}``
    (reported on `StackOverflow <https://stackoverflow.com/q/45681626/1307905>`__ by
    `mjalkio <https://stackoverflow.com/users/5130525/mjalkio>`_

0.15.29 (2017-08-14):
  - fix issue #51: different indents for mappings and sequences (reported by
    Alex Harvey)
  - fix for flow sequence/mapping as element/value of block sequence with
    sequence-indent minus dash-offset not equal two.

0.15.28 (2017-08-13):
  - fix issue #61: merge of merge cannot be __repr__-ed (reported by Tal Liron)

0.15.27 (2017-08-13):
  - fix issue 62, YAML 1.2 allows ``?`` and ``:`` in plain scalars if non-ambigious
    (reported by nowox)
  - fix lists within lists which would make comments disappear

0.15.26 (2017-08-10):
  - fix for disappearing comment after empty flow sequence (reported by
    oit-tzhimmash)

0.15.25 (2017-08-09):
  - fix for problem with dumping (unloaded) floats (reported by eyenseo)

0.15.24 (2017-08-09):
  - added ScalarFloat which supports roundtripping of 23.1, 23.100,
    42.00E+56, 0.0, -0.0 etc. while keeping the format. Underscores in mantissas
    are not preserved/supported (yet, is anybody using that?).
  - (finally) fixed longstanding issue 23 (reported by `Antony Sottile
    <https://bitbucket.org/asottile/>`__), now handling comment between block
    mapping key and value correctly
  - warn on YAML 1.1 float input that is incorrect (triggered by invalid YAML
    provided by Cecil Curry)
  - allow setting of boolean representation (`false`, `true`) by using:
    ``yaml.boolean_representation = [u'False', u'True']``

0.15.23 (2017-08-01):
  - fix for round_tripping integers on 2.7.X > sys.maxint (reported by ccatterina)

0.15.22 (2017-07-28):
  - fix for round_tripping singe excl. mark tags doubling (reported and fix by Jan Brezina)

0.15.21 (2017-07-25):
  - fix for writing unicode in new API, (reported on
    `StackOverflow <https://stackoverflow.com/a/45281922/1307905>`__

0.15.20 (2017-07-23):
  - wheels for windows including C extensions

0.15.19 (2017-07-13):
  - added object constructor for rt, decorator ``yaml_object`` to replace YAMLObject.
  - fix for problem using load_all with Path() instance
  - fix for load_all in combination with zero indent block style literal
    (``pure=True`` only!)

0.15.18 (2017-07-04):
  - missing ``pure`` attribute on ``YAML`` useful for implementing `!include` tag
    constructor for `including YAML files in a YAML file
    <https://stackoverflow.com/a/44913652/1307905>`__
  - some documentation improvements
  - trigger of doc build on new revision

0.15.17 (2017-07-03):
  - support for Unicode supplementary Plane **output**
    (input was already supported, triggered by
    `this <https://stackoverflow.com/a/44875714/1307905>`__ Stack Overflow Q&A)

0.15.16 (2017-07-01):
  - minor typing issues (reported and fix provided by
    `Manvendra Singh <https://bitbucket.org/manu-chroma/>`__
  - small doc improvements

0.15.15 (2017-06-27):
  - fix for issue 135, typ='safe' not dumping in Python 2.7
    (reported by Andrzej Ostrowski <https://bitbucket.org/aostr123/>`__)

0.15.14 (2017-06-25):
  - fix for issue 133, in setup.py: change ModuleNotFoundError to
    ImportError (reported and fix by
    `Asley Drake  <https://github.com/aldraco>`__)

0.15.13 (2017-06-24):
  - suppress duplicate key warning on mappings with merge keys (reported by
    Cameron Sweeney)

0.15.12 (2017-06-24):
  - remove fatal dependency of setup.py on wheel package (reported by
    Cameron Sweeney)

0.15.11 (2017-06-24):
  - fix for issue 130, regression in nested merge keys (reported by
    `David Fee <https://bitbucket.org/dfee/>`__)

0.15.10 (2017-06-23):
  - top level PreservedScalarString not indented if not explicitly asked to
  - remove Makefile (not very useful anyway)
  - some mypy additions

0.15.9 (2017-06-16):
  - fix for issue 127: tagged scalars were always quoted and seperated
    by a newline when in a block sequence (reported and largely fixed by
    `Tommy Wang <https://bitbucket.org/twang817/>`__)

0.15.8 (2017-06-15):
  - allow plug-in install via ``install ruamel.yaml[jinja2]``

0.15.7 (2017-06-14):
  - add plug-in mechanism for load/dump pre resp. post-processing

0.15.6 (2017-06-10):
  - a set() with duplicate elements now throws error in rt loading
  - support for toplevel column zero literal/folded scalar in explicit documents

0.15.5 (2017-06-08):
  - repeat `load()` on a single `YAML()` instance would fail.

0.15.4 (2017-06-08):
  - `transform` parameter on dump that expects a function taking a
    string and returning a string. This allows transformation of the output
    before it is written to stream. This forces creation of the complete output in memory!
  - some updates to the docs

0.15.3 (2017-06-07):
  - No longer try to compile C extensions on Windows. Compilation can be forced by setting
    the environment variable `RUAMEL_FORCE_EXT_BUILD` to some value
    before starting the `pip install`.

0.15.2 (2017-06-07):
  - update to conform to mypy 0.511: mypy --strict

0.15.1 (2017-06-07):
  - `duplicate keys  <http://yaml.readthedocs.io/en/latest/api.html#duplicate-keys>`__
    in mappings generate an error (in the old API this change generates a warning until 0.16)
  - dependecy on ruamel.ordereddict for 2.7 now via extras_require

0.15.0 (2017-06-04):
  - it is now allowed to pass in a ``pathlib.Path`` as "stream" parameter to all
    load/dump functions
  - passing in a non-supported object (e.g. a string) as "stream" will result in a
    much more meaningful YAMLStreamError.
  - assigning a normal string value to an existing CommentedMap key or CommentedSeq
    element will result in a value cast to the previous value's type if possible.
  - added ``YAML`` class for new API

0.14.12 (2017-05-14):
  - fix for issue 119, deepcopy not returning subclasses (reported and PR by
    Constantine Evans <cevans@evanslabs.org>)

0.14.11 (2017-05-01):
  - fix for issue 103 allowing implicit documents after document end marker line (``...``)
    in YAML 1.2

0.14.10 (2017-04-26):
  - fix problem with emitting using cyaml

0.14.9 (2017-04-22):
  - remove dependency on ``typing`` while still supporting ``mypy``
    (http://stackoverflow.com/a/43516781/1307905)
  - fix unclarity in doc that stated 2.6 is supported (reported by feetdust)

0.14.8 (2017-04-19):
  - fix Text not available on 3.5.0 and 3.5.1, now proactively setting version guards
    on all files (reported by `João Paulo Magalhães <https://bitbucket.org/jpmag/>`__)

0.14.7 (2017-04-18):
  - round trip of integers (decimal, octal, hex, binary) now preserve
    leading zero(s) padding and underscores. Underscores are presumed
    to be at regular distances (i.e. ``0o12_345_67`` dumps back as
    ``0o1_23_45_67`` as the space from the last digit to the
    underscore before that is the determining factor).

0.14.6 (2017-04-14):
  - binary, octal and hex integers are now preserved by default. This
    was a known deficiency. Working on this was prompted by the issue report (112)
    from devnoname120, as well as the additional experience with `.replace()`
    on `scalarstring` classes.
  - fix issues 114: cannot install on Buildozer (reported by mixmastamyk).
    Setting env. var ``RUAMEL_NO_PIP_INSTALL_CHECK`` will suppress ``pip``-check.

0.14.5 (2017-04-04):
  - fix issue 109: None not dumping correctly at top level (reported by Andrea Censi)
  - fix issue 110: .replace on Preserved/DoubleQuoted/SingleQuoted ScalarString
    would give back "normal" string (reported by sandres23)

0.14.4 (2017-03-31):
  - fix readme

0.14.3 (2017-03-31):
  - fix for 0o52 not being a string in YAML 1.1 (reported on
    `StackOverflow Q&A 43138503 <http://stackoverflow.com/a/43138503/1307905>`__ by
    `Frank D <http://stackoverflow.com/users/7796630/frank-d>`__)

0.14.2 (2017-03-23):
  - fix for old default pip on Ubuntu 14.04 (reported by Sébastien Maccagnoni-Munch)

0.14.1 (2017-03-22):
  - fix Text not available on 3.5.0 and 3.5.1 (reported by Charles Bouchard-Légaré)

0.14.0 (2017-03-21):
  - updates for mypy --strict
  - preparation for moving away from inheritance in Loader and Dumper, calls from e.g.
    the Representer to the Serializer.serialize() are now done via the attribute
    .serializer.serialize(). Usage of .serialize() outside of Serializer will be
    deprecated soon
  - some extra tests on main.py functions

----

For older changes see the file
`CHANGES <https://bitbucket.org/ruamel/yaml/src/default/CHANGES>`_
  * [ruffus-2.8.1](http://www.ruffus.org.uk) 
***************************************
Overview
***************************************


    The Ruffus module is a lightweight way to add support
    for running computational pipelines.

    Computational pipelines are often conceptually quite simple, especially
    if we breakdown the process into simple stages, or separate **tasks**.

    Each stage or **task** in a computational pipeline is represented by a python function
    Each python function can be called in parallel to run multiple **jobs**.

    Ruffus was originally designed for use in bioinformatics to analyse multiple genome
    data sets.

***************************************
Documentation
***************************************

    Ruffus documentation can be found `here <http://www.ruffus.org.uk>`__ ,
    with `download notes <http://www.ruffus.org.uk/installation.html>`__ ,
    a `tutorial <http://www.ruffus.org.uk/tutorials/new_tutorial/introduction.html>`__ and
    an `in-depth manual <http://www.ruffus.org.uk/tutorials/new_tutorial/manual_contents.html>`__ .


***************************************
Background
***************************************

    The purpose of a pipeline is to determine automatically which parts of a multi-stage
    process needs to be run and in what order in order to reach an objective ("targets")

    Computational pipelines, especially for analysing large scientific datasets are
    in widespread use.
    However, even a conceptually simple series of steps can be difficult to set up and
    maintain.

***************************************
Design
***************************************
    The ruffus module has the following design goals:

        * Lightweight
        * Scalable / Flexible / Powerful
        * Standard Python
        * Unintrusive
        * As simple as possible

***************************************
Features
***************************************

    Automatic support for

        * Managing dependencies
        * Parallel jobs, including dispatching work to computational clusters
        * Re-starting from arbitrary points, especially after errors (checkpointing)
        * Display of the pipeline as a flowchart
        * Managing complex pipeline topologies


***************************************
A Simple example
***************************************

        Use the **@follows(...)** python decorator before the function definitions::

            from ruffus import *
            import sys

            def first_task():
                print "First task"

            @follows(first_task)
            def second_task():
                print "Second task"

            @follows(second_task)
            def final_task():
                print "Final task"




        the ``@follows`` decorator indicate that the ``first_task`` function precedes ``second_task`` in
        the pipeline.

        The canonical Ruffus decorator is ``@transform`` which **transforms** data flowing down a
        computational pipeline from one stage to teh next.

********
Usage
********

    Each stage or **task** in a computational pipeline is represented by a python function
    Each python function can be called in parallel to run multiple **jobs**.

    1. Import module::

            import ruffus


    1. Annotate functions with python decorators

    2. Print dependency graph if you necessary

        - For a graphical flowchart in ``jpg``, ``svg``, ``dot``, ``png``, ``ps``, ``gif`` formats::

            pipeline_printout_graph ("flowchart.svg")

        This requires ``dot`` to be installed

        - For a text printout of all jobs ::

            pipeline_printout(sys.stdout)


    3. Run the pipeline::

        pipeline_run()



  * [s3fs-0.3.1](http://github.com/dask/s3fs/) s3fs
====

|Build Status| |Doc Status|

S3FS builds on boto3_ to provide a convenient Python filesystem interface for S3.

View the documentation_ for s3fs.

.. _documentation: http://s3fs.readthedocs.io/en/latest/
.. _boto3: https://boto3.readthedocs.io/en/latest/

.. |Build Status| image:: https://travis-ci.org/dask/s3fs.svg?branch=master
    :target: https://travis-ci.org/dask/s3fs
    :alt: Build Status
.. |Doc Status| image:: http://readthedocs.io/projects/s3fs/badge/?version=latest
    :target: http://s3fs.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status
  * [scandir-1.10.0](https://github.com/benhoyt/scandir) scandir, a better directory iterator and faster os.walk()
=========================================================

.. image:: https://img.shields.io/pypi/v/scandir.svg
   :target: https://pypi.python.org/pypi/scandir
   :alt: scandir on PyPI (Python Package Index)

.. image:: https://travis-ci.org/benhoyt/scandir.svg?branch=master
   :target: https://travis-ci.org/benhoyt/scandir
   :alt: Travis CI tests (Linux)

.. image:: https://ci.appveyor.com/api/projects/status/github/benhoyt/scandir?branch=master&svg=true
   :target: https://ci.appveyor.com/project/benhoyt/scandir
   :alt: Appveyor tests (Windows)


``scandir()`` is a directory iteration function like ``os.listdir()``,
except that instead of returning a list of bare filenames, it yields
``DirEntry`` objects that include file type and stat information along
with the name. Using ``scandir()`` increases the speed of ``os.walk()``
by 2-20 times (depending on the platform and file system) by avoiding
unnecessary calls to ``os.stat()`` in most cases.


Now included in a Python near you!
----------------------------------

``scandir`` has been included in the Python 3.5 standard library as
``os.scandir()``, and the related performance improvements to
``os.walk()`` have also been included. So if you're lucky enough to be
using Python 3.5 (release date September 13, 2015) you get the benefit
immediately, otherwise just
`download this module from PyPI <https://pypi.python.org/pypi/scandir>`_,
install it with ``pip install scandir``, and then do something like
this in your code:

.. code-block:: python

    # Use the built-in version of scandir/walk if possible, otherwise
    # use the scandir module version
    try:
        from os import scandir, walk
    except ImportError:
        from scandir import scandir, walk

`PEP 471 <https://www.python.org/dev/peps/pep-0471/>`_, which is the
PEP that proposes including ``scandir`` in the Python standard library,
was `accepted <https://mail.python.org/pipermail/python-dev/2014-July/135561.html>`_
in July 2014 by Victor Stinner, the BDFL-delegate for the PEP.

This ``scandir`` module is intended to work on Python 2.7+ and Python
3.4+ (and it has been tested on those versions).


Background
----------

Python's built-in ``os.walk()`` is significantly slower than it needs to be,
because -- in addition to calling ``listdir()`` on each directory -- it calls
``stat()`` on each file to determine whether the filename is a directory or not.
But both ``FindFirstFile`` / ``FindNextFile`` on Windows and ``readdir`` on Linux/OS
X already tell you whether the files returned are directories or not, so
no further ``stat`` system calls are needed. In short, you can reduce the number
of system calls from about 2N to N, where N is the total number of files and
directories in the tree.

In practice, removing all those extra system calls makes ``os.walk()`` about
**7-50 times as fast on Windows, and about 3-10 times as fast on Linux and Mac OS
X.** So we're not talking about micro-optimizations. See more benchmarks
in the "Benchmarks" section below.

Somewhat relatedly, many people have also asked for a version of
``os.listdir()`` that yields filenames as it iterates instead of returning them
as one big list. This improves memory efficiency for iterating very large
directories.

So as well as a faster ``walk()``, scandir adds a new ``scandir()`` function.
They're pretty easy to use, but see "The API" below for the full docs.


Benchmarks
----------

Below are results showing how many times as fast ``scandir.walk()`` is than
``os.walk()`` on various systems, found by running ``benchmark.py`` with no
arguments:

====================   ==============   =============
System version         Python version   Times as fast
====================   ==============   =============
Windows 7 64-bit       2.7.7 64-bit     10.4
Windows 7 64-bit SSD   2.7.7 64-bit     10.3
Windows 7 64-bit NFS   2.7.6 64-bit     36.8
Windows 7 64-bit SSD   3.4.1 64-bit     9.9
Windows 7 64-bit SSD   3.5.0 64-bit     9.5
Ubuntu 14.04 64-bit    2.7.6 64-bit     5.8
Mac OS X 10.9.3        2.7.5 64-bit     3.8
====================   ==============   =============

All of the above tests were done using the fast C version of scandir
(source code in ``_scandir.c``).

Note that the gains are less than the above on smaller directories and greater
on larger directories. This is why ``benchmark.py`` creates a test directory
tree with a standardized size.


The API
-------

walk()
~~~~~~

The API for ``scandir.walk()`` is exactly the same as ``os.walk()``, so just
`read the Python docs <https://docs.python.org/3.5/library/os.html#os.walk>`_.

scandir()
~~~~~~~~~

The full docs for ``scandir()`` and the ``DirEntry`` objects it yields are
available in the `Python documentation here <https://docs.python.org/3.5/library/os.html#os.scandir>`_. 
But below is a brief summary as well.

    scandir(path='.') -> iterator of DirEntry objects for given path

Like ``listdir``, ``scandir`` calls the operating system's directory
iteration system calls to get the names of the files in the given
``path``, but it's different from ``listdir`` in two ways:

* Instead of returning bare filename strings, it returns lightweight
  ``DirEntry`` objects that hold the filename string and provide
  simple methods that allow access to the additional data the
  operating system may have returned.

* It returns a generator instead of a list, so that ``scandir`` acts
  as a true iterator instead of returning the full list immediately.

``scandir()`` yields a ``DirEntry`` object for each file and
sub-directory in ``path``. Just like ``listdir``, the ``'.'``
and ``'..'`` pseudo-directories are skipped, and the entries are
yielded in system-dependent order. Each ``DirEntry`` object has the
following attributes and methods:

* ``name``: the entry's filename, relative to the scandir ``path``
  argument (corresponds to the return values of ``os.listdir``)

* ``path``: the entry's full path name (not necessarily an absolute
  path) -- the equivalent of ``os.path.join(scandir_path, entry.name)``

* ``is_dir(*, follow_symlinks=True)``: similar to
  ``pathlib.Path.is_dir()``, but the return value is cached on the
  ``DirEntry`` object; doesn't require a system call in most cases;
  don't follow symbolic links if ``follow_symlinks`` is False

* ``is_file(*, follow_symlinks=True)``: similar to
  ``pathlib.Path.is_file()``, but the return value is cached on the
  ``DirEntry`` object; doesn't require a system call in most cases; 
  don't follow symbolic links if ``follow_symlinks`` is False

* ``is_symlink()``: similar to ``pathlib.Path.is_symlink()``, but the
  return value is cached on the ``DirEntry`` object; doesn't require a
  system call in most cases

* ``stat(*, follow_symlinks=True)``: like ``os.stat()``, but the
  return value is cached on the ``DirEntry`` object; does not require a
  system call on Windows (except for symlinks); don't follow symbolic links
  (like ``os.lstat()``) if ``follow_symlinks`` is False

* ``inode()``: return the inode number of the entry; the return value
  is cached on the ``DirEntry`` object

Here's a very simple example of ``scandir()`` showing use of the
``DirEntry.name`` attribute and the ``DirEntry.is_dir()`` method:

.. code-block:: python

    def subdirs(path):
        """Yield directory names not starting with '.' under given path."""
        for entry in os.scandir(path):
            if not entry.name.startswith('.') and entry.is_dir():
                yield entry.name

This ``subdirs()`` function will be significantly faster with scandir
than ``os.listdir()`` and ``os.path.isdir()`` on both Windows and POSIX
systems, especially on medium-sized or large directories.


Further reading
---------------

* `The Python docs for scandir <https://docs.python.org/3.5/library/os.html#os.scandir>`_
* `PEP 471 <https://www.python.org/dev/peps/pep-0471/>`_, the
  (now-accepted) Python Enhancement Proposal that proposed adding
  ``scandir`` to the standard library -- a lot of details here,
  including rejected ideas and previous discussion


Flames, comments, bug reports
-----------------------------

Please send flames, comments, and questions about scandir to Ben Hoyt:

http://benhoyt.com/

File bug reports for the version in the Python 3.5 standard library
`here <https://docs.python.org/3.5/bugs.html>`_, or file bug reports
or feature requests for this module at the GitHub project page:

https://github.com/benhoyt/scandir



  * [scanpy-1.4.4](http://github.com/theislab/scanpy) |PyPI| |bioconda| |Docs| |Build Status|

.. |PyPI| image:: https://img.shields.io/pypi/v/scanpy.svg
   :target: https://pypi.org/project/scanpy
.. |bioconda| image:: https://img.shields.io/badge/bioconda-🐍-blue.svg
   :target: http://bioconda.github.io/recipes/scanpy/README.html
.. |Docs| image:: https://readthedocs.com/projects/icb-scanpy/badge/?version=latest
   :target: https://scanpy.readthedocs.io
.. |Build Status| image:: https://travis-ci.org/theislab/scanpy.svg?branch=master
   :target: https://travis-ci.org/theislab/scanpy
..
   .. |Coverage| image:: https://codecov.io/gh/theislab/scanpy/branch/master/graph/badge.svg
      :target: https://codecov.io/gh/theislab/scanpy

Scanpy – Single-Cell Analysis in Python
=======================================

Scanpy is a scalable toolkit for analyzing single-cell gene expression data.
It includes preprocessing, visualization, clustering, pseudotime and trajectory
inference and differential expression testing. The Python-based implementation
efficiently deals with datasets of more than one million cells.

Discuss usage on `Discourse <https://scanpy.discourse.group/>`__.
Read the documentation_.
If you'd like to contribute by opening an issue or creating a pull request, please take a look at our `contributing guide`_.
If Scanpy is useful for your research, consider citing `Genome Biology (2018)`_.

.. _documentation: https://scanpy.readthedocs.io
.. _contributing guide: CONTRIBUTING.md
.. _Genome Biology (2018): https://doi.org/10.1186/s13059-017-1382-0



  * [scanpydoc-0.3.4](https://github.com/theislab/scanpydoc/) scanpydoc |pypi| |docs| |travis| |codecov|
==========================================

A collection of Sphinx extensions similar to (but more flexible than) numpydoc.

Check the self-documenting documentation at https://icb-scanpydoc.readthedocs-hosted.com

.. |pypi| image:: https://img.shields.io/pypi/v/scanpydoc.svg
   :target: https://pypi.org/project/scanpydoc/
.. |docs| image:: https://readthedocs.com/projects/icb-scanpydoc/badge/
   :target: https://icb-scanpydoc.readthedocs-hosted.com/
.. |travis| image:: https://travis-ci.com/theislab/scanpydoc.svg
   :target: https://travis-ci.com/theislab/scanpydoc
.. |codecov| image:: https://codecov.io/gh/theislab/scanpydoc/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/theislab/scanpydoc

  * [schema-salad-4.5.20190621200723](https://github.com/common-workflow-language/schema_salad) |Linux Build Status| |Windows Build status| |Code coverage| |CII Best Practices|

.. |Linux Build Status| image:: https://img.shields.io/travis/common-workflow-language/schema_salad/master.svg?label=unix%20build
   :target: https://travis-ci.org/common-workflow-language/schema_salad
.. |Windows Build status| image:: https://img.shields.io/appveyor/ci/mr-c/schema-salad/master.svg?label=windows%20build
   :target: https://ci.appveyor.com/project/mr-c/schema-salad/branch/master
.. |Code coverage| image:: https://codecov.io/gh/common-workflow-language/schema_salad/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/common-workflow-language/schema_salad
.. |CII Best Practices| image:: https://bestpractices.coreinfrastructure.org/projects/1867/badge
   :target: https://bestpractices.coreinfrastructure.org/projects/1867

Schema Salad
------------

Salad is a schema language for describing JSON or YAML structured
linked data documents.  Salad schema describes rules for
preprocessing, structural validation, and hyperlink checking for
documents described by a Salad schema. Salad supports rich data
modeling with inheritance, template specialization, object
identifiers, object references, documentation generation, code
generation, and transformation to RDF_. Salad provides a bridge
between document and record oriented data modeling and the Semantic
Web.

Usage
-----

::

   $ pip install schema_salad

To install from source::

  git clone https://github.com/common-workflow-language/schema_salad
  cd schema_salad
  python setup.py install

Commands
--------

Schema salad can be used as a command line tool or imported as a Python module::

   $ schema-salad-tool
   usage: schema-salad-tool [-h] [--rdf-serializer RDF_SERIALIZER]
                         [--print-jsonld-context | --print-rdfs | --print-avro
                         | --print-rdf | --print-pre | --print-index
                         | --print-metadata | --print-inheritance-dot
                         | --print-fieldrefs-dot | --codegen language
                         | --print-oneline]
                         [--strict | --non-strict] [--verbose | --quiet
                         | --debug]
                         [--version]
                         [schema] [document]

   $ python
   >>> import schema_salad

Validate a schema::

   $ schema-salad-tool myschema.yml

Validate a document using a schema::

   $ schema-salad-tool myschema.yml mydocument.yml

Generate HTML documentation::

   $ schema-salad-tool myschema.yml > myschema.html

Get JSON-LD context::

   $ schema-salad-tool --print-jsonld-context myschema.yml mydocument.yml

Convert a document to JSON-LD::

   $ schema-salad-tool --print-pre myschema.yml mydocument.yml > mydocument.jsonld

Generate Python classes for loading/generating documents described by the schema::

   $ schema-salad-tool --codegen=python myschema.yml > myschema.py

Display inheritance relationship between classes as a graphviz 'dot' file and
render as SVG::

   $ schema-salad-tool --print-inheritance-dot myschema.yml | dot -Tsvg > myschema.svg


Quick Start
-----------

Let's say you have a 'basket' record that can contain items measured either by
weight or by count.  Here's an example::

   basket:
     - product: bananas
       price: 0.39
       per: pound
       weight: 1
     - product: cucumbers
       price: 0.79
       per: item
       count: 3

We want to validate that all the expected fields are present, the
measurement is known, and that "count" cannot be a fractional value.
Here is an example schema to do that::

   - name: Product
     doc: |
       The base type for a product.  This is an abstract type, so it
       can't be used directly, but can be used to define other types.
     type: record
     abstract: true
     fields:
       product: string
       price: float

   - name: ByWeight
     doc: |
       A product, sold by weight.  Products may be sold by pound or by
       kilogram.  Weights may be fractional.
     type: record
     extends: Product
     fields:
       per:
         type:
           type: enum
           symbols:
             - pound
             - kilogram
         jsonldPredicate: '#per'
       weight: float

   - name: ByCount
     doc: |
       A product, sold by count.  The count must be a integer value.
     type: record
     extends: Product
     fields:
       per:
         type:
           type: enum
           symbols:
             - item
         jsonldPredicate: '#per'
       count: int

   - name: Basket
     doc: |
       A basket of products.  The 'documentRoot' field indicates it is a
       valid starting point for a document.  The 'basket' field will
       validate subtypes of 'Product' (ByWeight and ByCount).
     type: record
     documentRoot: true
     fields:
       basket:
         type:
           type: array
           items: Product

You can check the schema and document in schema_salad/tests/basket_schema.yml
and schema_salad/tests/basket.yml::

   $ schema-salad-tool basket_schema.yml basket.yml
   Document `basket.yml` is valid


Documentation
-------------

See the specification_ and the metaschema_ (salad schema for itself).  For an
example application of Schema Salad see the Common Workflow Language_.


Rationale
---------

The JSON data model is an popular way to represent structured data.  It is
attractive because of it's relative simplicity and is a natural fit with the
standard types of many programming languages.  However, this simplicity comes
at the cost that basic JSON lacks expressive features useful for working with
complex data structures and document formats, such as schemas, object
references, and namespaces.

JSON-LD is a W3C standard providing a way to describe how to interpret a JSON
document as Linked Data by means of a "context".  JSON-LD provides a powerful
solution for representing object references and namespaces in JSON based on
standard web URIs, but is not itself a schema language.  Without a schema
providing a well defined structure, it is difficult to process an arbitrary
JSON-LD document as idiomatic JSON because there are many ways to express the
same data that are logically equivalent but structurally distinct.

Several schema languages exist for describing and validating JSON data, such as
JSON Schema and Apache Avro data serialization system, however none
understand linked data.  As a result, to fully take advantage of JSON-LD to
build the next generation of linked data applications, one must maintain
separate JSON schema, JSON-LD context, RDF schema, and human documentation,
despite significant overlap of content and obvious need for these documents to
stay synchronized.

Schema Salad is designed to address this gap.  It provides a schema language
and processing rules for describing structured JSON content permitting URI
resolution and strict document validation.  The schema language supports linked
data through annotations that describe the linked data interpretation of the
content, enables generation of JSON-LD context and RDF schema, and production
of RDF triples by applying the JSON-LD context.  The schema language also
provides for robust support of inline documentation.

.. _JSON-LD: http://json-ld.org
.. _Avro: http://avro.apache.org
.. _metaschema: https://github.com/common-workflow-language/schema_salad/blob/master/schema_salad/metaschema/metaschema.yml
.. _specification: http://www.commonwl.org/v1.0/SchemaSalad.html
.. _Language: https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/CommandLineTool.yml
.. _RDF: https://www.w3.org/RDF/



  * [sci-0.1.5](https://github.com/FredHutch/sci-package) sci Package
===========

A Python 3 package or simply a collection of convenience and wrapper functions supporting tasks
frequently needed by scientists.

install
-------

| ``pip3 install sci`` or
| ``pip3 install --user --upgrade sci``

use the ``--user`` flag on shared computers where you do not have root access. It will install the
package in a place like ``~/.local/lib/python3.x/site-packages/sci`` . You can delete that folder
later to remove the package.

design goals
------------

1. Simplicity: wrap frequently used workflows into simple functions that make life easier for
   scientists. Most import statements should move into functions (except for frequently used ones
   like os, sys) to avoid confusion during autocomplete with VS Code and other IDEs.
2. Verbose but easily consumable docstrings: Docstrings are accessible via code autocomplete in IDEs
   such as VS Code or Atom and through automated documentation environments such as sphinx. Each
   docsting should start with an example use case for the function.
3. functional programming paradigm: While using classes is permitted we encourage writing fewer
   classes and more functions (perhaps with decorators)
4. Cross-platform: The code should work on Linux (RHEL and Debian based), Windows and Mac OS X.
5. Python 3 only compatibility, Python 2.x is legacy and we do not want to invest any extra time in
   it.

How to contribute your own code
-------------------------------

1. ``git clone git@github.com:FredHutch/sci-pkg.git``
2. create a new branch (eg : sci-yourname)
3. paste your function into module sci/new.py.
4. Make sure you add an example call in the first line of the doc string.
5. Add your test case to sci-pkg/sci/tests

  * [scikit-bio-0.5.5](http://scikit-bio.org) 
.. image:: http://scikit-bio.org/assets/logo.svg
   :target: http://scikit-bio.org
   :alt: scikit-bio logo

|Build Status| |Coverage Status| |ASV Benchmarks| |Gitter Badge| |Depsy Badge| |Anaconda Build Platforms| |Anaconda Build Version| |License| |Downloads| |Install|

scikit-bio is an open-source, BSD-licensed Python 3 package providing data structures, algorithms and educational resources for bioinformatics.

To view scikit-bio's documentation, visit `scikit-bio.org
<http://scikit-bio.org>`__.

**Note:** scikit-bio is no longer compatible with Python 2. scikit-bio is compatible with Python 3.4 and later.

scikit-bio is currently in beta. We are very actively developing it, and **backward-incompatible interface changes can and will arise**. To avoid these types of changes being a surprise to our users, our public APIs are decorated to make it clear to users when an API can be relied upon (stable) and when it may be subject to change (experimental). See the `API stability docs <https://github.com/biocore/scikit-bio/blob/master/doc/source/user/api_stability.rst>`_ for more details, including what we mean by *stable* and *experimental* in this context.

Installing
----------

The recommended way to install scikit-bio is via the ``conda`` package manager available in `Anaconda <http://continuum.io/downloads>`_ or `miniconda <http://conda.pydata.org/miniconda.html>`_.

To install the latest release of scikit-bio::

    conda install -c conda-forge scikit-bio

Alternatively, you can install scikit-bio using ``pip``::

    pip install numpy
    pip install scikit-bio

You can verify your installation by running the scikit-bio unit tests::

    python -m skbio.test

For users of Debian, ``skbio`` is in the Debian software distribution and may
be installed using::

    sudo apt-get install python3-skbio python-skbio-doc


Getting help
------------

To get help with scikit-bio, you should use the `skbio <http://stackoverflow.com/questions/tagged/skbio>`_ tag on StackOverflow (SO). Before posting a question, check out SO's guide on how to `ask a question <http://stackoverflow.com/questions/how-to-ask>`_. The scikit-bio developers regularly monitor the ``skbio`` SO tag.

Projects using scikit-bio
-------------------------

Some of the projects that we know of that are using scikit-bio are:

- `QIIME <http://qiime.org/>`__
- `Emperor <http://biocore.github.io/emperor/>`__
- `An Introduction to Applied
  Bioinformatics <http://readIAB.org>`__
- `tax2tree <https://github.com/biocore/tax2tree>`__
- `Qiita <http://qiita.microbio.me>`__
- `ghost-tree <https://github.com/JTFouquier/ghost-tree>`__
- `Platypus-Conquistador <https://github.com/biocore/Platypus-Conquistador>`__

If you're using scikit-bio in your own projects, feel free to issue a pull request to add them to this list.

scikit-bio development
----------------------

If you're interested in getting involved in scikit-bio development, see `CONTRIBUTING.md <https://github.com/biocore/scikit-bio/blob/master/CONTRIBUTING.md>`__.

See the list of `scikit-bio's contributors
<https://github.com/biocore/scikit-bio/graphs/contributors>`__.

Licensing
---------

scikit-bio is available under the new BSD license. See
`COPYING.txt <https://github.com/biocore/scikit-bio/blob/master/COPYING.txt>`__ for scikit-bio's license, and the
`licenses directory <https://github.com/biocore/scikit-bio/tree/master/licenses>`_ for the licenses of third-party software that is
(either partially or entirely) distributed with scikit-bio.

The pre-history of scikit-bio
-----------------------------

scikit-bio began from code derived from `PyCogent
<http://www.pycogent.org>`__ and `QIIME <http://www.qiime.org>`__, and
the contributors and/or copyright holders have agreed to make the code
they wrote for PyCogent and/or QIIME available under the BSD
license. The contributors to PyCogent and/or QIIME modules that have
been ported to scikit-bio are: Rob Knight (`@rob-knight
<https://github.com/rob-knight>`__), Gavin Huttley (`@gavin-huttley
<https://github.com/gavin-huttley>`__), Daniel McDonald (`@wasade
<https://github.com/wasade>`__), Micah Hamady, Antonio Gonzalez
(`@antgonza <https://github.com/antgonza>`__), Sandra Smit, Greg
Caporaso (`@gregcaporaso <https://github.com/gregcaporaso>`__), Jai
Ram Rideout (`@jairideout <https://github.com/jairideout>`__),
Cathy Lozupone (`@clozupone <https://github.com/clozupone>`__), Mike Robeson
(`@mikerobeson <https://github.com/mikerobeson>`__), Marcin Cieslik,
Peter Maxwell, Jeremy Widmann, Zongzhi Liu, Michael Dwan, Logan Knecht
(`@loganknecht <https://github.com/loganknecht>`__), Andrew Cochran,
Jose Carlos Clemente (`@cleme <https://github.com/cleme>`__), Damien
Coy, Levi McCracken, Andrew Butterfield, Will Van Treuren (`@wdwvt1
<https://github.com/wdwvt1>`__), Justin Kuczynski (`@justin212k
<https://github.com/justin212k>`__), Jose Antonio Navas Molina
(`@josenavas <https://github.com/josenavas>`__), Matthew Wakefield
(`@genomematt <https://github.com/genomematt>`__) and Jens Reeder
(`@jensreeder <https://github.com/jensreeder>`__).

Logo
----

scikit-bio's logo was created by `Alina Prassas <http://cargocollective.com/alinaprassas>`_.

.. |Build Status| image:: https://travis-ci.org/biocore/scikit-bio.svg?branch=master
   :target: https://travis-ci.org/biocore/scikit-bio
.. |Coverage Status| image:: https://coveralls.io/repos/biocore/scikit-bio/badge.png
   :target: https://coveralls.io/r/biocore/scikit-bio
.. |ASV Benchmarks| image:: http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat
   :target: https://s3-us-west-2.amazonaws.com/scikit-bio.org/benchmarks/master/index.html
.. |Gitter Badge| image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/biocore/scikit-bio
   :target: https://gitter.im/biocore/scikit-bio?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
.. |Depsy Badge| image:: http://depsy.org/api/package/pypi/scikit-bio/badge.svg
   :target: http://depsy.org/package/python/scikit-bio
.. |Anaconda Build Platforms| image:: https://anaconda.org/conda-forge/scikit-bio/badges/platforms.svg
   :target: https://anaconda.org/conda-forge/scikit-bio
.. |Anaconda Build Version| image:: https://anaconda.org/conda-forge/scikit-bio/badges/version.svg
   :target: https://anaconda.org/conda-forge/scikit-bio
.. |License| image:: https://anaconda.org/conda-forge/scikit-bio/badges/license.svg
   :target: https://anaconda.org/conda-forge/scikit-bio
.. |Downloads| image:: https://anaconda.org/conda-forge/scikit-bio/badges/downloads.svg
   :target: https://anaconda.org/conda-forge/scikit-bio
.. |Install| image:: https://anaconda.org/conda-forge/scikit-bio/badges/installer/conda.svg
   :target: https://conda.anaconda.org/conda-forge

  * [scikit-build-0.10.0](https://github.com/scikit-build/scikit-build) ===============================
scikit-build
===============================

Improved build system generator for CPython C/C++/Fortran/Cython extensions.

Better support is available for additional compilers, build systems, cross
compilation, and locating dependencies and determining their build
requirements.

The **scikit-build** package is fundamentally just glue between
the `setuptools` Python module and `CMake <https://cmake.org/>`_.

To get started, see `this example <https://scikit-build.readthedocs.io/en/latest/usage.html#example-of-setup-py-cmakelists-txt-and-pyproject-toml>`_.


Latest Release
--------------

.. table::

  +-----------------------------------------------------------------------------+-------------------------------------------------------------------------------+
  | Versions                                                                    | Downloads                                                                     |
  +=============================================================================+===============================================================================+
  | .. image:: https://img.shields.io/pypi/v/scikit-build.svg                   | .. image:: https://img.shields.io/badge/downloads-130k%20total-green.svg      |
  |     :target: https://pypi.python.org/pypi/scikit-build                      |     :target: https://pypi.python.org/pypi/scikit-build                        |
  +-----------------------------------------------------------------------------+-------------------------------------------------------------------------------+
  | .. image:: https://anaconda.org/conda-forge/scikit-build/badges/version.svg | .. image:: https://anaconda.org/conda-forge/scikit-build/badges/downloads.svg |
  |     :target: https://anaconda.org/conda-forge/scikit-build                  |     :target: https://anaconda.org/conda-forge/scikit-build                    |
  +-----------------------------------------------------------------------------+-------------------------------------------------------------------------------+


Build Status
------------

.. table::

  +---------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+
  |               | Linux                                                                                   | MacOSX                                                                                  | Windows                                                                                                   |
  +===============+=========================================================================================+=========================================================================================+===========================================================================================================+
  | PyPI          | .. image:: https://circleci.com/gh/scikit-build/scikit-build.svg?style=shield           | .. image:: https://img.shields.io/travis/scikit-build/scikit-build.svg?maxAge=2592000   | .. image:: https://ci.appveyor.com/api/projects/status/77bjtsihsjaywjr0?svg=true                          |
  |               |     :target: https://circleci.com/gh/scikit-build/scikit-build                          |     :target: https://travis-ci.org/scikit-build/scikit-build                            |    :target: https://ci.appveyor.com/project/scikit-build/scikit-build/branch/master                       |
  +---------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+
  | Conda         | .. image:: https://circleci.com/gh/conda-forge/scikit-build-feedstock.svg?style=shield  | .. image:: https://travis-ci.org/conda-forge/scikit-build-feedstock.svg?branch=master   | .. image:: https://ci.appveyor.com/api/projects/status/github/conda-forge/scikit-build-feedstock?svg=True |
  |               |     :target: https://circleci.com/gh/conda-forge/scikit-build-feedstock                 |     :target: https://travis-ci.org/conda-forge/scikit-build-feedstock                   |    :target: https://ci.appveyor.com/project/conda-forge/scikit-build-feedstock/branch/master              |
  +---------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+

Overall Health
--------------

.. image:: https://requires.io/github/scikit-build/scikit-build/requirements.svg?branch=master
    :target: https://requires.io/github/scikit-build/scikit-build/requirements/?branch=master
    :alt: Requirements Status

.. image:: https://codecov.io/gh/scikit-build/scikit-build/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/scikit-build/scikit-build

.. image:: https://landscape.io/github/scikit-build/scikit-build/master/landscape.svg?style=flat
    :target: https://landscape.io/github/scikit-build/scikit-build
    :alt: Code Health

Miscellaneous
-------------

* Free software: MIT license
* Documentation: http://scikit-build.readthedocs.org
* Source code: https://github.com/scikit-build/scikit-build
* Mailing list: https://groups.google.com/forum/#!forum/scikit-build




History
-------

PyCMake was created at SciPy 2014 in response to general difficulties building
C++ and Fortran based Python extensions across platforms.  It was renamed to
"scikit-build" in 2016.



  * [scikit-image-0.15.0](https://scikit-image.org) Image Processing SciKit

Image processing algorithms for SciPy, including IO, morphology, filtering,
warping, color manipulation, object detection, etc.

Please refer to the online documentation at
https://scikit-image.org/



  * [scikit-learn-0.21.3](http://scikit-learn.org) .. -*- mode: rst -*-

|Azure|_ |Travis|_ |Codecov|_ |CircleCI|_ |Python35|_ |PyPi|_ |DOI|_

.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master
.. _Azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=master

.. |Travis| image:: https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master
.. _Travis: https://travis-ci.org/scikit-learn/scikit-learn

.. |Codecov| image:: https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&service=github
.. _Codecov: https://codecov.io/github/scikit-learn/scikit-learn?branch=master

.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&circle-token=:circle-token
.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn

.. |Python35| image:: https://img.shields.io/badge/python-3.5-blue.svg
.. _Python35: https://badge.fury.io/py/scikit-learn

.. |PyPi| image:: https://badge.fury.io/py/scikit-learn.svg
.. _PyPi: https://badge.fury.io/py/scikit-learn

.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg
.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn

scikit-learn
============

scikit-learn is a Python module for machine learning built on top of
SciPy and distributed under the 3-Clause BSD license.

The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the `About us <http://scikit-learn.org/dev/about.html#authors>`_ page
for a list of core contributors.

It is currently maintained by a team of volunteers.

Website: http://scikit-learn.org


Installation
------------

Dependencies
~~~~~~~~~~~~

scikit-learn requires:

- Python (>= 3.5)
- NumPy (>= 1.11.0)
- SciPy (>= 0.17.0)
- joblib (>= 0.11)

**Scikit-learn 0.20 was the last version to support Python2.7.**
Scikit-learn 0.21 and later require Python 3.5 or newer.

For running the examples Matplotlib >= 1.5.1 is required. A few examples
require scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0.

User installation
~~~~~~~~~~~~~~~~~

If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using ``pip`` ::

    pip install -U scikit-learn

or ``conda``::

    conda install scikit-learn

The documentation includes more detailed `installation instructions <http://scikit-learn.org/stable/install.html>`_.


Changelog
---------

See the `changelog <http://scikit-learn.org/dev/whats_new.html>`__
for a history of notable changes to scikit-learn.

Development
-----------

We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
`Development Guide <http://scikit-learn.org/stable/developers/index.html>`_
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.

Important links
~~~~~~~~~~~~~~~

- Official source code repo: https://github.com/scikit-learn/scikit-learn
- Download releases: https://pypi.org/project/scikit-learn/
- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues

Source code
~~~~~~~~~~~

You can check the latest sources with the command::

    git clone https://github.com/scikit-learn/scikit-learn.git

Contributing
~~~~~~~~~~~~

To learn more about making a contribution to scikit-learn, please see our
`Contributing guide
<https://scikit-learn.org/dev/developers/contributing.html>`_.

Testing
~~~~~~~

After installation, you can launch the test suite from outside the
source directory (you will need to have ``pytest`` >= 3.3.0 installed)::

    pytest sklearn

See the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing
for more information.

    Random number generation can be controlled during testing by setting
    the ``SKLEARN_SEED`` environment variable.

Submitting a Pull Request
~~~~~~~~~~~~~~~~~~~~~~~~~

Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: http://scikit-learn.org/stable/developers/index.html


Project History
---------------

The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  `About us <http://scikit-learn.org/dev/about.html#authors>`_ page
for a list of core contributors.

The project is currently maintained by a team of volunteers.

**Note**: `scikit-learn` was previously referred to as `scikits.learn`.


Help and Support
----------------

Documentation
~~~~~~~~~~~~~

- HTML documentation (stable release): http://scikit-learn.org
- HTML documentation (development version): http://scikit-learn.org/dev/
- FAQ: http://scikit-learn.org/stable/faq.html

Communication
~~~~~~~~~~~~~

- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``
- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn
- Website: http://scikit-learn.org

Citation
~~~~~~~~

If you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn



  * [scipy-1.1.0](https://www.scipy.org) SciPy (pronounced "Sigh Pie") is open-source software for mathematics,
science, and engineering. The SciPy library
depends on NumPy, which provides convenient and fast N-dimensional
array manipulation. The SciPy library is built to work with NumPy
arrays, and provides many user-friendly and efficient numerical
routines such as routines for numerical integration and optimization.
Together, they run on all popular operating systems, are quick to
install, and are free of charge.  NumPy and SciPy are easy to use,
but powerful enough to be depended upon by some of the world's
leading scientists and engineers. If you need to manipulate
numbers on a computer and display or publish the results,
give SciPy a try!




  * [scvi-0.4.1](https://github.com/YosefLab/scVI) ====
scVI
====

|PyPI| |bioconda| |Docs| |Build Status| |Coverage| |Code Style|

.. |PyPI| image:: https://img.shields.io/pypi/v/scVI.svg
    :target: https://pypi.org/project/scvi
.. |bioconda| image:: https://img.shields.io/badge/bioconda-blue.svg
    :target: http://bioconda.github.io/recipes/scvi/README.html
.. |Docs| image:: https://readthedocs.org/projects/scvi/badge/?version=latest
    :target: https://scvi.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status
.. |Build Status| image:: https://travis-ci.org/YosefLab/scVI.svg?branch=master
    :target: https://travis-ci.org/YosefLab/scVI
.. |Coverage| image:: https://codecov.io/gh/YosefLab/scVI/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/YosefLab/scVI
.. |Code Style| image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/python/black


Single-cell Variational Inference

* Free software: MIT license
* Documentation: https://scvi.readthedocs.io.


Quick Start
-----------

0. If you intend to use parallel implementation of our hyperparameter tuning feature, install MongoDb_.

.. _MongoDb: https://docs.mongodb.com/manual/installation/

1. Install Python 3.7. We typically use the Miniconda_ Python distribution.

.. _Miniconda: https://conda.io/miniconda.html

2. Install PyTorch_. If you have an Nvidia GPU, be sure to install a version of PyTorch that supports it -- scVI runs much faster with a discrete GPU.

.. _PyTorch: http://pytorch.org

3. Install scVI through conda:

    ``conda install scvi -c bioconda -c conda-forge``

   Alternatively, you may try pip (``pip install scvi``), or you may clone this repository and run ``python setup.py install``.
4. Follow along with our Jupyter notebooks to quickly get familiar with scVI!

   a. Getting started:
       * `data loading`__
       * `basic usage`__
   b. Analyzing several datasets:
       * `harmonization`__
       * `annotation`__
   c. Advanced topics:
       * `interaction with scanpy`__
       * `linear decoder for gene interpretation`__
       * `reproducing results from the scVI paper`__
       * `imputation of unobserved gene expression (gimVI)`__
       * `hyperparameter tuning for scVI with our autotune module`__


.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/data_loading.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/basic_tutorial.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/harmonization.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/annotation.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/scanpy_pbmc3k.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/linear_decoder.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/scVI_reproducibility.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/gimvi_tutorial.ipynb
.. __: https://github.com/YosefLab/scVI/blob/master/tests/notebooks/autotune_advanced_notebook.ipynb

References
----------

Romain Lopez, Jeffrey Regier, Michael Cole, Michael I. Jordan, Nir Yosef.
**"Deep generative modeling for single-cell transcriptomics."**
Nature Methods, 2018. `[pdf]`__

.. __: https://rdcu.be/bdHYQ

Chenling Xu∗, Romain Lopez∗, Edouard Mehlman∗, Jeffrey Regier, Michael I. Jordan, Nir Yosef.
**"Harmonization and Annotation of Single-cell Transcriptomics data with Deep Generative Models."**
Submitted, 2019. `[pdf]`__

.. __: https://www.biorxiv.org/content/biorxiv/early/2019/01/29/532895.full.pdf

Romain Lopez∗, Achille Nazaret∗, Maxime Langevin*, Jules Samaran*, Jeffrey Regier*, Michael I. Jordan, Nir Yosef.
**"A joint model of unpaired data from scRNA-seq and spatial transcriptomics for imputing missing gene expression measurements."**
ICML Workshop on Computational Biology, 2019. `[pdf]`__

.. __: https://arxiv.org/pdf/1905.02269.pdf



=======
History
=======

0.1.0 (2018-06-12)
0.1.1 (2018-06-14)
0.1.2 (2018-06-16)
------------------

* First release on PyPI.



  * [seaborn-0.9.0](https://seaborn.pydata.org) Seaborn is a library for making statistical graphics in Python. It is built on top of `matplotlib <https://matplotlib.org/>`_ and closely integrated with `pandas <https://pandas.pydata.org/>`_ data structures.

Here is some of the functionality that seaborn offers:

- A dataset-oriented API for examining relationships between multiple variables
- Specialized support for using categorical variables to show observations or aggregate statistics
- Options for visualizing univariate or bivariate distributions and for comparing them between subsets of data
- Automatic estimation and plotting of linear regression models for different kinds dependent variables
- Convenient views onto the overall structure of complex datasets
- High-level abstractions for structuring multi-plot grids that let you easily build complex visualizations
- Concise control over matplotlib figure styling with several built-in themes
- Tools for choosing color palettes that faithfully reveal patterns in your data

Seaborn aims to make visualization a central part of exploring and understanding data. Its dataset-oriented plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots.



  * [selenium-3.141.0](https://github.com/SeleniumHQ/selenium/) ======================
Selenium Client Driver
======================

Introduction
============

Python language bindings for Selenium WebDriver.

The `selenium` package is used to automate web browser interaction from Python.

+-----------+--------------------------------------------------------------------------------------+
| **Home**: | http://www.seleniumhq.org                                                            |
+-----------+--------------------------------------------------------------------------------------+
| **Docs**: | `selenium package API <https://seleniumhq.github.io/selenium/docs/api/py/api.html>`_ |
+-----------+--------------------------------------------------------------------------------------+
| **Dev**:  | https://github.com/SeleniumHQ/Selenium                                               |
+-----------+--------------------------------------------------------------------------------------+
| **PyPI**: | https://pypi.org/project/selenium/                                                   |
+-----------+--------------------------------------------------------------------------------------+
| **IRC**:  | **#selenium** channel on freenode                                                    |
+-----------+--------------------------------------------------------------------------------------+

Several browsers/drivers are supported (Firefox, Chrome, Internet Explorer), as well as the Remote protocol.

Supported Python Versions
=========================

* Python 2.7, 3.4+

Installing
==========

If you have `pip <https://pip.pypa.io/>`_ on your system, you can simply install or upgrade the Python bindings::

    pip install -U selenium

Alternately, you can download the source distribution from `PyPI <https://pypi.org/project/selenium/#files>`_ (e.g. selenium-3.141.0.tar.gz), unarchive it, and run::

    python setup.py install

Note: You may want to consider using `virtualenv <http://www.virtualenv.org/>`_ to create isolated Python environments.

Drivers
=======

Selenium requires a driver to interface with the chosen browser. Firefox,
for example, requires `geckodriver <https://github.com/mozilla/geckodriver/releases>`_, which needs to be installed before the below examples can be run. Make sure it's in your `PATH`, e. g., place it in `/usr/bin` or `/usr/local/bin`.

Failure to observe this step will give you an error `selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.`

Other supported browsers will have their own drivers available. Links to some of the more popular browser drivers follow.

+--------------+-----------------------------------------------------------------------+
| **Chrome**:  | https://sites.google.com/a/chromium.org/chromedriver/downloads        |
+--------------+-----------------------------------------------------------------------+
| **Edge**:    | https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/ |
+--------------+-----------------------------------------------------------------------+
| **Firefox**: | https://github.com/mozilla/geckodriver/releases                       |
+--------------+-----------------------------------------------------------------------+
| **Safari**:  | https://webkit.org/blog/6900/webdriver-support-in-safari-10/          |
+--------------+-----------------------------------------------------------------------+

Example 0:
==========

* open a new Firefox browser
* load the page at the given URL

.. code-block:: python

    from selenium import webdriver

    browser = webdriver.Firefox()
    browser.get('http://seleniumhq.org/')

Example 1:
==========

* open a new Firefox browser
* load the Yahoo homepage
* search for "seleniumhq"
* close the browser

.. code-block:: python

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys

    browser = webdriver.Firefox()

    browser.get('http://www.yahoo.com')
    assert 'Yahoo' in browser.title

    elem = browser.find_element_by_name('p')  # Find the search box
    elem.send_keys('seleniumhq' + Keys.RETURN)

    browser.quit()

Example 2:
==========

Selenium WebDriver is often used as a basis for testing web applications.  Here is a simple example using Python's standard `unittest <http://docs.python.org/3/library/unittest.html>`_ library:

.. code-block:: python

    import unittest
    from selenium import webdriver

    class GoogleTestCase(unittest.TestCase):

        def setUp(self):
            self.browser = webdriver.Firefox()
            self.addCleanup(self.browser.quit)

        def testPageTitle(self):
            self.browser.get('http://www.google.com')
            self.assertIn('Google', self.browser.title)

    if __name__ == '__main__':
        unittest.main(verbosity=2)

Selenium Server (optional)
==========================

For normal WebDriver scripts (non-Remote), the Java server is not needed.

However, to use Selenium Webdriver Remote or the legacy Selenium API (Selenium-RC), you need to also run the Selenium server.  The server requires a Java Runtime Environment (JRE).

Download the server separately, from: http://selenium-release.storage.googleapis.com/3.141/selenium-server-standalone-3.141.0.jar

Run the server from the command line::

    java -jar selenium-server-standalone-3.141.0.jar

Then run your Python client scripts.

Use The Source Luke!
====================

View source code online:

+-----------+-------------------------------------------------------+
| official: | https://github.com/SeleniumHQ/selenium/tree/master/py |
+-----------+-------------------------------------------------------+

  * [serializable-0.1.1](https://github.com/hamemrlab/serializable) |Build Status|

serializable
============

Base class with serialization methods for user-defined Python objects

Usage
-----

Classes which inherit from ``Serializable`` are enabled with default
implementations of ``to_json``, ``from_json``, ``__reduce__`` (for
pickling), and other serialization helpers. A derived class must either
have a member data matching the name of each argument to ``__init__``
or, alternatively, must provide a user-defined ``to_dict()`` method
which returns a dictionary whose keys match the arguments to
``__init__``.

Limitations
-----------

-  Serializable objects must inherit from ``Serializable``, be tuples or
   namedtuples, be serializble primitive types such as dict, list, int,
   float, or str.

-  The serialized representation of objects relies on reserved keywords
   (such as ``"__name__"``, and ``"__class__"``), so dictionaries are
   expected to not contain any keys which begin with two underscores.

.. |Build Status| image:: https://travis-ci.org/hammerlab/serializable.svg?branch=master
   :target: https://travis-ci.org/hammerlab/serializable
  * [setuptools-40.0.0](https://github.com/pypa/setuptools) .. image:: https://img.shields.io/pypi/v/setuptools.svg
   :target: https://pypi.org/project/setuptools

.. image:: https://img.shields.io/readthedocs/setuptools/latest.svg
    :target: https://setuptools.readthedocs.io

.. image:: https://img.shields.io/travis/pypa/setuptools/master.svg?label=Linux%20CI&logo=travis&logoColor=white
   :target: https://travis-ci.org/pypa/setuptools

.. image:: https://img.shields.io/appveyor/ci/pypa/setuptools/master.svg?label=Windows%20CI&logo=appveyor&logoColor=white
   :target: https://ci.appveyor.com/project/pypa/setuptools/branch/master

.. image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg?logo=codecov&logoColor=white
   :target: https://codecov.io/gh/pypa/setuptools

.. image:: https://tidelift.com/badges/github/pypa/setuptools?style=flat
   :target: https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=readme

.. image:: https://img.shields.io/pypi/pyversions/setuptools.svg

See the `Installation Instructions
<https://packaging.python.org/installing/>`_ in the Python Packaging
User's Guide for instructions on installing, upgrading, and uninstalling
Setuptools.

Questions and comments should be directed to the `distutils-sig
mailing list <http://mail.python.org/pipermail/distutils-sig/>`_.
Bug reports and especially tested patches may be
submitted directly to the `bug tracker
<https://github.com/pypa/setuptools/issues>`_.

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.


Code of Conduct
---------------

Everyone interacting in the setuptools project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the
`PyPA Code of Conduct <https://www.pypa.io/en/latest/code-of-conduct/>`_.



  * [setuptools-git-1.2](https://github.com/msabramo/setuptools-git) About
-----

This is a plugin for setuptools that enables git integration. Once
installed, Setuptools can be told to include in a package distribution
all the files tracked by git. This is an alternative to explicit
inclusion specifications with ``MANIFEST.in``.

A package distribution here refers to a package that you create using
setup.py, for example::

  $> python setup.py sdist
  $> python setup.py bdist_rpm
  $> python setup.py bdist_egg

This package was formerly known as gitlsfiles. The name change is the
result of an effort by the setuptools plugin developers to provide a
uniform naming convention.


Installation
------------

With easy_install::

  $> easy_install setuptools_git

Alternative manual installation::

  $> tar -zxvf setuptools_git-X.Y.Z.tar.gz
  $> cd setuptools_git-X.Y.Z
  $> python setup.py install

Where X.Y.Z is a version number.



Usage
-----

To activate this plugin, you must first package your python module
with ``setup.py`` and use setuptools. The former is well documented in
the `distutils manual <http://docs.python.org/dist/dist.html>`_.

To use setuptools instead of distutils, just edit ``setup.py`` and
change:

.. code-block:: python

  from distutils.core import setup

to:

.. code-block:: python

  from setuptools import setup, find_packages

When Setuptools builds a source package, it always includes all files
tracked by your revision control system, if it knows how to learn what
those files are.

When Setuptools builds a binary package, you can ask it to include all
files tracked by your revision control system, by adding these argument
to your invocation of `setup()`:

.. code-block:: python

  setup(...,
        packages=find_packages(),
        include_package_data=True,
        ...)

which will detect that a directory is a package if it contains a
``__init__.py`` file.  Alternatively, you can do without ``__init__.py``
files and tell Setuptools explicitly which packages to process:

.. code-block:: python

  setup(...,
        packages=["a_package", "another_one"],
        include_package_data=True,
        ...)

This plugin lets setuptools know what files are tracked by your git
revision control tool.  Setuptools ships with support for cvs and
subversion.  Other plugins like this one are available for bzr, darcs,
monotone, mercurial, and many others.

It might happen that you track files with your revision control system
that you don't want to include in your packages.  In that case, you
can prevent setuptools from packaging those files with a directive in
your ``MANIFEST.in``, for example::

  exclude .gitignore
  recursive-exclude images *.xcf *.blend

In this example, we prevent setuptools from packaging ``.gitignore`` and
the Gimp and Blender source files found under the ``images`` directory.

Files to exclude from the package can also be listed in the `setup()`
directive.  To do the same as the MANIFEST.in above, do:

.. code-block:: python

  setup(...,
        exclude_package_data={'': ['.gitignore'],
                              'images': ['*.xcf', '*.blend']},
        ...)

Here is another example:

.. code-block:: python

  setup(...,
        exclude_package_data={'': ['.gitignore', 'artwork/*'],
                              'model': ['config.py']},
        ...)


Gotchas
-------

Be aware that for this module to work properly, git and the git
meta-data must be available. That means that if someone tries to make
a package distribution out of a non-git distribution of yours, say a
tarball, setuptools will lack the information necessary to know which
files to include. A similar problem will happen if someone clones
your git repository but does not install this plugin.

Resolving those problems is out of the scope of this plugin; you
should add relevant warnings to your documentation if those situations
are a concern to you.

You can make sure that anyone who clones your git repository and uses
your setup.py file has this plugin by adding a `setup_requires`
argument:

.. code-block:: python

  setup(...,
        setup_requires=[ "setuptools_git >= 0.3", ],
        ...)


Changes
-------

1.2;  2017-02-17
~~~~~~~~~~~~~~~~
  - Add ability to get version from git tags (https://github.com/msabramo/setuptools-git/pull/9)
  - Return early if a directory isn't managed by git (https://github.com/msabramo/setuptools-git/pull/10)
  - Support universal wheels (https://github.com/msabramo/setuptools-git/pull/11)
  - Optimize directory scanning to skip ignored directories (https://github.com/msabramo/setuptools-git/pull/12)


References
----------

* `How to distribute Python modules with Distutils
  <http://docs.python.org/dist/dist.html>`_

* `Setuptools complete manual
  <http://peak.telecommunity.com/DevCenter/setuptools>`_

Thanks to `Zooko O'Whielacronx`_ for many improvements to the documentation.


.. _Zooko O'Whielacronx: https://bitbucket.org/zooko
  * [shap-0.29.3](http://github.com/slundberg/shap) SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.
  * [shellescape-3.4.1](https://github.com/chrissimpkins/shellescape) Source Repository: https://github.com/chrissimpkins/shellescape

Description
-----------

The shellescape Python module defines the ``shellescape.quote()`` function that returns a shell-escaped version of a Python string.  This is a backport of the ``shlex.quote()`` function from Python 3.4.3 that makes it accessible to users of Python 3 versions < 3.3 and all Python 2.x versions.

quote(s)
--------

From the Python documentation:

Return a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list.

This idiom would be unsafe:

.. code-block:: python

	>>> filename = 'somefile; rm -rf ~'
	>>> command = 'ls -l {}'.format(filename)
	>>> print(command)  # executed by a shell: boom!
	ls -l somefile; rm -rf ~


``quote()`` lets you plug the security hole:

.. code-block:: python

	>>> command = 'ls -l {}'.format(quote(filename))
	>>> print(command)
	ls -l 'somefile; rm -rf ~'
	>>> remote_command = 'ssh home {}'.format(quote(command))
	>>> print(remote_command)
	ssh home 'ls -l '"'"'somefile; rm -rf ~'"'"''


The quoting is compatible with UNIX shells and with ``shlex.split()``:

.. code-block:: python

	>>> remote_command = split(remote_command)
	>>> remote_command
	['ssh', 'home', "ls -l 'somefile; rm -rf ~'"]
	>>> command = split(remote_command[-1])
	>>> command
	['ls', '-l', 'somefile; rm -rf ~']


Usage
-----

Include ``shellescape`` in your project setup.py file ``install_requires`` dependency definition list:

.. code-block:: python

	setup(
	    ...
	    install_requires=['shellescape'],
	    ...
	)


Then import the ``quote`` function into your module(s) and use it as needed:

.. code-block:: python

	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	from shellescape import quote

	filename = "somefile; rm -rf ~"
	escaped_shell_command = 'ls -l {}'.format(quote(filename))


Issue Reporting
---------------

Issue reporting is available on the `GitHub repository <https://github.com/chrissimpkins/shellescape/issues>`_
  * [shove-0.6.6](https://bitbucket.org/lcrees/shove/) Common object storage frontend that supports
dictionary-style access, object serialization
and compression, and multiple storage and caching
backends.

Supported storage backends out of the box are:

- DBM
- Filesystem
- Memory
- sqlite (disk or memory)

Current supported caching backends are:

- Filesystem
- Memory
- sqlite (disk or memory)

The simplest *shove* use case...

>>> from shove import Shove
>>> store = Shove()

...which creates an in-memory store and cache.

Use of other backends for storage and caching involves
passing an module URI or existing store or cache instance
to *shove* following the form:

>>> from shove import Shove
>>> <storename> = Shove(<store_uri>, <cache_uri>)

Each module-specific URI form is documented in its module. The
URI form follows the URI form used by SQLAlchemy:

    http://www.sqlalchemy.org/docs/core/engines.html

*shove* implements the Python dictionary/mapping API:

    http://docs.python.org/lib/typesmapping.html
  * [simplegeneric-0.8.1](http://cheeseshop.python.org/pypi/simplegeneric) * New in 0.8: Source and tests are compatible with Python 3 (w/o ``setup.py``)

  * 0.8.1: setup.py is now compatible with Python 3 as well

* New in 0.7: `Multiple Types or Objects`_

* New in 0.6: `Inspection and Extension`_, and thread-safe method registration

The ``simplegeneric`` module lets you define simple single-dispatch
generic functions, akin to Python's built-in generic functions like
``len()``, ``iter()`` and so on.  However, instead of using
specially-named methods, these generic functions use simple lookup
tables, akin to those used by e.g. ``pickle.dump()`` and other
generic functions found in the Python standard library.

As you can see from the above examples, generic functions are actually
quite common in Python already, but there is no standard way to create
simple ones.  This library attempts to fill that gap, as generic
functions are an `excellent alternative to the Visitor pattern`_, as
well as being a great substitute for most common uses of adaptation.

This library tries to be the simplest possible implementation of generic
functions, and it therefore eschews the use of multiple or predicate
dispatch, as well as avoiding speedup techniques such as C dispatching
or code generation.  But it has absolutely no dependencies, other than
Python 2.4, and the implementation is just a single Python module of
less than 100 lines.


Usage
-----

Defining and using a generic function is straightforward::

    >>> from simplegeneric import generic
    >>> @generic
    ... def move(item, target):
    ...     """Default implementation goes here"""
    ...     print("what you say?!")

    >>> @move.when_type(int)
    ... def move_int(item, target):
    ...     print("In AD %d, %s was beginning." % (item, target))

    >>> @move.when_type(str)
    ... def move_str(item, target):
    ...     print("How are you %s!!" % item)
    ...     print("All your %s are belong to us." % (target,))

    >>> zig = object()
    >>> @move.when_object(zig)
    ... def move_zig(item, target):
    ...     print("You know what you %s." % (target,))
    ...     print("For great justice!")

    >>> move(2101, "war")
    In AD 2101, war was beginning.

    >>> move("gentlemen", "base")
    How are you gentlemen!!
    All your base are belong to us.

    >>> move(zig, "doing")
    You know what you doing.
    For great justice!

    >>> move(27.0, 56.2)
    what you say?!


Inheritance and Allowed Types
-----------------------------

Defining multiple methods for the same type or object is an error::

    >>> @move.when_type(str)
    ... def this_is_wrong(item, target):
    ...     pass
    Traceback (most recent call last):
    ...
    TypeError: <function move...> already has method for type <...'str'>

    >>> @move.when_object(zig)
    ... def this_is_wrong(item, target): pass
    Traceback (most recent call last):
      ...
    TypeError: <function move...> already has method for object <object ...>

And the ``when_type()`` decorator only accepts classes or types::

    >>> @move.when_type(23)
    ... def move_23(item, target):
    ...     print("You have no chance to survive!")
    Traceback (most recent call last):
      ...
    TypeError: 23 is not a type or class

Methods defined for supertypes are inherited following MRO order::

    >>> class MyString(str):
    ...     """String subclass"""

    >>> move(MyString("ladies"), "drinks")
    How are you ladies!!
    All your drinks are belong to us.

Classic class instances are also supported (although the lookup process
is slower than for new-style instances)::

    >>> class X: pass
    >>> class Y(X): pass

    >>> @move.when_type(X)
    ... def move_x(item, target):
    ...     print("Someone set us up the %s!!!" % (target,))

    >>> move(X(), "bomb")
    Someone set us up the bomb!!!

    >>> move(Y(), "dance")
    Someone set us up the dance!!!


Multiple Types or Objects
-------------------------

As a convenience, you can now pass more than one type or object to the
registration methods::

    >>> @generic
    ... def isbuiltin(ob):
    ...     return False
    >>> @isbuiltin.when_type(int, str, float, complex, type)
    ... @isbuiltin.when_object(None, Ellipsis)
    ... def yes(ob):
    ...     return True
    
    >>> isbuiltin(1)
    True
    >>> isbuiltin(object)
    True
    >>> isbuiltin(object())
    False
    >>> isbuiltin(X())
    False
    >>> isbuiltin(None)
    True
    >>> isbuiltin(Ellipsis)
    True


Defaults and Docs
-----------------

You can obtain a function's default implementation using its ``default``
attribute::

    >>> @move.when_type(Y)
    ... def move_y(item, target):
    ...     print("Someone set us up the %s!!!" % (target,))
    ...     move.default(item, target)

    >>> move(Y(), "dance")
    Someone set us up the dance!!!
    what you say?!


``help()`` and other documentation tools see generic functions as normal
function objects, with the same name, attributes, docstring, and module as
the prototype/default function::

    >>> help(move)
    Help on function move:
    ...
    move(*args, **kw)
        Default implementation goes here
    ...


Inspection and Extension
------------------------

You can find out if a generic function has a method for a type or object using
the ``has_object()`` and ``has_type()`` methods::

    >>> move.has_object(zig)
    True
    >>> move.has_object(42)
    False

    >>> move.has_type(X)
    True
    >>> move.has_type(float)
    False

Note that ``has_type()`` only queries whether there is a method registered for
the *exact* type, not subtypes or supertypes::

    >>> class Z(X): pass
    >>> move.has_type(Z)
    False

You can create a generic function that "inherits" from an existing generic
function by calling ``generic()`` on the existing function::

    >>> move2 = generic(move)
    >>> move(2101, "war")
    In AD 2101, war was beginning.

Any methods added to the new generic function override *all* methods in the
"base" function::

    >>> @move2.when_type(X)
    ... def move2_X(item, target):
    ...     print("You have no chance to survive make your %s!" % (target,))

    >>> move2(X(), "time")
    You have no chance to survive make your time!

    >>> move2(Y(), "time")
    You have no chance to survive make your time!

Notice that even though ``move()`` has a method for type ``Y``, the method
defined for ``X`` in ``move2()`` takes precedence.  This is because the
``move`` function is used as the ``default`` method of ``move2``, and ``move2``
has no method for type ``Y``::

    >>> move2.default is move
    True
    >>> move.has_type(Y)
    True
    >>> move2.has_type(Y)
    False


Limitations
-----------

* The first argument is always used for dispatching, and it must always be
  passed *positionally* when the function is called.

* Documentation tools don't see the function's original argument signature, so
  you have to describe it in the docstring.

* If you have optional arguments, you must duplicate them on every method in
  order for them to work correctly.  (On the plus side, it means you can have
  different defaults or required arguments for each method, although relying on
  that quirk probably isn't a good idea.)

These restrictions may be lifted in later releases, if I feel the need.  They
would require runtime code generation the way I do it in ``RuleDispatch``,
however, which is somewhat of a pain.  (Alternately I could use the
``BytecodeAssembler`` package to do the code generation, as that's a lot easier
to use than string-based code generation, but that would introduce more
dependencies, and I'm trying to keep this simple so I can just
toss it into Chandler without a big footprint increase.)

.. _excellent alternative to the Visitor pattern: http://peak.telecommunity.com/DevCenter/VisitorRevisited
  * [simplejson-3.16.0](https://github.com/simplejson/simplejson) simplejson
----------

.. image:: https://travis-ci.org/simplejson/simplejson.svg?branch=master
    :target: https://travis-ci.org/simplejson/simplejson

simplejson is a simple, fast, complete, correct and extensible
JSON <http://json.org> encoder and decoder for Python 2.5+
and Python 3.3+.  It is pure Python code with no dependencies,
but includes an optional C extension for a serious speed boost.

The latest documentation for simplejson can be read online here:
https://simplejson.readthedocs.io/

simplejson is the externally maintained development version of the
json library included with Python 2.6 and Python 3.0, but maintains
backwards compatibility with Python 2.5.

The encoder can be specialized to provide serialization in any kind of
situation, without any special support by the objects to be serialized
(somewhat like pickle). This is best done with the ``default`` kwarg
to dumps.

The decoder can handle incoming JSON strings of any specified encoding
(UTF-8 by default). It can also be specialized to post-process JSON
objects with the ``object_hook`` or ``object_pairs_hook`` kwargs. This
is particularly useful for implementing protocols such as JSON-RPC
that have a richer type system than JSON itself.

For those of you that have legacy systems to maintain, there is a
very old fork of simplejson in the `python2.2`_ branch that supports
Python 2.2. This is based off of a very old version of simplejson,
is not maintained, and should only be used as a last resort.

.. _python2.2: https://github.com/simplejson/simplejson/tree/python2.2



  * [singledispatch-3.4.0.3](http://docs.python.org/3/library/functools.html#functools.singledispatch) ==============
singledispatch
==============

`PEP 443 <http://www.python.org/dev/peps/pep-0443/>`_ proposed to expose
a mechanism in the ``functools`` standard library module in Python 3.4
that provides a simple form of generic programming known as
single-dispatch generic functions.

This library is a backport of this functionality to Python 2.6 - 3.3.

To define a generic function, decorate it with the ``@singledispatch``
decorator. Note that the dispatch happens on the type of the first
argument, create your function accordingly::

  >>> from singledispatch import singledispatch
  >>> @singledispatch
  ... def fun(arg, verbose=False):
  ...     if verbose:
  ...         print("Let me just say,", end=" ")
  ...     print(arg)

To add overloaded implementations to the function, use the
``register()`` attribute of the generic function. It is a decorator,
taking a type parameter and decorating a function implementing the
operation for that type::

  >>> @fun.register(int)
  ... def _(arg, verbose=False):
  ...     if verbose:
  ...         print("Strength in numbers, eh?", end=" ")
  ...     print(arg)
  ...
  >>> @fun.register(list)
  ... def _(arg, verbose=False):
  ...     if verbose:
  ...         print("Enumerate this:")
  ...     for i, elem in enumerate(arg):
  ...         print(i, elem)

To enable registering lambdas and pre-existing functions, the
``register()`` attribute can be used in a functional form::

  >>> def nothing(arg, verbose=False):
  ...     print("Nothing.")
  ...
  >>> fun.register(type(None), nothing)

The ``register()`` attribute returns the undecorated function which
enables decorator stacking, pickling, as well as creating unit tests for
each variant independently::

  >>> @fun.register(float)
  ... @fun.register(Decimal)
  ... def fun_num(arg, verbose=False):
  ...     if verbose:
  ...         print("Half of your number:", end=" ")
  ...     print(arg / 2)
  ...
  >>> fun_num is fun
  False

When called, the generic function dispatches on the type of the first
argument::

  >>> fun("Hello, world.")
  Hello, world.
  >>> fun("test.", verbose=True)
  Let me just say, test.
  >>> fun(42, verbose=True)
  Strength in numbers, eh? 42
  >>> fun(['spam', 'spam', 'eggs', 'spam'], verbose=True)
  Enumerate this:
  0 spam
  1 spam
  2 eggs
  3 spam
  >>> fun(None)
  Nothing.
  >>> fun(1.23)
  0.615

Where there is no registered implementation for a specific type, its
method resolution order is used to find a more generic implementation.
The original function decorated with ``@singledispatch`` is registered
for the base ``object`` type, which means it is used if no better
implementation is found.

To check which implementation will the generic function choose for
a given type, use the ``dispatch()`` attribute::

  >>> fun.dispatch(float)
  <function fun_num at 0x1035a2840>
  >>> fun.dispatch(dict)    # note: default implementation
  <function fun at 0x103fe0000>

To access all registered implementations, use the read-only ``registry``
attribute::

  >>> fun.registry.keys()
  dict_keys([<class 'NoneType'>, <class 'int'>, <class 'object'>,
            <class 'decimal.Decimal'>, <class 'list'>,
            <class 'float'>])
  >>> fun.registry[float]
  <function fun_num at 0x1035a2840>
  >>> fun.registry[object]
  <function fun at 0x103fe0000>

The vanilla documentation is available at
http://docs.python.org/3/library/functools.html#functools.singledispatch.


Versioning
----------

This backport is intended to keep 100% compatibility with the vanilla
release in Python 3.4+. To help maintaining a version you want and
expect, a versioning scheme is used where:

* the first three numbers indicate the version of Python 3.x from which the
  backport is done

* a backport release number is provided after the last dot

For example, ``3.4.0.0`` is the **first** release of ``singledispatch``
compatible with the library found in Python **3.4.0**.

A single exception from the 100% compatibility principle is that bugs
fixed before releasing another minor Python 3.x.y version **will be
included** in the backport releases done in the mean time. This rule
applies to bugs only.


Maintenance
-----------

This backport is maintained on BitBucket by Łukasz Langa, one of the
members of the core CPython team:

* `singledispatch Mercurial repository <https://bitbucket.org/ambv/singledispatch>`_

* `singledispatch issue tracker <https://bitbucket.org/ambv/singledispatch/issues>`_


Change Log
----------

3.4.0.3
~~~~~~~

Should now install flawlessly on PyPy as well. Thanks to Ryan Petrello
for finding and fixing the ``setup.py`` issue.

3.4.0.2
~~~~~~~

Updated to the reference implementation as of 02-July-2013.

* more predictable dispatch order when abstract base classes are in use:
  abstract base classes are now inserted into the MRO of the argument's
  class where their functionality is introduced, i.e. issubclass(cls,
  abc) returns True for the class itself but returns False for all its
  direct base classes. Implicit ABCs for a given class (either
  registered or inferred from the presence of a special method like
  __len__) are inserted directly after the last ABC explicitly listed in
  the MRO of said class. This also means there are less "ambiguous
  dispatch" exceptions raised.

* better test coverage and improved docstrings

3.4.0.1
~~~~~~~

Updated to the reference implementation as of 31-May-2013.

* better performance

* fixed a corner case with PEP 435 enums

* calls to `dispatch()` also cached

* dispatching algorithm now now a module-level routine called `_find_impl()`
  with a simplified implementation and proper documentation

* `dispatch()` now handles all caching-related activities

* terminology more consistent: "overload" -> "implementation"

3.4.0.0
~~~~~~~

* the first public release compatible with 3.4.0


Conversion Process
------------------

This section is technical and should bother you only if you are
wondering how this backport is produced. If the implementation details
of this backport are not important for you, feel free to ignore the
following content.

``singledispatch`` is converted using `six
<http://pypi.python.org/pypi/six>`_ so that a single codebase can be
used for all compatible Python versions.  Because a fully automatic
conversion was not doable, I took the following branching approach:

* the ``upstream`` branch holds unchanged files synchronized from the
  upstream CPython repository. The synchronization is currently done by
  manually copying the required code parts and stating from which
  CPython changeset they come from. The tests should pass on Python 3.4
  on this branch.

* the ``default`` branch holds the manually translated version and this
  is where all tests are run for all supported Python versions using
  Tox.
  * [six-1.11.0](https://github.com/benjaminp/six) .. image:: https://img.shields.io/pypi/v/six.svg
   :target: https://pypi.org/project/six/
   :alt: six on PyPI

.. image:: https://travis-ci.org/benjaminp/six.svg?branch=master
   :target: https://travis-ci.org/benjaminp/six
   :alt: six on TravisCI

.. image:: https://readthedocs.org/projects/six/badge/?version=latest
   :target: https://six.readthedocs.io/
   :alt: six's documentation on Read the Docs

.. image:: https://img.shields.io/badge/license-MIT-green.svg
   :target: https://github.com/benjaminp/six/blob/master/LICENSE
   :alt: MIT License badge

Six is a Python 2 and 3 compatibility library.  It provides utility functions
for smoothing over the differences between the Python versions with the goal of
writing Python code that is compatible on both Python versions.  See the
documentation for more information on what is provided.

Six supports every Python version since 2.6.  It is contained in only one Python
file, so it can be easily copied into your project. (The copyright and license
notice must be retained.)

Online documentation is at https://six.readthedocs.io/.

Bugs can be reported to https://github.com/benjaminp/six.  The code can also
be found there.

For questions about six or porting in general, email the python-porting mailing
list: https://mail.python.org/mailman/listinfo/python-porting



  * [sklearn-0.0](https://pypi.python.org/pypi/scikit-learn/) Use `scikit-learn <https://pypi.python.org/pypi/scikit-learn/>`_ instead.
  * [slurm-pipeline-3.1.0](https://github.com/acorg/slurm-pipeline) 
  * [slurmpy-0.0.7](https://github.com/brentp/slurmpy) quick and dirty lib for submitting jobs to slurm via python2/python3.

```Python
from slurmpy import Slurm

s = Slurm("job-name", {"account": "my-account", "partition": "my-parition"})
s.run("""
do
lots
of
stuff
""")

```

The above will submit the job to `sbatch` automatically write the script to `scripts/`
and automatically write logs/{name}.err and logs/{name}.out. It will have today's
date in the log and script names.

The script to run() can also contain `$variables` which are filled with the cmd_kwarg dict.
E.g. `echo $name` could be filled with `cmd_kwargs={'name': 'sally'}`

A command can be tested (not sent to queue) by setting the `_cmd` are to `run` as e.g. "ls".
The default is `sbatch` which submits jobs to slurm.

Dependencies
============

Each time `slurmpy.Slurm().run()` is called, it returns the job-id of the submitted job. This
can then be sent to a subsequent job:
```
s = Slurmp()
s.run(..., depends_on=[job_id])

```
to indicate that this job should not run until the the job with `job_id` has finished successfully.

Install
=======

```Shell
pip install slurmpy --user
```

  * [smmap2-2.0.5](https://github.com/gitpython-developers/smmap) ## Motivation

When reading from many possibly large files in a fashion similar to random access, it is usually the fastest and most efficient to use memory maps.

Although memory maps have many advantages, they represent a very limited system resource as every map uses one file descriptor, whose amount is limited per process. On 32 bit systems, the amount of memory you can have mapped at a time is naturally limited to theoretical 4GB of memory, which may not be enough for some applications.


## Limitations

* **System resources (file-handles) are likely to be leaked!** This is due to the library authors reliance on a deterministic `__del__()` destructor.
* The memory access is read-only by design.


## Overview

[![Build Status](https://travis-ci.org/gitpython-developers/smmap.svg?branch=master)](https://travis-ci.org/gitpython-developers/smmap)
[![Build status](https://ci.appveyor.com/api/projects/status/kuws846av5lvmugo?svg=true&passingText=windows%20OK&failingText=windows%20failed)](https://ci.appveyor.com/project/Byron/smmap)
[![Coverage Status](https://coveralls.io/repos/gitpython-developers/smmap/badge.png)](https://coveralls.io/r/gitpython-developers/smmap)
[![Issue Stats](http://www.issuestats.com/github/gitpython-developers/smmap/badge/pr)](http://www.issuestats.com/github/gitpython-developers/smmap)
[![Issue Stats](http://www.issuestats.com/github/gitpython-developers/smmap/badge/issue)](http://www.issuestats.com/github/gitpython-developers/smmap)

Smmap wraps an interface around mmap and tracks the mapped files as well as the amount of clients who use it. If the system runs out of resources, or if a memory limit is reached, it will automatically unload unused maps to allow continued operation.

To allow processing large files even on 32 bit systems, it allows only portions of the file to be mapped. Once the user reads beyond the mapped region, smmap will automatically map the next required region, unloading unused regions using a LRU algorithm.

The interface also works around the missing offset parameter in python implementations up to python 2.5.

Although the library can be used most efficiently with its native interface, a Buffer implementation is provided to hide these details behind a simple string-like interface.

For performance critical 64 bit applications, a simplified version of memory mapping is provided which always maps the whole file, but still provides the benefit of unloading unused mappings on demand.



## Prerequisites

* Python 2.7 or 3.4+
* OSX, Windows or Linux

The package was tested on all of the previously mentioned configurations.

## Installing smmap

[![Documentation Status](https://readthedocs.org/projects/smmap/badge/?version=latest)](https://readthedocs.org/projects/smmap/?badge=latest)

Its easiest to install smmap using the [pip](http://www.pip-installer.org/en/latest) program:

```bash
$ pip install smmap
```

As the command will install smmap in your respective python distribution, you will most likely need root permissions to authorize the required changes.

If you have downloaded the source archive, the package can be installed by running the `setup.py` script:

```bash
$ python setup.py install
```

It is advised to have a look at the **Usage Guide** for a brief introduction on the different database implementations.



## Homepage and Links

The project is home on github at https://github.com/gitpython-developers/smmap .

The latest source can be cloned from github as well:

* git://github.com/gitpython-developers/smmap.git


For support, please use the git-python mailing list:

* http://groups.google.com/group/git-python


Issues can be filed on github:

* https://github.com/gitpython-developers/smmap/issues


## License Information

*smmap* is licensed under the New BSD License.




  * [snakemake-5.5.4](https://snakemake.readthedocs.io) 
  * [snowballstemmer-1.9.0](https://github.com/snowballstem/snowball) 
It includes following language algorithms:

* Arabic
* Basque
* Catalan
* Danish
* Dutch
* English (Standard, Porter)
* Finnish
* French
* German
* Greek
* Hindi
* Hungarian
* Indonesian
* Irish
* Italian
* Lithuanian
* Nepali
* Norwegian
* Portuguese
* Romanian
* Russian
* Spanish
* Swedish
* Tamil
* Turkish

This is a pure Python stemming library. If `PyStemmer <https://pypi.org/project/PyStemmer/>`_ is available, this module uses
it to accelerate.



  * [softlayer_messaging-1.0.3](http://sldn.softlayer.com/reference/messagequeueapi) SoftLayer Message Queue - Python Client
========================================
This code provides bindings written in Python to communicate with the
[SoftLayer Message Queue API](http://sldn.softlayer.com/reference/messagequeueapi).

Installation
------------
Using pip:
```
pip install softlayer_messaging
```

Download source and run:
```
python setup.py install
```

Need more help?
---------------

For additional guidance and information, check out the
[Message Queue API reference](http://sldn.softlayer.com/reference/messagequeueapi) 
or the [SoftLayer Developer Network forum](https://forums.softlayer.com/forumdisplay.php?f=27).

For specific issues with the Python client library, get in touch with us via the
[SoftLayer Developer Network forum](https://forums.softlayer.com/forumdisplay.php?f=27)
or the [Issues page on our GitHub repository](https://github.com/softlayer/softlayer-message-queue-python/issues).

License
-------

Copyright (c) 2012 SoftLayer Technologies, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE
  * [sortedcollections-1.1.2](http://www.grantjenks.com/docs/sortedcollections/) Python Sorted Collections
=========================

`Sorted Collections`_ is an Apache2 licensed Python sorted collections library.

Features
--------

- Pure-Python
- Depends on the `Sorted Containers
  <http://www.grantjenks.com/docs/sortedcontainers/>`_ module.
- ValueSortedDict - Dictionary with (key, value) item pairs sorted by value.
- ItemSortedDict - Dictionary with key-function support for item pairs.
- OrderedDict - Ordered dictionary with numeric indexing support.
- OrderedSet - Ordered set with numeric indexing support.
- IndexableDict - Dictionary with numeric indexing support.
- IndexableSet - Set with numeric indexing support.
- SegmentList - List with fast random access insertion and deletion.
- 100% code coverage testing.
- Developed on Python 3.7
- Tested on CPython 2.7, 3.4, 3.5, 3.6, 3.7 and PyPy, PyPy3

.. image:: https://api.travis-ci.org/grantjenks/python-sortedcollections.svg?branch=master
    :target: http://www.grantjenks.com/docs/sortedcollections/

.. image:: https://ci.appveyor.com/api/projects/status/github/grantjenks/python-sortedcollections?branch=master&svg=true
   :target: http://www.grantjenks.com/docs/sortedcollections/

Quickstart
----------

Installing `Sorted Collections`_ is simple with `pip
<http://www.pip-installer.org/>`_::

    $ pip install sortedcollections

You can access documentation in the interpreter with Python's built-in `help`
function:

.. code-block:: python

    >>> from sortedcollections import ValueSortedDict
    >>> help(ValueSortedDict)

.. _`Sorted Collections`: http://www.grantjenks.com/docs/sortedcollections/

Recipes
-------

- `Value Sorted Dictionary Recipe`_
- `Item Sorted Dictionary Recipe`_
- `Ordered Dictionary Recipe`_
- `Ordered Set Recipe`_
- `Indexable Dictionary Recipe`_
- `Indexable Set Recipe`_
- `Segment List Recipe`_

.. _`Value Sorted Dictionary Recipe`: http://www.grantjenks.com/docs/sortedcollections/valuesorteddict.html
.. _`Item Sorted Dictionary Recipe`: http://www.grantjenks.com/docs/sortedcollections/itemsorteddict.html
.. _`Ordered Dictionary Recipe`: http://www.grantjenks.com/docs/sortedcollections/ordereddict.html
.. _`Ordered Set Recipe`: http://www.grantjenks.com/docs/sortedcollections/orderedset.html
.. _`Indexable Dictionary Recipe`: http://www.grantjenks.com/docs/sortedcollections/indexabledict.html
.. _`Indexable Set Recipe`: http://www.grantjenks.com/docs/sortedcollections/indexableset.html
.. _`Segment List Recipe`: http://www.grantjenks.com/docs/sortedcollections/segmentlist.html

Reference and Indices
---------------------

- `Sorted Collections Documentation`_
- `Sorted Collections at PyPI`_
- `Sorted Collections at Github`_
- `Sorted Collections Issue Tracker`_

.. _`Sorted Collections Documentation`: http://www.grantjenks.com/docs/sortedcollections/
.. _`Sorted Collections at PyPI`: https://pypi.python.org/pypi/sortedcollections/
.. _`Sorted Collections at Github`: https://github.com/grantjenks/python-sortedcollections
.. _`Sorted Collections Issue Tracker`: https://github.com/grantjenks/python-sortedcollections/issues

Sorted Collections License
--------------------------

Copyright 2015-2019 Grant Jenks

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.



  * [sortedcontainers-2.1.0](http://www.grantjenks.com/docs/sortedcontainers/) Python Sorted Containers
========================

`Sorted Containers`_ is an Apache2 licensed `sorted collections library`_,
written in pure-Python, and fast as C-extensions.

Python's standard library is great until you need a sorted collections
type. Many will attest that you can get really far without one, but the moment
you **really need** a sorted list, sorted dict, or sorted set, you're faced
with a dozen different implementations, most using C-extensions without great
documentation and benchmarking.

In Python, we can do better. And we can do it in pure-Python!

.. code-block:: python

    >>> from sortedcontainers import SortedList
    >>> sl = SortedList(['e', 'a', 'c', 'd', 'b'])
    >>> sl
    SortedList(['a', 'b', 'c', 'd', 'e'])
    >>> sl *= 10_000_000
    >>> sl.count('c')
    10000000
    >>> sl[-3:]
    ['e', 'e', 'e']
    >>> from sortedcontainers import SortedDict
    >>> sd = SortedDict({'c': 3, 'a': 1, 'b': 2})
    >>> sd
    SortedDict({'a': 1, 'b': 2, 'c': 3})
    >>> sd.popitem(index=-1)
    ('c', 3)
    >>> from sortedcontainers import SortedSet
    >>> ss = SortedSet('abracadabra')
    >>> ss
    SortedSet(['a', 'b', 'c', 'd', 'r'])
    >>> ss.bisect_left('c')
    2

All of the operations shown above run in faster than linear time. The above
demo also takes nearly a gigabyte of memory to run. When the sorted list is
multiplied by ten million, it stores ten million references to each of "a"
through "e". Each reference requires eight bytes in the sorted
container. That's pretty hard to beat as it's the cost of a pointer to each
object. It's also 66% less overhead than a typical binary tree implementation
(e.g. Red-Black Tree, AVL-Tree, AA-Tree, Splay-Tree, Treap, etc.) for which
every node must also store two pointers to children nodes.

`Sorted Containers`_ takes all of the work out of Python sorted collections -
making your deployment and use of Python easy. There's no need to install a C
compiler or pre-build and distribute custom extensions. Performance is a
feature and testing has 100% coverage with unit tests and hours of stress.

.. _`Sorted Containers`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`sorted collections library`: http://www.grantjenks.com/docs/sortedcontainers/

Testimonials
------------

**Alex Martelli**, `Fellow of the Python Software Foundation`_

"Good stuff! ... I like the `simple, effective implementation`_ idea of
splitting the sorted containers into smaller "fragments" to avoid the O(N)
insertion costs."

**Jeff Knupp**, `author of Writing Idiomatic Python and Python Trainer`_

"That last part, "fast as C-extensions," was difficult to believe. I would need
some sort of `Performance Comparison`_ to be convinced this is true. The author
includes this in the docs. It is."

**Kevin Samuel**, `Python and Django Trainer`_

I'm quite amazed, not just by the code quality (it's incredibly readable and
has more comment than code, wow), but the actual amount of work you put at
stuff that is *not* code: documentation, benchmarking, implementation
explanations. Even the git log is clean and the unit tests run out of the box
on Python 2 and 3.

**Mark Summerfield**, a short plea for `Python Sorted Collections`_

Python's "batteries included" standard library seems to have a battery
missing. And the argument that "we never had it before" has worn thin. It is
time that Python offered a full range of collection classes out of the box,
including sorted ones.

`Sorted Containers`_ is used in popular open source projects such as:
`Zipline`_, an algorithmic trading library from Quantopian; `Angr`_, a binary
analysis platform from UC Santa Barbara; `Trio`_, an async I/O library; and
`Dask Distributed`_, a distributed computation library supported by Continuum
Analytics.

.. _`Fellow of the Python Software Foundation`: https://en.wikipedia.org/wiki/Alex_Martelli
.. _`simple, effective implementation`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`author of Writing Idiomatic Python and Python Trainer`: https://jeffknupp.com/
.. _`Python and Django Trainer`: https://www.elephorm.com/formateur/kevin-samuel
.. _`Python Sorted Collections`: http://www.qtrac.eu/pysorted.html
.. _`Zipline`: https://github.com/quantopian/zipline
.. _`Angr`: https://github.com/angr/angr
.. _`Trio`: https://github.com/python-trio/trio
.. _`Dask Distributed`: https://github.com/dask/distributed

Features
--------

- Pure-Python
- Fully documented
- Benchmark comparison (alternatives, runtimes, load-factors)
- 100% test coverage
- Hours of stress testing
- Performance matters (often faster than C implementations)
- Compatible API (nearly identical to older blist and bintrees modules)
- Feature-rich (e.g. get the five largest keys in a sorted dict: d.keys()[-5:])
- Pragmatic design (e.g. SortedSet is a Python set with a SortedList index)
- Developed on Python 3.7
- Tested on CPython 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7 and PyPy, PyPy3

.. image:: https://api.travis-ci.org/grantjenks/python-sortedcontainers.svg?branch=master
   :target: http://www.grantjenks.com/docs/sortedcontainers/

.. image:: https://ci.appveyor.com/api/projects/status/github/grantjenks/python-sortedcontainers?branch=master&svg=true
   :target: http://www.grantjenks.com/docs/sortedcontainers/

Quickstart
----------

Installing `Sorted Containers`_ is simple with `pip
<https://pypi.org/project/pip/>`_::

    $ pip install sortedcontainers

You can access documentation in the interpreter with Python's built-in `help`
function. The `help` works on modules, classes and methods in `Sorted
Containers`_.

.. code-block:: python

    >>> import sortedcontainers
    >>> help(sortedcontainers)
    >>> from sortedcontainers import SortedDict
    >>> help(SortedDict)
    >>> help(SortedDict.popitem)

Documentation
-------------

Complete documentation including performance comparisons is available at
http://www.grantjenks.com/docs/sortedcontainers/

User Guide
..........

For those wanting more details, this part of the documentation describes
introduction, implementation, performance, and development.

- `Introduction`_
- `Performance Comparison`_
- `Load Factor Performance Comparison`_
- `Runtime Performance Comparison`_
- `Simulated Workload Performance Comparison`_
- `Implementation Details`_
- `Performance at Scale`_
- `Developing and Contributing`_
- `Release History`_

.. _`Introduction`: http://www.grantjenks.com/docs/sortedcontainers/introduction.html
.. _`Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance.html
.. _`Load Factor Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-load.html
.. _`Runtime Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-runtime.html
.. _`Simulated Workload Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-workload.html
.. _`Implementation Details`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`Performance at Scale`: http://www.grantjenks.com/docs/sortedcontainers/performance-scale.html
.. _`Developing and Contributing`: http://www.grantjenks.com/docs/sortedcontainers/development.html
.. _`Release History`: http://www.grantjenks.com/docs/sortedcontainers/history.html

API Documentation
.................

If you are looking for information on a specific function, class or method,
this part of the documentation is for you.

- `Sorted List`_
- `Sorted Dict`_
- `Sorted Set`_

.. _`Sorted List`: http://www.grantjenks.com/docs/sortedcontainers/sortedlist.html
.. _`Sorted Dict`: http://www.grantjenks.com/docs/sortedcontainers/sorteddict.html
.. _`Sorted Set`: http://www.grantjenks.com/docs/sortedcontainers/sortedset.html

Talks
-----

- `Python Sorted Collections | PyCon 2016 Talk`_
- `SF Python Holiday Party 2015 Lightning Talk`_
- `DjangoCon 2015 Lightning Talk`_

.. _`Python Sorted Collections | PyCon 2016 Talk`: http://www.grantjenks.com/docs/sortedcontainers/pycon-2016-talk.html
.. _`SF Python Holiday Party 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/sf-python-2015-lightning-talk.html
.. _`DjangoCon 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/djangocon-2015-lightning-talk.html

Useful Links
------------

- `Sorted Containers Documentation`_
- `Sorted Containers at PyPI`_
- `Sorted Containers at Github`_
- `Sorted Containers Issue Tracker`_

.. _`Sorted Containers Documentation`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`Sorted Containers at PyPI`: https://pypi.org/project/sortedcontainers/
.. _`Sorted Containers at Github`: https://github.com/grantjenks/python-sortedcontainers
.. _`Sorted Containers Issue Tracker`: https://github.com/grantjenks/python-sortedcontainers/issues

Sorted Containers License
-------------------------

Copyright 2014-2018 Grant Jenks

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.



  * [soupsieve-1.9.2](https://github.com/facelessuser/soupsieve) [![Unix Build Status][travis-image]][travis-link]
[![Windows Build Status][appveyor-image]][appveyor-link]
[![Coverage Status][codecov-image]][codecov-link]
[![PyPI Version][pypi-image]][pypi-link]
![License][license-image-mit]

# Soup Sieve

## Overview

Soup Sieve is a CSS selector library designed to be used with [Beautiful Soup 4][bs4]. It aims to provide selecting,
matching, and filtering using modern CSS selectors. Soup Sieve currently provides selectors from the CSS level 1
specifications up through the latest CSS level 4 drafts and beyond (though some are not yet implemented).

Soup Sieve was written with the intent to replace Beautiful Soup's builtin select feature, and as of Beautiful Soup
version 4.7.0, it now is :confetti_ball:. Soup Sieve can also be imported in order to use its API directly for
more controlled, specialized parsing.

Soup Sieve has implemented most of the CSS selectors up through the latest CSS draft specifications, though there are a
number that don't make sense in a non-browser environment. Selectors that cannot provide meaningful functionality simply
do not match anything. Some of the supported selectors are:

- `.classes`
- `#ids`
- `[attributes=value]`
- `parent child`
- `parent > child`
- `sibling ~ sibling`
- `sibling + sibling`
- `:not(element.class, element2.class)`
- `:is(element.class, element2.class)`
- `parent:has(> child)`
- and [many more](https://facelessuser.github.io/soupsieve/selectors/)


## Installation

You must have Beautiful Soup already installed:

```
pip install beautifulsoup4
```

In most cases, assuming you've installed version 4.7.0, that should be all you need to do, but if you've installed via
some alternative method, and Soup Sieve is not automatically installed for your, you can install it directly:

```
pip install soupsieve
```

If you want to manually install it from source, navigate to the root of the project and run

```
python setup.py build
python setup.py install
```

## Documentation

Documentation is found here: http://facelessuser.github.io/soupsieve/.

## License

MIT License

Copyright (c) 2018 - 2019 Isaac Muse <isaacmuse@gmail.com>

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

[bs4]: https://beautiful-soup-4.readthedocs.io/en/latest/#

[codecov-image]: https://img.shields.io/codecov/c/github/facelessuser/soupsieve/master.svg
[codecov-link]: https://codecov.io/github/facelessuser/soupsieve
[travis-image]: https://img.shields.io/travis/facelessuser/soupsieve/master.svg?label=Unix%20Build&logo=travis
[travis-link]: https://travis-ci.org/facelessuser/soupsieve
[appveyor-image]: https://img.shields.io/appveyor/ci/facelessuser/soupsieve/master.svg?label=Windows%20Build&logo=appveyor
[appveyor-link]: https://ci.appveyor.com/project/facelessuser/soupsieve
[pypi-image]: https://img.shields.io/pypi/v/soupsieve.svg?logo=python&logoColor=white
[pypi-link]: https://pypi.python.org/pypi/soupsieve
[license-image-mit]: https://img.shields.io/badge/license-MIT-blue.svg



  * [sphinx-autodoc-typehints-1.7.0]() sphinx-autodoc-typehints
========================

This extension allows you to use Python 3 annotations for documenting acceptable argument types
and return value types of functions. This allows you to use type hints in a very natural fashion,
allowing you to migrate from this:

.. code-block:: python

    def format_unit(value, unit):
        """
        Formats the given value as a human readable string using the given units.

        :param float|int value: a numeric value
        :param str unit: the unit for the value (kg, m, etc.)
        :rtype: str
        """
        return '{} {}'.format(value, unit)

to this:

.. code-block:: python

    from typing import Union

    def format_unit(value: Union[float, int], unit: str) -> str:
        """
        Formats the given value as a human readable string using the given units.

        :param value: a numeric value
        :param unit: the unit for the value (kg, m, etc.)
        """
        return '{} {}'.format(value, unit)


Installation and setup
----------------------

First, use pip to download and install the extension::

    $ pip install sphinx-autodoc-typehints

Then, add the extension to your ``conf.py``:

.. code-block:: python

    extensions = [
        'sphinx.ext.autodoc',
        'sphinx_autodoc_typehints'
    ]


Options
-------

The following configuration options are accepted:

* ``set_type_checking_flag`` (default: ``False``): if ``True``, set ``typing.TYPE_CHECKING`` to
  ``True`` to enable "expensive" typing imports
* ``typehints_fully_qualified`` (default: ``False``): if ``True``, class names are always fully
  qualified (e.g. ``module.for.Class``). If ``False``, just the class name displays (e.g.
  ``Class``)
* ``always_document_param_types`` (default: ``False``): If ``False``, do not add type info for
  undocumented parameters.  If ``True``, add stub documentation for undocumented parameters to
  be able to add type info.


How it works
------------

The extension listens to the ``autodoc-process-signature`` and ``autodoc-process-docstring``
Sphinx events. In the former, it strips the annotations from the function signature. In the latter,
it injects the appropriate ``:type argname:`` and ``:rtype:`` directives into the docstring.

Only arguments that have an existing ``:param:`` directive in the docstring get their respective
``:type:`` directives added. The ``:rtype:`` directive is added if and only if no existing
``:rtype:`` is found.


Compatibility with sphinx.ext.napoleon
--------------------------------------

To use `sphinx.ext.napoleon`_ with sphinx-autodoc-typehints, make sure you load
`sphinx.ext.napoleon`_ first, **before** sphinx-autodoc-typehints. See `Issue 15`_ on the issue
tracker for more information.

.. _sphinx.ext.napoleon: http://www.sphinx-doc.org/en/stable/ext/napoleon.html
.. _Issue 15: https://github.com/agronholm/sphinx-autodoc-typehints/issues/15


Dealing with circular imports
-----------------------------

Sometimes functions or classes from two different modules need to reference each other in their
type annotations. This creates a circular import problem. The solution to this is the following:

#. Import only the module, not the classes/functions from it
#. Use forward references in the type annotations (e.g.
   ``def methodname(self, param1: 'othermodule.OtherClass'):``)

On Python 3.7, you can even use ``from __future__ import annotations`` and remove the quotes.


Using type hint comments
------------------------

If you're documenting code that needs to stay compatible with Python 2.7, you cannot use regular
type annotations. Instead, you must either be using Python 3.8 or later or have typed_ast_
installed. The package extras ``type_comments`` will pull in the appropiate dependencies automatically.
Then you can add type hint comments in the following manner:

.. code-block:: python

    def myfunction(arg1, arg2):
        # type: (int, str) -> int
        return 42

or alternatively:

.. code-block:: python

    def myfunction(
        arg1,  # type: int
        arg2  # type: str
    ):
        # type: (...) -> int
        return 42

.. _typed_ast: https://pypi.org/project/typed-ast/



  * [sphinx-rtd-theme-0.4.3](https://github.com/rtfd/sphinx_rtd_theme/) 
**************************
Read the Docs Sphinx Theme
**************************

.. image:: https://img.shields.io/pypi/v/sphinx_rtd_theme.svg
   :target: https://pypi.python.org/pypi/sphinx_rtd_theme
   :alt: Pypi Version 
.. image:: https://travis-ci.org/rtfd/sphinx_rtd_theme.svg?branch=master
   :target: https://travis-ci.org/rtfd/sphinx_rtd_theme
   :alt: Build Status
.. image:: https://img.shields.io/pypi/l/sphinx_rtd_theme.svg
   :target: https://pypi.python.org/pypi/sphinx_rtd_theme/
   :alt: License
.. image:: https://readthedocs.org/projects/sphinx-rtd-theme/badge/?version=latest
  :target: http://sphinx-rtd-theme.readthedocs.io/en/latest/?badge=latest
  :alt: Documentation Status

The ``sphinx_rtd_theme`` is a sphinx_ theme designed to look modern and be mobile-friendly.
This theme is primarily focused to be used on readthedocs.org_ but can work with your
own sphinx projects. To read more and see a working demo_ head over to readthedocs.org_.

.. _sphinx: http://www.sphinx-doc.org
.. _readthedocs.org: http://www.readthedocs.org
.. _demo: https://sphinx-rtd-theme.readthedocs.io/en/latest/


Installing
==========

The theme is distributed on PyPI_ and can be installed with pip::

   pip install sphinx_rtd_theme

For more information read the full installing docs
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/installing.html>`__.

.. _PyPI: https://pypi.python.org/pypi/sphinx_rtd_theme


Configuration
=============

The ``sphinx_rtd_theme`` is highly customizable on both the page level and on a global level.
To see all the possible configuration options read the configuring docs
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/configuring.html>`__.


Contributing
============

If you would like to help improve the theme or have more control
over the theme in case of a fork please read our contributing guide
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/contributing.html>`__.



  * [sphinx_rtd_theme-0.4.3](https://github.com/rtfd/sphinx_rtd_theme/) 
**************************
Read the Docs Sphinx Theme
**************************

.. image:: https://img.shields.io/pypi/v/sphinx_rtd_theme.svg
   :target: https://pypi.python.org/pypi/sphinx_rtd_theme
   :alt: Pypi Version 
.. image:: https://travis-ci.org/rtfd/sphinx_rtd_theme.svg?branch=master
   :target: https://travis-ci.org/rtfd/sphinx_rtd_theme
   :alt: Build Status
.. image:: https://img.shields.io/pypi/l/sphinx_rtd_theme.svg
   :target: https://pypi.python.org/pypi/sphinx_rtd_theme/
   :alt: License
.. image:: https://readthedocs.org/projects/sphinx-rtd-theme/badge/?version=latest
  :target: http://sphinx-rtd-theme.readthedocs.io/en/latest/?badge=latest
  :alt: Documentation Status

The ``sphinx_rtd_theme`` is a sphinx_ theme designed to look modern and be mobile-friendly.
This theme is primarily focused to be used on readthedocs.org_ but can work with your
own sphinx projects. To read more and see a working demo_ head over to readthedocs.org_.

.. _sphinx: http://www.sphinx-doc.org
.. _readthedocs.org: http://www.readthedocs.org
.. _demo: https://sphinx-rtd-theme.readthedocs.io/en/latest/


Installing
==========

The theme is distributed on PyPI_ and can be installed with pip::

   pip install sphinx_rtd_theme

For more information read the full installing docs
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/installing.html>`__.

.. _PyPI: https://pypi.python.org/pypi/sphinx_rtd_theme


Configuration
=============

The ``sphinx_rtd_theme`` is highly customizable on both the page level and on a global level.
To see all the possible configuration options read the configuring docs
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/configuring.html>`__.


Contributing
============

If you would like to help improve the theme or have more control
over the theme in case of a fork please read our contributing guide
`here <https://sphinx-rtd-theme.readthedocs.io/en/latest/contributing.html>`__.



  * [sphinxcontrib-applehelp-1.0.1]() sphinxcontrib-applehelp is a sphinx extension which outputs Apple help books

Home-page: http://sphinx-doc.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: https://pypi.org/project/sphinxcontrib-applehelp/
Description: 
        sphinxcontrib-applehelp is a sphinx extension which outputs Apple help books
        
Platform: any
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Framework :: Sphinx
Classifier: Framework :: Sphinx :: Extension
Classifier: Topic :: Documentation
Classifier: Topic :: Documentation :: Sphinx
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.5
Provides-Extra: test

  * [sphinxcontrib-devhelp-1.0.1]() sphinxcontrib-devhelp is a sphinx extension which outputs Devhelp document.

Home-page: http://sphinx-doc.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: https://pypi.org/project/sphinxcontrib-devhelp/
Description: 
        sphinxcontrib-devhelp is a sphinx extension which outputs Devhelp document.
        
Platform: any
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Framework :: Sphinx
Classifier: Framework :: Sphinx :: Extension
Classifier: Topic :: Documentation
Classifier: Topic :: Documentation :: Sphinx
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.5
Provides-Extra: test

  * [sphinxcontrib-htmlhelp-1.0.2]() sphinxcontrib-htmlhelp is a sphinx extension which ...

Home-page: http://sphinx-doc.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: https://pypi.org/project/sphinxcontrib-htmlhelp/
Description: 
        sphinxcontrib-htmlhelp is a sphinx extension which ...
        
Platform: any
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Framework :: Sphinx
Classifier: Framework :: Sphinx :: Extension
Classifier: Topic :: Documentation
Classifier: Topic :: Documentation :: Sphinx
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.5
Provides-Extra: test

  * [sphinxcontrib-jsmath-1.0.1](http://sphinx-doc.org/) 
sphinxcontrib-jsmath is a sphinx extension which renders display math in HTML
via JavaScript.



  * [sphinxcontrib-qthelp-1.0.2]() sphinxcontrib-qthelp is a sphinx extension which outputs QtHelp document.

Home-page: http://sphinx-doc.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: https://pypi.org/project/sphinxcontrib-qthelp/
Description: 
        sphinxcontrib-qthelp is a sphinx extension which outputs QtHelp document.
        
Platform: any
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Framework :: Sphinx
Classifier: Framework :: Sphinx :: Extension
Classifier: Topic :: Documentation
Classifier: Topic :: Documentation :: Sphinx
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.5
Provides-Extra: test

  * [sphinxcontrib-serializinghtml-1.1.3]() sphinxcontrib-serializinghtml is a sphinx extension which outputs
"serialized" HTML files (json and pickle).

Home-page: http://sphinx-doc.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: https://pypi.org/project/sphinxcontrib-serializinghtml/
Description: 
        sphinxcontrib-serializinghtml is a sphinx extension which outputs
        "serialized" HTML files (json and pickle).
        
Platform: any
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: License :: OSI Approved :: BSD License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Framework :: Sphinx
Classifier: Framework :: Sphinx :: Extension
Classifier: Topic :: Documentation
Classifier: Topic :: Documentation :: Sphinx
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.5
Provides-Extra: test

  * [sphinxcontrib-websupport-1.1.2](http://sphinx-doc.org/) 
sphinxcontrib-websupport provides a Python API to easily integrate Sphinx
documentation into your Web application.



  * [sphinxcontrib_github_alt-1.1](https://github.com/jupyter/sphinxcontrib_github_alt) Link to GitHub issues, pull requests, commits and users for a particular project.

To use this extension, add it to the ``extensions`` list in ``conf.py``,
and set the variable ``github_project_url``:

.. code-block:: python

    extensions = [...
                  'sphinxcontrib_github_alt',
                 ]

    github_project_url = "https://github.com/ipython/ipython"

Then use these roles in your documentation:

.. code-block:: rst

    * :ghissue:`12` - link to issue #12
    * :ghpull:`35` - link to pull request #35
    * :ghcommit:`3a1cb54` - link to commit
    * :ghuser:`ipython` - link to a user or organisation


It's called 'alt' because `sphinxcontrib-github
<https://pypi.python.org/pypi/sphinxcontrib-github/>`__ already exists. IPython
& Jupyter projects have been using the syntax defined in this extension for some
time before we made it into its own package, so we didn't want to switch to
``sphinxcontrib-github``.

  * [sqlalchemy-migrate-0.12.0](http://www.openstack.org/) SQLAlchemy Migrate
==================

Fork from http://code.google.com/p/sqlalchemy-migrate/ to get it working with
SQLAlchemy 0.8.

Inspired by Ruby on Rails' migrations, Migrate provides a way to deal with
database schema changes in `SQLAlchemy <http://sqlalchemy.org>`_ projects.

Migrate extends SQLAlchemy to have database changeset handling. It provides a
database change repository mechanism which can be used from the command line as
well as from inside python code.

Help
----

Sphinx documentation is available at the project page `readthedocs.org
<https://sqlalchemy-migrate.readthedocs.org/>`_.

Users and developers can be found at #openstack-dev on Freenode IRC
network and at the public users mailing list `migrate-users
<http://groups.google.com/group/migrate-users>`_.

New releases and major changes are announced at the public announce mailing
list `openstack-dev
<http://lists.openstack.org/cgi-bin/mailman/listinfo/openstack-dev>`_
and at the Python package index `sqlalchemy-migrate
<http://pypi.python.org/pypi/sqlalchemy-migrate>`_.

Homepage is located at `stackforge
<http://github.com/stackforge/sqlalchemy-migrate/>`_

You can also clone a current `development version
<http://github.com/stackforge/sqlalchemy-migrate>`_

Tests and Bugs
--------------

To run automated tests:

* install tox: ``pip install -U tox``
* run tox: ``tox``
* to test only a specific Python version: ``tox -e py27`` (Python 2.7)

Please report any issues with sqlalchemy-migrate to the issue tracker at
`Launchpad issues
<https://bugs.launchpad.net/sqlalchemy-migrate>`_




  * [sqlparse-0.3.0](https://github.com/andialbrecht/sqlparse) 
``sqlparse`` is a non-validating SQL parser module.
It provides support for parsing, splitting and formatting SQL statements.

Visit the `project page <https://github.com/andialbrecht/sqlparse>`_ for
additional information and documentation.

**Example Usage**


Splitting SQL statements::

   >>> import sqlparse
   >>> sqlparse.split('select * from foo; select * from bar;')
   [u'select * from foo; ', u'select * from bar;']


Formatting statements::

   >>> sql = 'select * from foo where id in (select id from bar);'
   >>> print sqlparse.format(sql, reindent=True, keyword_case='upper')
   SELECT *
   FROM foo
   WHERE id IN
     (SELECT id
      FROM bar);


Parsing::

   >>> sql = 'select * from someschema.mytable where id = 1'
   >>> res = sqlparse.parse(sql)
   >>> res
   (<Statement 'select...' at 0x9ad08ec>,)
   >>> stmt = res[0]
   >>> str(stmt)  # converting it back to unicode
   'select * from someschema.mytable where id = 1'
   >>> # This is how the internal representation looks like:
   >>> stmt.tokens
   (<DML 'select' at 0x9b63c34>,
    <Whitespace ' ' at 0x9b63e8c>,
    <Operator '*' at 0x9b63e64>,
    <Whitespace ' ' at 0x9b63c5c>,
    <Keyword 'from' at 0x9b63c84>,
    <Whitespace ' ' at 0x9b63cd4>,
    <Identifier 'somes...' at 0x9b5c62c>,
    <Whitespace ' ' at 0x9b63f04>,
    <Where 'where ...' at 0x9b5caac>)




  * [ssh-import-id-5.6](https://launchpad.net/ssh-import-id) ssh-import-id
===========

You're logged onto a cloud instance working on a problem with your fellow devs, and you want to invite them to log in and take a look at these crazy log messages. What do?

Oh. You have to ask them to cat their public SSH key, paste it into IRC (wait, no, it's id\_rsa.pub, not id\_rsa silly!) then you copy it and cat it to the end of authorized\_hosts.

That's where ssh-import-id comes in. With ssh-import-id, you can add the public SSH keys from a known, trusted online identity to grant SSH access.

Currently supported identities include Github and Launchpad.

Usage
-----

ssh-import-id uses short prefix to indicate the location of the online identity. For now, these are:

	'gh:' for Github
	'lp:' for Launchpad

Command line help:

	usage: ssh-import-id [-h] [-o FILE] USERID [USERID ...]

	Authorize SSH public keys from trusted online identities.

	positional arguments:
  	USERID                User IDs to import

	optional arguments:
  	-h, --help            show this help message and exit
  	-o FILE, --output FILE
                        	Write output to file (default ~/.ssh/authorized_keys)

Example
-------

If you wanted me to be able to ssh into your server, as the desired user on that machine you would use:

	$ ssh-import-id gh:cmars

You can also import multiple users on the same line, even from different key services, like so:

	$ ssh-import-id gh:cmars lp:kirkland

Used with care, it's a great collaboration tool!

Installing
----------

ssh-import-id can be installed on Python >= 2.6 with a recent version of pip:

	$ pip install ssh-import-id

ssh-import-id requires a recent version of Requests (>=1.1.0) for verified SSL/TLS connections.

Extending
---------

You can add support for your own SSH public key providers by creating a script named ssh-import-id-*prefix*. Make the script executable and place it in the same bin directory as ssh-import-id.

The script should accept the identity username for the service it connects to, and output lines in the same format as an ~/.ssh/authorized\_keys file.

If you do develop such a handler, I recommend that you connect to the service with SSL/TLS, and require a valid certificate and matching hostname. Use Requests.get(url, verify=True), for example.

Credits
-------

This project is authored and maintained by Dustin Kirkland, Scott Moser, and Casey Marshall.
  * [statsmodels-0.10.1](https://www.statsmodels.org/) |Travis Build Status| |Azure CI Build Status| |Appveyor Build Status| |Coveralls Coverage|

About Statsmodels
=================

Statsmodels is a Python package that provides a complement to scipy for
statistical computations including descriptive statistics and estimation
and inference for statistical models.


Documentation
=============

The documentation for the latest release is at

https://www.statsmodels.org/stable/

The documentation for the development version is at

https://www.statsmodels.org/dev/

Recent improvements are highlighted in the release notes

https://www.statsmodels.org/stable/release/version0.9.html

Backups of documentation are available at https://statsmodels.github.io/stable/
and https://statsmodels.github.io/dev/.



Main Features
=============

* Linear regression models:

  - Ordinary least squares
  - Generalized least squares
  - Weighted least squares
  - Least squares with autoregressive errors
  - Quantile regression
  - Recursive least squares

* Mixed Linear Model with mixed effects and variance components
* GLM: Generalized linear models with support for all of the one-parameter
  exponential family distributions
* Bayesian Mixed GLM for Binomial and Poisson
* GEE: Generalized Estimating Equations for one-way clustered or longitudinal data
* Discrete models:

  - Logit and Probit
  - Multinomial logit (MNLogit)
  - Poisson and Generalized Poisson regression
  - Negative Binomial regression
  - Zero-Inflated Count models

* RLM: Robust linear models with support for several M-estimators.
* Time Series Analysis: models for time series analysis

  - Complete StateSpace modeling framework

    - Seasonal ARIMA and ARIMAX models
    - VARMA and VARMAX models
    - Dynamic Factor models
    - Unobserved Component models

  - Markov switching models (MSAR), also known as Hidden Markov Models (HMM)
  - Univariate time series analysis: AR, ARIMA
  - Vector autoregressive models, VAR and structural VAR
  - Vector error correction modle, VECM
  - exponential smoothing, Holt-Winters
  - Hypothesis tests for time series: unit root, cointegration and others
  - Descriptive statistics and process models for time series analysis

* Survival analysis:

  - Proportional hazards regression (Cox models)
  - Survivor function estimation (Kaplan-Meier)
  - Cumulative incidence function estimation

* Multivariate:

  - Principal Component Analysis with missing data
  - Factor Analysis with rotation
  - MANOVA
  - Canonical Correlation

* Nonparametric statistics: Univariate and multivariate kernel density estimators
* Datasets: Datasets used for examples and in testing
* Statistics: a wide range of statistical tests

  - diagnostics and specification tests
  - goodness-of-fit and normality tests
  - functions for multiple testing
  - various additional statistical tests

* Imputation with MICE, regression on order statistic and Gaussian imputation
* Mediation analysis
* Graphics includes plot functions for visual analysis of data and model results

* I/O

  - Tools for reading Stata .dta files, but pandas has a more recent version
  - Table output to ascii, latex, and html

* Miscellaneous models
* Sandbox: statsmodels contains a sandbox folder with code in various stages of
  developement and testing which is not considered "production ready".  This covers
  among others

  - Generalized method of moments (GMM) estimators
  - Kernel regression
  - Various extensions to scipy.stats.distributions
  - Panel data models
  - Information theoretic measures

How to get it
=============
The master branch on GitHub is the most up to date code

https://www.github.com/statsmodels/statsmodels

Source download of release tags are available on GitHub

https://github.com/statsmodels/statsmodels/tags

Binaries and source distributions are available from PyPi

https://pypi.org/project/statsmodels/

Binaries can be installed in Anaconda

conda install statsmodels


Installing from sources
=======================

See INSTALL.txt for requirements or see the documentation

https://statsmodels.github.io/dev/install.html

License
=======

Modified BSD (3-clause)

Discussion and Development
==========================

Discussions take place on our mailing list.

http://groups.google.com/group/pystatsmodels

We are very interested in feedback about usability and suggestions for
improvements.

Bug Reports
===========

Bug reports can be submitted to the issue tracker at

https://github.com/statsmodels/statsmodels/issues

.. |Travis Build Status| image:: https://travis-ci.org/statsmodels/statsmodels.svg?branch=master
   :target: https://travis-ci.org/statsmodels/statsmodels
.. |Azure CI Build Status| image:: https://dev.azure.com/statsmodels/statsmodels-testing/_apis/build/status/statsmodels.statsmodels?branch=master
   :target: https://dev.azure.com/statsmodels/statsmodels-testing/_build/latest?definitionId=1&branch=master
.. |Appveyor Build Status| image:: https://ci.appveyor.com/api/projects/status/gx18sd2wc63mfcuc/branch/master?svg=true
   :target: https://ci.appveyor.com/project/josef-pkt/statsmodels/branch/master
.. |Coveralls Coverage| image:: https://coveralls.io/repos/github/statsmodels/statsmodels/badge.svg?branch=master
   :target: https://coveralls.io/github/statsmodels/statsmodels?branch=master
  * [stdeb-0.8.5](http://github.com/astraw/stdeb) .. image:: https://travis-ci.org/astraw/stdeb.png?branch=master
        :target: https://travis-ci.org/astraw/stdeb

stdeb - Python to Debian source package conversion utility
==========================================================

`stdeb <http://github.com/astraw/stdeb>`_ produces Debian source
packages from Python packages via a new distutils command,
``sdist_dsc``. Automatic defaults are provided for the Debian package,
but many aspects of the resulting package can be customized (see the
customizing section, below). An additional command, ``bdist_deb``,
creates a Debian binary package, a .deb file. The ``install_deb``
command installs this .deb file. The ``debianize`` command builds a
``debian/`` directory directly alongside your setup.py.

Several convenience utilities are also provided:

* ``pypi-download`` will query the `Python Package Index (PyPI)
  <http://pypi.python.org/>`_ for a package and download it.
* ``pypi-install`` will query the `Python Package Index (PyPI)
  <http://pypi.python.org/>`_ for a package, download it, create a
  .deb from it, and then install the .deb.
* ``py2dsc`` will convert a distutils-built source tarball into a
  Debian source package.
* ``py2dsc-deb`` will convert a distutils-built source tarball into a
  Debian source package and then use the Debian machinery to build a
  .deb file from this.

.. contents::


Python 3 support
----------------

As explained in more detail below, the heart of stdeb is the sdist_dsc
distutils command. This command runs once to generate a Debian source
package. This Debian source package can specify building packages for
Python 2, Python 3, or both. Furthermore, this generation can be done
with the Python 2 or Python 3 interpreter. By default, only packages
are built for the version of Python being used. To override this, use
``--with-python2=True`` or ``--with-python3=True`` as an argument to
the sdist_dsc distutils command (or use both to be sure). For example,
to build only a Python 3 package using the Python 3 interpreter::

  python3 setup.py --command-packages=stdeb.command bdist_deb

To build both Python 2 and Python 3 packages using the Python 3
interpreter (and only the Python3 package installs scripts)::

  python3 setup.py --command-packages=stdeb.command sdist_dsc --with-python2=True --with-python3=True --no-python2-scripts=True bdist_deb

News
----

 * 2015-02-18: **Version 0.8.5**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.8.5>`__. Bugfixes: reverted
   change that installed into virtualenv when built in
   virtualenv. Improvements: Added
   `--allow-virtualenv-install-location` to allow installing into
   virtualenv location. Supports Debian Squeeze (6), Debian Wheezy
   (7), Ubuntu Precise (12.04), Ubuntu Trusty (14.04) and later
   releases.

 * 2015-02-16: **Version 0.8.4**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.8.4>`__. Bugfixes: works on
   Python 3.4 (e.g. Ubuntu Trusty) again. Improvements: Improved
   customization for Python 3 (Dirk Thomas added
   `force-x-python3-version` and `X-Python3-Version` and Louis for
   `Recommends3`, `Suggests3`, `Provides3` and `Replaces3`
   support. Supports Debian Squeeze (6), Debian Wheezy (7), Ubuntu
   Precise (12.04), Ubuntu Trusty (14.04) and later releases.

 * 2015-02-14: **Version 0.8.3**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.8.3>`__. This is a bugfix
   release which fixes several aspects of Unicode support. Tests pass
   on Debian Squeeze (6), Debian Wheezy (7), and Ubuntu Precise
   (12.04). Support for Python 3.4 (e.g. Ubuntu Trusty 14.04) was
   mistakenly broken and was fixed in the 0.8.3 release.

 * 2014-8-14: **Version 0.8.2**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.8.2>`__. This is a bugfix
   release fixing a serious issue that would cause a Python 2 package
   to be built if only a Python 3 package was requested in some
   circumstances.

 * 2014-8-10: **Version 0.8.1**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.8.1>`__. Due
   to bugs in 0.8.0, this release is the first announced from the 0.8
   series. Highlights since 0.7.1:

   - Full support for Python 3. This includes being run from Python 3
     and generating packages for Python 3. The default is to build
     Python 3 packages when run with Python 3 and to build Python 2
     packages when run from Python 2. Command line options can be used
     to build packages for the other Python interpreter, too.

   - Build .changes file for source package. While this still must be
     signed for upload to a PPA, for example, it should still be
     useful in some cases.

   - Switch to Debian source format 3.0 (quilt). Practically speaking,
     the .diff.gz file that used to come with a source package is now
     replaced by a .debian.tar.gz file.

   - Verify SSL certificates when talking to PyPI using
     Requests. (Verification requires Requests >= 0.8.8.)

   - Many bugfixes.

 * 2014-05-05: **Version 0.7.1**. See the `download page
   <https://pypi.python.org/pypi/stdeb/0.7.1>`__. Highlights for this
   release (you may also wish to consult the full `changelog
   <http://github.com/astraw/stdeb/blob/release-0.7.1/CHANGELOG.txt>`__). Due
   to bugs in 0.7.0, this release is the first announced from the 0.7
   series. Highlights since 0.6.0:

   - New commands: pypi-download and pypi-install to directly download
     and install packages from PyPI, respectively. py2dsc-deb directly
     creates a .deb file from a source tarball.

   - New distutils command: install_deb lets you directly install a
     python package as a standard system package.

   - Many bugfixes, including the new URL for PyPI.

   - Automated runs of test suite, thanks to Travis CI

   - Thanks to many, especially Piotr Ożarowski for help with stdeb.

 * 2010-06-18: **Version 0.6.0**. See the `download page
   <http://pypi.python.org/pypi/stdeb/0.6.0>`__. Highlights for this
   release (you may also wish to consult the full `changelog
   <http://github.com/astraw/stdeb/blob/release-0.6.0/CHANGELOG.txt>`__):

   - A new ``debianize`` command to build a ``debian/`` directory
     alongside your setup.py file.

   - Bugfixes.

 * 2010-01-09: **Version 0.5.1**. Bugfix release. See the `download
   page <http://pypi.python.org/pypi/stdeb/0.5.1>`__, the `changelog
   <http://github.com/astraw/stdeb/blob/release-0.5.1/CHANGELOG.txt>`__
   and `release notes
   <http://github.com/astraw/stdeb/blob/release-0.5.1/RELEASE_NOTES.txt>`__.

 * 2009-12-30: **Version 0.5.0**. See the `download page
   <http://pypi.python.org/pypi/stdeb/0.5.0>`__. Highlights for this
   release (you may also wish to consult the full `changelog
   <http://github.com/astraw/stdeb/blob/release-0.5.0/CHANGELOG.txt>`__):

   - A new ``pypi-install`` script will automatically download, make a
     .deb, and install packages from the `Python Package Index (PyPI)`_.

   - Removal of the setuptools dependency.

   - New option (`--guess-conflicts-provides-replaces`) to query
     original Debian packages for Conflicts/Provides/Replaces
     information.

   - As a result of these changes and to fix a couple bugs/warts, some
     minor backwards incompatible changes and deprecations were
     made. Please check the `release notes
     <http://github.com/astraw/stdeb/blob/release-0.5.0/RELEASE_NOTES.txt>`__.

 * 2009-12-28: Version 0.4.3 Released. See the `download page`__. See the
   `changelog`__ and `release notes`__.
 * 2009-11-02: Version 0.4.2 Released. See the `download page`__. See the
   `changelog`__ and `release notes`__.
 * 2009-10-04: Version 0.4.1 Released. See the `download page`__. See the
   `changelog`__ and `release notes`__.
 * 2009-09-27: Version 0.4 Released. See the `download page`__. This
   version switches to debhelper 7. See the `Changelog for 0.4`__.

__ http://pypi.python.org/pypi/stdeb/0.4.3
__ http://github.com/astraw/stdeb/blob/release-0.4.3/CHANGELOG.txt
__ http://github.com/astraw/stdeb/blob/release-0.4.3/RELEASE_NOTES.txt
__ http://pypi.python.org/pypi/stdeb/0.4.2
__ http://github.com/astraw/stdeb/blob/release-0.4.2/CHANGELOG.txt
__ http://github.com/astraw/stdeb/blob/release-0.4.2/RELEASE_NOTES.txt
__ http://pypi.python.org/pypi/stdeb/0.4.1
__ http://github.com/astraw/stdeb/blob/release-0.4.1/CHANGELOG.txt
__ http://github.com/astraw/stdeb/blob/release-0.4.1/RELEASE_NOTES.txt
__ http://pypi.python.org/pypi/stdeb/0.4
__ http://github.com/astraw/stdeb/blob/release-0.4/CHANGELOG.txt

Releases up to and including 0.3.2 are compatible with Ubuntu Hardy.

 * 2009-10-04: Version 0.3.2 Released. See the `download page`__. See the `Changelog for 0.3.2`__
 * 2009-09-27: Version 0.3.1 Released. See the `download page`__. See the `Changelog for 0.3.1`__
 * 2009-03-21: Version 0.3 Released. See the `download page`__. See the `Changelog for 0.3`__
 * 2009-02-17: Version 0.2.3 Released. See the `download page`__. See the `Changelog for 0.2.3`__
 * 2009-01-29: Version 0.2.2 Released. See the `download page`__. See the `Changelog for 0.2.2`__
 * 2008-04-26: Version 0.2.1 Released. See the `download page`__. See the `Changelog for 0.2.1`__
 * 2008-04-26: Version 0.2 Released. See the `download page`__. See the `Changelog for 0.2`__
 * 2007-04-02: Version 0.2.a1 Released. See the `old download page`_.
 * 2006-06-19: Version 0.1 Released. See the `old download page`_.

__ http://pypi.python.org/pypi/stdeb/0.3.2
__ http://github.com/astraw/stdeb/blob/release-0.3.2/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.3.1
__ http://github.com/astraw/stdeb/blob/release-0.3.1/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.3
__ http://github.com/astraw/stdeb/blob/release-0.3/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.2.3
__ http://github.com/astraw/stdeb/blob/release-0.2.3/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.2.2
__ http://github.com/astraw/stdeb/blob/release-0.2.2/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.2.1
__ http://github.com/astraw/stdeb/blob/release-0.2.1/CHANGELOG.txt
__ http://pypi.python.org/pypi/stdeb/0.2
__ http://github.com/astraw/stdeb/blob/release-0.2/CHANGELOG.txt

The commands
------------

pypi-download, command-line command
```````````````````````````````````

``pypi-download`` takes a package name, queries PyPI for it and downloads
it::

  pypi-download [options] mypackage

pypi-install, command-line command
``````````````````````````````````

``pypi-install`` takes a package name, queries PyPI for it, downloads
it, builds a Debian source package and then .deb from it, and this
installs it::

  pypi-install [options] mypackage

py2dsc, command-line command
````````````````````````````

``py2dsc`` takes a .tar.gz source package and build a Debian source
package from it::

  py2dsc [options] mypackage-0.1.tar.gz # uses pre-built Python source package

py2dsc-deb, command-line command
````````````````````````````````

``py2dsc-deb`` takes a .tar.gz source package and build a Debian source
package and then a .deb file from it::

  py2dsc-deb [options] mypackage-0.1.tar.gz # uses pre-built Python source package

sdist_dsc, distutils command
````````````````````````````
All methods eventually result in a call to the ``sdist_dsc`` distutils
command. You may prefer to do so directly::

  python setup.py --command-packages=stdeb.command sdist_dsc

A Debian source package is produced from unmodified
Python packages. The following files are produced in a newly created
subdirectory ``deb_dist``:

 * ``packagename_versionname.orig.tar.gz``
 * ``packagename_versionname-debianversion.dsc``
 * ``packagename_versionname-debianversion.diff.gz``

These can then be compiled into binary packages using the standard
Debian machinery (e.g. dpkg-buildpackage).

bdist_deb, distutils command
````````````````````````````
A ``bdist_deb`` distutils command is installed. This calls the
sdist_dsc command and then runs dpkg-buildpackage on the result::

  python setup.py --command-packages=stdeb.command bdist_deb

install_deb, distutils command
``````````````````````````````

The ``install_deb`` distutils command calls the bdist_deb command and
then installs the result. You need to run this with superuser privilege::

  sudo python setup.py --command-packages=stdeb.command install_deb

debianize, distutils command
````````````````````````````
The ``debianize`` distutils command builds the same ``debian/``
directory as used in the previous command, but the output is placed
directly in the project's root folder (alongside setup.py). This is
useful for customizing the Debian package directly (rather than using
the various stdeb options to tune the generated package).

::

  python setup.py --command-packages=stdeb.command debianize

A note about telling distutils to use the stdeb distutils commands
``````````````````````````````````````````````````````````````````

Distutils command packages can also be specified in distutils
configuration files (rather than using the ``--command-packages``
command line argument to ``setup.py``), as specified in the `distutils
documentation
<https://docs.python.org/2/distutils/extending.html>`_. Specifically,
you could include this in your ``~/.pydistutils.cfg`` file::

  [global]
  command-packages: stdeb.command

Examples
--------

These all assume you have stdeb installed in your system Python
path. stdeb also works from a non-system Python path (e.g. a
`virtualenv <http://pypi.python.org/pypi/virtualenv>`_).

Quickstart 1: Install something from PyPI now, I don't care about anything else
```````````````````````````````````````````````````````````````````````````````

Do this from the command line::

  pypi-install mypackage

**Warning: Despite doing its best, there is absolutely no way stdeb
can guarantee all the Debian package dependencies will be properly
fulfilled without manual intervention. Using pypi-install bypasses
your ability to customize stdeb's behavior. Read the rest of this
document to understand how to make better packages.**

Quickstart 2: Just tell me the fastest way to make a .deb
`````````````````````````````````````````````````````````

(First, install stdeb as you normally install Python packages.)

Do this from the directory with your `setup.py` file::

  python setup.py --command-packages=stdeb.command bdist_deb

This will make a Debian source package (.dsc, .orig.tar.gz and
.diff.gz files) and then compile it to a Debian binary package (.deb)
for your current system. The result will be in ``deb_dist``.

**Warning: installing the .deb file on other versions of Ubuntu or
Debian than the one on which it was compiled will result in undefined
behavior. If you have extension modules, they will probably
break. Even in the absence of extension modules, bad stuff will likely
happen.**

For this reason, it is much better to build the Debian source package
and then compile that (e.g. using `Ubuntu's PPA`__) for each target
version of Debian or Ubuntu.

__ https://help.launchpad.net/Packaging/PPA

Quickstart 3: I read the warning, so show me how to make a source package, then compile it
``````````````````````````````````````````````````````````````````````````````````````````

This generates a source package::

  wget http://pypi.python.org/packages/source/R/Reindent/Reindent-0.1.0.tar.gz
  py2dsc Reindent-0.1.0.tar.gz

This turns it into a .deb using the standard Debian tools. (Do *this*
on the same source package for each target distribution)::

  cd deb_dist/reindent-0.1.0/
  dpkg-buildpackage -rfakeroot -uc -us

This installs it::

  cd ..
  sudo dpkg -i python-reindent_0.1.0-1_all.deb

Quickstart 4: Install from a Python package direct to a debian system package
`````````````````````````````````````````````````````````````````````````````

(First, install stdeb as you normally install Python packages.)

Do this from the directory with your `setup.py` file::

  python setup.py --command-packages=stdeb.command install_deb

This will make a Debian source package (.dsc, .orig.tar.gz and
.diff.gz files), compile it to a Debian binary package (.deb) for your
current system and then install it using ``dpkg``.


Another example, with more explanation
``````````````````````````````````````

This example is more useful if you don't have a Python source package
(.tar.gz file generated by ``python setup.py sdist``). For the sake of
illustration, we do download such a tarball, but immediately unpack it
(alternatively, use a version control system to grab the unpacked
source of a package)::

  wget http://pypi.python.org/packages/source/R/Reindent/Reindent-0.1.0.tar.gz
  tar xzf Reindent-0.1.0.tar.gz
  cd Reindent-0.1.0

The following will generate a directory ``deb_dist`` containing the
files ``reindent_0.1.0-1.dsc``, ``reindent_0.1.0.orig.tar.gz`` and
``reindent_0.1.0-1.diff.gz``, which, together, are a debian source
package::

  python setup.py --command-packages=stdeb.command sdist_dsc

The source generated in the above way is also extracted (using
``dpkg-source -x``) and placed in the ``deb_dist`` subdirectory. To
continue the example above::

  cd deb_dist/reindent-0.1.0
  dpkg-buildpackage -rfakeroot -uc -us

Finally, the generated package can be installed::

  cd ..
  sudo dpkg -i python-reindent_0.1.0-1_all.deb

For yet another example of use, with still more explanation, see
`allmydata-tahoe ticket 251`_.

.. _allmydata-tahoe ticket 251: http://allmydata.org/trac/tahoe/ticket/251

Download
--------

Files are available at the `download page`_ (for ancient releases, see
the `old download page`_).

.. _download page: https://pypi.python.org/pypi/stdeb
.. _old download page: http://stdeb.python-hosting.com/wiki/Download

The git repository is available at
http://github.com/astraw/stdeb

Install (or, using stdeb to create an stdeb installer)
------------------------------------------------------

For a bit of fun, here's how to install stdeb using stdeb. Note that
stdeb is also in Debian and Ubuntu, so this recipe is only necessary
to install a more recent stdeb.

::

  STDEB_VERSION="0.8.5"

  # Download stdeb
  wget http://pypi.python.org/packages/source/s/stdeb/stdeb-$STDEB_VERSION.tar.gz

  # Extract it
  tar xzf stdeb-$STDEB_VERSION.tar.gz

  # Enter extracted source package
  cd stdeb-$STDEB_VERSION

  # Build .deb (making use of stdeb package directory in sys.path).
  python setup.py --command-packages=stdeb.command bdist_deb

  # Install it
  sudo dpkg -i deb_dist/python-stdeb_$STDEB_VERSION-1_all.deb

Background
----------

For the average Python package, its source distribution
(python_package.tar.gz created with ``python setup.py sdist``)
contains nearly everything necessary to make a Debian source
package. This near-equivalence encouraged me to write this distutils
extension, which executes the setup.py file to extract relevant
information. `setuptools
<http://peak.telecommunity.com/DevCenter/setuptools>`_ may optionally
be used.

I wrote this initially to Debianize several Python packages of my own,
but I have the feeling it could be generally useful. It appears
similar, at least in theory, to easydeb_, `Logilab's Devtools`_,
bdist_dpkg_, bdist_deb_, pkgme_ and `dh-virtualenv
<https://github.com/spotify/dh-virtualenv>`__.

.. _easydeb: http://easy-deb.sourceforge.net/
.. _Logilab's DevTools: http://www.logilab.org/projects/devtools
.. _bdist_dpkg: http://svn.python.org/view/sandbox/trunk/Lib/bdist_dpkg.py
.. _bdist_deb: http://bugs.python.org/issue1054967
.. _pkgme: https://launchpad.net/pkgme

Features
--------

* Create a package for all Python versions supported by
  python-support. (Limiting this range is possible with the
  ``XS-Python-Version:`` config option.)

* Automatic conversion of Python package names into valid Debian
  package names.

* Attempt to automatically convert version numbers such that ordering
  is maintained. See also the config option
  ``Forced-Upstream-Version``.

* Fine grained control of version numbers. (``Debian-Version``,
  ``Forced-Upstream-Version``, ``Upstream-Version-Prefix``,
  ``Upstream-Version-Suffix`` config options.)

* Install .desktop files. (``MIME-Desktop-Files`` config option.)

* Install .mime and .sharedmimeinfo files. (``MIME-File`` and
  ``Shared-MIME-File`` config options.)

* Install copyright files. (``Copyright-File`` config option.)

* Apply patches to upstream sources. (``Stdeb-Patch-File`` config
  option.)

* Pass environment variables to setup.py script. (``Setup-Env-Vars``
  config option.)

Customizing the produced Debian source package (config options)
---------------------------------------------------------------

stdeb will attempt to provide reasonable defaults, but these are only
guesses.

There are two ways to customize the Debian source package produced by
stdeb. First, you may provide options to the distutils
commands. Second, you may provide an ``stdeb.cfg`` file.

stdeb distutils command options
```````````````````````````````

The sdist_dsc command takes command-line options to the distutils
command. For example::

  python setup.py --command-packages=stdeb.command sdist_dsc --debian-version 0MyName1

This creates a Debian package with the Debian version set to
"0MyName1".

These options can also be set via distutils configuration
files. (These are the ``setup.cfg`` file alongside ``setup.py`` and
the ~/.pydistutils.cfg file.) In that case, put the arguments in the
``[sdist_dsc]`` section. For example, a project's ``~/.setup.cfg``
file might have this::

  [sdist_dsc]
  debian-version: 0MyName1

To pass these commands to sdist_dsc when calling bdist_deb, do this::

  python setup.py sdist_dsc --debian-version 0MyName1 bdist_deb

====================================== =========================================
        Command line option                      Effect
====================================== =========================================
  --with-python2                       build Python 2 package (default=True)
  --with-python3                       build Python 3 package (default=False)
  --no-python2-scripts                 disable installation of Python 2 scripts (default=False)
  --no-python3-scripts                 disable installation of Python 3 scripts (default=False)
  --force-x-python3-version            Override default minimum python3:any
                                       dependency with value from x-python3-
                                       version
  --allow-virtualenv-install-location  Allow installing into
                                       /some/random/virtualenv-path
  --dist-dir (-d)                      directory to put final built
                                       distributions in (default='deb_dist')
  --patch-already-applied (-a)         patch was already applied (used when
                                       py2dsc calls sdist_dsc)
  --default-distribution               deprecated (see --suite)
  --suite (-z)                         distribution name to use if not
                                       specified in .cfg (default='unstable')
  --default-maintainer                 deprecated (see --maintainer)
  --maintainer (-m)                    maintainer name and email to use if not
                                       specified in .cfg (default from
                                       setup.py)
  --extra-cfg-file (-x)                additional .cfg file (in addition to
                                       stdeb.cfg if present)
  --patch-file (-p)                    patch file applied before setup.py
                                       called (incompatible with file
                                       specified in .cfg)
  --patch-level (-l)                   patch file applied before setup.py
                                       called (incompatible with file
                                       specified in .cfg)
  --patch-posix (-q)                   apply the patch with --posix mode
  --remove-expanded-source-dir (-r)    remove the expanded source directory
  --ignore-install-requires (-i)       ignore the requirements from
                                       requires.txt in the egg-info directory
  --no-backwards-compatibility         This option has no effect, is here for
                                       backwards compatibility, and may be
                                       removed someday.
  --guess-conflicts-provides-replaces  If True, attempt to guess
                                       Conflicts/Provides/Replaces in
                                       debian/control based on apt-cache
                                       output. (Default=False).
  --use-premade-distfile (-P)          use .zip or .tar.gz file already made
                                       by sdist command
  --source                             debian/control Source: (Default:
                                       <source-debianized-setup-name>)
  --package                            debian/control Package: (Default:
                                       python-<debianized-setup-name>)
  --suite                              suite (e.g. stable, lucid) in changelog
                                       (Default: unstable)
  --maintainer                         debian/control Maintainer: (Default:
                                       <setup-maintainer-or-author>)
  --debian-version                     debian version (Default: 1)
  --section                            debian/control Section: (Default:
                                       python)
  --epoch                              version epoch
  --forced-upstream-version            forced upstream version
  --upstream-version-prefix            upstream version prefix
  --upstream-version-suffix            upstream version suffix
  --uploaders                          uploaders
  --copyright-file                     copyright file
  --build-depends                      debian/control Build-Depends:
  --build-conflicts                    debian/control Build-Conflicts:
  --stdeb-patch-file                   file containing patches for stdeb to
                                       apply
  --stdeb-patch-level                  patch level provided to patch command
  --depends                            debian/control Depends:
  --suggests                           debian/control Suggests:
  --recommends                         debian/control Recommends:
  --xs-python-version                  debian/control XS-Python-Version:
  --x-python3-version                  debian/control X-Python3-Version:
  --dpkg-shlibdeps-params              parameters passed to dpkg-shlibdeps
  --conflicts                          debian/control Conflicts:
  --provides                           debian/control Provides:
  --replaces                           debian/control Replaces:
  --mime-desktop-files                 MIME desktop files
  --mime-file                          MIME file
  --shared-mime-file                   shared MIME file
  --setup-env-vars                     environment variables passed to
                                       setup.py
  --udev-rules                         file with rules to install to udev

====================================== =========================================


You may also pass any arguments described below for the stdeb.cfg file
via distutils options. Passing the arguments this way (either on the
command line, or in the ``[sdist_dsc]`` section of a distutils .cfg
file) will take precedence. The option name should be given in lower
case.

stdeb.cfg configuration file
````````````````````````````

You may write config files of the format understood by `ConfigParser
<http://docs.python.org/lib/module-ConfigParser.html>`_. When building
each package, stdeb looks for the existance of a ``stdeb.cfg`` in the
directory with ``setup.py``. You may specify an additional config file
with the command-line option --extra-cfg-file. The section should
should either be [DEFAULT] or [package_name], which package_name is
specified as the name argument to the setup() command. An example
stdeb.cfg file is::

  [DEFAULT]
  Depends: python-numpy
  XS-Python-Version: >= 2.6

All available options:

====================================== =========================================
  Config file option                     Effect
====================================== =========================================
  Source                               debian/control Source: (Default:
                                       <source-debianized-setup-name>)
  Package                              debian/control Package: (Default:
                                       python-<debianized-setup-name>)
  Suite                                suite (e.g. stable, lucid) in changelog
                                       (Default: unstable)
  Maintainer                           debian/control Maintainer: (Default:
                                       <setup-maintainer-or-author>)
  Debian-Version                       debian version (Default: 1)
  Section                              debian/control Section: (Default:
                                       python)
  Epoch                                version epoch
  Forced-Upstream-Version              forced upstream version
  Upstream-Version-Prefix              upstream version prefix
  Upstream-Version-Suffix              upstream version suffix
  Uploaders                            uploaders
  Copyright-File                       copyright file
  Build-Depends                        debian/control Build-Depends:
  Build-Conflicts                      debian/control Build-Conflicts:
  Stdeb-Patch-File                     file containing patches for stdeb to
                                       apply
  Stdeb-Patch-Level                    patch level provided to patch command
  Depends                              debian/control Depends:
  Depends3                             debian/control Depends: for python3
  Suggests                             debian/control Suggests:
  Suggests3                            debian/control Suggests: for python3
  Recommends                           debian/control Recommends:
  Recommends3                          debian/control Recommends: for python3
  XS-Python-Version                    debian/control XS-Python-Version:
  X-Python3-Version                    debian/control X-Python3-Version:
  Dpkg-Shlibdeps-Params                parameters passed to dpkg-shlibdeps
  Conflicts                            debian/control Conflicts:
  Conflicts3                           debian/control Conflicts: for python3
  Provides                             debian/control Provides:
  Provides3                            debian/control Provides: for python3
  Replaces                             debian/control Replaces:
  Replaces3                            debian/control Replaces: for python3
  MIME-Desktop-Files                   MIME desktop files
  MIME-File                            MIME file
  Shared-MIME-File                     shared MIME file
  Setup-Env-Vars                       environment variables passed to
                                       setup.py
  Udev-Rules                           file with rules to install to udev
====================================== =========================================

The option names in stdeb.cfg files are not case sensitive.

Prerequisites
-------------

 * Python 2.7 or Python 3.x
 * Standard Debian utilities such as ``date``, ``dpkg-source`` and
   Debhelper 7 (use stdeb 0.3.x if you need to support older
   distributions without dh7)
 * If your setup.py uses the setuptools features ``setup_requires`` or
   ``install_requires``, you must run ``apt-file update`` prior to
   running any stdeb command.

TODO
----

* Make output meet `Debian Python Policy`_ specifications or the `new
  python policy`_. This will include several things, among which are:

  - the ability to make custom changelogs
  - the ability to include project-supplied documentation as a -doc package
  - include license information in debian/copyright
  - the ability to include project-supplied examples, tests, and data
    as a separate package
  - much more not listed

* Create (better) documentation

* Log output using standard distutils mechanisms

* Refactor the source code to have a simpler, more sane design

.. _debian python policy: http://www.debian.org/doc/packaging-manuals/python-policy/
.. _new python policy: http://wiki.debian.org/DebianPython/NewPolicy

Call for volunteers
-------------------

I don't have a lot of time for this. This project stands a very real
chance of being only a shadow of its potential self unless people step
up and contribute. There are numerous ways in which people could
help. In particular, I'd be interested in finding a co-maintainer or
maintainer if the project generates any interest. Secondarily, I would
appreciate advice from Debian developers or Ubuntu MOTUs about the
arcane details of Python packaging.

Mailing list
------------

Please address all questions to the distutils-SIG_

.. _distutils-SIG: http://mail.python.org/mailman/listinfo/distutils-sig

License
-------

MIT-style license. Copyright (c) 2006-2015 stdeb authors.

See the LICENSE.txt file provided with the source distribution for
full details.

Authors
-------

* Andrew Straw <strawman@astraw.com>
* Pedro Algarvio, aka, s0undt3ch <ufs@ufsoft.org>
* Gerry Reno (initial bdist_deb implementation)

Additional Credits
------------------

* Zooko O'Whielacronx for the autofind-depends patch.
* Brett (last name unknown) for the --ignore-install-requires patch.
* Ximin Luo for a bug fix.
* Alexander D. Sedov for bug fixes and suggestions.
* Michele Mattioni for bug fix.
* Alexander V. Nikolaev for the debhelper buildsystem specification.
* Roland Sommer for the description field bugfix.
* Barry Warsaw for suggesting the debianize command.
* Asheesh Laroia for updating the PyPI URL.
* Piotr Ożarowski for implementing dh_python2 support.
* Nikita Burtsev for unicode tests and fixes
* Mikołaj Siedlarek for a bugfix
* Dirk Thomas for --force-x-python3-version and X-Python3-Version
* Louis for Recommends3, Suggests3, Provides3 and Replaces3 support
* kzwin for interop with virtualenv
* GitHub_ for hosting services.
* WebFaction_ (aka `python-hosting`_) for previous hosting services.
* TravisCI_ for continuous integration

.. _GitHub: http://github.com/
.. _WebFaction: http://webfaction.com/
.. _python-hosting: http://python-hosting.com/
..  _TravisCI: http://travis-ci.org/
  * [stestr-2.4.0](http://stestr.readthedocs.io/en/latest/) stestr
======

.. image:: https://img.shields.io/travis/mtreinish/stestr/master.svg?style=flat-square
    :target: https://travis-ci.org/mtreinish/stestr
    :alt: Build status

.. image:: https://dev.azure.com/stestr/stestr/_apis/build/status/mtreinish.stestr?branchName=master
    :target: https://dev.azure.com/stestr/stestr/_build/latest?definitionId=1&branchName=master
    :alt: Azure DevOps build status

.. image:: https://img.shields.io/coveralls/github/mtreinish/stestr/master.svg?style=flat-square
    :target: https://coveralls.io/github/mtreinish/stestr?branch=master
    :alt: Code coverage

.. image:: https://img.shields.io/pypi/v/stestr.svg?style=flat-square
    :target: https://pypi.python.org/pypi/stestr
    :alt: Latest Version

* Read this in other languages: `English`_, `日本語`_
* You can see the full rendered docs at: http://stestr.readthedocs.io/en/latest/
* The code of the project is on Github: https://github.com/mtreinish/stestr

.. _English: https://github.com/mtreinish/stestr/blob/master/README.rst
.. _日本語: https://github.com/mtreinish/stestr/blob/master/README_ja.rst

.. note:: stestr v2.x.x release series will be the last series that supports
    Python 2. Support for Python 2.7 will be dropped in stestr release 3.0.0
    which is being planned for early 2020.

Overview
--------

stestr is parallel Python test runner designed to execute `unittest`_ test
suites using multiple processes to split up execution of a test suite. It also
will store a history of all test runs to help in debugging failures and
optimizing the scheduler to improve speed. To accomplish this goal it uses the
`subunit`_ protocol to facilitate streaming and storing results from multiple
workers.

.. _unittest: https://docs.python.org/3/library/unittest.html
.. _subunit: https://github.com/testing-cabal/subunit

stestr originally started as a fork of the `testrepository`_ project. But,
instead of being an interface for any test runner that used subunit, like
testrepository, stestr concentrated on being a dedicated test runner for python
projects. While stestr was originally forked from testrepository it is not
backwards compatible with testrepository. At a high level the basic concepts of
operation are shared between the two projects but the actual usage is not
exactly the same.

.. _testrepository: https://testrepository.readthedocs.org/en/latest


Installing stestr
-----------------

stestr is available via pypi, so all you need to do is run::

  pip install -U stestr

to get stestr on your system. If you need to use a development version of
stestr you can clone the repo and install it locally with::

  git clone https://github.com/mtreinish/stestr.git && pip install -e stestr

which will install stestr in your python environment in editable mode for local
development

Using stestr
------------

After you install stestr to use it to run tests is pretty straightforward. The
first thing you'll want to do is create a ``.stestr.conf`` file for your
project. This file is used to tell stestr where to find tests and basic
information about how tests are run. A basic minimal example of the
contents of this is::

  [DEFAULT]
  test_path=./project_source_dir/tests

which just tells stestr the relative path for the directory to use for
test discovery. This is the same as ``--start-directory`` in the standard
`unittest discovery`_.

.. _unittest discovery: https://docs.python.org/3/library/unittest.html#test-discovery

After this file is created you should be all set to start using stestr to run
tests. To run tests just use::

    stestr run

it will first create a results repository at ``.stestr/`` in the current
working directory and then execute all the tests found by test discovery. If
you're just running a single test (or module) and want to avoid the overhead of
doing test discovery you can use the ``--no-discover``/``-n`` option to specify
that test.

For all the details on these commands and more thorough explanation of options
see the stestr manual: https://stestr.readthedocs.io/en/latest/MANUAL.html

Migrating from testrepository
-----------------------------

If you have a project that is already using testrepository stestr's source repo
contains a helper script for migrating your repo to use stestr. This script
just creates a ``.stestr.conf`` file from a ``.testr.conf`` file.
(assuming it uses a standard subunit.run test command format) To run
this from your project repo just call::

    $STESTR_SOURCE_DIR/tools/testr_to_stestr.py

and you'll have a ``.stestr.conf`` created.

Building a manpage
------------------

The stestr manual has been formatted so that it renders well as html and as a
manpage. The html output and is autogenerated and published to:
https://stestr.readthedocs.io/en/latest/MANUAL.html but the manpage has to be
generated by hand. To do this you have to manually run sphinx-build with the
manpage builder. This has been automated in a small script that should be run
from the root of the stestr repository::

  tools/build_manpage.sh

which will generate the troff file in doc/build/man/stestr.1 which is ready to
be packaged and or put in your system's man pages.

Contributing
------------

To browse the latest code, see: https://github.com/mtreinish/stestr
To clone the latest code, use: ``git clone https://github.com/mtreinish/stestr.git``

Guidelines for contribution are documented at: http://stestr.readthedocs.io/en/latest/developer_guidelines.html

Use `github pull requests`_ to submit patches. Before you submit a pull request
ensure that all the automated testing will pass by running ``tox`` locally.
This will run the test suite and also the automated style rule checks just as
they will in CI. If CI fails on your change it will not be able to merge.

.. _github pull requests: https://help.github.com/articles/about-pull-requests/

Community
---------

Besides Github interactions there is also a stestr IRC channel:

#stestr on Freenode

feel free to join to ask questions, or just discuss stestr.




  * [stevedore-1.30.1](https://docs.openstack.org/stevedore/latest/) ===========================================================
stevedore -- Manage dynamic plugins for Python applications
===========================================================

.. image:: https://img.shields.io/pypi/v/stevedore.svg
    :target: https://pypi.org/project/stevedore/
    :alt: Latest Version

.. image:: https://governance.openstack.org/tc/badges/stevedore.svg
    :target: https://governance.openstack.org/tc/reference/tags/index.html

Python makes loading code dynamically easy, allowing you to configure
and extend your application by discovering and loading extensions
("*plugins*") at runtime. Many applications implement their own
library for doing this, using ``__import__`` or ``importlib``.
stevedore avoids creating yet another extension
mechanism by building on top of `setuptools entry points`_. The code
for managing entry points tends to be repetitive, though, so stevedore
provides manager classes for implementing common patterns for using
dynamically loaded extensions.

.. _setuptools entry points: http://setuptools.readthedocs.io/en/latest/pkg_resources.html?#entry-points

* Free software: Apache license
* Documentation: https://docs.openstack.org/stevedore/latest
* Source: https://opendev.org/openstack/stevedore
* Bugs: https://bugs.launchpad.net/python-stevedore




  * [strict-rfc3339-0.7](http://www.danielrichman.co.uk/libraries/strict-rfc3339.html) Strict, simple, lightweight RFC3339 functions
=============================================

Goals
-----

 - Convert unix timestamps to and from RFC3339.
 - Either produce RFC3339 strings with a UTC offset (Z) or with the offset
   that the C time module reports is the local timezone offset.
 - Simple with minimal dependencies/libraries.
 - Avoid timezones as much as possible.
 - Be very strict and follow RFC3339.

Caveats
-------

 - Leap seconds are not quite supported, since timestamps do not support them,
   and it requires access to timezone data.
 - You may be limited by the size of `time_t` on 32 bit systems.

In both cases, see 'Notes' below.

Rationale
---------

 - A lot of libraries have trouble with DST transitions and ambiguous times.
 - Generally, using the python datetime object causes trouble, introducing
   problems with timezones.
 - The excellent `pytz` library seems to achieve timezone perfection, however
   it didn't (at the time of writing) have a method for getting the local
   timezone or the 'now' time in the local zone.
 - I saw a lot of problems ultimately due to information lost when converting
   or transferring between two libraries (e.g., `time` -> `datetime` loses DST
   info in the tuple)

Usage
-----

Validation:

    >>> strict_rfc3339.validate_rfc3339("some rubbish")
    False
    >>> strict_rfc3339.validate_rfc3339("2013-03-25T12:42:31+00:32")
    True

Indeed, we can then:

    >>> strict_rfc3339.rfc3339_to_timestamp("2013-03-25T12:42:31+00:32")
    1364213431
    >>> tuple(time.gmtime(1364213431))[:6]
    (2013, 3, 25, 12, 10, 31)

No need for two function calls:

    >>> strict_rfc3339.rfc3339_to_timestamp("some rubbish")
    Traceback [...]
    strict_rfc3339.InvalidRFC3339Error

Producing strings (for this example `TZ=America/New_York`):

    >>> strict_rfc3339.timestamp_to_rfc3339_utcoffset(1364213431)
    '2013-03-25T12:10:31Z'
    >>> strict_rfc3339.timestamp_to_rfc3339_localoffset(1364213431)
    '2013-03-25T08:10:31-04:00'

And with `TZ=Europe/London`:

    >>> strict_rfc3339.timestamp_to_rfc3339_localoffset(1364213431)
    '2013-03-25T12:10:31+00:00'

Convenience functions:

    >>> strict_rfc3339.now_to_rfc3339_utcoffset()
    '2013-03-25T21:39:35Z'
    >>> strict_rfc3339.now_to_rfc3339_localoffset()
    '2013-03-25T17:39:39-04:00'

Floats:

    >>> strict_rfc3339.now_to_rfc3339_utcoffset(integer=True) # The default
    '2013-03-25T22:04:01Z'
    >>> strict_rfc3339.now_to_rfc3339_utcoffset(integer=False)
    '2013-03-25T22:04:01.04399Z'
    >>> strict_rfc3339.rfc3339_to_timestamp("2013-03-25T22:04:10.04399Z")
    1364249050.0439899

Behind the scenes
-----------------

These functions are essentially string formatting and arithmetic only.  A very
small number of functions do the heavy lifting. These come from two modules:
`time` and `calendar`.

`time` is a thin wrapper around the C time functions. I'm working on the
assumption that these are usually of high quality and are correct. From the
`time` module, `strict_rfc3339` uses:

 - `time`: (actually calls `gettimeofday`) to get the current timestamp / "now"
 - `gmtime`: splits a timestamp into a UTC time tuple
 - `localtime`: splits a timestamp into a local time tuple

Based on the assumption that they are correct, we can use the difference
between the values returned by `gmtime` and `localtime` to find the local
offset.  As clunky as it sounds, it's far easier than using a fully fledged
timezone library.

`calendar` is implemented in python. From `calendar`, `strict_rfc3339` uses:

 - `timegm`: turns a UTC time tuple into a timestamp. This essentially just
   multiplies each number in the tuple by the number of seconds in it. It does
   use `datetime.date` to work out the number of days between Jan 1 1970 and the
   Y-M-D in the tuple, but this is fine. It does not perform much validation at
   all.
 - `monthrange`: gives the number of days in a (year, month). I checked and
   (at least in my copy of python 2.6) the function used for leap years is
   identical to the one specified in RFC3339 itself.

Notes
-----

 - RFC3339 specifies an offset, not a timezone, and the difference is
   important. Timezones are evil.
 - It is perhaps simpler to think of a RFC3339 string as a human readable
   method of specifying a moment in time (only). These functions merely provide
   access to the one-to-many timestamp-to-RFC3339 mapping.
 - Timestamps don't support leap seconds: a day is always 86400 "long".
   Also, validating leap seconds is particularly fiddly, because not only do
   you need some data, but it must be kept up to date.
   For this reason, `strict_rfc3339` does not support leap seconds: in validation,
   `seconds == 60` or `seconds == 61` is rejected.
   In the case of reverse leap seconds, calendar.timegm will blissfully accept
   it. The result would be about as correct as you could get.
 - RFC3339 generation using `gmtime` or `localtime` may be limited by the size
   of `time_t` on the system: if it is 32 bit, you're limited to dates between
   (approx) 1901 and 2038. This does not affect `rfc3339_to_timestamp`.
  * [stuf-0.9.16](https://bitbucket.org/lcrees/stuf) A collection of Python dictionary types that support attribute-style access.
Includes *defaultdict*,  *OrderedDict*, restricted, *ChainMap*, *Counter*, and
frozen implementations plus miscellaneous utilities for writing Python software.
  * [subunit2sql-1.10.0]() ==================
subunit2SQL README
==================

subunit2SQL is a tool for storing test results data in a SQL database. Like
it's name implies it was originally designed around converting `subunit`_
streams to data in a SQL database and the packaged utilities assume a subunit
stream as the input format. However, the data model used for the DB does not
preclude using any test result format. Additionally the analysis tooling built
on top of a database is data format agnostic. However if you choose to use a
different result format as an input for the database additional tooling using
the DB api would need to be created to parse a different test result output
format. It's also worth pointing out that subunit has several language library
bindings available. So as a user you could create a small filter to convert a
different format to subunit. Creating a filter should be fairly easy and then
you don't have to worry about writing a tool like :ref:`subunit2sql` to use a
different format.

.. _subunit: https://github.com/testing-cabal/subunit/blob/master/README.rst

For multiple distributed test runs that are generating subunit output it is
useful to store the results in a unified repository. This is the motivation for
the `testrepository`_ project which does a good job for centralizing the
results from multiple test runs.

.. _testrepository: http://testrepository.readthedocs.org/en/latest/

However, imagine something like the OpenStack CI system where the same basic
test suite is normally run several hundreds of times a day. To provide useful
introspection on the data from those runs and to build trends over time
the test results need to be stored in a format that allows for easy querying.
Using a SQL database makes a lot of sense for doing this, which was the
original motivation for the project.

At a high level subunit2SQL uses alembic migrations to setup a DB schema that
can then be used by the :ref:`subunit2sql` tool to parse subunit streams and
populate the DB. Then there are tools for interacting with the stored data in
the :ref:`subunit2sql-graph` command as well as the :ref:`sql2subunit`
command to create a subunit stream from data in the database. Additionally,
subunit2sql provides a Python DB API that can be used to query information from
the stored data to build other tooling.

- Source: http://git.openstack.org/cgit/openstack-infra/subunit2sql
- Bugs, Stories: https://storyboard.openstack.org/#!/project/747

Usage
=====

DB Setup
--------

The usage of subunit2sql is split into 2 stages. First you need to prepare a
database with the proper schema; subunit2sql-db-manage should be used to do
this. The utility requires db connection info which can be specified on the
command or with a config file. Obviously the sql connector type, user,
password, address, and database name should be specific to your environment.
subunit2sql-db-manage will use alembic to setup the db schema. You can run the
db migrations with the command::

    subunit2sql-db-manage --database-connection mysql://subunit:pass@127.0.0.1/subunit upgrade head

or with a config file::

    subunit2sql-db-manage --config-file subunit2sql.conf upgrade head

This will bring the DB schema up to the latest version for subunit2sql.

.. _subunit2sql:

subunit2sql
-----------

Once you have a database setup with the proper database schema you can then use
the subunit2sql command to populate the database with data from your test runs.
subunit2sql takes in a subunit v2 either through stdin or by passing it file
paths as positional arguments to the script. If only a subunit v1 stream is
available, it can be converted to a subunit v2 stream using the subunit-1to2
utility.

There are several options for running subunit2sql, they can be listed with::

    subunit2sql --help

The only required option is --database-connection. The options can either be
used on the CLI, or put in a config file. If a config file is used you need to
specify the location on the CLI.

Most of the optional arguments deal with how subunit2sql interacts with the
SQL DB. However, it is worth pointing out that the artifacts option and the
run_meta option are used to pass additional metadata into the database for the
run(s) being added. The artifacts option should be used to pass in a url or
path that points to any logs or other external test artifacts related to the
run being added. The run_meta option takes in a dictionary which will be added
to the database as key value pairs associated with the run being added.

.. _sql2subunit:

sql2subunit
-----------

The sql2subunit utility is used for taking a run_id and creating a subunit
v2 stream from the data in the DB about that run. To create a new subunit
stream run::

    sql2subunit $RUN_ID

along with any options that you would normally use to either specify a config
file or the DB connection info. Running this command will print to stdout the
subunit v2 stream for the run specified by $RUN_ID, unless the --out_path
argument is specified to write it to a file instead.

Development
===========

For development and testing, you need a local database setup. Check
``tools/test-setup.sh`` on how the databases need to be configured.

ChangeLog
=========

To see the release notes go here: `http://docs.openstack.org/releasenotes/subunit2sql/ <http://docs.openstack.org/releasenotes/subunit2sql/>`_




  * [swiftly-2.06](https://github.com/gholt/swiftly/) Swiftly - A client for Swift

Provides a command line tool as well as Client and other classes for common
Swift functions. Works both with access as a standard external user and as an
internal administrator of a cluster with direct access to the rings and to all
back end servers.

You can read the Sphinx-built documentation at
http://gholt.github.io/swiftly/
  * [sybil-1.2.0](https://github.com/cjw296/sybil) =====
sybil
=====

Automated testing for the examples in your documentation.

The latest documentation can be found at:
http://sybil.readthedocs.org/en/latest/

Development takes place here:
https://github.com/cjw296/sybil/



  * [sympy-1.4](https://sympy.org) 
SymPy is a Python library for symbolic mathematics. It aims to become a
full-featured computer algebra system (CAS) while keeping the code as simple
as possible in order to be comprehensible and easily extensible.  SymPy is
written entirely in Python. It depends on mpmath, and other external libraries
may be optionally for things like plotting support.

See the webpage for more information and documentation:

    https://sympy.org




  * [synapseclient-1.9.3](http://synapse.sagebase.org/) Python Synapse Client
=====================

 CI | Branch  | Build Status
 ---|---------|-------------
Travis | develop | [![Build Status develop branch](https://travis-ci.org/Sage-Bionetworks/synapsePythonClient.svg?branch=develop)](https://travis-ci.org/Sage-Bionetworks/synapsePythonClient)
Travis | master  | [![Build Status master branch](https://travis-ci.org/Sage-Bionetworks/synapsePythonClient.svg?branch=master)](https://travis-ci.org/Sage-Bionetworks/synapsePythonClient)
AppVeyor | develop | [![AppVeyor branch](https://img.shields.io/appveyor/ci/SageBionetworks/synapsePythonClient/master.svg)](https://ci.appveyor.com/project/SageBionetworks/synapsepythonclient)
AppVeyor | master | [![AppVeyor branch](https://img.shields.io/appveyor/ci/SageBionetworks/synapsePythonClient/master.svg)](https://ci.appveyor.com/project/SageBionetworks/synapsepythonclient)


[![Get the synapseclient from PyPI](https://img.shields.io/pypi/v/synapseclient.svg)](https://pypi.python.org/pypi/synapseclient/) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/synapseclient.svg)](https://pypi.python.org/pypi/synapseclient/) 

A Python client for [Sage Bionetworks'](https://www.sagebase.org) [Synapse](https://www.synapse.org/), a collaborative compute space that allows scientists to share and analyze data together. The Python client can be used as a library for development of software that communicates with Synapse or as a command-line utility.

There is also a [Synapse client for R](https://github.com/Sage-Bionetworks/synapser/).


Python 2 Support
----------------

The sun is setting on Python 2. Many major open source Python packages are moving to require Python 3.

The Synapse engineering team will step down Python 2.7 support to only bug fixes, and require Python 3 on new feature releases. **Starting with Synapse Python client version 2.0 (will be released in Q1 2019), Synapse Python client will require Python 3.**


Documentation
-------------

For more information about the Python client, see:

 * [Python client API docs](https://python-docs.synapse.org) 

For more information about interacting with Synapse, see:

 * [Synapse API docs](http://docs.synapse.org/rest/)
 * [User guides (including Python examples)](http://docs.synapse.org/articles/)
 * [Getting Started Guide to Synapse](http://docs.synapse.org/articles/getting_started.html)


Installation
------------

The Python Synapse client has been tested on Python 2.7, 3.5 and 3.6 on Mac OS X, Ubuntu Linux and Windows.

### Install using pip

The [Python Synapse Client is on PyPI](https://pypi.python.org/pypi/synapseclient) and can be installed with pip:

    (sudo) pip install synapseclient[pandas,pysftp]

...or to upgrade an existing installation of the Synapse client:

    (sudo) pip install --upgrade synapseclient

The dependencies on `pandas` and `pysftp` are optional. Synapse [Tables](http://docs.synapse.org/python/#tables) integrate
with [Pandas](http://pandas.pydata.org/). The library `pysftp` is required for users of
[SFTP](http://docs.synapse.org/python/sftp.html) file storage. Both libraries require native code
to be compiled or installed separately from prebuilt binaries.

### Install from source

Clone the [source code repository](https://github.com/Sage-Bionetworks/synapsePythonClient).

    git clone git://github.com/Sage-Bionetworks/synapsePythonClient.git
    cd synapsePythonClient
    python setup.py install

#### Install release candidate branch

For validation, validators would install a release candidate branch to verify that a bug has been fix or a new feature/ an improvement works as expected. To prevent overwriting your working environment, using [virtualenv](https://virtualenv.pypa.io/) to create an isolated test environment is a good idea.

    git clone git://github.com/Sage-Bionetworks/synapsePythonClient.git
    cd synapsePythonClient
    git checkout v1.8.2-rc
    python setup.py install

Replace `python setup.py install` with `python setup.py develop` to make the installation follow the head without having to reinstall.

#### Installing a tagged version

After a version is release, a tag will be created for the released version. To install a specific release, instead of checking out the release candidate branch, check out the tag instead, for example:

    git checkout v1.8.2



Command line usage
------------------

The synapse client can be used from the shell command prompt. Valid commands
include: query, get, cat, add, update, delete, and onweb. A few examples are
shown.

### downloading test data from synapse

    synapse -u my_username -p my_password get syn1528299

### getting help

    synapse -h

Note that a [synapse account](https://www.synapse.org/#RegisterAccount:0) is required.


Usage as a library
------------------

The synapse client can be used to write software that interacts with the Sage Synapse repository.

### Example

    import synapseclient

    syn = synapseclient.Synapse()

    ## log in using username and password
    syn.login('my_username', 'my_password')

    ## retrieve a 100 by 4 matrix
    matrix = syn.get('syn1901033')

    ## inspect its properties
    print(matrix.name)
    print(matrix.description)
    print(matrix.path)

    ## load the data matrix into a dictionary with an entry for each column
    with open(matrix.path, 'r') as f:
        labels = f.readline().strip().split('\t')
        data = {label: [] for label in labels}
        for line in f:
            values = [float(x) for x in line.strip().split('\t')]
            for i in range(len(labels)):
                data[labels[i]].append(values[i])

    ## load the data matrix into a numpy array
    import numpy as np
    np.loadtxt(fname=matrix.path, skiprows=1)


Authentication
--------------
Authentication toward [synapse](https://www.synapse.org/#RegisterAccount:0) can be accomplished in a few different ways. One is by passing username and password to the `syn.login` function.

    import synapseclient
    syn = synapseclient.Synapse()
    syn.login('my_username', 'my_password')

It is much more convenient to use an API key, which can be generated and cached locally by doing the following _once_:

    syn.login('my_username', 'my_password', rememberMe=True)

Then, in subsequent interactions, specifying username and password is optional and only needed to login as a different user. Calling `login` with no arguments uses cached credentials when they are available.

    syn.login('my_username')

As a short-cut, creating the `Synapse` object and logging in can be done in one step:

    import synapseclient
    syn = synapseclient.login()

Caching credentials can also be done from the command line client:

    synapse login -u my_username -p my_password --rememberMe


Synapse Utilities (synapseutils)
--------------------------------

The purpose of synapseutils is to create a space filled with convenience functions that includes traversing through large projects, copying entities, recursively downloading files and many more.

### Example

    import synapseutils
    import synapseclient
    syn = synapseclient.login()
    
    #COPY: copies all synapse entities to a destination location
    synapseutils.copy(syn, "syn1234", destinationId = "syn2345")
    
    #COPY WIKI: copies the wiki from the entity to a destination entity. Only a project can have sub wiki pages.
    synapseutils.copyWiki(syn, "syn1234", destinationId = "syn2345")


    #WALK: Traverses through synapse directories, behaves exactly like os.walk()
    walkedPath = synapseutils.walk(syn, "syn1234")

    for dirpath, dirname, filename in walkedPath:
        print(dirpath)
        print(dirname)
        print(filename)
        

License and Copyright
---------------------

&copy; Copyright 2013-18 Sage Bionetworks

This software is licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
  * [tabulate-0.8.2](https://github.com/astanin/python-tabulate) python-tabulate
===============

Pretty-print tabular data in Python, a library and a command-line
utility.

The main use cases of the library are:

-   printing small tables without hassle: just one function call,
    formatting is guided by the data itself
-   authoring tabular data for lightweight plain-text markup: multiple
    output formats suitable for further editing or transformation
-   readable presentation of mixed textual and numeric data: smart
    column alignment, configurable number formatting, alignment by a
    decimal point

Installation
------------

To install the Python library and the command line utility, run:

    pip install tabulate

The command line utility will be installed as `tabulate` to `bin` on
Linux (e.g. `/usr/bin`); or as `tabulate.exe` to `Scripts` in your
Python installation on Windows (e.g.
`C:\Python27\Scripts\tabulate.exe`).

You may consider installing the library only for the current user:

    pip install tabulate --user

In this case the command line utility will be installed to
`~/.local/bin/tabulate` on Linux and to
`%APPDATA%\Python\Scripts\tabulate.exe` on Windows.

To install just the library on Unix-like operating systems:

    TABULATE_INSTALL=lib-only pip install tabulate

On Windows:

    set TABULATE_INSTALL=lib-only
    pip install tabulate

The module provides just one function, `tabulate`, which takes a list of
lists or another tabular data type as the first argument, and outputs a
nicely formatted plain-text table:

    >>> from tabulate import tabulate

    >>> table = [["Sun",696000,1989100000],["Earth",6371,5973.6],
    ...          ["Moon",1737,73.5],["Mars",3390,641.85]]
    >>> print(tabulate(table))
    -----  ------  -------------
    Sun    696000     1.9891e+09
    Earth    6371  5973.6
    Moon     1737    73.5
    Mars     3390   641.85
    -----  ------  -------------

The following tabular data types are supported:

-   list of lists or another iterable of iterables
-   list or another iterable of dicts (keys as columns)
-   dict of iterables (keys as columns)
-   two-dimensional NumPy array
-   NumPy record arrays (names as columns)
-   pandas.DataFrame

Examples in this file use Python2. Tabulate supports Python3 too.

### Headers

The second optional argument named `headers` defines a list of column
headers to be used:

    >>> print(tabulate(table, headers=["Planet","R (km)", "mass (x 10^29 kg)"]))
    Planet      R (km)    mass (x 10^29 kg)
    --------  --------  -------------------
    Sun         696000           1.9891e+09
    Earth         6371        5973.6
    Moon          1737          73.5
    Mars          3390         641.85

If `headers="firstrow"`, then the first row of data is used:

    >>> print(tabulate([["Name","Age"],["Alice",24],["Bob",19]],
    ...                headers="firstrow"))
    Name      Age
    ------  -----
    Alice      24
    Bob        19

If `headers="keys"`, then the keys of a dictionary/dataframe, or column
indices are used. It also works for NumPy record arrays and lists of
dictionaries or named tuples:

    >>> print(tabulate({"Name": ["Alice", "Bob"],
    ...                 "Age": [24, 19]}, headers="keys"))
      Age  Name
    -----  ------
       24  Alice
       19  Bob

### Row Indices

By default, only pandas.DataFrame tables have an additional column
called row index. To add a similar column to any other type of table,
pass `showindex="always"` or `showindex=True` argument to `tabulate()`.
To suppress row indices for all types of data, pass `showindex="never"`
or `showindex=False`. To add a custom row index column, pass
`showindex=rowIDs`, where `rowIDs` is some iterable:

    >>> print(tabulate([["F",24],["M",19]], showindex="always"))
    -  -  --
    0  F  24
    1  M  19
    -  -  --

### Table format

There is more than one way to format a table in plain text. The third
optional argument named `tablefmt` defines how the table is formatted.

Supported table formats are:

-   "plain"
-   "simple"
-   "github"
-   "grid"
-   "fancy\_grid"
-   "pipe"
-   "orgtbl"
-   "jira"
-   "presto"
-   "psql"
-   "rst"
-   "mediawiki"
-   "moinmoin"
-   "youtrack"
-   "html"
-   "latex"
-   "latex\_raw"
-   "latex\_booktabs"
-   "textile"

`plain` tables do not use any pseudo-graphics to draw lines:

    >>> table = [["spam",42],["eggs",451],["bacon",0]]
    >>> headers = ["item", "qty"]
    >>> print(tabulate(table, headers, tablefmt="plain"))
    item      qty
    spam       42
    eggs      451
    bacon       0

`simple` is the default format (the default may change in future
versions). It corresponds to `simple_tables` in [Pandoc Markdown
extensions](http://johnmacfarlane.net/pandoc/README.html#tables):

    >>> print(tabulate(table, headers, tablefmt="simple"))
    item      qty
    ------  -----
    spam       42
    eggs      451
    bacon       0

`github` follows the conventions of Github flavored Markdown. It
corresponds to the `pipe` format without alignment colons:

    >>> print(tabulate(table, headers, tablefmt="github"))
    | item   | qty   |
    |--------|-------|
    | spam   | 42    |
    | eggs   | 451   |
    | bacon  | 0     |

`grid` is like tables formatted by Emacs'
[table.el](http://table.sourceforge.net/) package. It corresponds to
`grid_tables` in Pandoc Markdown extensions:

    >>> print(tabulate(table, headers, tablefmt="grid"))
    +--------+-------+
    | item   |   qty |
    +========+=======+
    | spam   |    42 |
    +--------+-------+
    | eggs   |   451 |
    +--------+-------+
    | bacon  |     0 |
    +--------+-------+

`fancy_grid` draws a grid using box-drawing characters:

    >>> print(tabulate(table, headers, tablefmt="fancy_grid"))
    ╒════════╤═══════╕
    │ item   │   qty │
    ╞════════╪═══════╡
    │ spam   │    42 │
    ├────────┼───────┤
    │ eggs   │   451 │
    ├────────┼───────┤
    │ bacon  │     0 │
    ╘════════╧═══════╛

`presto` is like tables formatted by Presto cli:

    >>> print(tabulate(table, headers, tablefmt="presto"))
     item   |   qty
    --------+-------
     spam   |    42
     eggs   |   451
     bacon  |     0

`psql` is like tables formatted by Postgres' psql cli:

    >>> print(tabulate(table, headers, tablefmt="psql"))
    +--------+-------+
    | item   |   qty |
    |--------+-------|
    | spam   |    42 |
    | eggs   |   451 |
    | bacon  |     0 |
    +--------+-------+

`pipe` follows the conventions of [PHP Markdown
Extra](http://michelf.ca/projects/php-markdown/extra/#table) extension.
It corresponds to `pipe_tables` in Pandoc. This format uses colons to
indicate column alignment:

    >>> print(tabulate(table, headers, tablefmt="pipe"))
    | item   |   qty |
    |:-------|------:|
    | spam   |    42 |
    | eggs   |   451 |
    | bacon  |     0 |

`orgtbl` follows the conventions of Emacs
[org-mode](http://orgmode.org/manual/Tables.html), and is editable also
in the minor orgtbl-mode. Hence its name:

    >>> print(tabulate(table, headers, tablefmt="orgtbl"))
    | item   |   qty |
    |--------+-------|
    | spam   |    42 |
    | eggs   |   451 |
    | bacon  |     0 |

`jira` follows the conventions of Atlassian Jira markup language:

    >>> print(tabulate(table, headers, tablefmt="jira"))
    || item   ||   qty ||
    | spam   |    42 |
    | eggs   |   451 |
    | bacon  |     0 |

`rst` formats data like a simple table of the
[reStructuredText](http://docutils.sourceforge.net/docs/user/rst/quickref.html#tables)
format:

    >>> print(tabulate(table, headers, tablefmt="rst"))
    ======  =====
    item      qty
    ======  =====
    spam       42
    eggs      451
    bacon       0
    ======  =====

`mediawiki` format produces a table markup used in
[Wikipedia](http://www.mediawiki.org/wiki/Help:Tables) and on other
MediaWiki-based sites:

    >>> print(tabulate(table, headers, tablefmt="mediawiki"))
    {| class="wikitable" style="text-align: left;"
    |+ <!-- caption -->
    |-
    ! item   !! align="right"|   qty
    |-
    | spam   || align="right"|    42
    |-
    | eggs   || align="right"|   451
    |-
    | bacon  || align="right"|     0
    |}

`moinmoin` format produces a table markup used in
[MoinMoin](https://moinmo.in/) wikis:

    >>> print(tabulate(table, headers, tablefmt="moinmoin"))
    || ''' item   ''' || ''' quantity   ''' ||
    ||  spam    ||  41.999      ||
    ||  eggs    ||  451         ||
    ||  bacon   ||              ||

`youtrack` format produces a table markup used in Youtrack tickets:

    >>> print(tabulate(table, headers, tablefmt="youtrack"))
    ||  item    ||  quantity   ||
    |   spam    |  41.999      |
    |   eggs    |  451         |
    |   bacon   |              |

`textile` format produces a table markup used in
[Textile](http://redcloth.org/hobix.com/textile/) format:

    >>> print(tabulate(table, headers, tablefmt="textile"))
    |_.  item   |_.   qty |
    |<. spam    |>.    42 |
    |<. eggs    |>.   451 |
    |<. bacon   |>.     0 |

`html` produces standard HTML markup:

    >>> print(tabulate(table, headers, tablefmt="html"))
    <table>
    <tbody>
    <tr><th>item  </th><th style="text-align: right;">  qty</th></tr>
    <tr><td>spam  </td><td style="text-align: right;">   42</td></tr>
    <tr><td>eggs  </td><td style="text-align: right;">  451</td></tr>
    <tr><td>bacon </td><td style="text-align: right;">    0</td></tr>
    </tbody>
    </table>

`latex` format creates a `tabular` environment for LaTeX markup,
replacing special characters like `_` or `\` to their LaTeX
correspondents:

    >>> print(tabulate(table, headers, tablefmt="latex"))
    \begin{tabular}{lr}
    \hline
     item   &   qty \\
    \hline
     spam   &    42 \\
     eggs   &   451 \\
     bacon  &     0 \\
    \hline
    \end{tabular}

`latex_raw` behaves like `latex` but does not escape LaTeX commands and
special characters.

`latex_booktabs` creates a `tabular` environment for LaTeX markup using
spacing and style from the `booktabs` package.

### Column alignment

`tabulate` is smart about column alignment. It detects columns which
contain only numbers, and aligns them by a decimal point (or flushes
them to the right if they appear to be integers). Text columns are
flushed to the left.

You can override the default alignment with `numalign` and `stralign`
named arguments. Possible column alignments are: `right`, `center`,
`left`, `decimal` (only for numbers), and `None` (to disable alignment).

Aligning by a decimal point works best when you need to compare numbers
at a glance:

    >>> print(tabulate([[1.2345],[123.45],[12.345],[12345],[1234.5]]))
    ----------
        1.2345
      123.45
       12.345
    12345
     1234.5
    ----------

Compare this with a more common right alignment:

    >>> print(tabulate([[1.2345],[123.45],[12.345],[12345],[1234.5]], numalign="right"))
    ------
    1.2345
    123.45
    12.345
     12345
    1234.5
    ------

For `tabulate`, anything which can be parsed as a number is a number.
Even numbers represented as strings are aligned properly. This feature
comes in handy when reading a mixed table of text and numbers from a
file:

    >>> import csv ; from StringIO import StringIO
    >>> table = list(csv.reader(StringIO("spam, 42\neggs, 451\n")))
    >>> table
    [['spam', ' 42'], ['eggs', ' 451']]
    >>> print(tabulate(table))
    ----  ----
    spam    42
    eggs   451
    ----  ----

### Custom column alignment

`tabulate` allows a custom column alignment to override the above. The
`colalign` argument can be a list or a tuple of `stralign` named
arguments. Possible column alignments are: `right`, `center`, `left`,
`decimal` (only for numbers), and `None` (to disable alignment).
Omitting an alignment uses the default. For example:

    >>> print(tabulate([["one", "two"], ["three", "four"]], colalign=("right",))
    -----  ----
      one  two
    three  four
    -----  ----

### Number formatting

`tabulate` allows to define custom number formatting applied to all
columns of decimal numbers. Use `floatfmt` named argument:

    >>> print(tabulate([["pi",3.141593],["e",2.718282]], floatfmt=".4f"))
    --  ------
    pi  3.1416
    e   2.7183
    --  ------

`floatfmt` argument can be a list or a tuple of format strings, one per
column, in which case every column may have different number formatting:

    >>> print(tabulate([[0.12345, 0.12345, 0.12345]], floatfmt=(".1f", ".3f")))
    ---  -----  -------
    0.1  0.123  0.12345
    ---  -----  -------

### Text formatting

By default, `tabulate` removes leading and trailing whitespace from text
columns. To disable whitespace removal, set the global module-level flag
`PRESERVE_WHITESPACE`:

    import tabulate
    tabulate.PRESERVE_WHITESPACE = True

### Wide (fullwidth CJK) symbols

To properly align tables which contain wide characters (typically
fullwidth glyphs from Chinese, Japanese or Korean languages), the user
should install `wcwidth` library. To install it together with
`tabulate`:

    pip install tabulate[widechars]

Wide character support is enabled automatically if `wcwidth` library is
already installed. To disable wide characters support without
uninstalling `wcwidth`, set the global module-level flag
`WIDE_CHARS_MODE`:

    import tabulate
    tabulate.WIDE_CHARS_MODE = False

### Multiline cells

Most table formats support multiline cell text (text containing newline
characters). The newline characters are honored as line break
characters.

Multiline cells are supported for data rows and for header rows.

Further automatic line breaks are not inserted. Of course, some output
formats such as latex or html handle automatic formatting of the cell
content on their own, but for those that don't, the newline characters
in the input cell text are the only means to break a line in cell text.

Note that some output formats (e.g. simple, or plain) do not represent
row delimiters, so that the representation of multiline cells in such
formats may be ambiguous to the reader.

The following examples of formatted output use the following table with
a multiline cell, and headers with a multiline cell:

    >>> table = [["eggs",451],["more\nspam",42]]
    >>> headers = ["item\nname", "qty"]

`plain` tables:

    >>> print(tabulate(table, headers, tablefmt="plain"))
    item      qty
    name
    eggs      451
    more       42
    spam

`simple` tables:

    >>> print(tabulate(table, headers, tablefmt="simple"))
    item      qty
    name
    ------  -----
    eggs      451
    more       42
    spam

`github` tables:

    >>> print(tabulate(table, headers, tablefmt="github"))
    | item   | qty   |
    | name   |       |
    |--------|-------|
    | eggs   | 451   |
    | more   | 42    |
    | spam   |       |

`grid` tables:

    >>> print(tabulate(table, headers, tablefmt="grid"))
    +--------+-------+
    | item   |   qty |
    | name   |       |
    +========+=======+
    | eggs   |   451 |
    +--------+-------+
    | more   |    42 |
    | spam   |       |
    +--------+-------+

`fancy_grid` tables:

    >>> print(tabulate(table, headers, tablefmt="fancy_grid"))
    ╒════════╤═══════╕
    │ item   │   qty │
    │ name   │       │
    ╞════════╪═══════╡
    │ eggs   │   451 │
    ├────────┼───────┤
    │ more   │    42 │
    │ spam   │       │
    ╘════════╧═══════╛

`pipe` tables:

    >>> print(tabulate(table, headers, tablefmt="pipe"))
    | item   |   qty |
    | name   |       |
    |:-------|------:|
    | eggs   |   451 |
    | more   |    42 |
    | spam   |       |

`orgtbl` tables:

    >>> print(tabulate(table, headers, tablefmt="orgtbl"))
    | item   |   qty |
    | name   |       |
    |--------+-------|
    | eggs   |   451 |
    | more   |    42 |
    | spam   |       |

`jira` tables:

    >>> print(tabulate(table, headers, tablefmt="jira"))
    | item   |   qty |
    | name   |       |
    |:-------|------:|
    | eggs   |   451 |
    | more   |    42 |
    | spam   |       |

`presto` tables:

    >>> print(tabulate(table, headers, tablefmt="presto"))
     item   |   qty
     name   |
    --------+-------
     eggs   |   451
     more   |    42
     spam   |

`psql` tables:

    >>> print(tabulate(table, headers, tablefmt="psql"))
    +--------+-------+
    | item   |   qty |
    | name   |       |
    |--------+-------|
    | eggs   |   451 |
    | more   |    42 |
    | spam   |       |
    +--------+-------+

`rst` tables:

    >>> print(tabulate(table, headers, tablefmt="rst"))
    ======  =====
    item      qty
    name
    ======  =====
    eggs      451
    more       42
    spam
    ======  =====

Multiline cells are not well supported for the other table formats.

Usage of the command line utility
---------------------------------

    Usage: tabulate [options] [FILE ...]

    FILE                      a filename of the file with tabular data;
                              if "-" or missing, read data from stdin.

    Options:

    -h, --help                show this message
    -1, --header              use the first row of data as a table header
    -o FILE, --output FILE    print table to FILE (default: stdout)
    -s REGEXP, --sep REGEXP   use a custom column separator (default: whitespace)
    -F FPFMT, --float FPFMT   floating point number format (default: g)
    -f FMT, --format FMT      set output table format; supported formats:
                              plain, simple, github, grid, fancy_grid, pipe,
                              orgtbl, rst, mediawiki, html, latex, latex_raw,
                              latex_booktabs, tsv
                              (default: simple)

Performance considerations
--------------------------

Such features as decimal point alignment and trying to parse everything
as a number imply that `tabulate`:

-   has to "guess" how to print a particular tabular data type
-   needs to keep the entire table in-memory
-   has to "transpose" the table twice
-   does much more work than it may appear

It may not be suitable for serializing really big tables (but who's
going to do that, anyway?) or printing tables in performance sensitive
applications. `tabulate` is about two orders of magnitude slower than
simply joining lists of values with a tab, coma or other separator.

In the same time `tabulate` is comparable to other table
pretty-printers. Given a 10x10 table (a list of lists) of mixed text and
numeric data, `tabulate` appears to be slower than `asciitable`, and
faster than `PrettyTable` and `texttable` The following mini-benchmark
was run in Python 3.6.8 on Ubuntu 18.04 in WSL:

    ===========================  ==========  ===========
    Table formatter                time, μs    rel. time
    ===========================  ==========  ===========
    csv to StringIO                     8.2          1.0
    join with tabs and newlines        10.8          1.3
    asciitable (0.8.0)                205.2         24.9
    tabulate (0.8.5)                  421.7         51.2
    PrettyTable (0.7.2)               787.2         95.6
    texttable (1.6.2)                1123.4        136.4
    ===========================  ==========  ===========


Version history
---------------

The full version history can be found at the [changelog](./CHANGELOG).

How to contribute
-----------------

Contributions should include tests and an explanation for the changes
they propose. Documentation (examples, docstrings, README.md) should be
updated accordingly.

This project uses [nose](https://nose.readthedocs.org/) testing
framework and [tox](https://tox.readthedocs.io/) to automate testing in
different environments. Add tests to one of the files in the `test/`
folder.

To run tests on all supported Python versions, make sure all Python
interpreters, `nose` and `tox` are installed, then run `tox` in the root
of the project source tree.

On Linux `tox` expects to find executables like `python2.6`,
`python2.7`, `python3.4` etc. On Windows it looks for
`C:\Python26\python.exe`, `C:\Python27\python.exe` and
`C:\Python34\python.exe` respectively.

To test only some Python environements, use `-e` option. For example, to
test only against Python 2.7 and Python 3.6, run:

    tox -e py27,py36

in the root of the project source tree.

To enable NumPy and Pandas tests, run:

    tox -e py27-extra,py36-extra

(this may take a long time the first time, because NumPy and Pandas will
have to be installed in the new virtual environments)

See `tox.ini` file to learn how to use `nosetests` directly to test
individual Python versions.

Contributors
------------

Sergey Astanin, Pau Tallada Crespí, Erwin Marsi, Mik Kocikowski, Bill
Ryder, Zach Dwiel, Frederik Rietdijk, Philipp Bogensberger, Greg
(anonymous), Stefan Tatschner, Emiel van Miltenburg, Brandon Bennett,
Amjith Ramanujam, Jan Schulz, Simon Percivall, Javier Santacruz
López-Cepero, Sam Denton, Alexey Ziyangirov, acaird, Cesar Sanchez,
naught101, John Vandenberg, Zack Dever, Christian Clauss, Benjamin
Maier, Andy MacKinlay, Thomas Roten, Jue Wang, Joe King, Samuel Phan,
Nick Satterly, Daniel Robbins, Dmitry B, Lars Butler, Andreas Maier,
Dick Marinus, Sébastien Celles, Yago González, Andrew Gaul, Wim Glenn,
Jean Michel Rouly, Tim Gates, John Vandenberg.
  * [tabulator-1.23.0](https://github.com/frictionlessdata/tabulator-py) # tabulator-py

[![Travis](https://img.shields.io/travis/frictionlessdata/tabulator-py/master.svg)](https://travis-ci.org/frictionlessdata/tabulator-py)
[![Coveralls](http://img.shields.io/coveralls/frictionlessdata/tabulator-py.svg?branch=master)](https://coveralls.io/r/frictionlessdata/tabulator-py?branch=master)
[![PyPi](https://img.shields.io/pypi/v/tabulator.svg)](https://pypi.python.org/pypi/tabulator)
[![Gitter](https://img.shields.io/gitter/room/frictionlessdata/chat.svg)](https://gitter.im/frictionlessdata/chat)

A library for reading and writing tabular data (csv/xls/json/etc).

## Features

- **Supports most common tabular formats**: CSV, XLS, ODS, JSON, Google Sheets, SQL, and others. See complete list [below](#supported-file-formats).
- **Loads local and remote data**: Supports HTTP, FTP and S3.
- **Low memory usage**: Only the current row is kept in memory, so you can
  large datasets.
- **Supports compressed files**: Using ZIP or GZIP algorithms.
- **Extensible**: You can add support for custom file formats and loaders (e.g.
  FTP).

## Contents

<!--TOC-->

  - [Getting started](#getting-started)
    - [Installation](#installation)
    - [Running on CLI](#running-on-cli)
    - [Running on Python](#running-on-python)
  - [Documentation](#documentation)
    - [Stream](#stream)
      - [Headers](#headers)
      - [Encoding](#encoding)
      - [Compression (Python3-only)](#compression-python3-only)
      - [Allow html](#allow-html)
      - [Sample size](#sample-size)
      - [Bytes sample size](#bytes-sample-size)
      - [Ignore blank headers](#ignore-blank-headers)
      - [Force strings](#force-strings)
      - [Force parse](#force-parse)
      - [Skip rows](#skip-rows)
      - [Post parse](#post-parse)
      - [Keyed and extended rows](#keyed-and-extended-rows)
    - [Supported schemes](#supported-schemes)
      - [s3](#s3)
      - [file](#file)
      - [http/https/ftp/ftps](#httphttpsftpftps)
      - [stream](#stream-1)
      - [text](#text)
    - [Supported file formats](#supported-file-formats)
      - [csv (read & write)](#csv-read--write)
      - [xls/xlsx (read only)](#xlsxlsx-read-only)
      - [ods (read only)](#ods-read-only)
      - [gsheet (read only)](#gsheet-read-only)
      - [sql (read only)](#sql-read-only)
      - [Data Package (read only)](#data-package-read-only)
      - [inline (read only)](#inline-read-only)
      - [json (read only)](#json-read-only)
      - [ndjson (read only)](#ndjson-read-only)
      - [tsv (read only)](#tsv-read-only)
    - [Adding support for new file sources, formats, and writers](#adding-support-for-new-file-sources-formats-and-writers)
      - [Custom loaders](#custom-loaders)
      - [Custom parsers](#custom-parsers)
      - [Custom writers](#custom-writers)
    - [Validate](#validate)
    - [Exceptions](#exceptions)
  - [API Reference](#api-reference)
  - [Contributing](#contributing)
  - [Changelog](#changelog)

<!--TOC-->

## Getting started

### Installation

```bash
$ pip install tabulator
```

### Running on CLI

Tabulator ships with a simple CLI called `tabulator` to read tabular data. For
example:

```bash
$ tabulator https://github.com/frictionlessdata/tabulator-py/raw/4c1b3943ac98be87b551d87a777d0f7ca4904701/data/table.csv.gz
id,name
1,english
2,中国人
```

You can see all supported options by running `tabulator --help`.

### Running on Python

```python
from tabulator import Stream

with Stream('data.csv', headers=1) as stream:
    stream.headers # [header1, header2, ..]
    for row in stream:
        print(row)  # [value1, value2, ..]
```

You can find other examples in the [examples][examples-dir] directory.

## Documentation

In the following sections, we'll walk through some usage examples of
this library. All examples were tested with Python 3.6, but should
run fine with Python 3.3+.

### Stream

The `Stream` class represents a tabular stream. It takes the file path as the
`source` argument. For example:

```
<scheme>://path/to/file.<format>
```

It uses this path to determine the file format (e.g. CSV or XLS) and scheme
(e.g. HTTP or postgresql). It also supports format extraction from URLs like `http://example.com?format=csv`. If necessary, you also can define these explicitly.

Let's try it out. First, we create a `Stream` object passing the path to a CSV file.

```python
import tabulator

stream = tabulator.Stream('data.csv')
```

At this point, the file haven't been read yet. Let's open the stream so we can
read the contents.

```python
try:
    stream.open()
except tabulator.TabulatorException as e:
    pass  # Handle exception
```

This will open the underlying data stream, read a small sample to detect the
file encoding, and prepare the data to be read. We catch
`tabulator.TabulatorException` here, in case something goes wrong.

We can now read the file contents. To iterate over each row, we do:

```python
for row in stream.iter():
    print(row)  # [value1, value2, ...]
```

The `stream.iter()` method will return each row data as a list of values. If
you prefer, you could call `stream.iter(keyed=True)` instead, which returns a
dictionary with the column names as keys. Either way, this method keeps only a
single row in memory at a time. This means it can handle handle large files
without consuming too much memory.

If you want to read the entire file, use `stream.read()`. It accepts the same
arguments as `stream.iter()`, but returns all rows at once.

```python
stream.reset()
rows = stream.read()
```

Notice that we called `stream.reset()` before reading the rows. This is because
internally, tabulator only keeps a pointer to its current location in the file.
If we didn't reset this pointer, we would read starting from where we stopped.
For example, if we ran `stream.read()` again, we would get an empty list, as
the internal file pointer is at the end of the file (because we've already read
it all). Depending on the file location, it might be necessary to download the
file again to rewind (e.g. when the file was loaded from the web).

After we're done, close the stream with:

```python
stream.close()
```

The entire example looks like:

```python
import tabulator

stream = tabulator.Stream('data.csv')
try:
    stream.open()
except tabulator.TabulatorException as e:
    pass  # Handle exception

for row in stream.iter():
    print(row)  # [value1, value2, ...]

stream.reset()  # Rewind internal file pointer
rows = stream.read()

stream.close()
```

It could be rewritten to use Python's context manager interface as:

```python
import tabulator

try:
    with tabulator.Stream('data.csv') as stream:
        for row in stream.iter():
            print(row)

        stream.reset()
        rows = stream.read()
except tabulator.TabulatorException as e:
    pass
```

This is the preferred way, as Python closes the stream automatically, even if some exception was thrown along the way.

The full API documentation is available as docstrings in the [Stream source code][stream.py].

#### Headers

By default, tabulator considers that all file rows are values (i.e. there is no
header).

```python
with Stream([['name', 'age'], ['Alex', 21]]) as stream:
  stream.headers # None
  stream.read() # [['name', 'age'], ['Alex', 21]]
```

If you have a header row, you can use the `headers` argument with the its row
number (starting from 1).

```python
# Integer
with Stream([['name', 'age'], ['Alex', 21]], headers=1) as stream:
  stream.headers # ['name', 'age']
  stream.read() # [['Alex', 21]]
```

You can also pass a lists of strings to define the headers explicitly:

```python
with Stream([['Alex', 21]], headers=['name', 'age']) as stream:
  stream.headers # ['name', 'age']
  stream.read() # [['Alex', 21]]
```

Tabulator also supports multiline headers for the `xls` and `xlsx` formats.

```python
with Stream('data.xlsx', headers=[1, 3], fill_merged_cells=True) as stream:
  stream.headers # ['header from row 1-3']
  stream.read() # [['value1', 'value2', 'value3']]
```

#### Encoding

You can specify the file encoding (e.g. `utf-8` and `latin1`) via the `encoding`
argument.

```python
with Stream(source, encoding='latin1') as stream:
  stream.read()
```

If this argument isn't set, Tabulator will try to infer it from the data. If you
get a `UnicodeDecodeError` while loading a file, try setting the encoding to
`utf-8`.

#### Compression (Python3-only)

Tabulator supports both ZIP and GZIP compression methods. By default it'll infer from the file name:

```python
with Stream('http://example.com/data.csv.zip') as stream:
  stream.read()
```

You can also set it explicitly:

```python
with Stream('data.csv.ext', compression='gz') as stream:
  stream.read()
```
###### Options

- **filename**: filename in zip file to process (default is first file)

#### Allow html

The `Stream` class raises `tabulator.exceptions.FormatError` if it detects HTML
contents. This helps avoiding the relatively common mistake of trying to load a
CSV file inside an HTML page, for example on GitHub.

You can disable this behaviour using the `allow_html` option:

```python
with Stream(source_with_html, allow_html=True) as stream:
  stream.read() # no exception on open
```

#### Sample size

To detect the file's headers, and run other checks like validating that the file
doesn't contain HTML, Tabulator reads a sample of rows on the `stream.open()`
method. This data is available via the `stream.sample` property. The number of
rows used can be defined via the `sample_size` parameters (defaults to 100).

```python
with Stream(two_rows_source, sample_size=1) as stream:
  stream.sample # only first row
  stream.read() # first and second rows
```

You can disable this by setting `sample_size` to zero. This way, no data will be
read on `stream.open()`.

#### Bytes sample size

Tabulator needs to read a part of the file to infer its encoding. The
`bytes_sample_size` arguments controls how many bytes will be read for this
detection (defaults to 10000).

```python
source = 'data/special/latin1.csv'
with Stream(source) as stream:
    stream.encoding # 'iso8859-2'
```

You can disable this by setting `bytes_sample_size` to zero, in which case it'll
use the machine locale's default encoding.

#### Ignore blank headers

When `True`, tabulator will ignore columns that have blank headers (defaults to
`False`).

```python
# Default behaviour
source = 'text://header1,,header3\nvalue1,value2,value3'
with Stream(source, format='csv', headers=1) as stream:
    stream.headers # ['header1', '', 'header3']
    stream.read(keyed=True) # {'header1': 'value1', '': 'value2', 'header3': 'value3'}

# Ignoring columns with blank headers
source = 'text://header1,,header3\nvalue1,value2,value3'
with Stream(source, format='csv', headers=1, ignore_blank_headers=True) as stream:
    stream.headers # ['header1', 'header3']
    stream.read(keyed=True) # {'header1': 'value1', 'header3': 'value3'}
```

#### Force strings

When `True`, all rows' values will be converted to strings (defaults to
`False`). `None` values will be converted to empty strings.

```python
# Default behaviour
with Stream([['string', 1, datetime.datetime(2017, 12, 1, 17, 00)]]) as stream:
  stream.read() # [['string', 1, datetime.dateime(2017, 12, 1, 17, 00)]]

# Forcing rows' values as strings
with Stream([['string', 1]], force_strings=True) as stream:
  stream.read() # [['string', '1', '2017-12-01 17:00:00']]
```

#### Force parse

When `True`, don't raise an exception when parsing a malformed row, but simply
return an empty row. Otherwise, tabulator raises
`tabulator.exceptions.SourceError` when a row can't be parsed. Defaults to `False`.

```python
# Default behaviour
with Stream([[1], 'bad', [3]]) as stream:
  stream.read() # raises tabulator.exceptions.SourceError

# With force_parse
with Stream([[1], 'bad', [3]], force_parse=True) as stream:
  stream.read() # [[1], [], [3]]
```

#### Skip rows

List of row numbers and/or strings to skip.
If it's a string, all rows that begin with it will be skipped (e.g. '#' and '//').
If it's the empty string, all rows that begin with an empty column will be skipped.

```python
source = [['John', 1], ['Alex', 2], ['#Sam', 3], ['Mike', 4], ['John', 5]]
with Stream(source, skip_rows=[1, 2, -1, '#']) as stream:
  stream.read() # [['Mike', 4]]
```

If the `headers` parameter is also set to be an integer, it will use the first not skipped row as a headers.

```python
source = [['#comment'], ['name', 'order'], ['John', 1], ['Alex', 2]]
with Stream(source, headers=1, skip_rows=['#']) as stream:
  stream.headers # [['name', 'order']]
  stream.read() # [['Jogn', 1], ['Alex', 2]]
```

#### Post parse

List of functions that can filter or transform rows after they are parsed. These
functions receive the `extended_rows` containing the row's number, headers
list, and the row values list. They then process the rows, and yield or discard
them, modified or not.

```python
def skip_odd_rows(extended_rows):
    for row_number, headers, row in extended_rows:
        if not row_number % 2:
            yield (row_number, headers, row)

def multiply_by_two(extended_rows):
    for row_number, headers, row in extended_rows:
        doubled_row = list(map(lambda value: value * 2, row))
        yield (row_number, headers, doubled_row)

rows = [
  [1],
  [2],
  [3],
  [4],
]
with Stream(rows, post_parse=[skip_odd_rows, multiply_by_two]) as stream:
  stream.read() # [[4], [8]]
```

These functions are applied in order, as a simple data pipeline. In the example
above, `multiply_by_two` just sees the rows yielded by `skip_odd_rows`.

#### Keyed and extended rows

The methods `stream.iter()` and `stream.read()` accept the `keyed` and
`extended` flag arguments to modify how the rows are returned.

By default, every row is returned as a list of its cells values:

```python
with Stream([['name', 'age'], ['Alex', 21]]) as stream:
  stream.read() # [['Alex', 21]]
```

With `keyed=True`, the rows are returned as dictionaries, mapping the column names to their values in the row:

```python
with Stream([['name', 'age'], ['Alex', 21]]) as stream:
  stream.read(keyed=True) # [{'name': 'Alex', 'age': 21}]
```

And with `extended=True`, the rows are returned as a tuple of `(row_number,
headers, row)`, there `row_number` is the current row number (starting from 1),
`headers` is a list with the headers names, and `row` is a list with the rows
values:

```python
with Stream([['name', 'age'], ['Alex', 21]]) as stream:
  stream.read(extended=True) # (1, ['name', 'age'], ['Alex', 21])
```

### Supported schemes

#### s3

It loads data from AWS S3. For private files you should provide credentials supported by the `boto3` library, for example, corresponding environment variables. Read more about [configuring `boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html).

```python
stream = Stream('s3://bucket/data.csv')
```

###### Options

- **s3\_endpoint\_url** - the endpoint URL to use. By default it's `https://s3.amazonaws.com`. For complex use cases, for example, `goodtables`'s runs on a data package this option can be provided as an environment variable `S3_ENDPOINT_URL`.

#### file

The default scheme, a file in the local filesystem.

```python
stream = Stream('data.csv')
```

#### http/https/ftp/ftps

> In Python 2, `tabulator` can't stream remote data sources because of a limitation in the underlying libraries. The whole data source will be loaded to the memory. In Python 3 there is no such problem and remote files are streamed.

```python
stream = Stream('https://example.com/data.csv')
```

###### Options

- **http\_session** - a `requests.Session` object. Read more in the [requests docs][requests-session].
- **http\_stream** - Enables or disables HTTP streaming, when possible (enabled by default). Disable it if you'd like to preload the whole file into memory.

#### stream

The source is a file-like Python object.


```python
with open('data.csv') as fp:
    stream = Stream(fp)
```

#### text

The source is a string containing the tabular data. Both `scheme` and `format`
must be set explicitly, as it's not possible to infer them.

```python
stream = Stream(
    'name,age\nJohn, 21\n',
    scheme='text',
    format='csv'
)
```

### Supported file formats

In this section, we'll describe the supported file formats, and their respective
configuration options and operations. Some formats only support read operations,
while others support both reading and writing.

#### csv (read & write)

```python
stream = Stream('data.csv', delimiter=',')
```

###### Options

It supports all options from the Python CSV library. Check [their
documentation][pydoc-csv] for more information.

#### xls/xlsx (read only)

> Tabulator is unable to stream `xls` files, so the entire file is loaded in
> memory. Streaming is supported for `xlsx` files.

```python
stream = Stream('data.xls', sheet=1)
```

###### Options

- **sheet**: Sheet name or number (starting from 1)
- **fill_merged_cells**: if `True` it will unmerge and fill all merged cells by
  a visible value. With this option enabled the parser can't stream data and
  load the whole document into memory.
- **preserve_formatting**: if `True` it will try to preserve text formatting of numeric and temporal cells returning it as strings according to how it looks in a spreadsheet (EXPERIMETAL)

#### ods (read only)

> This format is not included to package by default. To use it please install `tabulator` with an `ods` extras: `$ pip install tabulator[ods]`

Source should be a valid Open Office document.

```python
stream = Stream('data.ods', sheet=1)
```

###### Options

- **sheet**: Sheet name or number (starting from 1)

#### gsheet (read only)

A publicly-accessible Google Spreadsheet.

```python
stream = Stream('https://docs.google.com/spreadsheets/d/<id>?usp=sharing')
stream = Stream('https://docs.google.com/spreadsheets/d/<id>edit#gid=<gid>')
```

#### sql (read only)

Any database URL supported by [sqlalchemy][sqlalchemy].

```python
stream = Stream('postgresql://name:pass@host:5432/database', table='data')
```

###### Options

- **table (required)**: Database table name
- **order_by**: SQL expression for row ordering (e.g. `name DESC`)

#### Data Package (read only)

> This format is not included to package by default. You can enable it by
> installing tabulator using `pip install tabulator[datapackage]`.

A [Tabular Data Package][tdp].

```python
stream = Stream('datapackage.json', resource=1)
```

###### Options

- **resource**: Resource name or index (starting from 0)

#### inline (read only)

Either a list of lists, or a list of dicts mapping the column names to their
respective values.

```python
stream = Stream([['name', 'age'], ['John', 21], ['Alex', 33]])
stream = Stream([{'name': 'John', 'age': 21}, {'name': 'Alex', 'age': 33}])
```

#### json (read only)

JSON document containing a list of lists, or a list of dicts mapping the column
names to their respective values (see the `inline` format for an example).

```python
stream = Stream('data.json', property='key1.key2')
```

###### Options

- **property**: JSON Path to the property containing the tabular data. For example, considering the JSON `{"response": {"data": [...]}}`, the `property` should be set to `response.data`.

#### ndjson (read only)

```python
stream = Stream('data.ndjson')
```

#### tsv (read only)

```python
stream = Stream('data.tsv')
```

### Adding support for new file sources, formats, and writers

Tabulator is written with extensibility in mind, allowing you to add support for
new tabular file formats, schemes (e.g. ssh), and writers (e.g. MongoDB). There
are three components that allow this:

* Loaders
  * Loads a stream from some location (e.g. ssh)
* Parsers
  * Parses a stream of tabular data in some format (e.g. xls)
* Writers
  * Writes tabular data to some destination (e.g. MongoDB)

In this section, we'll see how to write custom classes to extend any of these components.

#### Custom loaders

You can add support for a new scheme (e.g. ssh) by creating a custom loader.
Custom loaders are implemented by inheriting from the `Loader` class, and
implementing its methods. This loader can then be used by `Stream` to load data
by passing it via the `custom_loaders={'scheme': CustomLoader}` argument.

The skeleton of a custom loader looks like:

```python
from tabulator import Loader

class CustomLoader(Loader):
  options = []

  def __init__(self, bytes_sample_size, **options):
      pass

  def load(self, source, mode='t', encoding=None):
      # load logic

with Stream(source, custom_loaders={'custom': CustomLoader}) as stream:
  stream.read()
```

You can see examples of how the loaders are implemented by looking in the
`tabulator.loaders` module.

#### Custom parsers

You can add support for a new file format by creating a custom parser. Similarly
to custom loaders, custom parsers are implemented by inheriting from the
`Parser` class, and implementing its methods. This parser can then be used by
`Stream` to parse data by passing it via the `custom_parsers={'format':
CustomParser}` argument.

The skeleton of a custom parser looks like:

```python
from tabulator import Parser

class CustomParser(Parser):
    options = []

    def __init__(self, loader, force_parse, **options):
        self.__loader = loader

    def open(self, source, encoding=None):
        # open logic

    def close(self):
        # close logic

    def reset(self):
        # reset logic

    @property
    def closed(self):
        return False

    @property
    def extended_rows(self):
        # extended rows logic

with Stream(source, custom_parsers={'custom': CustomParser}) as stream:
  stream.read()
```

You can see examples of how parsers are implemented by looking in the
`tabulator.parsers` module.

#### Custom writers

You can add support to write files in a specific format by creating a custom
writer. The custom writers are implemented by inheriting from the base `Writer`
class, and implementing its methods. This writer can then be used by `Stream` to
write data via the `custom_writers={'format': CustomWriter}` argument.

The skeleton of a custom writer looks like:

```python
from tabulator import Writer

class CustomWriter(Writer):
  options = []

  def __init__(self, **options):
      pass

  def write(self, source, target, headers=None, encoding=None):
      # write logic

with Stream(source, custom_writers={'custom': CustomWriter}) as stream:
  stream.save(target)
```

You can see examples of how parsers are implemented by looking in the
`tabulator.writers` module.

### Validate

You can check if a source can be loaded by tabulator using the `validate` function.

```python
from tabulator import validate, exceptions

try:
    tabular = validate('data.csv')
except exceptions.SchemeError:
    # The file scheme isn't supported
except exceptions.FormatError:
    # The file format isn't supported
```

### Exceptions

All the exceptions thrown by tabulator inherit from
`tabulator.exceptions.TabulatorException`, so you can use it as a way to catch
any tabulator exception. You can learn about the other exceptions thrown by
looking into the [tabulator.exceptions][tabulator.exceptions] module.

## API Reference

The API reference is written as docstrings in the tabulator classes. A good
place to start is the [Stream](tabulator/stream.py) class, which manages all
loading and parsing of data files.

## Contributing

This project follows the [Open Knowledge International coding standards](https://github.com/okfn/coding-standards).

We recommend you to use `virtualenv` to isolate this project from the rest of the
packages in your machine.

To install the project and its development dependencies, run:

```bash
$ make install
```

To run the tests, use:

```bash
$ make test
```

## Changelog

Here described only breaking and the most important changes. The full changelog and documentation for all released versions could be found in nicely formatted [commit history](https://github.com/frictionlessdata/tabulator-py/commits/master).

###### v1.26

- Added `stream.fragment` field showing e.g. Excel sheet's or DP resource's name

###### v1.25

- Added support for the `s3` file scheme (data loading from AWS S3)

###### v1.24

- Added support for compressed file-like objects

###### v1.23

- Added a setter for the `stream.headers` property

###### v1.22

- The `headers` parameter will now use the first not skipped row if the `skip_rows` parameter is provided and there are comments on the top of a data source (see #264)

###### v1.21

- Implemented experimental `preserve_formatting` for xlsx

###### v1.20

- Added support for specifying filename in zip source

###### v1.19

Updated behaviour:
- For `ods` format the boolean, integer and datatime native types are detected now

###### v1.18

Updated behaviour:
- For `xls` format the boolean, integer and datatime native types are detected now

###### v1.17

Updated behaviour:
- Added support for Python 3.7

###### v1.16

New API added:
- `skip_rows` support for an empty string to skip rows with an empty first column

###### v1.15

New API added:
- Format will be extracted from URLs like `http://example.com?format=csv`

###### v1.14

Updated behaviour:
- Now `xls` booleans will be parsed as booleans not integers

###### v1.13

New API added:
- The `skip_rows` argument now supports negative numbers to skip rows starting from the end

###### v1.12

Updated behaviour:
- Instead of raising an exception, a `UserWarning` warning will be emitted if an option isn't recognized.

###### v1.11

New API added:
- Added `http_session` argument for the `http/https` format (it uses `requests` now)
- Added support for multiline headers: `headers` argument accept ranges like `[1,3]`

###### v1.10

New API added:
- Added support for compressed files i.e. `zip` and `gz` on Python3
- The `Stream` constructor now accepts a `compression` argument
- The `http/https` scheme now accepts a `http_stream` flag

###### v1.9

Improved behaviour:
- The `headers` argument allows to set the order for keyed sources and cherry-pick values

###### v1.8

New API added:
- Formats `XLS/XLSX/ODS` supports sheet names passed via the `sheet` argument
- The `Stream` constructor accepts an `ignore_blank_headers` option

###### v1.7

Improved behaviour:
- Rebased `datapackage` format on `datapackage@1` library

###### v1.6

New API added:
- Argument `source` for the `Stream` constructor can be a `pathlib.Path`

###### v1.5

New API added:
- Argument `bytes_sample_size` for the `Stream` constructor

###### v1.4

Improved behaviour:
- Updated encoding name to a canonical form

###### v1.3

New API added:
- `stream.scheme`
- `stream.format`
- `stream.encoding`

Promoted provisional API to stable API:
- `Loader` (custom loaders)
- `Parser` (custom parsers)
- `Writer` (custom writers)
- `validate`

###### v1.2

Improved behaviour:
- Autodetect common CSV delimiters

###### v1.1

New API added:
- Added `fill_merged_cells` option to `xls/xlsx` formats

###### v1.0

New API added:
- published `Loader/Parser/Writer` API
- Added `Stream` argument `force_strings`
- Added `Stream` argument `force_parse`
- Added `Stream` argument `custom_writers`

Deprecated API removal:
- removed `topen` and `Table` - use `Stream` instead
- removed `Stream` arguments `loader/parser_options` - use `**options` instead

Provisional API changed:
- Updated the `Loader/Parser/Writer` API - please use an updated version

###### v0.15

Provisional API added:
- Unofficial support for `Stream` arguments `custom_loaders/parsers`


[stream.py]: tabulator/stream.py
[examples-dir]: examples "Examples"
[requests-session]: https://docs.puthon-requests.org/en/master/user/advanced/#session-objects
[pydoc-csv]: https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters "Python CSV options"
[sqlalchemy]: https://www.sqlalchemy.org/
[tdp]: https://frictionlessdata.io/specs/tabular-data-package/ "Tabular Data Package"
[tabulator.exceptions]: tabulator/exceptions.py "Tabulator Exceptions"


  * [tblib-1.4.0](https://github.com/ionelmc/python-tblib) ========
Overview
========



Traceback serialization library.

* Free software: BSD license

It allows you to:

* `Pickle <https://docs.python.org/3/library/pickle.html>`_ tracebacks and raise exceptions
  with pickled tracebacks in different processes. This allows better error handling when running
  code over multiple processes (imagine multiprocessing, billiard, futures, celery etc).
* Create traceback objects from strings (the ``from_string`` method). *No pickling is used*.
* Serialize tracebacks to/from plain dicts (the ``from_dict`` and ``to_dict`` methods). *No pickling is used*.
* Raise the tracebacks created from the aforementioned sources.

**Again, note that using the pickle support is completely optional. You are solely responsible for
security problems should you decide to use the pickle support.**

Installation
============

::

    pip install tblib

Documentation
=============

.. contents::
   :local:

Pickling tracebacks
~~~~~~~~~~~~~~~~~~~

**Note**: The traceback objects that come out are stripped of some attributes (like variables). But you'll be able to raise exceptions with
those tracebacks or print them - that should cover 99% of the usecases.

::

    >>> from tblib import pickling_support
    >>> pickling_support.install()
    >>> import pickle, sys
    >>> def inner_0():
    ...     raise Exception('fail')
    ...
    >>> def inner_1():
    ...     inner_0()
    ...
    >>> def inner_2():
    ...     inner_1()
    ...
    >>> try:
    ...     inner_2()
    ... except:
    ...     s1 = pickle.dumps(sys.exc_info())
    ...
    >>> len(s1) > 1
    True
    >>> try:
    ...     inner_2()
    ... except:
    ...     s2 = pickle.dumps(sys.exc_info(), protocol=pickle.HIGHEST_PROTOCOL)
    ...
    >>> len(s2) > 1
    True

    >>> try:
    ...     import cPickle
    ... except ImportError:
    ...     import pickle as cPickle
    >>> try:
    ...     inner_2()
    ... except:
    ...     s3 = cPickle.dumps(sys.exc_info(), protocol=pickle.HIGHEST_PROTOCOL)
    ...
    >>> len(s3) > 1
    True

Unpickling
~~~~~~~~~~

::

    >>> pickle.loads(s1)
    (<...Exception'>, Exception('fail'...), <traceback object at ...>)

    >>> pickle.loads(s2)
    (<...Exception'>, Exception('fail'...), <traceback object at ...>)

    >>> pickle.loads(s3)
    (<...Exception'>, Exception('fail'...), <traceback object at ...>)

Raising
~~~~~~~

::

    >>> from six import reraise
    >>> reraise(*pickle.loads(s1))
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[14]>", line 1, in <module>
        reraise(*pickle.loads(s2))
      File "<doctest README.rst[8]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail
    >>> reraise(*pickle.loads(s2))
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[14]>", line 1, in <module>
        reraise(*pickle.loads(s2))
      File "<doctest README.rst[8]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail
    >>> reraise(*pickle.loads(s3))
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[14]>", line 1, in <module>
        reraise(*pickle.loads(s2))
      File "<doctest README.rst[8]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail

What if we have a local stack, does it show correctly ?
-------------------------------------------------------

Yes it does::

    >>> exc_info = pickle.loads(s3)
    >>> def local_0():
    ...     reraise(*exc_info)
    ...
    >>> def local_1():
    ...     local_0()
    ...
    >>> def local_2():
    ...     local_1()
    ...
    >>> local_2()
    Traceback (most recent call last):
      File "...doctest.py", line ..., in __run
        compileflags, 1) in test.globs
      File "<doctest README.rst[24]>", line 1, in <module>
        local_2()
      File "<doctest README.rst[23]>", line 2, in local_2
        local_1()
      File "<doctest README.rst[22]>", line 2, in local_1
        local_0()
      File "<doctest README.rst[21]>", line 2, in local_0
        reraise(*exc_info)
      File "<doctest README.rst[11]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail

It also supports more contrived scenarios
-----------------------------------------

Like tracebacks with syntax errors::

    >>> from tblib import Traceback
    >>> from examples import bad_syntax
    >>> try:
    ...     bad_syntax()
    ... except:
    ...     et, ev, tb = sys.exc_info()
    ...     tb = Traceback(tb)
    ...
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[58]>", line 1, in <module>
        reraise(et, ev, tb.as_traceback())
      File "<doctest README.rst[57]>", line 2, in <module>
        bad_syntax()
      File "...tests...examples.py", line 18, in bad_syntax
        import badsyntax
      File "...tests...badsyntax.py", line 5
        is very bad
         ^
    SyntaxError: invalid syntax

Or other import failures::

    >>> from examples import bad_module
    >>> try:
    ...     bad_module()
    ... except:
    ...     et, ev, tb = sys.exc_info()
    ...     tb = Traceback(tb)
    ...
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[61]>", line 1, in <module>
        reraise(et, ev, tb.as_traceback())
      File "<doctest README.rst[60]>", line 2, in <module>
        bad_module()
      File "...tests...examples.py", line 23, in bad_module
        import badmodule
      File "...tests...badmodule.py", line 3, in <module>
        raise Exception("boom!")
    Exception: boom!

Or a traceback that's caused by exceeding the recursion limit (here we're
forcing the type and value to have consistency across platforms)::

    >>> def f(): f()
    >>> try:
    ...    f()
    ... except RuntimeError:
    ...    et, ev, tb = sys.exc_info()
    ...    tb = Traceback(tb)
    ...
    >>> reraise(RuntimeError, RuntimeError("maximum recursion depth exceeded"), tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[32]>", line 1, in f
        def f(): f()
      File "<doctest README.rst[32]>", line 1, in f
        def f(): f()
      File "<doctest README.rst[32]>", line 1, in f
        def f(): f()
      ...
    RuntimeError: maximum recursion depth exceeded

Reference
~~~~~~~~~

tblib.Traceback
---------------

It is used by the ``pickling_support``. You can use it too if you want more flexibility::

    >>> from tblib import Traceback
    >>> try:
    ...     inner_2()
    ... except:
    ...     et, ev, tb = sys.exc_info()
    ...     tb = Traceback(tb)
    ...
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[21]>", line 6, in <module>
        reraise(et, ev, tb.as_traceback())
      File "<doctest README.rst[21]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail

tblib.Traceback.to_dict
```````````````````````

You can use the ``to_dict`` method and the ``from_dict`` classmethod to
convert a Traceback into and from a dictionary serializable by the stdlib
json.JSONDecoder::

    >>> import json
    >>> from pprint import pprint
    >>> try:
    ...     inner_2()
    ... except:
    ...     et, ev, tb = sys.exc_info()
    ...     tb = Traceback(tb)
    ...     tb_dict = tb.to_dict()
    ...     pprint(tb_dict)
    {'tb_frame': {'f_code': {'co_filename': '<doctest README.rst[...]>',
                             'co_name': '<module>'},
                  'f_globals': {'__name__': '__main__'}},
     'tb_lineno': 2,
     'tb_next': {'tb_frame': {'f_code': {'co_filename': ...
                                         'co_name': 'inner_2'},
                              'f_globals': {'__name__': '__main__'}},
                 'tb_lineno': 2,
                 'tb_next': {'tb_frame': {'f_code': {'co_filename': ...
                                                     'co_name': 'inner_1'},
                                          'f_globals': {'__name__': '__main__'}},
                             'tb_lineno': 2,
                             'tb_next': {'tb_frame': {'f_code': {'co_filename': ...
                                                                 'co_name': 'inner_0'},
                                                      'f_globals': {'__name__': '__main__'}},
                                         'tb_lineno': 2,
                                         'tb_next': None}}}}

tblib.Traceback.from_dict
`````````````````````````

Building on the previous example::

    >>> tb_json = json.dumps(tb_dict)
    >>> tb = Traceback.from_dict(json.loads(tb_json))
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[21]>", line 6, in <module>
        reraise(et, ev, tb.as_traceback())
      File "<doctest README.rst[21]>", line 2, in <module>
        inner_2()
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail

tblib.Traceback.from_string
```````````````````````````

::

    >>> tb = Traceback.from_string("""
    ... File "skipped.py", line 123, in func_123
    ... Traceback (most recent call last):
    ...   File "tests/examples.py", line 2, in func_a
    ...     func_b()
    ...   File "tests/examples.py", line 6, in func_b
    ...     func_c()
    ...   File "tests/examples.py", line 10, in func_c
    ...     func_d()
    ...   File "tests/examples.py", line 14, in func_d
    ... Doesn't: matter
    ... """)
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[42]>", line 6, in <module>
        reraise(et, ev, tb.as_traceback())
      File "...examples.py", line 2, in func_a
        func_b()
      File "...examples.py", line 6, in func_b
        func_c()
      File "...examples.py", line 10, in func_c
        func_d()
      File "...examples.py", line 14, in func_d
        raise Exception("Guessing time !")
    Exception: fail


If you use the ``strict=False`` option then parsing is a bit more lax::

    >>> tb = Traceback.from_string("""
    ... File "bogus.py", line 123, in bogus
    ... Traceback (most recent call last):
    ...  File "tests/examples.py", line 2, in func_a
    ...   func_b()
    ...    File "tests/examples.py", line 6, in func_b
    ...     func_c()
    ...    File "tests/examples.py", line 10, in func_c
    ...   func_d()
    ...  File "tests/examples.py", line 14, in func_d
    ... Doesn't: matter
    ... """, strict=False)
    >>> reraise(et, ev, tb.as_traceback())
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[42]>", line 6, in <module>
        reraise(et, ev, tb.as_traceback())
      File "bogus.py", line 123, in bogus
      File "...examples.py", line 2, in func_a
        func_b()
      File "...examples.py", line 6, in func_b
        func_c()
      File "...examples.py", line 10, in func_c
        func_d()
      File "...examples.py", line 14, in func_d
        raise Exception("Guessing time !")
    Exception: fail

tblib.decorators.return_error
-----------------------------

::

    >>> from tblib.decorators import return_error
    >>> inner_2r = return_error(inner_2)
    >>> e = inner_2r()
    >>> e
    <tblib.decorators.Error object at ...>
    >>> e.reraise()
    Traceback (most recent call last):
      ...
      File "<doctest README.rst[26]>", line 1, in <module>
        e.reraise()
      File "...tblib...decorators.py", line 19, in reraise
        reraise(self.exc_type, self.exc_value, self.traceback)
      File "...tblib...decorators.py", line 25, in return_exceptions_wrapper
        return func(*args, **kwargs)
      File "<doctest README.rst[5]>", line 2, in inner_2
        inner_1()
      File "<doctest README.rst[4]>", line 2, in inner_1
        inner_0()
      File "<doctest README.rst[3]>", line 2, in inner_0
        raise Exception('fail')
    Exception: fail

How's this useful? Imagine you're using multiprocessing like this::

    # Note that Python 3.4 and later will show the remote traceback (but as a string sadly) so we skip testing this.
    >>> import traceback
    >>> from multiprocessing import Pool
    >>> from examples import func_a
    >>> pool = Pool()  # doctest: +SKIP
    >>> try:  # doctest: +SKIP
    ...     for i in pool.map(func_a, range(5)):
    ...         print(i)
    ... except:
    ...     print(traceback.format_exc())
    ...
    Traceback (most recent call last):
      File "<doctest README.rst[...]>", line 2, in <module>
        for i in pool.map(func_a, range(5)):
      File "...multiprocessing...pool.py", line ..., in map
        ...
      File "...multiprocessing...pool.py", line ..., in get
        ...
    Exception: Guessing time !
    <BLANKLINE>
    >>> pool.terminate()  # doctest: +SKIP

Not very useful is it? Let's sort this out::

    >>> from tblib.decorators import apply_with_return_error, Error
    >>> from itertools import repeat
    >>> pool = Pool()
    >>> try:
    ...     for i in pool.map(apply_with_return_error, zip(repeat(func_a), range(5))):
    ...         if isinstance(i, Error):
    ...             i.reraise()
    ...         else:
    ...             print(i)
    ... except:
    ...     print(traceback.format_exc())
    ...
    Traceback (most recent call last):
      File "<doctest README.rst[...]>", line 4, in <module>
        i.reraise()
      File "...tblib...decorators.py", line ..., in reraise
        reraise(self.exc_type, self.exc_value, self.traceback)
      File "...tblib...decorators.py", line ..., in return_exceptions_wrapper
        return func(*args, **kwargs)
      File "...tblib...decorators.py", line ..., in apply_with_return_error
        return args[0](*args[1:])
      File "...examples.py", line 2, in func_a
        func_b()
      File "...examples.py", line 6, in func_b
        func_c()
      File "...examples.py", line 10, in func_c
        func_d()
      File "...examples.py", line 14, in func_d
        raise Exception("Guessing time !")
    Exception: Guessing time !
    <BLANKLINE>
    >>> pool.terminate()

Much better !

What if we have a local call stack ?
````````````````````````````````````

::

    >>> def local_0():
    ...     pool = Pool()
    ...     for i in pool.map(apply_with_return_error, zip(repeat(func_a), range(5))):
    ...         if isinstance(i, Error):
    ...             i.reraise()
    ...         else:
    ...             print(i)
    ...
    >>> def local_1():
    ...     local_0()
    ...
    >>> def local_2():
    ...     local_1()
    ...
    >>> try:
    ...     local_2()
    ... except:
    ...     print(traceback.format_exc())
    Traceback (most recent call last):
      File "<doctest README.rst[...]>", line 2, in <module>
        local_2()
      File "<doctest README.rst[...]>", line 2, in local_2
        local_1()
      File "<doctest README.rst[...]>", line 2, in local_1
        local_0()
      File "<doctest README.rst[...]>", line 5, in local_0
        i.reraise()
      File "...tblib...decorators.py", line 20, in reraise
        reraise(self.exc_type, self.exc_value, self.traceback)
      File "...tblib...decorators.py", line 27, in return_exceptions_wrapper
        return func(*args, **kwargs)
      File "...tblib...decorators.py", line 47, in apply_with_return_error
        return args[0](*args[1:])
      File "...tests...examples.py", line 2, in func_a
        func_b()
      File "...tests...examples.py", line 6, in func_b
        func_c()
      File "...tests...examples.py", line 10, in func_c
        func_d()
      File "...tests...examples.py", line 14, in func_d
        raise Exception("Guessing time !")
    Exception: Guessing time !
    <BLANKLINE>

Other weird stuff
`````````````````

Clearing traceback works (Python 3.4 and up)::

    >>> tb = Traceback.from_string("""
    ... File "skipped.py", line 123, in func_123
    ... Traceback (most recent call last):
    ...   File "tests/examples.py", line 2, in func_a
    ...     func_b()
    ...   File "tests/examples.py", line 6, in func_b
    ...     func_c()
    ...   File "tests/examples.py", line 10, in func_c
    ...     func_d()
    ...   File "tests/examples.py", line 14, in func_d
    ... Doesn't: matter
    ... """)
    >>> import traceback, sys
    >>> if sys.version_info > (3, 4):
    ...     traceback.clear_frames(tb)

Credits
=======

* `mitsuhiko/jinja2 <https://github.com/mitsuhiko/jinja2>`_ for figuring a way to create traceback objects.


Changelog
=========

1.4.0 (2019-05-02)
~~~~~~~~~~~~~~~~~~

* Remove support for end of life Python 3.3.
* Fixed tests for Python 3.7. Contributed by Elliott Sales de Andrade in
  `#36 <https://github.com/ionelmc/python-tblib/issues/36>`_.
* Fixed compatibility issue with Twised (``twisted.python.failure.Failure`` expected a ``co_code`` attribute).

1.3.2 (2017-04-09)
~~~~~~~~~~~~~~~~~~

* Add support for PyPy3.5-5.7.1-beta. Previously ``AttributeError:
  'Frame' object has no attribute 'clear'``  could be raised. See PyPy
  issue `#2532 <https://bitbucket.org/pypy/pypy/issues/2532/pypy3-attributeerror-frame-object-has-no>`_.

1.3.1 (2017-03-27)
~~~~~~~~~~~~~~~~~~

* Fixed handling for tracebacks due to exceeding the recursion limit.
  Fixes `#15 <https://github.com/ionelmc/python-tblib/issues/15>`_.

1.3.0 (2016-03-08)
~~~~~~~~~~~~~~~~~~

* Added ``Traceback.from_string``.

1.2.0 (2015-12-18)
~~~~~~~~~~~~~~~~~~

* Fixed handling for tracebacks from generators and other internal improvements
  and optimizations. Contributed by DRayX in `#10 <https://github.com/ionelmc/python-tblib/issues/10>`_
  and `#11 <https://github.com/ionelmc/python-tblib/pull/11>`_.

1.1.0 (2015-07-27)
~~~~~~~~~~~~~~~~~~

* Added support for Python 2.6. Contributed by Arcadiy Ivanov in
  `#8 <https://github.com/ionelmc/python-tblib/pull/8>`_.

1.0.0 (2015-03-30)
~~~~~~~~~~~~~~~~~~

* Added ``to_dict`` method and ``from_dict`` classmethod on Tracebacks.
  Contributed by beckjake in `#5 <https://github.com/ionelmc/python-tblib/pull/5>`_.



  * [termcolor-1.1.0](http://pypi.python.org/pypi/termcolor) Example
=======
    ::

        import sys
        from termcolor import colored, cprint

        text = colored('Hello, World!', 'red', attrs=['reverse', 'blink'])
        print(text)
        cprint('Hello, World!', 'green', 'on_red')

        print_red_on_cyan = lambda x: cprint(x, 'red', 'on_cyan')
        print_red_on_cyan('Hello, World!')
        print_red_on_cyan('Hello, Universe!')

        for i in range(10):
            cprint(i, 'magenta', end=' ')

        cprint("Attention!", 'red', attrs=['bold'], file=sys.stderr)

Text Properties
===============

  Text colors:

      - grey
      - red
      - green
      - yellow
      - blue
      - magenta
      - cyan
      - white

  Text highlights:

      - on_grey
      - on_red
      - on_green
      - on_yellow
      - on_blue
      - on_magenta
      - on_cyan
      - on_white

  Attributes:

      - bold
      - dark
      - underline
      - blink
      - reverse
      - concealed

Terminal properties
===================

    ============ ======= ==== ========= ========== ======= =========
    Terminal     bold    dark underline blink      reverse concealed
    ------------ ------- ---- --------- ---------- ------- ---------
    xterm        yes     no   yes       bold       yes     yes
    linux        yes     yes  bold      yes        yes     no
    rxvt         yes     no   yes       bold/black yes     no
    dtterm       yes     yes  yes       reverse    yes     yes
    teraterm     reverse no   yes       rev/red    yes     no
    aixterm      normal  no   yes       no         yes     yes
    PuTTY        color   no   yes       no         yes     no
    Windows      no      no   no        no         yes     no
    Cygwin SSH   yes     no   color     color      color   yes
    Mac Terminal yes     no   yes       yes        yes     yes
    ============ ======= ==== ========= ========== ======= =========


CHANGES
=======

1.1.0 (13.01.2011)
------------------

- Added cprint function.

1.0.1 (13.01.2011)
------------------

- Updated README.rst.

1.0.0 (13.01.2011)
------------------

- Changed license to MIT.
- Updated copyright.
- Refactored source code.

0.2 (07.09.2010)
----------------

- Added support of Python 3.x.

0.1.2 (04.06.2009)
------------------

- Fixed bold characters. (Thanks Tibor Fekete)

0.1.1 (05.03.2009)
------------------

- Some refactoring.
- Updated copyright.
- Fixed reset colors.
- Updated documentation.

0.1 (09.06.2008)
----------------

- Initial release.
  * [terminado-0.8.2](https://github.com/jupyter/terminado) This is a `Tornado <http://tornadoweb.org/>`_ websocket backend for the
`Xterm.js <https://xtermjs.org/>`_ Javascript terminal emulator
library.

It evolved out of `pyxterm <https://github.com/mitotic/pyxterm>`_, which was
part of `GraphTerm <https://github.com/mitotic/graphterm>`_ (as lineterm.py),
v0.57.0 (2014-07-18), and ultimately derived from the public-domain `Ajaxterm
<http://antony.lesuisse.org/software/ajaxterm/>`_ code, v0.11 (2008-11-13) (also
on Github as part of `QWeb <https://github.com/antonylesuisse/qweb>`_).

Modules:

* ``terminado.management``: controls launching virtual terminals,
  connecting them to Tornado's event loop, and closing them down.
* ``terminado.websocket``: Provides a websocket handler for communicating with
  a terminal.
* ``terminado.uimodule``: Provides a ``Terminal`` Tornado `UI Module
  <http://www.tornadoweb.org/en/stable/guide/templates.html#ui-modules>`_.

JS:

* ``terminado/_static/terminado.js``: A lightweight wrapper to set up a
  term.js terminal with a websocket.

Usage example:

.. code:: python

    import os.path
    import tornado.web
    import tornado.ioloop
    # This demo requires tornado_xstatic and XStatic-term.js
    import tornado_xstatic

    import terminado
    STATIC_DIR = os.path.join(os.path.dirname(terminado.__file__), "_static")

    class TerminalPageHandler(tornado.web.RequestHandler):
        def get(self):
            return self.render("termpage.html", static=self.static_url,
                               xstatic=self.application.settings['xstatic_url'],
                               ws_url_path="/websocket")

    if __name__ == '__main__':
        term_manager = terminado.SingleTermManager(shell_command=['bash'])
        handlers = [
                    (r"/websocket", terminado.TermSocket,
                         {'term_manager': term_manager}),
                    (r"/", TerminalPageHandler),
                    (r"/xstatic/(.*)", tornado_xstatic.XStaticFileHandler,
                         {'allowed_modules': ['termjs']})
                   ]
        app = tornado.web.Application(handlers, static_path=STATIC_DIR,
                          xstatic_url = tornado_xstatic.url_maker('/xstatic/'))
        # Serve at http://localhost:8765/ N.B. Leaving out 'localhost' here will
        # work, but it will listen on the public network interface as well.
        # Given what terminado does, that would be rather a security hole.
        app.listen(8765, 'localhost')
        try:
            tornado.ioloop.IOLoop.instance().start()
        finally:
            term_manager.shutdown()

See the `demos directory <https://github.com/takluyver/terminado/tree/master/demos>`_
for more examples. This is a simplified version of the ``single.py`` demo.

Run the unit tests with:

    $ nosetests

  * [testfixtures-6.10.0](https://github.com/Simplistix/testfixtures) Testfixtures
============

|CircleCI|_ |Docs|_

.. |CircleCI| image:: https://circleci.com/gh/Simplistix/testfixtures/tree/master.svg?style=shield
.. _CircleCI: https://circleci.com/gh/Simplistix/testfixtures/tree/master

.. |Docs| image:: https://readthedocs.org/projects/testfixtures/badge/?version=latest
.. _Docs: http://testfixtures.readthedocs.org/en/latest/

Testfixtures is a collection of helpers and mock objects that are useful when
writing automated tests in Python.

The areas of testing this package can help with are listed below:

**Comparing objects and sequences**

Better feedback when the results aren't as you expected along with
support for comparison of objects that don't normally support
comparison and comparison of deeply nested datastructures.

**Mocking out objects and methods**

Easy to use ways of stubbing out objects, classes or individual
methods for both doc tests and unit tests. Special helpers are
provided for testing with dates and times.

**Testing logging**

Helpers for capturing logging output in both doc tests and
unit tests.

**Testing stream output**

Helpers for capturing stream output, such as that from print
statements, and making assertion about it.

**Testing with files and directories**

Support for creating and checking files and directories in sandboxes
for both doc tests and unit tests.

**Testing exceptions and warnings**

Easy to use ways of checking that a certain exception is raised,
or a warning is issued, even down the to the parameters provided.

**Testing subprocesses**

A handy mock for testing code that uses subprocesses.

**Testing when using django**

Helpers for comparing instances of django models.

**Testing when using zope.component**

An easy to use sterile component registry.



  * [testpath-0.4.2](https://github.com/jupyter/testpath) Testpath is a collection of utilities for Python code working with files and commands.

It contains functions to check things on the filesystem, and tools for mocking
system commands and recording calls to those.

`Documentation on ReadTheDocs <https://testpath.readthedocs.io/en/latest/>`_

e.g.::

    import testpath
    testpath.assert_isfile(path)
    
    with testpath.assert_calls('git', ['add', path]):
        function_under_test()

  * [testrepository-0.0.20](https://launchpad.net/testrepository) Test Repository
+++++++++++++++

Overview
~~~~~~~~

This project provides a database of test results which can be used as part of
developer workflow to ensure/check things like:

* No commits without having had a test failure, test fixed cycle.
* No commits without new tests being added.
* What tests have failed since the last commit (to run just a subset).
* What tests are currently failing and need work.

Test results are inserted using subunit (and thus anything that can output
subunit or be converted into a subunit stream can be accepted).

A mailing list for discussion, usage and development is at
https://launchpad.net/~testrepository-dev - all are welcome to join. Some folk
hang out on #testrepository on irc.freenode.net.

CI for the project is at http://build.robertcollins.net/job/testrepository-default/.

Licensing
~~~~~~~~~

Test Repository is under BSD / Apache 2.0 licences. See the file COPYING in the source for details.

Quick Start
~~~~~~~~~~~

Create a config file::
  $ touch .testr.conf

Create a repository::
  $ testr init

Load a test run into the repository::
  $ testr load < testrun

Query the repository::
  $ testr stats
  $ testr last
  $ testr failing

Delete a repository::
  $ rm -rf .testrepository

Documentation
~~~~~~~~~~~~~

More detailed documentation including design and implementation details, a
user manual, and guidelines for development of Test Repository itself can be
found at https://testrepository.readthedocs.org/en/latest, or in the source
tree at doc/ (run make -C doc html).
  * [testresources-2.0.1](https://launchpad.net/testresources) testresources: extensions to python unittest to allow declarative use
of resources by test cases.

Copyright (C) 2005-2013  Robert Collins <robertc@robertcollins.net>

  Licensed under either the Apache License, Version 2.0 or the BSD 3-clause
  license at the users choice. A copy of both licenses are available in the
  project source as Apache-2.0 and BSD. You may not use this file except in
  compliance with one of these two licences.

  Unless required by applicable law or agreed to in writing, software
  distributed under these licenses is distributed on an "AS IS" BASIS, WITHOUT
  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
  license you chose for the specific language governing permissions and
  limitations under that license.

  See the COPYING file for full details on the licensing of Testresources.


Testresources
+++++++++++++

testresources extends unittest with a clean and simple api to provide test
optimisation where expensive common resources are needed for test cases - for
example sample working trees for VCS systems, reference databases for
enterprise applications, or web servers ... let imagination run wild.

Dependencies to build/selftest
==============================

* Python 2.6+ (or 3.3+)
* docutils
* testtools (http://pypi.python.org/pypi/testtools/)
* fixtures (http://pypi.python.org/pypi/fixtures)

Dependencies to use testresources
=================================

* Python 2.6+ (or 3.3+)

For older versions of Python, testresources <= 1.0.0 supported 2.4, 2.5 and
3.2.

How testresources Works
=======================

The basic idea of testresources is:

* Tests declare the resources they need in a ``resources`` attribute.
* When the test is run, the required resource objects are allocated (either
  newly constructed, or reused), and assigned to attributes of the TestCase.

testresources distinguishes a 'resource manager' (a subclass of
``TestResourceManager``) which acts as a kind of factory, and a 'resource'
which can be any kind of object returned from the manager class's
``getResource`` method.

Resources are either clean or dirty.  Being clean means they have same state in
all important ways as a newly constructed instance and they can therefore be
safely reused.

At this time, testresources is incompatible with setUpClass and setUpModule -
when an OptimisingTestSuite is wrapped around a test suite using those
features, the result will be flattened for optimisation and those setup's will
not run at all.

Main Classes
============

testresources.ResourcedTestCase
-------------------------------

By extending or mixing-in this class, tests can have necessary resources
automatically allocated and disposed or recycled.

ResourceTestCase can be used as a base class for tests, and when that is done
tests will have their ``resources`` attribute automatically checked for
resources by both OptimisingTestSuite and their own setUp() and tearDown()
methods. (This allows tests to remain functional without needing this specific
TestSuite as a container). Alternatively, you can call setUpResources(self,
resources, test_result) and tearDownResources(self, resources, test_result)
from your own classes setUp and tearDown and the same behaviour will be
activated.

To declare the use of a resource, set the ``resources`` attribute to a list of
tuples of ``(attribute_name, resource_manager)``.

During setUp, for each declared requirement, the test gains an attribute
pointing to an allocated resource, which is the result of calling
``resource_manager.getResource()``.  ``finishedWith`` will be called on each
resource during tearDown().

For example::

    class TestLog(testresources.ResourcedTestCase):

        resources = [('branch', BzrPopulatedBranch())]

        def test_log(self):
            show_log(self.branch, ...)

testresources.TestResourceManager
---------------------------------

A TestResourceManager is an object that tests can use to create resources.  It
can be overridden to manage different types of resources.  Normally test code
doesn't need to call any methods on it, as this will be arranged by the
testresources machinery.

When implementing a new ``TestResourceManager`` subclass you should consider
overriding these methods:

``make``
    Must be overridden in every concrete subclass.

    Returns a new instance of the resource object
    (the actual resource, not the TestResourceManager).  Doesn't need to worry about
    reuse, which is taken care of separately.  This method is only called when a
    new resource is definitely needed.

    ``make`` is called by ``getResource``; you should not normally need to override
    the latter.

``clean``
    Cleans up an existing resource instance, eg by deleting a directory or
    closing a network connection.  By default this does nothing, which may be
    appropriate for resources that are automatically garbage collected.

``_reset``
    Reset a no-longer-used dirty resource to a clean state.  By default this
    just discards it and creates a new one, but for some resources there may be a
    faster way to reset them.

``isDirty``
    Check whether an existing resource is dirty.  By default this just reports
    whether ``TestResourceManager.dirtied`` has been called or any of the
    dependency resources are dirty.

For instance::

    class TemporaryDirectoryResource(TestResourceManager):

        def clean(self, resource):
            shutil.rmtree(resource)

        def make(self):
            return tempfile.mkdtemp()

        def isDirty(self, resource):
            # Can't detect when the directory is written to, so assume it
            # can never be reused.  We could list the directory, but that might
            # not catch it being open as a cwd etc.
            return True

The ``resources`` list on the TestResourceManager object is used to declare
dependencies. For instance, a DataBaseResource that needs a TemporaryDirectory
might be declared with a resources list::

    class DataBaseResource(TestResourceManager):

        resources = [("scratchdir", TemporaryDirectoryResource())]

Most importantly, two getResources to the same TestResourceManager with no
finishedWith call in the middle, will return the same object as long as it is
not dirty.

When a Test has a dependency and that dependency successfully completes but
returns None, the framework does *not* consider this an error: be sure to always
return a valid resource, or raise an error. Error handling hasn't been heavily
exercised, but any bugs in this area will be promptly dealt with.

A sample TestResourceManager can be found in the doc/ folder.

See pydoc testresources.TestResourceManager for details.

testresources.GenericResource
-----------------------------

Glue to adapt testresources to an existing resource-like class.

testresources.FixtureResource
-----------------------------

Glue to adapt testresources to the simpler fixtures.Fixture API. Long
term testresources is likely to consolidate on that simpler API as the
recommended method of writing resources.

testresources.OptimisingTestSuite
---------------------------------

This TestSuite will introspect all the test cases it holds directly and if
they declare needed resources, will run the tests in an order that attempts to
minimise the number of setup and tear downs required. It attempts to achieve
this by callling getResource() and finishedWith() around the sequence of tests
that use a specific resource.

Tests are added to an OptimisingTestSuite as normal. Any standard library
TestSuite objects will be flattened, while any custom TestSuite subclasses
will be distributed across their member tests. This means that any custom
logic in test suites should be preserved, at the price of some level of
optimisation.

Because the test suite does the optimisation, you can control the amount of
optimising that takes place by adding more or fewer tests to a single
OptimisingTestSuite. You could add everything to a single OptimisingTestSuite,
getting global optimisation or you could use several smaller
OptimisingTestSuites.


testresources.TestLoader
------------------------

This is a trivial TestLoader that creates OptimisingTestSuites by default.

unittest.TestResult
-------------------

testresources will log activity about resource creation and destruction to the
result object tests are run with. 6 extension methods are looked for:
``startCleanResource``, ``stopCleanResource``, ``startMakeResource``,
``stopMakeResource``, ``startResetResource`` and finally ``stopResetResource``.
``testresources.tests.ResultWithResourceExtensions`` is
an example of a ``TestResult`` with these methods present.

Controlling Resource Reuse
==========================

When or how do I mark the resource dirtied?

The simplest approach is to have ``TestResourceManager.make`` call ``self.dirtied``:
the resource is always immediately dirty and will never be reused without first
being reset.  This is appropriate when the underlying resource is cheap to
reset or recreate, or when it's hard to detect whether it's been dirtied or to
trap operations that change it.

Alternatively, override ``TestResourceManager.isDirty`` and inspect the resource to
see if it is safe to reuse.

Finally, you can arrange for the returned resource to always call back to
``TestResourceManager.dirtied`` on the first operation that mutates it.

FAQ
===

* Can I dynamically request resources inside a test method?

  Generally, no, you shouldn't do this.  The idea is that the resources are
  declared statically, so that testresources can "smooth" resource usage across
  several tests.

  But, you may be able to find some object that is statically declared and reusable
  to act as the resource, which can then provide methods to generate sub-elements
  of itself during a test.

* If the resource is held inside the TestResourceManager object, and the
  TestResourceManager is typically constructed inline in the test case
  ``resources`` attribute, how can they be shared across different test
  classes?

  Good question.

  I guess you should arrange for a single instance to be held in an appropriate
  module scope, then referenced by the test classes that want to share it.

Releasing
=========

1. Add a section to NEWS (after In Development).
2. git tag -s
3. python setup.py sdist bdist_wheel upload -s
  * [testscenarios-0.5.0](https://launchpad.net/testscenarios) *****************************************************************
testscenarios: extensions to python unittest to support scenarios
*****************************************************************

  Copyright (c) 2009, Robert Collins <robertc@robertcollins.net>
  
  Licensed under either the Apache License, Version 2.0 or the BSD 3-clause
  license at the users choice. A copy of both licenses are available in the
  project source as Apache-2.0 and BSD. You may not use this file except in
  compliance with one of these two licences.
  
  Unless required by applicable law or agreed to in writing, software
  distributed under these licenses is distributed on an "AS IS" BASIS, WITHOUT
  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
  license you chose for the specific language governing permissions and
  limitations under that license.


testscenarios provides clean dependency injection for python unittest style
tests. This can be used for interface testing (testing many implementations via
a single test suite) or for classic dependency injection (provide tests with
dependencies externally to the test code itself, allowing easy testing in
different situations).

Dependencies
============

* Python 2.6+
* testtools <https://launchpad.net/testtools>


Why TestScenarios
=================

Standard Python unittest.py provides on obvious method for running a single
test_foo method with two (or more) scenarios: by creating a mix-in that
provides the functions, objects or settings that make up the scenario. This is
however limited and unsatisfying. Firstly, when two projects are cooperating
on a test suite (for instance, a plugin to a larger project may want to run
the standard tests for a given interface on its implementation), then it is
easy for them to get out of sync with each other: when the list of TestCase
classes to mix-in with changes, the plugin will either fail to run some tests
or error trying to run deleted tests. Secondly, its not as easy to work with
runtime-created-subclasses (a way of dealing with the aforementioned skew)
because they require more indirection to locate the source of the test, and will
often be ignored by e.g. pyflakes pylint etc.

It is the intent of testscenarios to make dynamically running a single test
in multiple scenarios clear, easy to debug and work with even when the list
of scenarios is dynamically generated.


Defining Scenarios
==================

A **scenario** is a tuple of a string name for the scenario, and a dict of
parameters describing the scenario.  The name is appended to the test name, and
the parameters are made available to the test instance when it's run.

Scenarios are presented in **scenario lists** which are typically Python lists
but may be any iterable.


Getting Scenarios applied
=========================

At its heart the concept is simple. For a given test object with a list of
scenarios we prepare a new test object for each scenario. This involves:

* Clone the test to a new test with a new id uniquely distinguishing it.
* Apply the scenario to the test by setting each key, value in the scenario
  as attributes on the test object.

There are some complicating factors around making this happen seamlessly. These
factors are in two areas:

* Choosing what scenarios to use. (See Setting Scenarios For A Test).
* Getting the multiplication to happen. 

Subclasssing
++++++++++++

If you can subclass TestWithScenarios, then the ``run()`` method in
TestWithScenarios will take care of test multiplication. It will at test
execution act as a generator causing multiple tests to execute. For this to 
work reliably TestWithScenarios must be first in the MRO and you cannot
override run() or __call__. This is the most robust method, in the sense
that any test runner or test loader that obeys the python unittest protocol
will run all your scenarios.

Manual generation
+++++++++++++++++

If you cannot subclass TestWithScenarios (e.g. because you are using
TwistedTestCase, or TestCaseWithResources, or any one of a number of other
useful test base classes, or need to override run() or __call__ yourself) then 
you can cause scenario application to happen later by calling
``testscenarios.generate_scenarios()``. For instance::

  >>> import unittest
  >>> try:
  ...     from StringIO import StringIO
  ... except ImportError:
  ...     from io import StringIO
  >>> from testscenarios.scenarios import generate_scenarios

This can work with loaders and runners from the standard library, or possibly other
implementations::

  >>> loader = unittest.TestLoader()
  >>> test_suite = unittest.TestSuite()
  >>> runner = unittest.TextTestRunner(stream=StringIO())

  >>> mytests = loader.loadTestsFromNames(['doc.test_sample'])
  >>> test_suite.addTests(generate_scenarios(mytests))
  >>> runner.run(test_suite)
  <unittest...TextTestResult run=1 errors=0 failures=0>

Testloaders
+++++++++++

Some test loaders support hooks like ``load_tests`` and ``test_suite``.
Ensuring your tests have had scenario application done through these hooks can
be a good idea - it means that external test runners (which support these hooks
like ``nose``, ``trial``, ``tribunal``) will still run your scenarios. (Of
course, if you are using the subclassing approach this is already a surety).
With ``load_tests``::

  >>> def load_tests(standard_tests, module, loader):
  ...     result = loader.suiteClass()
  ...     result.addTests(generate_scenarios(standard_tests))
  ...     return result

as a convenience, this is available in ``load_tests_apply_scenarios``, so a
module using scenario tests need only say ::

  >>> from testscenarios import load_tests_apply_scenarios as load_tests

Python 2.7 and greater support a different calling convention for `load_tests``
<https://bugs.launchpad.net/bzr/+bug/607412>.  `load_tests_apply_scenarios`
copes with both.

With ``test_suite``::

  >>> def test_suite():
  ...     loader = TestLoader()
  ...     tests = loader.loadTestsFromName(__name__)
  ...     result = loader.suiteClass()
  ...     result.addTests(generate_scenarios(tests))
  ...     return result


Setting Scenarios for a test
============================

A sample test using scenarios can be found in the doc/ folder.

See `pydoc testscenarios` for details.

On the TestCase
+++++++++++++++

You can set a scenarios attribute on the test case::

  >>> class MyTest(unittest.TestCase):
  ...
  ...     scenarios = [
  ...         ('scenario1', dict(param=1)),
  ...         ('scenario2', dict(param=2)),]

This provides the main interface by which scenarios are found for a given test.
Subclasses will inherit the scenarios (unless they override the attribute).

After loading
+++++++++++++

Test scenarios can also be generated arbitrarily later, as long as the test has
not yet run. Simply replace (or alter, but be aware that many tests may share a
single scenarios attribute) the scenarios attribute. For instance in this
example some third party tests are extended to run with a custom scenario. ::

  >>> import testtools
  >>> class TestTransport:
  ...     """Hypothetical test case for bzrlib transport tests"""
  ...     pass
  ...
  >>> stock_library_tests = unittest.TestLoader().loadTestsFromNames(
  ...     ['doc.test_sample'])
  ...
  >>> for test in testtools.iterate_tests(stock_library_tests):
  ...     if isinstance(test, TestTransport):
  ...         test.scenarios = test.scenarios + [my_vfs_scenario]
  ...
  >>> suite = unittest.TestSuite()
  >>> suite.addTests(generate_scenarios(stock_library_tests))

Generated tests don't have a ``scenarios`` list, because they don't normally
require any more expansion.  However, you can add a ``scenarios`` list back on
to them, and then run them through ``generate_scenarios`` again to generate the
cross product of tests. ::

  >>> class CrossProductDemo(unittest.TestCase):
  ...     scenarios = [('scenario_0_0', {}),
  ...                  ('scenario_0_1', {})]
  ...     def test_foo(self):
  ...         return
  ...
  >>> suite = unittest.TestSuite()
  >>> suite.addTests(generate_scenarios(CrossProductDemo("test_foo")))
  >>> for test in testtools.iterate_tests(suite):
  ...     test.scenarios = [
  ...         ('scenario_1_0', {}), 
  ...         ('scenario_1_1', {})]
  ...
  >>> suite2 = unittest.TestSuite()
  >>> suite2.addTests(generate_scenarios(suite))
  >>> print(suite2.countTestCases())
  4

Dynamic Scenarios
+++++++++++++++++

A common use case is to have the list of scenarios be dynamic based on plugins
and available libraries. An easy way to do this is to provide a global scope
scenarios somewhere relevant to the tests that will use it, and then that can
be customised, or dynamically populate your scenarios from a registry etc.
For instance::

  >>> hash_scenarios = []
  >>> try:
  ...     from hashlib import md5
  ... except ImportError:
  ...     pass
  ... else:
  ...     hash_scenarios.append(("md5", dict(hash=md5)))
  >>> try:
  ...     from hashlib import sha1
  ... except ImportError:
  ...     pass
  ... else:
  ...     hash_scenarios.append(("sha1", dict(hash=sha1)))
  ...
  >>> class TestHashContract(unittest.TestCase):
  ...
  ...     scenarios = hash_scenarios
  ...
  >>> class TestHashPerformance(unittest.TestCase):
  ...
  ...     scenarios = hash_scenarios


Forcing Scenarios
+++++++++++++++++

The ``apply_scenarios`` function can be useful to apply scenarios to a test
that has none applied. ``apply_scenarios`` is the workhorse for
``generate_scenarios``, except it takes the scenarios passed in rather than
introspecting the test object to determine the scenarios. The
``apply_scenarios`` function does not reset the test scenarios attribute,
allowing it to be used to layer scenarios without affecting existing scenario
selection.


Generating Scenarios
====================

Some functions (currently one :-) are available to ease generation of scenario
lists for common situations.

Testing Per Implementation Module
+++++++++++++++++++++++++++++++++

It is reasonably common to have multiple Python modules that provide the same
capabilities and interface, and to want apply the same tests to all of them.

In some cases, not all of the statically defined implementations will be able
to be used in a particular testing environment.  For example, there may be both
a C and a pure-Python implementation of a module.  You want to test the C
module if it can be loaded, but also to have the tests pass if the C module has
not been compiled.

The ``per_module_scenarios`` function generates a scenario for each named
module. The module object of the imported module is set in the supplied
attribute name of the resulting scenario.
Modules which raise ``ImportError`` during import will have the
``sys.exc_info()`` of the exception set instead of the module object. Tests
can check for the attribute being a tuple to decide what to do (e.g. to skip).

Note that for the test to be valid, all access to the module under test must go
through the relevant attribute of the test object.  If one of the
implementations is also directly imported by the test module or any other,
testscenarios will not magically stop it being used.


Advice on Writing Scenarios
===========================

If a parameterised test is because of a bug run without being parameterized,
it should fail rather than running with defaults, because this can hide bugs.


Producing Scenarios
===================

The `multiply_scenarios` function produces the cross-product of the scenarios
passed in::

  >>> from testscenarios.scenarios import multiply_scenarios
  >>> 
  >>> scenarios = multiply_scenarios(
  ...      [('scenario1', dict(param1=1)), ('scenario2', dict(param1=2))],
  ...      [('scenario2', dict(param2=1))],
  ...      )
  >>> scenarios == [('scenario1,scenario2', {'param2': 1, 'param1': 1}),
  ...               ('scenario2,scenario2', {'param2': 1, 'param1': 2})]
  True
  * [testtools-2.3.0](https://github.com/testing-cabal/testtools) ======================================
testtools: tasteful testing for Python
======================================

testtools is a set of extensions to the Python standard library's unit testing
framework. These extensions have been derived from many years of experience
with unit testing in Python and come from many different sources.

What better way to start than with a contrived code snippet?::

  from testtools import TestCase
  from testtools.content import Content
  from testtools.content_type import UTF8_TEXT
  from testtools.matchers import Equals

  from myproject import SillySquareServer

  class TestSillySquareServer(TestCase):

      def setUp(self):
          super(TestSillySquareServer, self).setUp()
          self.server = self.useFixture(SillySquareServer())
          self.addCleanup(self.attach_log_file)

      def attach_log_file(self):
          self.addDetail(
              'log-file',
              Content(UTF8_TEXT,
                      lambda: open(self.server.logfile, 'r').readlines()))

      def test_server_is_cool(self):
          self.assertThat(self.server.temperature, Equals("cool"))

      def test_square(self):
          self.assertThat(self.server.silly_square_of(7), Equals(49))


Why use testtools?
==================

Better assertion methods
------------------------

The standard assertion methods that come with unittest aren't as helpful as
they could be, and there aren't quite enough of them.  testtools adds
``assertIn``, ``assertIs``, ``assertIsInstance`` and their negatives.


Matchers: better than assertion methods
---------------------------------------

Of course, in any serious project you want to be able to have assertions that
are specific to that project and the particular problem that it is addressing.
Rather than forcing you to define your own assertion methods and maintain your
own inheritance hierarchy of ``TestCase`` classes, testtools lets you write
your own "matchers", custom predicates that can be plugged into a unit test::

  def test_response_has_bold(self):
     # The response has bold text.
     response = self.server.getResponse()
     self.assertThat(response, HTMLContains(Tag('bold', 'b')))


More debugging info, when you need it
--------------------------------------

testtools makes it easy to add arbitrary data to your test result.  If you
want to know what's in a log file when a test fails, or what the load was on
the computer when a test started, or what files were open, you can add that
information with ``TestCase.addDetail``, and it will appear in the test
results if that test fails.


Extend unittest, but stay compatible and re-usable
--------------------------------------------------

testtools goes to great lengths to allow serious test authors and test
*framework* authors to do whatever they like with their tests and their
extensions while staying compatible with the standard library's unittest.

testtools has completely parametrized how exceptions raised in tests are
mapped to ``TestResult`` methods and how tests are actually executed (ever
wanted ``tearDown`` to be called regardless of whether ``setUp`` succeeds?)

It also provides many simple but handy utilities, like the ability to clone a
test, a ``MultiTestResult`` object that lets many result objects get the
results from one test suite, adapters to bring legacy ``TestResult`` objects
into our new golden age.


Cross-Python compatibility
--------------------------

testtools gives you the very latest in unit testing technology in a way that
will work with Python 2.7, 3.3, 3.4, 3.5, and pypy.

If you wish to use testtools with Python 2.4 or 2.5, then please use testtools
0.9.15.

If you wish to use testtools with Python 2.6 or 3.2, then please use testtools
1.9.0.
  * [textblob-0.15.3](https://github.com/sloria/TextBlob) 
TextBlob: Simplified Text Processing
====================================

.. image:: https://badgen.net/pypi/v/TextBlob
    :target: https://pypi.org/project/textblob/
    :alt: Latest version

.. image:: https://badgen.net/travis/sloria/TextBlob/dev
    :target: https://travis-ci.org/sloria/TextBlob
    :alt: Travis-CI

Homepage: `https://textblob.readthedocs.io/ <https://textblob.readthedocs.io/>`_

`TextBlob` is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.


.. code-block:: python

    from textblob import TextBlob

    text = '''
    The titular threat of The Blob has always struck me as the ultimate movie
    monster: an insatiably hungry, amoeba-like mass able to penetrate
    virtually any safeguard, capable of--as a doomed doctor chillingly
    describes it--"assimilating flesh on contact.
    Snide comparisons to gelatin be damned, it's a concept with the most
    devastating of potential consequences, not unlike the grey goo scenario
    proposed by technological theorists fearful of
    artificial intelligence run rampant.
    '''

    blob = TextBlob(text)
    blob.tags           # [('The', 'DT'), ('titular', 'JJ'),
                        #  ('threat', 'NN'), ('of', 'IN'), ...]

    blob.noun_phrases   # WordList(['titular threat', 'blob',
                        #            'ultimate movie monster',
                        #            'amoeba-like mass', ...])

    for sentence in blob.sentences:
        print(sentence.sentiment.polarity)
    # 0.060
    # -0.341

    blob.translate(to="es")  # 'La amenaza titular de The Blob...'

TextBlob stands on the giant shoulders of `NLTK`_ and `pattern`_, and plays nicely with both.

Features
--------

- Noun phrase extraction
- Part-of-speech tagging
- Sentiment analysis
- Classification (Naive Bayes, Decision Tree)
- Language translation and detection powered by Google Translate
- Tokenization (splitting text into words and sentences)
- Word and phrase frequencies
- Parsing
- `n`-grams
- Word inflection (pluralization and singularization) and lemmatization
- Spelling correction
- Add new models or languages through extensions
- WordNet integration

Get it now
----------
::

    $ pip install -U textblob
    $ python -m textblob.download_corpora

Examples
--------

See more examples at the `Quickstart guide`_.

.. _`Quickstart guide`: https://textblob.readthedocs.io/en/latest/quickstart.html#quickstart


Documentation
-------------

Full documentation is available at https://textblob.readthedocs.io/.

Requirements
------------

- Python >= 2.7 or >= 3.4

Project Links
-------------

- Docs: https://textblob.readthedocs.io/
- Changelog: https://textblob.readthedocs.io/en/latest/changelog.html
- PyPI: https://pypi.python.org/pypi/TextBlob
- Issues: https://github.com/sloria/TextBlob/issues

License
-------

MIT licensed. See the bundled `LICENSE <https://github.com/sloria/TextBlob/blob/master/LICENSE>`_ file for more details.

.. _pattern: http://www.clips.ua.ac.be/pattern
.. _NLTK: http://nltk.org/



  * [tifffile-2019.7.26](https://www.lfd.uci.edu/~gohlke/) Read and write TIFF(r) files
============================

Tifffile is a Python library to

(1) store numpy arrays in TIFF (Tagged Image File Format) files, and
(2) read image and metadata from TIFF-like files used in bioimaging.

Image and metadata can be read from TIFF, BigTIFF, OME-TIFF, STK, LSM, SGI,
NIHImage, ImageJ, MicroManager, FluoView, ScanImage, SEQ, GEL, SVS, SCN, SIS,
ZIF, QPI, NDPI, and GeoTIFF files.

Numpy arrays can be written to TIFF, BigTIFF, and ImageJ hyperstack compatible
files in multi-page, memory-mappable, tiled, predicted, or compressed form.

Only a subset of the TIFF specification is supported, mainly uncompressed and
losslessly compressed 1, 8, 16, 32 and 64-bit integer, 16, 32 and 64-bit float,
grayscale and RGB(A) images.
Specifically, reading slices of image data, CCITT and OJPEG compression,
chroma subsampling without JPEG compression, or IPTC and XMP metadata are not
implemented.

TIFF(r), the Tagged Image File Format, is a trademark and under control of
Adobe Systems Incorporated. BigTIFF allows for files greater than 4 GB.
STK, LSM, FluoView, SGI, SEQ, GEL, and OME-TIFF, are custom extensions
defined by Molecular Devices (Universal Imaging Corporation), Carl Zeiss
MicroImaging, Olympus, Silicon Graphics International, Media Cybernetics,
Molecular Dynamics, and the Open Microscopy Environment consortium
respectively.

For command line usage run ``python -m tifffile --help``

:Author:
  `Christoph Gohlke <https://www.lfd.uci.edu/~gohlke/>`_

:Organization:
  Laboratory for Fluorescence Dynamics, University of California, Irvine

:License: 3-clause BSD

:Version: 2019.7.26

Requirements
------------
This release has been tested with the following requirements and dependencies
(other versions may work):

* `CPython 2.7.16, 3.5.4, 3.6.8, 3.7.4, 64-bit <https://www.python.org>`_
* `Numpy 1.16.4 <https://www.numpy.org>`_
* `Imagecodecs 2019.5.22 <https://pypi.org/project/imagecodecs/>`_
  (optional; used for encoding and decoding LZW, JPEG, etc.)
* `Matplotlib 3.1 <https://www.matplotlib.org>`_ (optional; used for plotting)
* Python 2.7 requires 'futures', 'enum34', and 'pathlib'.

Revisions
---------
2019.7.26
    Pass 2869 tests.
    Fix infinite loop reading more than two tags of same code in IFD.
    Delay import of logging module.
2019.7.20
    Fix OME-XML detection for files created by Imaris.
    Remove or replace assert statements.
2019.7.2
    Do not write SampleFormat tag for unsigned data types.
    Write ByteCount tag values as SHORT or LONG if possible.
    Allow to specify axes in FileSequence pattern via group names.
    Add option to concurrently read FileSequence using threads.
    Derive TiffSequence from FileSequence.
    Use str(datetime.timedelta) to format Timer duration.
    Use perf_counter for Timer if possible.
2019.6.18
    Fix reading planar RGB ImageJ files created by Bio-Formats.
    Fix reading single-file, multi-image OME-TIFF without UUID.
    Presume LSM stores uncompressed images contiguously per page.
    Reformat some complex expressions.
2019.5.30
    Ignore invalid frames in OME-TIFF.
    Set default subsampling to (2, 2) for RGB JPEG compression.
    Fix reading and writing planar RGB JPEG compression.
    Replace buffered_read with FileHandle.read_segments.
    Include page or frame numbers in exceptions and warnings.
    Add Timer class.
2019.5.22
    Add optional chroma subsampling for JPEG compression.
    Enable writing PNG, JPEG, JPEGXR, and JPEG2000 compression (WIP).
    Fix writing tiled images with WebP compression.
    Improve handling GeoTIFF sparse files.
2019.3.18
    Fix regression decoding JPEG with RGB photometrics.
    Fix reading OME-TIFF files with corrupted but unused pages.
    Allow to load TiffFrame without specifying keyframe.
    Calculate virtual TiffFrames for non-BigTIFF ScanImage files > 2GB.
    Rename property is_chroma_subsampled to is_subsampled (breaking).
    Make more attributes and methods private (WIP).
2019.3.8
    Fix MemoryError when RowsPerStrip > ImageLength.
    Fix SyntaxWarning on Python 3.8.
    Fail to decode JPEG to planar RGB (tentative).
    Separate public from private test files (WIP).
    Allow testing without data files or imagecodecs.
2019.2.22
    Use imagecodecs-lite as a fallback for imagecodecs.
    Simplify reading numpy arrays from file.
    Use TiffFrames when reading arrays from page sequences.
    Support slices and iterators in TiffPageSeries sequence interface.
    Auto-detect uniform series.
    Use page hash to determine generic series.
    Turn off page cache (tentative).
    Pass through more parameters in imread.
    Discontinue movie parameter in imread and TiffFile (breaking).
    Discontinue bigsize parameter in imwrite (breaking).
    Raise TiffFileError in case of issues with TIFF structure.
    Return TiffFile.ome_metadata as XML (breaking).
    Ignore OME series when last dimensions are not stored in TIFF pages.
2019.2.10
    Assemble IFDs in memory to speed-up writing on some slow media.
    Handle discontinued arguments fastij, multifile_close, and pages.
2019.1.30
    Use black background in imshow.
    Do not write datetime tag by default (breaking).
    Fix OME-TIFF with SamplesPerPixel > 1.
    Allow 64-bit IFD offsets for NDPI (files > 4GB still not supported).
2019.1.4
    Fix decoding deflate without imagecodecs.
2019.1.1
    Update copyright year.
    Require imagecodecs >= 2018.12.16.
    Do not use JPEG tables from keyframe.
    Enable decoding large JPEG in NDPI.
    Decode some old-style JPEG.
    Reorder OME channel axis to match PlanarConfiguration storage.
    Return tiled images as contiguous arrays.
    Add decode_lzw proxy function for compatibility with old czifile module.
    Use dedicated logger.
2018.11.28
    Make SubIFDs accessible as TiffPage.pages.
    Make parsing of TiffSequence axes pattern optional (breaking).
    Limit parsing of TiffSequence axes pattern to file names, not path names.
    Do not interpolate in imshow if image dimensions <= 512, else use bilinear.
    Use logging.warning instead of warnings.warn in many cases.
    Fix numpy FutureWarning for out == 'memmap'.
    Adjust ZSTD and WebP compression to libtiff-4.0.10 (WIP).
    Decode old-style LZW with imagecodecs >= 2018.11.8.
    Remove TiffFile.qptiff_metadata (QPI metadata are per page).
    Do not use keyword arguments before variable positional arguments.
    Make either all or none return statements in a function return expression.
    Use pytest parametrize to generate tests.
    Replace test classes with functions.
2018.11.6
    Rename imsave function to imwrite.
    Readd Python implementations of packints, delta, and bitorder codecs.
    Fix TiffFrame.compression AttributeError.
2018.10.18
    Rename tiffile package to tifffile.
2018.10.10
    Read ZIF, the Zoomable Image Format (WIP).
    Decode YCbCr JPEG as RGB (tentative).
    Improve restoration of incomplete tiles.
    Allow to write grayscale with extrasamples without specifying planarconfig.
    Enable decoding of PNG and JXR via imagecodecs.
    Deprecate 32-bit platforms (too many memory errors during tests).
2018.9.27
    Read Olympus SIS (WIP).
    Allow to write non-BigTIFF files up to ~4 GB (fix).
    Fix parsing date and time fields in SEM metadata.
    Detect some circular IFD references.
    Enable WebP codecs via imagecodecs.
    Add option to read TiffSequence from ZIP containers.
    Remove TiffFile.isnative.
    Move TIFF struct format constants out of TiffFile namespace.
2018.8.31
    Fix wrong TiffTag.valueoffset.
    Towards reading Hamamatsu NDPI (WIP).
    Enable PackBits compression of byte and bool arrays.
    Fix parsing NULL terminated CZ_SEM strings.
2018.8.24
    Move tifffile.py and related modules into tiffile package.
    Move usage examples to module docstring.
    Enable multi-threading for compressed tiles and pages by default.
    Add option to concurrently decode image tiles using threads.
    Do not skip empty tiles (fix).
    Read JPEG and J2K compressed strips and tiles.
    Allow floating-point predictor on write.
    Add option to specify subfiletype on write.
    Depend on imagecodecs package instead of _tifffile, lzma, etc modules.
    Remove reverse_bitorder, unpack_ints, and decode functions.
    Use pytest instead of unittest.
2018.6.20
    Save RGBA with unassociated extrasample by default (breaking).
    Add option to specify ExtraSamples values.
2018.6.17 (included with 0.15.1)
    Towards reading JPEG and other compressions via imagecodecs package (WIP).
    Read SampleFormat VOID as UINT.
    Add function to validate TIFF using 'jhove -m TIFF-hul'.
    Save bool arrays as bilevel TIFF.
    Accept pathlib.Path as filenames.
    Move 'software' argument from TiffWriter __init__ to save.
    Raise DOS limit to 16 TB.
    Lazy load LZMA and ZSTD compressors and decompressors.
    Add option to save IJMetadata tags.
    Return correct number of pages for truncated series (fix).
    Move EXIF tags to TIFF.TAG as per TIFF/EP standard.
2018.2.18
    Always save RowsPerStrip and Resolution tags as required by TIFF standard.
    Do not use badly typed ImageDescription.
    Coherce bad ASCII string tags to bytes.
    Tuning of __str__ functions.
    Fix reading 'undefined' tag values.
    Read and write ZSTD compressed data.
    Use hexdump to print byte strings.
    Determine TIFF byte order from data dtype in imsave.
    Add option to specify RowsPerStrip for compressed strips.
    Allow memory-map of arrays with non-native byte order.
    Attempt to handle ScanImage <= 5.1 files.
    Restore TiffPageSeries.pages sequence interface.
    Use numpy.frombuffer instead of fromstring to read from binary data.
    Parse GeoTIFF metadata.
    Add option to apply horizontal differencing before compression.
    Towards reading PerkinElmer QPI (QPTIFF, no test files).
    Do not index out of bounds data in tifffile.c unpackbits and decodelzw.
2017.9.29
    Many backward incompatible changes improving speed and resource usage:
    Add detail argument to __str__ function. Remove info functions.
    Fix potential issue correcting offsets of large LSM files with positions.
    Remove TiffFile sequence interface; use TiffFile.pages instead.
    Do not make tag values available as TiffPage attributes.
    Use str (not bytes) type for tag and metadata strings (WIP).
    Use documented standard tag and value names (WIP).
    Use enums for some documented TIFF tag values.
    Remove 'memmap' and 'tmpfile' options; use out='memmap' instead.
    Add option to specify output in asarray functions.
    Add option to concurrently decode pages using threads.
    Add TiffPage.asrgb function (WIP).
    Do not apply colormap in asarray.
    Remove 'colormapped', 'rgbonly', and 'scale_mdgel' options from asarray.
    Consolidate metadata in TiffFile _metadata functions.
    Remove non-tag metadata properties from TiffPage.
    Add function to convert LSM to tiled BIN files.
    Align image data in file.
    Make TiffPage.dtype a numpy.dtype.
    Add 'ndim' and 'size' properties to TiffPage and TiffPageSeries.
    Allow imsave to write non-BigTIFF files up to ~4 GB.
    Only read one page for shaped series if possible.
    Add memmap function to create memory-mapped array stored in TIFF file.
    Add option to save empty arrays to TIFF files.
    Add option to save truncated TIFF files.
    Allow single tile images to be saved contiguously.
    Add optional movie mode for files with uniform pages.
    Lazy load pages.
    Use lightweight TiffFrame for IFDs sharing properties with key TiffPage.
    Move module constants to 'TIFF' namespace (speed up module import).
    Remove 'fastij' option from TiffFile.
    Remove 'pages' parameter from TiffFile.
    Remove TIFFfile alias.
    Deprecate Python 2.
    Require enum34 and futures packages on Python 2.7.
    Remove Record class and return all metadata as dict instead.
    Add functions to parse STK, MetaSeries, ScanImage, SVS, Pilatus metadata.
    Read tags from EXIF and GPS IFDs.
    Use pformat for tag and metadata values.
    Fix reading some UIC tags.
    Do not modify input array in imshow (fix).
    Fix Python implementation of unpack_ints.
2017.5.23
    Write correct number of SampleFormat values (fix).
    Use Adobe deflate code to write ZIP compressed files.
    Add option to pass tag values as packed binary data for writing.
    Defer tag validation to attribute access.
    Use property instead of lazyattr decorator for simple expressions.
2017.3.17
    Write IFDs and tag values on word boundaries.
    Read ScanImage metadata.
    Remove is_rgb and is_indexed attributes from TiffFile.
    Create files used by doctests.
2017.1.12 (included with scikit-image 0.14.x)
    Read Zeiss SEM metadata.
    Read OME-TIFF with invalid references to external files.
    Rewrite C LZW decoder (5x faster).
    Read corrupted LSM files missing EOI code in LZW stream.
2017.1.1
    ...

Refer to the CHANGES file for older revisions.

Notes
-----
The API is not stable yet and might change between revisions.

Tested on little-endian platforms only.

Python 2.7 and 32-bit versions are deprecated.

Tifffile relies on the `imagecodecs <https://pypi.org/project/imagecodecs/>`_
package for encoding and decoding LZW, JPEG, and other compressed images.
The `imagecodecs-lite <https://pypi.org/project/imagecodecs-lite/>`_ package,
which is easier to build, can be used for decoding LZW compressed images
instead.

Several TIFF-like formats do not strictly adhere to the TIFF6 specification,
some of which allow file or data sizes to exceed the 4 GB limit:

* *BigTIFF* is identified by version number 43 and uses different file
  header, IFD, and tag structures with 64-bit offsets. It adds more data types.
  Tifffile can read and write BigTIFF files.
* *ImageJ* hyperstacks store all image data, which may exceed 4 GB,
  contiguously after the first IFD. Files > 4 GB contain one IFD only.
  The size (shape and dtype) of the up to 6-dimensional image data can be
  determined from the ImageDescription tag of the first IFD, which is Latin-1
  encoded. Tifffile can read and write ImageJ hyperstacks.
* *OME-TIFF* stores up to 8-dimensional data in one or multiple TIFF of BigTIFF
  files. The 8-bit UTF-8 encoded OME-XML metadata found in the ImageDescription
  tag of the first IFD defines the position of TIFF IFDs in the high
  dimensional data. Tifffile can read OME-TIFF files, except when the OME-XML
  metadata is stored in a separate file.
* *LSM* stores all IFDs below 4 GB but wraps around 32-bit StripOffsets.
  The StripOffsets of each series and position require separate unwrapping.
  The StripByteCounts tag contains the number of bytes for the uncompressed
  data. Tifffile can read large LSM files.
* *NDPI* uses some 64-bit offsets in the file header, IFD, and tag structures
  and might require correcting 32-bit offsets found in tags.
  JPEG compressed tiles with dimensions > 65536 are not readable with libjpeg.
  Tifffile can read NDPI files < 4 GB and decompress large JPEG tiles using
  the imagecodecs library on Windows.
* *ScanImage* optionally allows corrupt non-BigTIFF files > 2 GB. The values
  of StripOffsets and StripByteCounts can be recovered using the constant
  differences of the offsets of IFD and tag values throughout the file.
  Tifffile can read such files on Python 3 if the image data is stored
  contiguously in each page.
* *GeoTIFF* sparse files allow strip or tile offsets and byte counts to be 0.
  Such segments are implicitly set to 0 or the NODATA value on reading.
  Tifffile can read GeoTIFF sparse files.

Other libraries for reading scientific TIFF files from Python:

* `Python-bioformats <https://github.com/CellProfiler/python-bioformats>`_
* `Imread <https://github.com/luispedro/imread>`_
* `GDAL <https://github.com/OSGeo/gdal/tree/master/gdal/swig/python>`_
* `OpenSlide-python <https://github.com/openslide/openslide-python>`_
* `PyLibTiff <https://github.com/pearu/pylibtiff>`_
* `SimpleITK <https://github.com/SimpleITK/SimpleITK>`_
* `PyLSM <https://launchpad.net/pylsm>`_
* `PyMca.TiffIO.py <https://github.com/vasole/pymca>`_ (same as fabio.TiffIO)
* `BioImageXD.Readers <http://www.bioimagexd.net/>`_
* `CellCognition <https://cellcognition-project.org/>`_
* `pymimage <https://github.com/ardoi/pymimage>`_
* `pytiff <https://github.com/FZJ-INM1-BDA/pytiff>`_
* `ScanImageTiffReaderPython
  <https://gitlab.com/vidriotech/scanimagetiffreader-python>`_
* `bigtiff <https://pypi.org/project/bigtiff>`_

Some libraries are using tifffile to write OME-TIFF files:

* `Zeiss Apeer OME-TIFF library
  <https://github.com/apeer-micro/apeer-ometiff-library>`_
* `Allen Institute for Cell Science imageio
  <https://pypi.org/project/aicsimageio>`_

References
----------
1)  TIFF 6.0 Specification and Supplements. Adobe Systems Incorporated.
    https://www.adobe.io/open/standards/TIFF.html
2)  TIFF File Format FAQ. https://www.awaresystems.be/imaging/tiff/faq.html
3)  MetaMorph Stack (STK) Image File Format.
    http://mdc.custhelp.com/app/answers/detail/a_id/18862
4)  Image File Format Description LSM 5/7 Release 6.0 (ZEN 2010).
    Carl Zeiss MicroImaging GmbH. BioSciences. May 10, 2011
5)  The OME-TIFF format.
    https://docs.openmicroscopy.org/ome-model/5.6.4/ome-tiff/
6)  UltraQuant(r) Version 6.0 for Windows Start-Up Guide.
    http://www.ultralum.com/images%20ultralum/pdf/UQStart%20Up%20Guide.pdf
7)  Micro-Manager File Formats.
    https://micro-manager.org/wiki/Micro-Manager_File_Formats
8)  Tags for TIFF and Related Specifications. Digital Preservation.
    https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml
9)  ScanImage BigTiff Specification - ScanImage 2016.
    http://scanimage.vidriotechnologies.com/display/SI2016/
    ScanImage+BigTiff+Specification
10) CIPA DC-008-2016: Exchangeable image file format for digital still cameras:
    Exif Version 2.31.
    http://www.cipa.jp/std/documents/e/DC-008-Translation-2016-E.pdf
11) ZIF, the Zoomable Image File format. http://zif.photo/
12) GeoTIFF File Format https://www.gdal.org/frmt_gtiff.html

Examples
--------
Save a 3D numpy array to a multi-page, 16-bit grayscale TIFF file:

>>> data = numpy.random.randint(0, 2**12, (4, 301, 219), 'uint16')
>>> imwrite('temp.tif', data, photometric='minisblack')

Read the whole image stack from the TIFF file as numpy array:

>>> image_stack = imread('temp.tif')
>>> image_stack.shape
(4, 301, 219)
>>> image_stack.dtype
dtype('uint16')

Read the image from first page in the TIFF file as numpy array:

>>> image = imread('temp.tif', key=0)
>>> image.shape
(301, 219)

Read images from a sequence of TIFF files as numpy array:

>>> image_sequence = imread(['temp.tif', 'temp.tif'])
>>> image_sequence.shape
(2, 4, 301, 219)

Save a numpy array to a single-page RGB TIFF file:

>>> data = numpy.random.randint(0, 255, (256, 256, 3), 'uint8')
>>> imwrite('temp.tif', data, photometric='rgb')

Save a floating-point array and metadata, using zlib compression:

>>> data = numpy.random.rand(2, 5, 3, 301, 219).astype('float32')
>>> imwrite('temp.tif', data, compress=6, metadata={'axes': 'TZCYX'})

Save a volume with xyz voxel size 2.6755x2.6755x3.9474 Âµm^3 to an ImageJ file:

>>> volume = numpy.random.randn(57*256*256).astype('float32')
>>> volume.shape = 1, 57, 1, 256, 256, 1  # dimensions in TZCYXS order
>>> imwrite('temp.tif', volume, imagej=True, resolution=(1./2.6755, 1./2.6755),
...         metadata={'spacing': 3.947368, 'unit': 'um'})

Get the shape and dtype of the images stored in the TIFF file:

>>> tif = TiffFile('temp.tif')
>>> len(tif.pages)  # number of pages in the file
57
>>> page = tif.pages[0]  # get shape and dtype of the image in the first page
>>> page.shape
(256, 256)
>>> page.dtype
dtype('float32')
>>> page.axes
'YX'
>>> series = tif.series[0]  # get shape and dtype of the first image series
>>> series.shape
(57, 256, 256)
>>> series.dtype
dtype('float32')
>>> series.axes
'ZYX'
>>> tif.close()

Read hyperstack and metadata from the ImageJ file:

>>> with TiffFile('temp.tif') as tif:
...     imagej_hyperstack = tif.asarray()
...     imagej_metadata = tif.imagej_metadata
>>> imagej_hyperstack.shape
(57, 256, 256)
>>> imagej_metadata['slices']
57

Read the "XResolution" tag from the first page in the TIFF file:

>>> with TiffFile('temp.tif') as tif:
...     tag = tif.pages[0].tags['XResolution']
>>> tag.value
(2000, 5351)
>>> tag.name
'XResolution'
>>> tag.code
282
>>> tag.count
1
>>> tag.dtype
'2I'
>>> tag.valueoffset
360

Read images from a selected range of pages:

>>> image = imread('temp.tif', key=range(4, 40, 2))
>>> image.shape
(18, 256, 256)

Create an empty TIFF file and write to the memory-mapped numpy array:

>>> memmap_image = memmap('temp.tif', shape=(256, 256), dtype='float32')
>>> memmap_image[255, 255] = 1.0
>>> memmap_image.flush()
>>> memmap_image.shape, memmap_image.dtype
((256, 256), dtype('float32'))
>>> del memmap_image

Memory-map image data of the first page in the TIFF file:

>>> memmap_image = memmap('temp.tif', page=0)
>>> memmap_image[255, 255]
1.0
>>> del memmap_image

Successively append images to a BigTIFF file, which can exceed 4 GB:

>>> data = numpy.random.randint(0, 255, (5, 2, 3, 301, 219), 'uint8')
>>> with TiffWriter('temp.tif', bigtiff=True) as tif:
...     for i in range(data.shape[0]):
...         tif.save(data[i], compress=6, photometric='minisblack')

Iterate over pages and tags in the TIFF file and successively read images:

>>> with TiffFile('temp.tif') as tif:
...     image_stack = tif.asarray()
...     for page in tif.pages:
...         for tag in page.tags.values():
...             tag_name, tag_value = tag.name, tag.value
...         image = page.asarray()

Save two image series to a TIFF file:

>>> data0 = numpy.random.randint(0, 255, (301, 219, 3), 'uint8')
>>> data1 = numpy.random.randint(0, 255, (5, 301, 219), 'uint16')
>>> with TiffWriter('temp.tif') as tif:
...     tif.save(data0, compress=6, photometric='rgb')
...     tif.save(data1, compress=6, photometric='minisblack', contiguous=False)

Read the second image series from the TIFF file:

>>> series1 = imread('temp.tif', series=1)
>>> series1.shape
(5, 301, 219)

Read an image stack from a series of TIFF files with a file name pattern:

>>> imwrite('temp_C001T001.tif', numpy.random.rand(64, 64))
>>> imwrite('temp_C001T002.tif', numpy.random.rand(64, 64))
>>> image_sequence = TiffSequence('temp_C001*.tif', pattern='axes')
>>> image_sequence.shape
(1, 2)
>>> image_sequence.axes
'CT'
>>> data = image_sequence.asarray()
>>> data.shape
(1, 2, 64, 64)



  * [tinytimer-0.0.0](https://github.com/iskandr/microbench) Tiny Timer
==========

A tiny benchmarking library for Python
  * [tokenize-rt-3.2.0](https://github.com/asottile/tokenize-rt) [![Build Status](https://dev.azure.com/asottile/asottile/_apis/build/status/asottile.tokenize-rt?branchName=master)](https://dev.azure.com/asottile/asottile/_build/latest?definitionId=25&branchName=master)
[![Azure DevOps coverage](https://img.shields.io/azure-devops/coverage/asottile/asottile/25/master.svg)](https://dev.azure.com/asottile/asottile/_build/latest?definitionId=25&branchName=master)

tokenize-rt
===========

The stdlib `tokenize` module does not properly roundtrip.  This wrapper
around the stdlib provides two additional tokens `ESCAPED_NL` and
`UNIMPORTANT_WS`, and a `Token` data type.  Use `src_to_tokens` and
`tokens_to_src` to roundtrip.

This library is useful if you're writing a refactoring tool based on the
python tokenization.

## Installation

`pip install tokenize-rt`

## Usage

### datastructures

#### `tokenize_rt.Offset(line=None, utf8_byte_offset=None)`

A token offset, useful as a key when cross referencing the `ast` and the
tokenized source.

#### `tokenize_rt.Token(name, src, line=None, utf8_byte_offset=None)`

Construct a token

- `name`: one of the token names listed in `token.tok_name` or
  `ESCAPED_NL` or `UNIMPORTANT_WS`
- `src`: token's source as text
- `line`: the line number that this token appears on.  This will be `None` for
   `ESCAPED_NL` and `UNIMPORTANT_WS` tokens.
- `utf8_byte_offset`: the utf8 byte offset that this token appears on in the
  line.  This will be `None` for `ESCAPED_NL` and `UNIMPORTANT_WS` tokens.

#### `tokenize_rt.Token.offset`

Retrieves an `Offset` for this token.

### converting to and from `Token` representations

#### `tokenize_rt.src_to_tokens(text: str) -> List[Token]`

#### `tokenize_rt.tokens_to_src(Iterable[Token]) -> str`

### additional tokens added by `tokenize-rt`

#### `tokenize_rt.ESCAPED_NL`

#### `tokenize_rt.UNIMPORTANT_WS`

### helpers

#### `tokenize_rt.NON_CODING_TOKENS`

A `frozenset` containing tokens which may appear between others while not
affecting control flow or code:
- `COMMENT`
- `ESCAPED_NL`
- `NL`
- `UNIMPORTANT_WS`

#### `tokenize_rt.parse_string_literal(text: str) -> Tuple[str, str]`

parse a string literal into its prefix and string content

```pycon
>>> parse_string_literal('f"foo"')
('f', '"foo"')
```

#### `tokenize_rt.reversed_enumerate(Sequence[Token]) -> Iterator[Tuple[int, Token]]`

yields `(index, token)` pairs.  Useful for rewriting source.

#### `tokenize_rt.rfind_string_parts(Sequence[Token], i) -> Tuple[int, ...]`

find the indices of the string parts of a (joined) string literal

- `i` should start at the end of the string literal
- returns `()` (an empty tuple) for things which are not string literals

```pycon
>>> tokens = src_to_tokens('"foo" "bar".capitalize()')
>>> rfind_string_parts(tokens, 2)
(0, 2)
>>> tokens = src_to_tokens('("foo" "bar").capitalize()')
>>> rfind_string_parts(tokens, 4)
(1, 3)
```

## Differences from `tokenize`

- `tokenize-rt` adds `ESCAPED_NL` for a backslash-escaped newline "token"
- `tokenize-rt` adds `UNIMPORTANT_WS` for whitespace (discarded in `tokenize`)
- `tokenize-rt` normalizes string prefixes, even if they are not parsed -- for
  instance, this means you'll see `Token('STRING', "f'foo'", ...)` even in
  python 2.
- `tokenize-rt` normalizes python 2 long literals (`4l` / `4L`) and octal
  literals (`0755`) in python 3 (for easier rewriting of python 2 code while
  running python 3).

## Sample usage

- https://github.com/asottile/add-trailing-comma
- https://github.com/asottile/future-fstrings
- https://github.com/asottile/pyupgrade
- https://github.com/asottile/yesqa



  * [toml-0.10.0](https://github.com/uiri/toml) ****
TOML
****

.. image:: https://badge.fury.io/py/toml.svg
    :target: https://badge.fury.io/py/toml

.. image:: https://travis-ci.org/uiri/toml.svg?branch=master
    :target: https://travis-ci.org/uiri/toml

.. image:: https://img.shields.io/pypi/pyversions/toml.svg
    :target: https://pypi.org/project/toml/


A Python library for parsing and creating `TOML <https://en.wikipedia.org/wiki/TOML>`_.

The module passes `the TOML test suite <https://github.com/BurntSushi/toml-test>`_.

See also:

* `The TOML Standard <https://github.com/toml-lang/toml>`_
* `The currently supported TOML specification <https://github.com/toml-lang/toml/blob/v0.5.0/README.md>`_

Installation
============

To install the latest release on `PyPI <https://pypi.org/project/toml/>`_,
simply run:

::

  pip install toml

Or to install the latest development version, run:

::

  git clone https://github.com/uiri/toml.git
  cd toml
  python setup.py install

Quick Tutorial
==============

*toml.loads* takes in a string containing standard TOML-formatted data and
returns a dictionary containing the parsed data.

.. code:: pycon

  >>> import toml
  >>> toml_string = """
  ... # This is a TOML document.
  ...
  ... title = "TOML Example"
  ...
  ... [owner]
  ... name = "Tom Preston-Werner"
  ... dob = 1979-05-27T07:32:00-08:00 # First class dates
  ...
  ... [database]
  ... server = "192.168.1.1"
  ... ports = [ 8001, 8001, 8002 ]
  ... connection_max = 5000
  ... enabled = true
  ...
  ... [servers]
  ...
  ...   # Indentation (tabs and/or spaces) is allowed but not required
  ...   [servers.alpha]
  ...   ip = "10.0.0.1"
  ...   dc = "eqdc10"
  ...
  ...   [servers.beta]
  ...   ip = "10.0.0.2"
  ...   dc = "eqdc10"
  ...
  ... [clients]
  ... data = [ ["gamma", "delta"], [1, 2] ]
  ...
  ... # Line breaks are OK when inside arrays
  ... hosts = [
  ...   "alpha",
  ...   "omega"
  ... ]
  ... """
  >>> parsed_toml = toml.loads(toml_string)


*toml.dumps* takes a dictionary and returns a string containing the
corresponding TOML-formatted data.

.. code:: pycon

  >>> new_toml_string = toml.dumps(parsed_toml)
  >>> print(new_toml_string)
  title = "TOML Example"
  [owner]
  name = "Tom Preston-Werner"
  dob = 1979-05-27T07:32:00Z
  [database]
  server = "192.168.1.1"
  ports = [ 8001, 8001, 8002,]
  connection_max = 5000
  enabled = true
  [clients]
  data = [ [ "gamma", "delta",], [ 1, 2,],]
  hosts = [ "alpha", "omega",]
  [servers.alpha]
  ip = "10.0.0.1"
  dc = "eqdc10"
  [servers.beta]
  ip = "10.0.0.2"
  dc = "eqdc10"

For more functions, view the API Reference below.

API Reference
=============

``toml.load(f, _dict=dict)``
  Parse a file or a list of files as TOML and return a dictionary.

  :Args:
    * ``f``: A path to a file, list of filepaths (to be read into single
      object) or a file descriptor
    * ``_dict``: The class of the dictionary object to be returned

  :Returns:
    A dictionary (or object ``_dict``) containing parsed TOML data

  :Raises:
    * ``TypeError``: When ``f`` is an invalid type or is a list containing
      invalid types
    * ``TomlDecodeError``: When an error occurs while decoding the file(s)

``toml.loads(s, _dict=dict)``
  Parse a TOML-formatted string to a dictionary.

  :Args:
    * ``s``: The TOML-formatted string to be parsed
    * ``_dict``: Specifies the class of the returned toml dictionary

  :Returns:
    A dictionary (or object ``_dict``) containing parsed TOML data

  :Raises:
    * ``TypeError``: When a non-string object is passed
    * ``TomlDecodeError``: When an error occurs while decoding the
      TOML-formatted string

``toml.dump(o, f)``
  Write a dictionary to a file containing TOML-formatted data

  :Args:
    * ``o``: An object to be converted into TOML
    * ``f``: A File descriptor where the TOML-formatted output should be stored

  :Returns:
    A string containing the TOML-formatted data corresponding to object ``o``

  :Raises:
    * ``TypeError``: When anything other than file descriptor is passed

``toml.dumps(o)``
  Create a TOML-formatted string from an input object

  :Args:
    * ``o``: An object to be converted into TOML

  :Returns:
    A string containing the TOML-formatted data corresponding to object ``o``

Licensing
=========

This project is released under the terms of the MIT Open Source License. View
*LICENSE.txt* for more information.



  * [tomlkit-0.5.5](https://github.com/sdispater/tomlkit) [github_release]: https://img.shields.io/github/release/sdispater/tomlkit.svg?logo=github&logoColor=white
[pypi_version]: https://img.shields.io/pypi/v/tomlkit.svg?logo=python&logoColor=white
[python_versions]: https://img.shields.io/pypi/pyversions/tomlkit.svg?logo=python&logoColor=white
[github_license]: https://img.shields.io/github/license/sdispater/tomlkit.svg?logo=github&logoColor=white
[travisci]: https://img.shields.io/travis/com/sdispater/tomlkit/master.svg?logo=travis&logoColor=white&label=Travis%20CI
[appveyor]: https://img.shields.io/appveyor/ci/sdispater/tomlkit/master.svg?logo=appveyor&logoColor=white&label=AppVeyor
<!--Codecov logo not offered by shields.io or simpleicons.org, this is Codecov's SVG image modified to be white-->
[codecov]: https://img.shields.io/codecov/c/github/sdispater/tomlkit/master.svg?logo=data:image/svg+xml;base64,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&label=Codecov

[![GitHub Release][github_release]](https://github.com/sdispater/tomlkit/releases/)
[![PyPI Version][pypi_version]](https://pypi.python.org/pypi/tomlkit/)
[![Python Versions][python_versions]](https://pypi.python.org/pypi/tomlkit/)
[![License][github_license]](https://github.com/sdispater/tomlkit/blob/master/LICENSE)
<br>
[![Travis CI][travisci]](https://travis-ci.com/sdispater/tomlkit)
[![AppVeyor][appveyor]](https://ci.appveyor.com/project/sdispater/tomlkit)
[![Codecov][codecov]](https://codecov.io/gh/sdispater/tomlkit)

# TOML Kit - Style-preserving TOML library for Python

TOML Kit is a **0.5.0-compliant** [TOML](https://github.com/toml-lang/toml) library.

It includes a parser that preserves all comments, indentations, whitespace and internal element ordering,
and makes them accessible and editable via an intuitive API.

You can also create new TOML documents from scratch using the provided helpers.

Part of the implementation as been adapted, improved and fixed from [Molten](https://github.com/LeopoldArkham/Molten).

## Usage

### Parsing

TOML Kit comes with a fast and style-preserving parser to help you access
the content of TOML files and strings.

```python
>>> from tomlkit import dumps
>>> from tomlkit import parse  # you can also use loads

>>> content = """[table]
... foo = "bar"  # String
... """
>>> doc = parse(content)

# doc is a TOMLDocument instance that holds all the information
# about the TOML string.
# It behaves like a standard dictionary.

>>> assert doc["table"]["foo"] == "bar"

# The string generated from the document is exactly the same
# as the original string
>>> assert dumps(doc) == content
```

### Modifying

TOML Kit provides an intuitive API to modify TOML documents.

```python
>>> from tomlkit import dumps
>>> from tomlkit import parse
>>> from tomlkit import table

>>> doc = parse("""[table]
... foo = "bar"  # String
... """)

>>> doc["table"]["baz"] = 13

>>> dumps(doc)
"""[table]
foo = "bar"  # String
baz = 13
"""

# Add a new table
>>> tab = table()
>>> tab.add("array", [1, 2, 3])

>>> doc["table2"] = tab

>>> dumps(doc)
"""[table]
foo = "bar"  # String
baz = 13

[table2]
array = [1, 2, 3]
"""

# Remove the newly added table
>>> doc.remove("table2")
# del doc["table2] is also possible
```

### Writing

You can also write a new TOML document from scratch.

Let's say we want to create this following document:

```toml
# This is a TOML document.

title = "TOML Example"

[owner]
name = "Tom Preston-Werner"
organization = "GitHub"
bio = "GitHub Cofounder & CEO\nLikes tater tots and beer."
dob = 1979-05-27T07:32:00Z # First class dates? Why not?

[database]
server = "192.168.1.1"
ports = [ 8001, 8001, 8002 ]
connection_max = 5000
enabled = true
```

It can be created with the following code:

```python
>>> from tomlkit import comment
>>> from tomlkit import document
>>> from tomlkit import nl
>>> from tomlkit import table

>>> doc = document()
>>> doc.add(comment("This is a TOML document."))
>>> doc.add(nl())
>>> doc.add("title", "TOML Example")
# Using doc["title"] = "TOML Example" is also possible

>>> owner = table()
>>> owner.add("name", "Tom Preston-Werner")
>>> owner.add("organization", "GitHub")
>>> owner.add("bio", "GitHub Cofounder & CEO\nLikes tater tots and beer.")
>>> owner.add("dob", datetime(1979, 5, 27, 7, 32, tzinfo=utc))
>>> owner["dob"].comment("First class dates? Why not?")

# Adding the table to the document
>>> doc.add("owner", owner)

>>> database = table()
>>> database["server"] = "192.168.1.1"
>>> database["ports"] = [8001, 8001, 8002]
>>> database["connection_max"] = 5000
>>> database["enabled"] = True

>>> doc["database"] = database
```


## Installation

If you are using [Poetry](https://poetry.eustace.io),
add `tomlkit` to your `pyproject.toml` file by using:

```bash
poetry add tomlkit
```

If not, you can use `pip`:

```bash
pip install tomlkit
```

  * [toolz-0.10.0](https://github.com/pytoolz/toolz/) Toolz
=====

|Build Status| |Coverage Status| |Version Status|

A set of utility functions for iterators, functions, and dictionaries.

See the PyToolz documentation at https://toolz.readthedocs.io

LICENSE
-------

New BSD. See `License File <https://github.com/pytoolz/toolz/blob/master/LICENSE.txt>`__.

Install
-------

``toolz`` is on the Python Package Index (PyPI):

::

    pip install toolz

Structure and Heritage
----------------------

``toolz`` is implemented in three parts:

|literal itertoolz|_, for operations on iterables. Examples: ``groupby``,
``unique``, ``interpose``,

|literal functoolz|_, for higher-order functions. Examples: ``memoize``,
``curry``, ``compose``,

|literal dicttoolz|_, for operations on dictionaries. Examples: ``assoc``,
``update-in``, ``merge``.

.. |literal itertoolz| replace:: ``itertoolz``
.. _literal itertoolz: https://github.com/pytoolz/toolz/blob/master/toolz/itertoolz.py

.. |literal functoolz| replace:: ``functoolz``
.. _literal functoolz: https://github.com/pytoolz/toolz/blob/master/toolz/functoolz.py

.. |literal dicttoolz| replace:: ``dicttoolz``
.. _literal dicttoolz: https://github.com/pytoolz/toolz/blob/master/toolz/dicttoolz.py

These functions come from the legacy of functional languages for list
processing. They interoperate well to accomplish common complex tasks.

Read our `API
Documentation <https://toolz.readthedocs.io/en/latest/api.html>`__ for
more details.

Example
-------

This builds a standard wordcount function from pieces within ``toolz``:

.. code:: python

    >>> def stem(word):
    ...     """ Stem word to primitive form """
    ...     return word.lower().rstrip(",.!:;'-\"").lstrip("'\"")

    >>> from toolz import compose, frequencies, partial
    >>> from toolz.curried import map
    >>> wordcount = compose(frequencies, map(stem), str.split)

    >>> sentence = "This cat jumped over this other cat!"
    >>> wordcount(sentence)
    {'this': 2, 'cat': 2, 'jumped': 1, 'over': 1, 'other': 1}

Dependencies
------------

``toolz`` supports Python 2.7 and Python 3.4+ with a common codebase.
It is pure Python and requires no dependencies beyond the standard
library.

It is, in short, a lightweight dependency.


CyToolz
-------

The ``toolz`` project has been reimplemented in `Cython <http://cython.org>`__.
The ``cytoolz`` project is a drop-in replacement for the Pure Python
implementation.
See `CyToolz GitHub Page <https://github.com/pytoolz/cytoolz/>`__ for more
details.

See Also
--------

-  `Underscore.js <https://underscorejs.org/>`__: A similar library for
   JavaScript
-  `Enumerable <https://ruby-doc.org/core-2.0.0/Enumerable.html>`__: A
   similar library for Ruby
-  `Clojure <https://clojure.org/>`__: A functional language whose
   standard library has several counterparts in ``toolz``
-  `itertools <https://docs.python.org/2/library/itertools.html>`__: The
   Python standard library for iterator tools
-  `functools <https://docs.python.org/2/library/functools.html>`__: The
   Python standard library for function tools

Contributions Welcome
---------------------

``toolz`` aims to be a repository for utility functions, particularly
those that come from the functional programming and list processing
traditions. We welcome contributions that fall within this scope.

We also try to keep the API small to keep ``toolz`` manageable.  The ideal
contribution is significantly different from existing functions and has
precedent in a few other functional systems.

Please take a look at our
`issue page <https://github.com/pytoolz/toolz/issues>`__
for contribution ideas.

Community
---------

See our `mailing list <https://groups.google.com/forum/#!forum/pytoolz>`__.
We're friendly.

.. |Build Status| image:: https://travis-ci.org/pytoolz/toolz.svg?branch=master
   :target: https://travis-ci.org/pytoolz/toolz
.. |Coverage Status| image:: https://coveralls.io/repos/pytoolz/toolz/badge.svg?branch=master
   :target: https://coveralls.io/r/pytoolz/toolz
.. |Version Status| image:: https://badge.fury.io/py/toolz.svg
   :target: https://badge.fury.io/py/toolz
  * [torch-1.2.0](https://pytorch.org/) 
  * [torchfile-0.1.0](https://github.com/bshillingford/python-torchfile) UNKNOWN
  * [tornado-6.0.3](http://www.tornadoweb.org/) Tornado Web Server
==================

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/tornadoweb/tornado
   :target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

`Tornado <http://www.tornadoweb.org>`_ is a Python web framework and
asynchronous networking library, originally developed at `FriendFeed
<http://friendfeed.com>`_.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
`long polling <http://en.wikipedia.org/wiki/Push_technology#Long_Polling>`_,
`WebSockets <http://en.wikipedia.org/wiki/WebSocket>`_, and other
applications that require a long-lived connection to each user.

Hello, world
------------

Here is a simple "Hello, world" example web app for Tornado:

.. code-block:: python

    import tornado.ioloop
    import tornado.web

    class MainHandler(tornado.web.RequestHandler):
        def get(self):
            self.write("Hello, world")

    def make_app():
        return tornado.web.Application([
            (r"/", MainHandler),
        ])

    if __name__ == "__main__":
        app = make_app()
        app.listen(8888)
        tornado.ioloop.IOLoop.current().start()

This example does not use any of Tornado's asynchronous features; for
that see this `simple chat room
<https://github.com/tornadoweb/tornado/tree/stable/demos/chat>`_.

Documentation
-------------

Documentation and links to additional resources are available at
http://www.tornadoweb.org



  * [tqdm-4.32.2](https://github.com/tqdm/tqdm) |Logo|

tqdm
====

|PyPI-Versions| |PyPI-Status| |Conda-Forge-Status| |Docker| |Snapcraft|

|Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade| |Libraries-Rank| |PyPI-Downloads|

|DOI| |LICENCE| |OpenHub-Status| |binder-demo| |notebook-demo| |awesome-python|

``tqdm`` means "progress" in Arabic (*taqadum*, تقدّم)
and is an abbreviation for "I love you so much" in Spanish (*te quiero demasiado*).

Instantly make your loops show a smart progress meter - just wrap any
iterable with ``tqdm(iterable)``, and you're done!

.. code:: python

    from tqdm import tqdm
    for i in tqdm(range(10000)):
        ...

``76%|████████████████████████████         | 7568/10000 [00:33<00:10, 229.00it/s]``

``trange(N)`` can be also used as a convenient shortcut for
``tqdm(xrange(N))``.

|Screenshot|
    REPL: `ptpython <https://github.com/jonathanslenders/ptpython>`__ |
    PyData London: `video <https://tqdm.github.io/video>`__
    / `slides <https://tqdm.github.io/PyData2019/slides.html>`__

It can also be executed as a module with pipes:

.. code:: sh

    $ seq 9999999 | tqdm --bytes | wc -l
    75.2MB [00:00, 217MB/s]
    9999999
    $ 7z a -bd -r backup.7z docs/ | grep Compressing | \
        tqdm --total $(find docs/ -type f | wc -l) --unit files >> backup.log
    100%|███████████████████████████████▉| 8014/8014 [01:37<00:00, 82.29files/s]

Overhead is low -- about 60ns per iteration (80ns with ``tqdm.gui``), and is
unit tested against performance regression.
By comparison, the well-established
`ProgressBar <https://github.com/niltonvolpato/python-progressbar>`__ has
an 800ns/iter overhead.

In addition to its low overhead, ``tqdm`` uses smart algorithms to predict
the remaining time and to skip unnecessary iteration displays, which allows
for a negligible overhead in most cases.

``tqdm`` works on any platform
(Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),
in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.

``tqdm`` does not require any dependencies (not even ``curses``!), just
Python and an environment supporting ``carriage return \r`` and
``line feed \n`` control characters.

------------------------------------------

.. contents:: Table of contents
   :backlinks: top
   :local:


Installation
------------

Latest PyPI stable release
~~~~~~~~~~~~~~~~~~~~~~~~~~

|PyPI-Status| |PyPI-Downloads| |Libraries-Dependents|

.. code:: sh

    pip install tqdm

Latest development release on GitHub
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

|GitHub-Status| |GitHub-Stars| |GitHub-Commits| |GitHub-Forks| |GitHub-Updated|

Pull and install in the current directory:

.. code:: sh

    pip install -e git+https://github.com/tqdm/tqdm.git@master#egg=tqdm

Latest Conda release
~~~~~~~~~~~~~~~~~~~~

|Conda-Forge-Status|

.. code:: sh

    conda install -c conda-forge tqdm

Latest Snapcraft release
~~~~~~~~~~~~~~~~~~~~~~~~

|Snapcraft|

.. code:: sh

    snap install tqdm

Latest Docker release
~~~~~~~~~~~~~~~~~~~~~

|Docker|

.. code:: sh

    docker pull tqdm/tqdm
    docker run -i --rm tqdm/tqdm --help

Changelog
---------

The list of all changes is available either on GitHub's Releases:
|GitHub-Status|, on the
`wiki <https://github.com/tqdm/tqdm/wiki/Releases>`__, on the
`website <https://tqdm.github.io/releases/>`__, or on crawlers such as
`allmychanges.com <https://allmychanges.com/p/python/tqdm/>`_.


Usage
-----

``tqdm`` is very versatile and can be used in a number of ways.
The three main ones are given below.

Iterable-based
~~~~~~~~~~~~~~

Wrap ``tqdm()`` around any iterable:

.. code:: python

    from tqdm import tqdm
    import time

    text = ""
    for char in tqdm(["a", "b", "c", "d"]):
        time.sleep(0.25)
        text = text + char

``trange(i)`` is a special optimised instance of ``tqdm(range(i))``:

.. code:: python

    for i in trange(100):
        time.sleep(0.01)

Instantiation outside of the loop allows for manual control over ``tqdm()``:

.. code:: python

    pbar = tqdm(["a", "b", "c", "d"])
    for char in pbar:
        time.sleep(0.25)
        pbar.set_description("Processing %s" % char)

Manual
~~~~~~

Manual control on ``tqdm()`` updates by using a ``with`` statement:

.. code:: python

    with tqdm(total=100) as pbar:
        for i in range(10):
            time.sleep(0.1)
            pbar.update(10)

If the optional variable ``total`` (or an iterable with ``len()``) is
provided, predictive stats are displayed.

``with`` is also optional (you can just assign ``tqdm()`` to a variable,
but in this case don't forget to ``del`` or ``close()`` at the end:

.. code:: python

    pbar = tqdm(total=100)
    for i in range(10):
        time.sleep(0.1)
        pbar.update(10)
    pbar.close()

Module
~~~~~~

Perhaps the most wonderful use of ``tqdm`` is in a script or on the command
line. Simply inserting ``tqdm`` (or ``python -m tqdm``) between pipes will pass
through all ``stdin`` to ``stdout`` while printing progress to ``stderr``.

The example below demonstrated counting the number of lines in all Python files
in the current directory, with timing information included.

.. code:: sh

    $ time find . -name '*.py' -type f -exec cat \{} \; | wc -l
    857365

    real    0m3.458s
    user    0m0.274s
    sys     0m3.325s

    $ time find . -name '*.py' -type f -exec cat \{} \; | tqdm | wc -l
    857366it [00:03, 246471.31it/s]
    857365

    real    0m3.585s
    user    0m0.862s
    sys     0m3.358s

Note that the usual arguments for ``tqdm`` can also be specified.

.. code:: sh

    $ find . -name '*.py' -type f -exec cat \{} \; |
        tqdm --unit loc --unit_scale --total 857366 >> /dev/null
    100%|███████████████████████████████████| 857K/857K [00:04<00:00, 246Kloc/s]

Backing up a large directory?

.. code:: sh

    $ 7z a -bd -r backup.7z docs/ | grep Compressing |
        tqdm --total $(find docs/ -type f | wc -l) --unit files >> backup.log
    100%|███████████████████████████████▉| 8014/8014 [01:37<00:00, 82.29files/s]


FAQ and Known Issues
--------------------

|GitHub-Issues|

The most common issues relate to excessive output on multiple lines, instead
of a neat one-line progress bar.

- Consoles in general: require support for carriage return (``CR``, ``\r``).
- Nested progress bars:

  * Consoles in general: require support for moving cursors up to the
    previous line. For example,
    `IDLE <https://github.com/tqdm/tqdm/issues/191#issuecomment-230168030>`__,
    `ConEmu <https://github.com/tqdm/tqdm/issues/254>`__ and
    `PyCharm <https://github.com/tqdm/tqdm/issues/203>`__ (also
    `here <https://github.com/tqdm/tqdm/issues/208>`__,
    `here <https://github.com/tqdm/tqdm/issues/307>`__, and
    `here <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__)
    lack full support.
  * Windows: additionally may require the Python module ``colorama``
    to ensure nested bars stay within their respective lines.

- Unicode:

  * Environments which report that they support unicode will have solid smooth
    progressbars. The fallback is an ```ascii``-only bar.
  * Windows consoles often only partially support unicode and thus
    `often require explicit ascii=True <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__
    (also `here <https://github.com/tqdm/tqdm/issues/499>`__). This is due to
    either normal-width unicode characters being incorrectly displayed as
    "wide", or some unicode characters not rendering.

- Wrapping enumerated iterables: use ``enumerate(tqdm(...))`` instead of
  ``tqdm(enumerate(...))``. The same applies to ``numpy.ndenumerate``.
  This is because enumerate functions tend to hide the length of iterables.
  ``tqdm`` does not.
- Wrapping zipped iterables has similar issues due to internal optimisations.
  ``tqdm(zip(a, b))`` should be replaced with ``zip(tqdm(a), b)`` or even
  ``zip(tqdm(a), tqdm(b))``.
- `Hanging pipes in python2 <https://github.com/tqdm/tqdm/issues/359>`__:
  when using ``tqdm`` on the CLI, you may need to use python 3.5+ for correct
  buffering.

If you come across any other difficulties, browse and file |GitHub-Issues|.

Documentation
-------------

|PyPI-Versions| |README-Hits| (Since 19 May 2016)

.. code:: python

    class tqdm():
      """
      Decorate an iterable object, returning an iterator which acts exactly
      like the original iterable, but prints a dynamically updating
      progressbar every time a value is requested.
      """

      def __init__(self, iterable=None, desc=None, total=None, leave=True,
                   file=None, ncols=None, mininterval=0.1,
                   maxinterval=10.0, miniters=None, ascii=None, disable=False,
                   unit='it', unit_scale=False, dynamic_ncols=False,
                   smoothing=0.3, bar_format=None, initial=0, position=None,
                   postfix=None, unit_divisor=1000):

Parameters
~~~~~~~~~~

* iterable  : iterable, optional  
    Iterable to decorate with a progressbar.
    Leave blank to manually manage the updates.
* desc  : str, optional  
    Prefix for the progressbar.
* total  : int, optional  
    The number of expected iterations. If unspecified,
    len(iterable) is used if possible. If float("inf") or as a last
    resort, only basic progress statistics are displayed
    (no ETA, no progressbar).
    If ``gui`` is True and this parameter needs subsequent updating,
    specify an initial arbitrary large positive integer,
    e.g. int(9e9).
* leave  : bool, optional  
    If [default: True], keeps all traces of the progressbar
    upon termination of iteration.
    If ``None``, will leave only if ``position`` is ``0``.
* file  : ``io.TextIOWrapper`` or ``io.StringIO``, optional  
    Specifies where to output the progress messages
    (default: sys.stderr). Uses ``file.write(str)`` and ``file.flush()``
    methods.  For encoding, see ``write_bytes``.
* ncols  : int, optional  
    The width of the entire output message. If specified,
    dynamically resizes the progressbar to stay within this bound.
    If unspecified, attempts to use environment width. The
    fallback is a meter width of 10 and no limit for the counter and
    statistics. If 0, will not print any meter (only stats).
* mininterval  : float, optional  
    Minimum progress display update interval [default: 0.1] seconds.
* maxinterval  : float, optional  
    Maximum progress display update interval [default: 10] seconds.
    Automatically adjusts ``miniters`` to correspond to ``mininterval``
    after long display update lag. Only works if ``dynamic_miniters``
    or monitor thread is enabled.
* miniters  : int, optional  
    Minimum progress display update interval, in iterations.
    If 0 and ``dynamic_miniters``, will automatically adjust to equal
    ``mininterval`` (more CPU efficient, good for tight loops).
    If > 0, will skip display of specified number of iterations.
    Tweak this and ``mininterval`` to get very efficient loops.
    If your progress is erratic with both fast and slow iterations
    (network, skipping items, etc) you should set miniters=1.
* ascii  : bool or str, optional  
    If unspecified or False, use unicode (smooth blocks) to fill
    the meter. The fallback is to use ASCII characters " 123456789#".
* disable  : bool, optional  
    Whether to disable the entire progressbar wrapper
    [default: False]. If set to None, disable on non-TTY.
* unit  : str, optional  
    String that will be used to define the unit of each iteration
    [default: it].
* unit_scale  : bool or int or float, optional  
    If 1 or True, the number of iterations will be reduced/scaled
    automatically and a metric prefix following the
    International System of Units standard will be added
    (kilo, mega, etc.) [default: False]. If any other non-zero
    number, will scale ``total`` and ``n``.
* dynamic_ncols  : bool, optional  
    If set, constantly alters ``ncols`` to the environment (allowing
    for window resizes) [default: False].
* smoothing  : float, optional  
    Exponential moving average smoothing factor for speed estimates
    (ignored in GUI mode). Ranges from 0 (average speed) to 1
    (current/instantaneous speed) [default: 0.3].
* bar_format  : str, optional  
    Specify a custom bar string formatting. May impact performance.
    [default: '{l_bar}{bar}{r_bar}'], where
    l_bar='{desc}: {percentage:3.0f}%|' and
    r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, '
    '{rate_fmt}{postfix}]'
    Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,
    percentage, elapsed, elapsed_s, ncols, desc, unit,
    rate, rate_fmt, rate_noinv, rate_noinv_fmt,
    rate_inv, rate_inv_fmt, postfix, unit_divisor,
    remaining, remaining_s.
    Note that a trailing ": " is automatically removed after {desc}
    if the latter is empty.
* initial  : int, optional  
    The initial counter value. Useful when restarting a progress
    bar [default: 0].
* position  : int, optional  
    Specify the line offset to print this bar (starting from 0)
    Automatic if unspecified.
    Useful to manage multiple bars at once (eg, from threads).
* postfix  : dict or ``*``, optional  
    Specify additional stats to display at the end of the bar.
    Calls ``set_postfix(**postfix)`` if possible (dict).
* unit_divisor  : float, optional  
    [default: 1000], ignored unless ``unit_scale`` is True.
* write_bytes  : bool, optional  
    If (default: None) and ``file`` is unspecified,
    bytes will be written in Python 2. If ``True`` will also write
    bytes. In all other cases will default to unicode.

Extra CLI Options
~~~~~~~~~~~~~~~~~

* delim  : chr, optional  
    Delimiting character [default: '\n']. Use '\0' for null.
    N.B.: on Windows systems, Python converts '\n' to '\r\n'.
* buf_size  : int, optional  
    String buffer size in bytes [default: 256]
    used when ``delim`` is specified.
* bytes  : bool, optional  
    If true, will count bytes, ignore ``delim``, and default
    ``unit_scale`` to True, ``unit_divisor`` to 1024, and ``unit`` to 'B'.
* manpath  : str, optional  
    Directory in which to install tqdm man pages.
* log  : str, optional  
    CRITICAL|FATAL|ERROR|WARN(ING)|[default: 'INFO']|DEBUG|NOTSET.

Returns
~~~~~~~

* out  : decorated iterator.  

.. code:: python

    class tqdm():
      def update(self, n=1):
          """
          Manually update the progress bar, useful for streams
          such as reading files.
          E.g.:
          >>> t = tqdm(total=filesize) # Initialise
          >>> for current_buffer in stream:
          ...    ...
          ...    t.update(len(current_buffer))
          >>> t.close()
          The last line is highly recommended, but possibly not necessary if
          ``t.update()`` will be called in such a way that ``filesize`` will be
          exactly reached and printed.

          Parameters
          ----------
          n  : int, optional
              Increment to add to the internal counter of iterations
              [default: 1].
          """

      def close(self):
          """Cleanup and (if leave=False) close the progressbar."""

      def clear(self, nomove=False):
          """Clear current bar display."""

      def refresh(self):
          """Force refresh the display of this bar."""

      def unpause(self):
          """Restart tqdm timer from last print time."""

      def reset(self, total=None):
          """
          Resets to 0 iterations for repeated use.

          Consider combining with ``leave=True``.

          Parameters
          ----------
          total  : int, optional. Total to use for the new bar.
          """

      def set_description(self, desc=None, refresh=True):
          """
          Set/modify description of the progress bar.

          Parameters
          ----------
          desc  : str, optional
          refresh  : bool, optional
              Forces refresh [default: True].
          """

      def set_postfix(self, ordered_dict=None, refresh=True, **kwargs):
          """
          Set/modify postfix (additional stats)
          with automatic formatting based on datatype.

          Parameters
          ----------
          ordered_dict  : dict or OrderedDict, optional
          refresh  : bool, optional
              Forces refresh [default: True].
          kwargs  : dict, optional
          """

      @classmethod
      def write(cls, s, file=sys.stdout, end="\n"):
          """Print a message via tqdm (without overlap with bars)."""

      @property
      def format_dict(self):
          """Public API for read-only member access."""

      def display(self, msg=None, pos=None):
          """
          Use ``self.sp`` to display ``msg`` in the specified ``pos``.

          Consider overloading this function when inheriting to use e.g.:
          ``self.some_frontend(**self.format_dict)`` instead of ``self.sp``.

          Parameters
          ----------
          msg  : str, optional. What to display (default: ``repr(self)``).
          pos  : int, optional. Position to ``moveto``
            (default: ``abs(self.pos)``).
          """

    def trange(*args, **kwargs):
        """
        A shortcut for tqdm(xrange(*args), **kwargs).
        On Python3+ range is used instead of xrange.
        """

    class tqdm.gui.tqdm(tqdm.tqdm):
        """Experimental GUI version"""

    def tqdm.gui.trange(*args, **kwargs):
        """Experimental GUI version of trange"""

    class tqdm.notebook.tqdm(tqdm.tqdm):
        """Experimental IPython/Jupyter Notebook widget"""

    def tqdm.notebook.trange(*args, **kwargs):
        """Experimental IPython/Jupyter Notebook widget version of trange"""


Examples and Advanced Usage
---------------------------

- See the `examples <https://github.com/tqdm/tqdm/tree/master/examples>`__
  folder;
- import the module and run ``help()``;
- consult the `wiki <https://github.com/tqdm/tqdm/wiki>`__;

  * this has an
    `excellent article <https://github.com/tqdm/tqdm/wiki/How-to-make-a-great-Progress-Bar>`__
    on how to make a **great** progressbar;

- run the |notebook-demo| or |binder-demo|, or
- check out the `slides from PyData London <https://tqdm.github.io/PyData2019/slides.html>`__.

Description and additional stats
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Custom information can be displayed and updated dynamically on ``tqdm`` bars
with the ``desc`` and ``postfix`` arguments:

.. code:: python

    from tqdm import trange
    from random import random, randint
    from time import sleep

    with trange(10) as t:
        for i in t:
            # Description will be displayed on the left
            t.set_description('GEN %i' % i)
            # Postfix will be displayed on the right,
            # formatted automatically based on argument's datatype
            t.set_postfix(loss=random(), gen=randint(1,999), str='h',
                          lst=[1, 2])
            sleep(0.1)

    with tqdm(total=10, bar_format="{postfix[0]} {postfix[1][value]:>8.2g}",
              postfix=["Batch", dict(value=0)]) as t:
        for i in range(10):
            sleep(0.1)
            t.postfix[1]["value"] = i / 2
            t.update()

Points to remember when using ``{postfix[...]}`` in the ``bar_format`` string:

- ``postfix`` also needs to be passed as an initial argument in a compatible
  format, and
- ``postfix`` will be auto-converted to a string if it is a ``dict``-like
  object. To prevent this behaviour, insert an extra item into the dictionary
  where the key is not a string.

Additional ``bar_format`` parameters may also be defined by overriding
``format_dict``, and the bar itself may be modified using ``ascii``:

.. code:: python

    from tqdm import tqdm
    class TqdmExtraFormat(tqdm):
        """Provides a `total_time` format parameter"""
        @property
        def format_dict(self):
            d = super(TqdmExtraFormat, self).format_dict
            total_time = d["elapsed"] * (d["total"] or 0) / max(d["n"], 1)
            d.update(total_time=self.format_interval(total_time) + " in total")
            return d

    for i in TqdmExtraFormat(
          range(10), ascii=" .oO0",
          bar_format="{total_time}: {percentage:.0f}%|{bar}{r_bar}"):
        pass

.. code::

    00:01 in total: 40%|000o     | 4/10 [00:00<00:00,  9.96it/s]

Note that ``{bar}`` also supports a format specifier ``[width][type]``.

- ``width``

  * unspecified (default): automatic to fill ``ncols``
  * ``int >= 0``: fixed width overriding ``ncols`` logic
  * ``int < 0``: subtract from the automatic default

- ``type``

  * ``a``: ascii (``ascii=True`` override)
  * ``u``: unicode (``ascii=False`` override)
  * ``b``: blank (``ascii="  "`` override)

This means a fixed bar with right-justified text may be created by using:
``bar_format="{l_bar}{bar:10}|{bar:-10b}right-justified"``

Nested progress bars
~~~~~~~~~~~~~~~~~~~~

``tqdm`` supports nested progress bars. Here's an example:

.. code:: python

    from tqdm import trange
    from time import sleep

    for i in trange(4, desc='1st loop'):
        for j in trange(5, desc='2nd loop'):
            for k in trange(50, desc='3nd loop', leave=False):
                sleep(0.01)

On Windows `colorama <https://github.com/tartley/colorama>`__ will be used if
available to keep nested bars on their respective lines.

For manual control over positioning (e.g. for multi-processing use),
you may specify ``position=n`` where ``n=0`` for the outermost bar,
``n=1`` for the next, and so on:

.. code:: python

    from time import sleep
    from tqdm import trange, tqdm
    from multiprocessing import Pool, freeze_support

    L = list(range(9))

    def progresser(n):
        interval = 0.001 / (n + 2)
        total = 5000
        text = "#{}, est. {:<04.2}s".format(n, interval * total)
        for _ in trange(total, desc=text, position=n):
            sleep(interval)

    if __name__ == '__main__':
        freeze_support()  # for Windows support
        p = Pool(len(L), initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),))
        p.map(progresser, L)

Note that in python 3, threads do not require manual positioning,
and ``tqdm.write`` is safe to use:

.. code:: python

    from time import sleep
    from tqdm import tqdm, trange
    from concurrent.futures import ThreadPoolExecutor

    L = list(range(9))

    def progresser(n):
        interval = 0.001 / (n + 2)
        total = 5000
        text = "#{}, est. {:<04.2}s".format(n, interval * total)
        for _ in trange(total, desc=text):
            sleep(interval)
        if n == 6:
            tqdm.write("n == 6 completed.")
            tqdm.write("`tqdm.write()` is thread-safe in py3!")

    if __name__ == '__main__':
        with ThreadPoolExecutor() as p:
            p.map(progresser, L)

Hooks and callbacks
~~~~~~~~~~~~~~~~~~~

``tqdm`` can easily support callbacks/hooks and manual updates.
Here's an example with ``urllib``:

**urllib.urlretrieve documentation**

    | [...]
    | If present, the hook function will be called once
    | on establishment of the network connection and once after each block read
    | thereafter. The hook will be passed three arguments; a count of blocks
    | transferred so far, a block size in bytes, and the total size of the file.
    | [...]

.. code:: python

    import urllib, os
    from tqdm import tqdm

    class TqdmUpTo(tqdm):
        """Provides `update_to(n)` which uses `tqdm.update(delta_n)`."""
        def update_to(self, b=1, bsize=1, tsize=None):
            """
            b  : int, optional
                Number of blocks transferred so far [default: 1].
            bsize  : int, optional
                Size of each block (in tqdm units) [default: 1].
            tsize  : int, optional
                Total size (in tqdm units). If [default: None] remains unchanged.
            """
            if tsize is not None:
                self.total = tsize
            self.update(b * bsize - self.n)  # will also set self.n = b * bsize

    eg_link = "https://caspersci.uk.to/matryoshka.zip"
    with TqdmUpTo(unit='B', unit_scale=True, miniters=1,
                  desc=eg_link.split('/')[-1]) as t:  # all optional kwargs
        urllib.urlretrieve(eg_link, filename=os.devnull,
                           reporthook=t.update_to, data=None)

Inspired by `twine#242 <https://github.com/pypa/twine/pull/242>`__.
Functional alternative in
`examples/tqdm_wget.py <https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py>`__.

It is recommend to use ``miniters=1`` whenever there is potentially
large differences in iteration speed (e.g. downloading a file over
a patchy connection).

Pandas Integration
~~~~~~~~~~~~~~~~~~

Due to popular demand we've added support for ``pandas`` -- here's an example
for ``DataFrame.progress_apply`` and ``DataFrameGroupBy.progress_apply``:

.. code:: python

    import pandas as pd
    import numpy as np
    from tqdm import tqdm

    df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))

    # Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`
    # (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)
    tqdm.pandas(desc="my bar!")

    # Now you can use `progress_apply` instead of `apply`
    # and `progress_map` instead of `map`
    df.progress_apply(lambda x: x**2)
    # can also groupby:
    # df.groupby(0).progress_apply(lambda x: x**2)

In case you're interested in how this works (and how to modify it for your
own callbacks), see the
`examples <https://github.com/tqdm/tqdm/tree/master/examples>`__
folder or import the module and run ``help()``.

IPython/Jupyter Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

IPython/Jupyter is supported via the ``tqdm.notebook`` submodule:

.. code:: python

    from tqdm.notebook import trange, tqdm
    from time import sleep

    for i in trange(3, desc='1st loop'):
        for j in tqdm(range(100), desc='2nd loop'):
            sleep(0.01)

In addition to ``tqdm`` features, the submodule provides a native Jupyter
widget (compatible with IPython v1-v4 and Jupyter), fully working nested bars
and colour hints (blue: normal, green: completed, red: error/interrupt,
light blue: no ETA); as demonstrated below.

|Screenshot-Jupyter1|
|Screenshot-Jupyter2|
|Screenshot-Jupyter3|

It is also possible to let ``tqdm`` automatically choose between
console or notebook versions by using the ``autonotebook`` submodule:

.. code:: python

    from tqdm.autonotebook import tqdm
    tqdm.pandas()

Note that this will issue a ``TqdmExperimentalWarning`` if run in a notebook
since it is not meant to be possible to distinguish between ``jupyter notebook``
and ``jupyter console``. Use ``auto`` instead of ``autonotebook`` to suppress
this warning.

Custom Integration
~~~~~~~~~~~~~~~~~~

``tqdm`` may be inherited from to create custom callbacks (as with the
``TqdmUpTo`` example `above <#hooks-and-callbacks>`__) or for custom frontends
(e.g. GUIs such as notebook or plotting packages). In the latter case:

1. ``def __init__()`` to call ``super().__init__(..., gui=True)`` to disable
   terminal ``status_printer`` creation.
2. Redefine: ``close()``, ``clear()``, ``display()``.

Consider overloading ``display()`` to use e.g.
``self.frontend(**self.format_dict)`` instead of ``self.sp(repr(self))``.

`tqdm/notebook.py <https://github.com/tqdm/tqdm/blob/master/tqdm/notebook.py>`__
and `tqdm/gui.py <https://github.com/tqdm/tqdm/blob/master/tqdm/gui.py>`__
submodules are examples of inheritance which don't (yet) strictly conform to the
above recommendation.

Dynamic Monitor/Meter
~~~~~~~~~~~~~~~~~~~~~

You can use a ``tqdm`` as a meter which is not monotonically increasing.
This could be because ``n`` decreases (e.g. a CPU usage monitor) or ``total``
changes.

One example would be recursively searching for files. The ``total`` is the
number of objects found so far, while ``n`` is the number of those objects which
are files (rather than folders):

.. code:: python

    from tqdm import tqdm
    import os.path

    def find_files_recursively(path, show_progress=True):
        files = []
        # total=1 assumes `path` is a file
        t = tqdm(total=1, unit="file", disable=not show_progress)
        if not os.path.exists(path):
            raise IOError("Cannot find:" + path)

        def append_found_file(f):
            files.append(f)
            t.update()

        def list_found_dir(path):
            """returns os.listdir(path) assuming os.path.isdir(path)"""
            listing = os.listdir(path)
            # subtract 1 since a "file" we found was actually this directory
            t.total += len(listing) - 1
            # fancy way to give info without forcing a refresh
            t.set_postfix(dir=path[-10:], refresh=False)
            t.update(0)  # may trigger a refresh
            return listing

        def recursively_search(path):
            if os.path.isdir(path):
                for f in list_found_dir(path):
                    recursively_search(os.path.join(path, f))
            else:
                append_found_file(path)

        recursively_search(path)
        t.set_postfix(dir=path)
        t.close()
        return files

Using ``update(0)`` is a handy way to let ``tqdm`` decide when to trigger a
display refresh to avoid console spamming.

Writing messages
~~~~~~~~~~~~~~~~

This is a work in progress (see
`#737 <https://github.com/tqdm/tqdm/issues/737>`__).

Since ``tqdm`` uses a simple printing mechanism to display progress bars,
you should not write any message in the terminal using ``print()`` while
a progressbar is open.

To write messages in the terminal without any collision with ``tqdm`` bar
display, a ``.write()`` method is provided:

.. code:: python

    from tqdm import tqdm, trange
    from time import sleep

    bar = trange(10)
    for i in bar:
        # Print using tqdm class method .write()
        sleep(0.1)
        if not (i % 3):
            tqdm.write("Done task %i" % i)
        # Can also use bar.write()

By default, this will print to standard output ``sys.stdout``. but you can
specify any file-like object using the ``file`` argument. For example, this
can be used to redirect the messages writing to a log file or class.

Redirecting writing
~~~~~~~~~~~~~~~~~~~

If using a library that can print messages to the console, editing the library
by  replacing ``print()`` with ``tqdm.write()`` may not be desirable.
In that case, redirecting ``sys.stdout`` to ``tqdm.write()`` is an option.

To redirect ``sys.stdout``, create a file-like class that will write
any input string to ``tqdm.write()``, and supply the arguments
``file=sys.stdout, dynamic_ncols=True``.

A reusable canonical example is given below:

.. code:: python

    from time import sleep
    import contextlib
    import sys
    from tqdm import tqdm

    class DummyTqdmFile(object):
        """Dummy file-like that will write to tqdm"""
        file = None
        def __init__(self, file):
            self.file = file

        def write(self, x):
            # Avoid print() second call (useless \n)
            if len(x.rstrip()) > 0:
                tqdm.write(x, file=self.file)

        def flush(self):
            return getattr(self.file, "flush", lambda: None)()

    @contextlib.contextmanager
    def std_out_err_redirect_tqdm():
        orig_out_err = sys.stdout, sys.stderr
        try:
            sys.stdout, sys.stderr = map(DummyTqdmFile, orig_out_err)
            yield orig_out_err[0]
        # Relay exceptions
        except Exception as exc:
            raise exc
        # Always restore sys.stdout/err if necessary
        finally:
            sys.stdout, sys.stderr = orig_out_err

    def some_fun(i):
        print("Fee, fi, fo,".split()[i])

    # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`)
    with std_out_err_redirect_tqdm() as orig_stdout:
        # tqdm needs the original stdout
        # and dynamic_ncols=True to autodetect console width
        for i in tqdm(range(3), file=orig_stdout, dynamic_ncols=True):
            sleep(.5)
            some_fun(i)

    # After the `with`, printing is restored
    print("Done!")

Monitoring thread, intervals and miniters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``tqdm`` implements a few tricks to to increase efficiency and reduce overhead.

- Avoid unnecessary frequent bar refreshing: ``mininterval`` defines how long
  to wait between each refresh. ``tqdm`` always gets updated in the background,
  but it will display only every ``mininterval``.
- Reduce number of calls to check system clock/time.
- ``mininterval`` is more intuitive to configure than ``miniters``.
  A clever adjustment system ``dynamic_miniters`` will automatically adjust
  ``miniters`` to the amount of iterations that fit into time ``mininterval``.
  Essentially, ``tqdm`` will check if it's time to print without actually
  checking time. This behaviour can be still be bypassed by manually setting
  ``miniters``.

However, consider a case with a combination of fast and slow iterations.
After a few fast iterations, ``dynamic_miniters`` will set ``miniters`` to a
large number. When iteration rate subsequently slows, ``miniters`` will
remain large and thus reduce display update frequency. To address this:

- ``maxinterval`` defines the maximum time between display refreshes.
  A concurrent monitoring thread checks for overdue updates and forces one
  where necessary.

The monitoring thread should not have a noticeable overhead, and guarantees
updates at least every 10 seconds by default.
This value can be directly changed by setting the ``monitor_interval`` of
any ``tqdm`` instance (i.e. ``t = tqdm.tqdm(...); t.monitor_interval = 2``).
The monitor thread may be disabled application-wide by setting
``tqdm.tqdm.monitor_interval = 0`` before instantiation of any ``tqdm`` bar.


Contributions
-------------

|GitHub-Commits| |GitHub-Issues| |GitHub-PRs| |OpenHub-Status| |GitHub-Contributions|

All source code is hosted on `GitHub <https://github.com/tqdm/tqdm>`__.
Contributions are welcome.

See the
`CONTRIBUTING <https://raw.githubusercontent.com/tqdm/tqdm/master/CONTRIBUTING.md>`__
file for more information.

Developers who have made significant contributions, ranked by *LoC*
(surviving lines of code,
`git fame <https://github.com/casperdcl/git-fame>`__ ``-wMC``),
are:

==================== ================================================== ==== ================================
Name                 ID                                                 LoC  Notes
==================== ================================================== ==== ================================
Casper da Costa-Luis `casperdcl <https://github.com/casperdcl>`__       ~3/4 primary maintainer |Gift-Casper|
Stephen Larroque     `lrq3000 <https://github.com/lrq3000>`__           ~10% team member
Kyle Altendorf       `altendky <https://github.com/altendky>`__         ~2%
Guangshuo Chen       `chengs <https://github.com/chengs>`__             ~1%
Matthew Stevens      `mjstevens777 <https://github.com/mjstevens777>`__ ~1%
Noam Yorav-Raphael   `noamraph <https://github.com/noamraph>`__         ~1%  original author
Hadrien Mary         `hadim <https://github.com/hadim>`__               ~1%  team member
Mikhail Korobov      `kmike <https://github.com/kmike>`__               ~1%  team member
==================== ================================================== ==== ================================

|sourcerer-0| |sourcerer-1| |sourcerer-2| |sourcerer-3| |sourcerer-4| |sourcerer-5| |sourcerer-7|

Ports to Other Languages
~~~~~~~~~~~~~~~~~~~~~~~~

A list is available on
`this wiki page <https://github.com/tqdm/tqdm/wiki/tqdm-ports>`__.


LICENCE
-------

Open Source (OSI approved): |LICENCE|

Citation information: |DOI| (publication), |DOI-code| (code)

|README-Hits| (Since 19 May 2016)

.. |Logo| image:: https://raw.githubusercontent.com/tqdm/tqdm/master/images/logo.gif
.. |Screenshot| image:: https://raw.githubusercontent.com/tqdm/tqdm/master/images/tqdm.gif
.. |Build-Status| image:: https://img.shields.io/travis/tqdm/tqdm/master.svg?logo=travis
   :target: https://travis-ci.org/tqdm/tqdm
.. |Coverage-Status| image:: https://coveralls.io/repos/tqdm/tqdm/badge.svg?branch=master
   :target: https://coveralls.io/github/tqdm/tqdm
.. |Branch-Coverage-Status| image:: https://codecov.io/gh/tqdm/tqdm/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/tqdm/tqdm
.. |Codacy-Grade| image:: https://api.codacy.com/project/badge/Grade/3f965571598f44549c7818f29cdcf177
   :target: https://www.codacy.com/app/tqdm/tqdm/dashboard
.. |GitHub-Status| image:: https://img.shields.io/github/tag/tqdm/tqdm.svg?maxAge=86400&logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/releases
.. |GitHub-Forks| image:: https://img.shields.io/github/forks/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/network
.. |GitHub-Stars| image:: https://img.shields.io/github/stars/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/stargazers
.. |GitHub-Commits| image:: https://img.shields.io/github/commit-activity/y/tqdm/tqdm.svg?logo=git&logoColor=white
   :target: https://github.com/tqdm/tqdm/graphs/commit-activity
.. |GitHub-Issues| image:: https://img.shields.io/github/issues-closed/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/issues?q=
.. |GitHub-PRs| image:: https://img.shields.io/github/issues-pr-closed/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/pulls
.. |GitHub-Contributions| image:: https://img.shields.io/github/contributors/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/graphs/contributors
.. |GitHub-Updated| image:: https://img.shields.io/github/last-commit/tqdm/tqdm/master.svg?logo=github&logoColor=white&label=pushed
   :target: https://github.com/tqdm/tqdm/pulse
.. |Gift-Casper| image:: https://img.shields.io/badge/dynamic/json.svg?color=ff69b4&label=gifts%20received&prefix=%C2%A3&query=%24..sum&url=https%3A%2F%2Fcaspersci.uk.to%2Fgifts.json
   :target: https://caspersci.uk.to/donate
.. |PyPI-Status| image:: https://img.shields.io/pypi/v/tqdm.svg
   :target: https://pypi.org/project/tqdm
.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/tqdm.svg?label=pypi%20downloads&logo=python&logoColor=white
   :target: https://pypi.org/project/tqdm
.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/tqdm.svg?logo=python&logoColor=white
   :target: https://pypi.org/project/tqdm
.. |Conda-Forge-Status| image:: https://img.shields.io/conda/v/conda-forge/tqdm.svg?label=conda-forge&logo=conda-forge
   :target: https://anaconda.org/conda-forge/tqdm
.. |Snapcraft| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
   :target: https://snapcraft.io/tqdm
.. |Docker| image:: https://img.shields.io/badge/docker-pull-blue.svg?logo=docker
   :target: https://hub.docker.com/r/tqdm/tqdm
.. |Libraries-Rank| image:: https://img.shields.io/librariesio/sourcerank/pypi/tqdm.svg?logo=koding&logoColor=white
   :target: https://libraries.io/pypi/tqdm
.. |Libraries-Dependents| image:: https://img.shields.io/librariesio/dependent-repos/pypi/tqdm.svg?logo=koding&logoColor=white
    :target: https://github.com/tqdm/tqdm/network/dependents
.. |OpenHub-Status| image:: https://www.openhub.net/p/tqdm/widgets/project_thin_badge?format=gif
   :target: https://www.openhub.net/p/tqdm?ref=Thin+badge
.. |awesome-python| image:: https://awesome.re/mentioned-badge.svg
   :target: https://github.com/vinta/awesome-python
.. |LICENCE| image:: https://img.shields.io/pypi/l/tqdm.svg
   :target: https://raw.githubusercontent.com/tqdm/tqdm/master/LICENCE
.. |DOI| image:: https://img.shields.io/badge/DOI-10.21105/joss.01277-green.svg
   :target: https://doi.org/10.21105/joss.01277
.. |DOI-code| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.595120-blue.svg
   :target: https://doi.org/10.5281/zenodo.595120
.. |notebook-demo| image:: https://img.shields.io/badge/launch-notebook-orange.svg?logo=jupyter
   :target: https://notebooks.ai/demo/gh/tqdm/tqdm
.. |binder-demo| image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/tqdm/tqdm/master?filepath=DEMO.ipynb
.. |Screenshot-Jupyter1| image:: https://raw.githubusercontent.com/tqdm/tqdm/master/images/tqdm-jupyter-1.gif
.. |Screenshot-Jupyter2| image:: https://raw.githubusercontent.com/tqdm/tqdm/master/images/tqdm-jupyter-2.gif
.. |Screenshot-Jupyter3| image:: https://raw.githubusercontent.com/tqdm/tqdm/master/images/tqdm-jupyter-3.gif
.. |README-Hits| image:: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&style=social&r=https://github.com/tqdm/tqdm&l=https://caspersci.uk.to/images/tqdm.png&f=https://raw.githubusercontent.com/tqdm/tqdm/master/images/logo.gif
   :target: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&a=plot&r=https://github.com/tqdm/tqdm&l=https://caspersci.uk.to/images/tqdm.png&f=https://raw.githubusercontent.com/tqdm/tqdm/master/images/logo.gif&style=social
.. |sourcerer-0| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/0
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/0
.. |sourcerer-1| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/1
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/1
.. |sourcerer-2| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/2
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/2
.. |sourcerer-3| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/3
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/3
.. |sourcerer-4| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/4
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/4
.. |sourcerer-5| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/5
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/5
.. |sourcerer-6| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/6
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/6
.. |sourcerer-7| image:: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/images/7
   :target: https://sourcerer.io/fame/casperdcl/tqdm/tqdm/links/7



  * [traitlets-4.3.2](http://ipython.org) A configuration system for Python applications.



  * [treq-18.6.0](https://github.com/twisted/treq) treq
====

|pypi|_
|build|_
|coverage|_

``treq`` is an HTTP library inspired by
`requests <http://www.python-requests.org>`_ but written on top of
`Twisted <http://www.twistedmatrix.com>`_'s
`Agents <http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html>`_.

It provides a simple, higher level API for making HTTP requests when
using Twisted.

.. code-block:: python

    >>> from treq import get

    >>> def done(response):
    ...     print response.code
    ...     reactor.stop()

    >>> get("http://www.github.com").addCallback(done)

    >>> from twisted.internet import reactor
    >>> reactor.run()
    200

For more info `read the docs <http://treq.readthedocs.org>`_.

Contribute
==========

``treq`` is hosted on `GitHub <http://github.com/twisted/treq>`_.

Feel free to fork and send contributions over.

Developing
==========

Install dependencies:

::

    pip install treq[dev]

Run Tests (unit & integration):

::

    trial treq

Lint:

::

    pep8 treq
    pyflakes treq

Build docs::

    tox -e docs

.. |build| image:: https://api.travis-ci.org/twisted/treq.svg?branch=master
.. _build: https://travis-ci.org/twisted/treq

.. |coverage| image:: https://codecov.io/github/twisted/treq/coverage.svg?branch=master
.. _coverage: https://codecov.io/github/twisted/treq

.. |pypi| image:: https://img.shields.io/pypi/v/treq.svg
.. _pypi: https://pypi.python.org/pypi/treq



  * [twine-1.13.0](https://twine.readthedocs.io/) .. image:: https://img.shields.io/travis/pypa/twine/master.svg?label=travis-ci
   :target: https://travis-ci.org/pypa/twine

twine
=====

.. rtd-inclusion-marker-do-not-remove

Twine is `a utility`_ for `publishing`_ Python packages on `PyPI`_.

It provides build system independent uploads of source and binary
`distribution artifacts <distributions>`_ for both new and existing
`projects`_.


Why Should I Use This?
----------------------

The goal of ``twine`` is to improve PyPI interaction by improving
security and testability.

The biggest reason to use ``twine`` is that it securely authenticates
you to `PyPI`_ over HTTPS using a verified connection regardless of
the underlying Python version, while whether or not
``python setup.py upload`` will work correctly and securely depends
on your build system, your Python version and the underlying operating
system.

Secondly, it allows you to precreate your distribution files.
``python setup.py upload`` only allows you to upload something that you're
building with ``distutils`` or ``setuptools``, and created in the same
command invocation. This means that you cannot test the
exact file you're going to upload to PyPI to ensure that it works before
uploading it.

Finally, ``twine`` allows you to pre-sign your files and pass the
``.asc`` files into the command line invocation (``twine upload
myproject-1.0.1.tar.gz myproject-1.0.1.tar.gz.asc``). This enables you
to be assured that you're typing your ``gpg`` passphrase into ``gpg``
itself and not anything else, since *you* will be the one directly
executing ``gpg --detach-sign -a <filename>``.


Features
--------

- Verified HTTPS connections
- Uploading doesn't require executing ``setup.py``
- Uploading files that have already been created, allowing testing of
  distributions before release
- Supports uploading any packaging format (including `wheels`_)


Installation
------------

.. code-block:: console

    $ pip install twine


Using Twine
-----------

1. Create some distributions in the normal way:

   .. code-block:: console

       $ python setup.py sdist bdist_wheel

2. Upload with ``twine`` to `Test PyPI`_ and verify things look right. Twine will automatically prompt for your username and password:

   .. code-block:: console

       $ twine upload --repository-url https://test.pypi.org/legacy/ dist/*
       username: ...
       password:
       ...

3. Upload to `PyPI`_:

   .. code-block:: console

       $ twine upload dist/*

4. Done!

More documentation on using ``twine`` to upload packages to PyPI is in
the `Python Packaging User Guide`_.

Keyring Support
---------------

Instead of typing in your password every time you upload a distribution, Twine
allows you to store your username and password securely using `keyring`_.

To use the keyring, you must first install the keyring packages:

- On Windows and MacOS you just need to install ``keyring``, for example,
  ``pip install --user keyring``.
- On Linux, in addition to the ``keyring`` package you also need to ensure the
  ``python3-dbus`` system package is installed. For example, ``apt install
  python3-dbus``. See `Keyring's installation instructions`_ for more details.

Once keyring is installed you can use the ``keyring`` program to set your
username and password to use for each package index (repository) you want to
upload to using Twine.

To set your username and password for test PyPI run the following command.
``keyring`` will prompt you for your password:

.. code-block:: console

    $ keyring set https://test.pypi.org/legacy/ your-username
    # or
    $ python3 -m keyring set https://test.pypi.org/legacy/ your-username

To set your username and password for PyPI run this command, again, ``keyring``
will prompt for the password:

.. code-block:: console

    $ keyring set https://upload.pypi.org/legacy/ your-username
    # or
    $ python3 -m keyring set https://upload.pypi.org/legacy/ your-username


The next time you run ``twine`` it will prompt you for a username and will grab the appropriate password from the keyring.

.. Note:: If you are using Linux in a headless environment (such as on a
    server) you'll need to do some additional steps to ensure that Keyring can
    store secrets securely. See `Using Keyring on headless systems`_.

.. _`keyring`: https://pypi.org/project/keyring/
.. _`Keyring's installation instructions`:
    https://keyring.readthedocs.io/en/latest#installation-instructions
.. _`Using Keyring on headless systems`:
    https://keyring.readthedocs.io/en/latest/#using-keyring-on-headless-linux-systems

Disabling Keyring
^^^^^^^^^^^^^^^^^

In some cases, the presence of keyring may be problematic. To disable
keyring and defer to a prompt for passwords, uninstall ``keyring``
or if that's not an option, you can also configure keyring to be disabled.

See `twine 338 <https://github.com/pypa/twine/issues/338>`_ for a
discussion on ways to do that.

Options
-------

``twine upload``
^^^^^^^^^^^^^^^^

Uploads one or more distributions to a repository.

.. code-block:: console

    $ twine upload -h

    usage: twine upload [-h] [-r REPOSITORY] [--repository-url REPOSITORY_URL]
                        [-s] [--sign-with SIGN_WITH] [-i IDENTITY] [-u USERNAME]
                        [-p PASSWORD] [-c COMMENT] [--config-file CONFIG_FILE]
                        [--skip-existing] [--cert path] [--client-cert path]
                        [--verbose] [--disable-progress-bar]
                        dist [dist ...]

    positional arguments:
      dist                  The distribution files to upload to the repository
                            (package index). Usually dist/* . May additionally
                            contain a .asc file to include an existing signature
                            with the file upload.

    optional arguments:
      -h, --help            show this help message and exit
      -r REPOSITORY, --repository REPOSITORY
                            The repository (package index) to upload the package
                            to. Should be a section in the config file (default:
                            pypi). (Can also be set via TWINE_REPOSITORY
                            environment variable.)
      --repository-url REPOSITORY_URL
                            The repository (package index) URL to upload the
                            package to. This overrides --repository. (Can also be
                            set via TWINE_REPOSITORY_URL environment variable.)
      -s, --sign            Sign files to upload using GPG.
      --sign-with SIGN_WITH
                            GPG program used to sign uploads (default: gpg).
      -i IDENTITY, --identity IDENTITY
                            GPG identity used to sign files.
      -u USERNAME, --username USERNAME
                            The username to authenticate to the repository
                            (package index) as. (Can also be set via
                            TWINE_USERNAME environment variable.)
      -p PASSWORD, --password PASSWORD
                            The password to authenticate to the repository
                            (package index) with. (Can also be set via
                            TWINE_PASSWORD environment variable.)
      -c COMMENT, --comment COMMENT
                            The comment to include with the distribution file.
      --config-file CONFIG_FILE
                            The .pypirc config file to use.
      --skip-existing       Continue uploading files if one already exists. (Only
                            valid when uploading to PyPI. Other implementations
                            may not support this.)
      --cert path           Path to alternate CA bundle (can also be set via
                            TWINE_CERT environment variable).
      --client-cert path    Path to SSL client certificate, a single file
                            containing the private key and the certificate in PEM
                            format.
      --verbose             Show verbose output.
      --disable-progress-bar
                            Disable the progress bar.

``twine check``
^^^^^^^^^^^^^^^

Checks whether your distributions long description will render correctly on PyPI.

.. code-block:: console

    $ twine check -h
    usage: twine check [-h] dist [dist ...]

    positional arguments:
    dist        The distribution files to check, usually dist/*

    optional arguments:
    -h, --help  show this help message and exit

``twine register``
^^^^^^^^^^^^^^^^^^

**WARNING**: The ``register`` command is `no longer necessary if you are uploading to
pypi.org`_.  As such, it is `no longer supported`_ in `Warehouse`_ (the new
PyPI software running on pypi.org). However, you may need this if you are using
a different package index.

For completeness, its usage:

.. code-block:: console

    $ twine register -h

    usage: twine register [-h] -r REPOSITORY [--repository-url REPOSITORY_URL]
                          [-u USERNAME] [-p PASSWORD] [-c COMMENT]
                          [--config-file CONFIG_FILE] [--cert path]
                          [--client-cert path]
                          package

    positional arguments:
      package               File from which we read the package metadata.

    optional arguments:
      -h, --help            show this help message and exit
      -r REPOSITORY, --repository REPOSITORY
                            The repository (package index) to register the package
                            to. Should be a section in the config file. (Can also
                            be set via TWINE_REPOSITORY environment variable.)
                            Initial package registration no longer necessary on
                            pypi.org:
                            https://packaging.python.org/guides/migrating-to-pypi-
                            org/
      --repository-url REPOSITORY_URL
                            The repository (package index) URL to register the
                            package to. This overrides --repository. (Can also be
                            set via TWINE_REPOSITORY_URL environment variable.)
      -u USERNAME, --username USERNAME
                            The username to authenticate to the repository
                            (package index) as. (Can also be set via
                            TWINE_USERNAME environment variable.)
      -p PASSWORD, --password PASSWORD
                            The password to authenticate to the repository
                            (package index) with. (Can also be set via
                            TWINE_PASSWORD environment variable.)
      -c COMMENT, --comment COMMENT
                            The comment to include with the distribution file.
      --config-file CONFIG_FILE
                            The .pypirc config file to use.
      --cert path           Path to alternate CA bundle (can also be set via
                            TWINE_CERT environment variable).
      --client-cert path    Path to SSL client certificate, a single file
                            containing the private key and the certificate in PEM
                            format.

Environment Variables
^^^^^^^^^^^^^^^^^^^^^

Twine also supports configuration via environment variables. Options passed on
the command line will take precedence over options set via environment
variables. Definition via environment variable is helpful in environments where
it is not convenient to create a `.pypirc` file, such as a CI/build server, for
example.

* ``TWINE_USERNAME`` - the username to use for authentication to the repository.
* ``TWINE_PASSWORD`` - the password to use for authentication to the repository.
* ``TWINE_REPOSITORY`` - the repository configuration, either defined as a
  section in `.pypirc` or provided as a full URL.
* ``TWINE_REPOSITORY_URL`` - the repository URL to use.
* ``TWINE_CERT`` - custom CA certificate to use for repositories with
  self-signed or untrusted certificates.

Resources
---------

* `IRC <https://webchat.freenode.net/?channels=%23pypa>`_
  (``#pypa`` - irc.freenode.net)
* `GitHub repository <https://github.com/pypa/twine>`_
* User and developer `documentation`_
* `Python Packaging User Guide`_

Contributing
------------

See our `developer documentation`_ for how to get started, an
architectural overview, and our future development plans.

Code of Conduct
---------------

Everyone interacting in the ``twine`` project's codebases, issue
trackers, chat rooms, and mailing lists is expected to follow the
`PyPA Code of Conduct`_.

.. _`a utility`: https://pypi.org/project/twine/
.. _`publishing`: https://packaging.python.org/tutorials/distributing-packages/
.. _`PyPI`: https://pypi.org
.. _`Test PyPI`: https://packaging.python.org/guides/using-testpypi/
.. _`Python Packaging User Guide`: https://packaging.python.org/tutorials/distributing-packages/
.. _`documentation`: https://twine.readthedocs.io/
.. _`developer documentation`: https://twine.readthedocs.io/en/latest/contributing.html
.. _`projects`: https://packaging.python.org/glossary/#term-project
.. _`distributions`: https://packaging.python.org/glossary/#term-distribution-package
.. _`PyPA Code of Conduct`: https://www.pypa.io/en/latest/code-of-conduct/
.. _`Warehouse`: https://github.com/pypa/warehouse
.. _`wheels`: https://packaging.python.org/glossary/#term-wheel
.. _`no longer necessary if you are uploading to pypi.org`: https://packaging.python.org/guides/migrating-to-pypi-org/#registering-package-names-metadata
.. _`no longer supported`: https://github.com/pypa/warehouse/issues/1627



  * [twobitreader-3.1.7](https://github.com/benjschiller/twobitreader) 
  * [typechecks-0.1.0](https://github.com/openvax/typechecks) typechecks
==========

Type checking helpers for Python

  * [typed-ast-1.4.0](https://github.com/python/typed_ast) `typed_ast` is a Python 3 package that provides a Python 2.7 and Python 3
parser similar to the standard `ast` library.  Unlike `ast`, the parsers in
`typed_ast` include PEP 484 type comments and are independent of the version of
Python under which they are run.  The `typed_ast` parsers produce the standard
Python AST (plus type comments), and are both fast and correct, as they are
based on the CPython 2.7 and 3.6 parsers.


  * [typing-extensions-3.7.4](https://github.com/python/typing/blob/master/typing_extensions/README.rst) Typing Extensions -- Backported and Experimental Type Hints for Python

The ``typing`` module was added to the standard library in Python 3.5 on
a provisional basis and will no longer be provisional in Python 3.7. However,
this means users of Python 3.5 - 3.6 who are unable to upgrade will not be
able to take advantage of new types added to the ``typing`` module, such as
``typing.Text`` or ``typing.Coroutine``.

The ``typing_extensions`` module contains both backports of these changes
as well as experimental types that will eventually be added to the ``typing``
module, such as ``Protocol`` or ``TypedDict``.

Users of other Python versions should continue to install and use
the ``typing`` module from PyPi instead of using this one unless specifically
writing code that must be compatible with multiple Python versions or requires
experimental types.



  * [tzlocal-2.0.0](https://github.com/regebro/tzlocal) tzlocal
=======

This Python module returns a ``tzinfo`` object with the local timezone information under Unix and Win-32.
It requires ``pytz``, and returns ``pytz`` ``tzinfo`` objects.

This module attempts to fix a glaring hole in ``pytz``, that there is no way to
get the local timezone information, unless you know the zoneinfo name, and
under several Linux distros that's hard or impossible to figure out.

Also, with Windows different timezone system using pytz isn't of much use
unless you separately configure the zoneinfo timezone name.

With ``tzlocal`` you only need to call ``get_localzone()`` and you will get a
``tzinfo`` object with the local time zone info. On some Unices you will still
not get to know what the timezone name is, but you don't need that when you
have the tzinfo file. However, if the timezone name is readily available it
will be used.


Supported systems
-----------------

These are the systems that are in theory supported:

 * Windows 2000 and later

 * Any unix-like system with a ``/etc/localtime`` or ``/usr/local/etc/localtime``

If you have one of the above systems and it does not work, it's a bug.
Please report it.

Please note that if you getting a time zone called ``local``, this is not a bug, it's
actually the main feature of ``tzlocal``, that even if your system does NOT have a configuration file
with the zoneinfo name of your time zone, it will still work.

You can also use ``tzlocal`` to get the name of your local timezone, but only if your system is
configured to make that possible. ``tzlocal`` looks for the timezone name in ``/etc/timezone``, ``/var/db/zoneinfo``,
``/etc/sysconfig/clock`` and ``/etc/conf.d/clock``. If your ``/etc/localtime`` is a symlink it can also extract the
name from that symlink.

If you need the name of your local time zone, then please make sure your system is properly configured to allow that.
If it isn't configured, tzlocal will default to UTC.

Usage
-----

Load the local timezone:

    >>> from tzlocal import get_localzone
    >>> tz = get_localzone()
    >>> tz
    <DstTzInfo 'Europe/Warsaw' WMT+1:24:00 STD>

Create a local datetime:

    >>> from datetime import datetime
    >>> dt = tz.localize(datetime(2015, 4, 10, 7, 22))
    >>> dt
    datetime.datetime(2015, 4, 10, 7, 22, tzinfo=<DstTzInfo 'Europe/Warsaw' CEST+2:00:00 DST>)

Lookup another timezone with `pytz`:

    >>> import pytz
    >>> eastern = pytz.timezone('US/Eastern')

Convert the datetime:

    >>> dt.astimezone(eastern)
    datetime.datetime(2015, 4, 10, 1, 22, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)


Maintainer
----------

* Lennart Regebro, regebro@gmail.com

Contributors
------------

* Marc Van Olmen
* Benjamen Meyer
* Manuel Ebert
* Xiaokun Zhu
* Cameris
* Edward Betts
* McK KIM
* Cris Ewing
* Ayala Shachar
* Lev Maximov
* Jakub Wilk
* John Quarles
* Preston Landers
* Victor Torres
* Jean Jordaan
* Zackary Welch
* Mickaël Schoentgen
* Gabriel Corona

(Sorry if I forgot someone)

License
-------

* MIT https://opensource.org/licenses/MIT


Changes
=======

2.0.0 (2019-07-23)
------------------

- No differences since 2.0.0b3

Major differences since 1.5.1
.............................

- When no time zone configuration can be find, tzlocal now return UTC.
  This is a major difference from 1.x, where an exception would be raised.
  This change is because Docker images often have no configuration at all,
  and the unix utilities will then default to UTC, so we follow that.

- If tzlocal on Unix finds a timezone name in a /etc config file, then 
  tzlocal now verifies that the timezone it fouds has the same offset as
  the local computer is configured with. If it doesn't, something is
  configured incorrectly. (Victor Torres, regebro)

- Get timezone via Termux `getprop` wrapper on Android. It's not officially
  supported because we can't test it, but at least we make an effort.
  (Jean Jordaan)

Minor differences and bug fixes
...............................

- Skip comment lines when parsing /etc/timezone. (Edward Betts)

- Don't load timezone from current directory. (Gabriel Corona)

- Now verifies that the config files actually contain something before
  reading them. (Zackary Welch, regebro)

- Got rid of a BytesWarning (Mickaël Schoentgen)

- Now handles if config file paths exists, but are directories.

- Moved tests out from distributions

- Support wheels


1.5.1 (2017-12-01)
------------------

- 1.5 had a bug that slipped through testing, fixed that,
  increased test coverage.


1.5 (2017-11-30)
----------------

- No longer treats macOS as special, but as a unix.

- get_windows_info.py is renamed to update_windows_mappings.py

- Windows mappings now also contain mappings from deprecated zoneinfo names.
  (Preston-Landers, regebro)


1.4 (2017-04-18)
----------------

- I use MIT on my other projects, so relicensing.


1.4b1 (2017-04-14)
------------------

- Dropping support for Python versions nobody uses (2.5, 3.1, 3.2), adding 3.6
  Python 3.1 and 3.2 still works, 2.5 has been broken for some time.

- Ayalash's OS X fix didn't work on Python 2.7, fixed that.


1.3.2 (2017-04-12)
------------------

- Ensure closing of subprocess on OS X (ayalash)

- Removed unused imports (jwilk)

- Closes stdout and stderr to get rid of ResourceWarnings (johnwquarles)

- Updated Windows timezones (axil)


1.3 (2016-10-15)
----------------

- #34: Added support for /var/db/zoneinfo


1.2.2 (2016-03-02)
------------------

- #30: Fixed a bug on OS X.


1.2.1 (2016-02-28)
------------------

- Tests failed if TZ was set in the environment. (EdwardBetts)

- Replaces os.popen() with subprocess.Popen() for OS X to
  handle when systemsetup doesn't exist. (mckabi, cewing)


1.2 (2015-06-14)
----------------

- Systemd stores no time zone name, forcing us to look at the name of the file
  that localtime symlinks to. (cameris)


1.1.2 (2014-10-18)
------------------

- Timezones that has 3 items did not work on Mac OS X.
  (Marc Van Olmen)

- Now doesn't fail if the TZ environment variable isn't an Olsen time zone.

- Some timezones on Windows can apparently be empty (perhaps the are deleted).
  Now these are ignored.
  (Xiaokun Zhu)


1.1.1 (2014-01-29)
------------------

- I forgot to add Etc/UTC as an alias for Etc/GMT.


1.1 (2014-01-28)
----------------

- Adding better support for OS X.

- Added support to map from tzdata/Olsen names to Windows names.
  (Thanks to Benjamen Meyer).


1.0 (2013-05-29)
----------------

- Fixed some more cases where spaces needs replacing with underscores.

- Better handling of misconfigured /etc/timezone.

- Better error message on Windows if we can't find a timezone at all.


0.3 (2012-09-13)
----------------

- Windows 7 support.

- Python 2.5 supported; because it only needed a __future__ import.

- Python 3.3 tested, it worked.

- Got rid of relative imports, because I don't actually like them,
  so I don't know why I used them in the first place.

- For each Windows zone, use the default zoneinfo zone, not the last one.


0.2 (2012-09-12)
----------------

- Python 3 support.


0.1 (2012-09-11)
----------------

- Initial release.



  * [ujson-1.35](http://www.esn.me) UltraJSON
=============
.. image:: https://travis-ci.org/esnme/ultrajson.svg?branch=master
    :target: https://travis-ci.org/esnme/ultrajson

UltraJSON is an ultra fast JSON encoder and decoder written in pure C with bindings for Python 2.5+ and 3.

For a more painless day to day C/C++ JSON decoder experience please checkout ujson4c_, based on UltraJSON.

.. _ujson4c: http://github.com/esnme/ujson4c/

| Please checkout the rest of the projects in the Ultra series:
| http://github.com/esnme/ultramemcache
| http://github.com/esnme/ultramysql

To install it just run Pip as usual::

    $ pip install ujson

============
Usage
============
May be used as a drop in replacement for most other JSON parsers for Python::

    >>> import ujson
    >>> ujson.dumps([{"key": "value"}, 81, True])
    '[{"key":"value"},81,true]'
    >>> ujson.loads("""[{"key": "value"}, 81, true]""")
    [{u'key': u'value'}, 81, True]

~~~~~~~~~~~~~~~
Encoder options
~~~~~~~~~~~~~~~
encode_html_chars
-----------------
Used to enable special encoding of "unsafe" HTML characters into safer Unicode sequences. Default is false::

    >>> ujson.dumps("<script>John&Doe", encode_html_chars=True)
    '"\\u003cscript\\u003eJohn\\u0026Doe"'

ensure_ascii
-------------
Limits output to ASCII and escapes all extended characters above 127. Default is true. If your end format supports UTF-8 setting this option to false is highly recommended to save space::

    >>> ujson.dumps(u"\xe5\xe4\xf6")
    '"\\u00e5\\u00e4\\u00f6"'
    >>> ujson.dumps(u"\xe5\xe4\xf6", ensure_ascii=False)
    '"\xc3\xa5\xc3\xa4\xc3\xb6"'

double_precision
----------------
Controls how many decimals to encode for double or decimal values. Default is 9::

    >>> ujson.dumps(math.pi)
    '3.1415926536'
    >>> ujson.dumps(math.pi, double_precision=1)
    '3.1'
    >>> ujson.dumps(math.pi, double_precision=0)
    '3'
    >>> ujson.dumps(math.pi, double_precision=4)
    '3.1416'

escape_forward_slashes
----------------------
Controls whether forward slashes (``/``) are escaped. Default is True::

    >>> ujson.dumps("http://esn.me")
    '"http:\/\/esn.me"'
    >>> ujson.dumps("http://esn.me", escape_forward_slashes=False)
    '"http://esn.me"'

indent
----------------------
Controls whether indention ("pretty output") is enabled. Default is 0 (disabled)::

    >>> ujson.dumps({"foo": "bar"})
    '{"foo":"bar"}'
    >>> ujson.dumps({"foo": "bar"}, indent=4)
    {
        "foo":"bar"
    }

~~~~~~~~~~~~~~~~
Decoders options
~~~~~~~~~~~~~~~~
precise_float
-------------
Set to enable usage of higher precision (strtod) function when decoding string to double values. Default is to use fast but less precise builtin functionality::

    >>> ujson.loads("4.56")
    4.5600000000000005
    >>> ujson.loads("4.56", precise_float=True)
    4.5599999999999996

~~~~~~~~~~~~~
Test machine:
~~~~~~~~~~~~~

Linux 3.13.0-66-generic x86_64 #108-Ubuntu SMP Wed Oct 7 15:20:27 UTC 2015

~~~~~~~~~
Versions:
~~~~~~~~~

- CPython 2.7.6 (default, Jun 22 2015, 17:58:13) [GCC 4.8.2]
- blist     : 1.3.6
- simplejson: 3.8.1
- ujson     : 1.34 (0c52200eb4e2d97e548a765d5f089858c41967b0)
- yajl      : 0.3.5

+-------------------------------------------------------------------------------+------------+------------+------------+------------+
|                                                                               | ujson      | yajl       | simplejson | json       |
+===============================================================================+============+============+============+============+
| Array with 256 doubles                                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |    3508.19 |    5742.00 |    3232.38 |    3309.09 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   25103.37 |   11257.83 |   11696.26 |   11871.04 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 UTF-8 strings                                                  |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |    3189.71 |    2717.14 |    2006.38 |    2961.72 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |    1354.94 |     630.54 |     356.35 |     344.05 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 strings                                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   18127.47 |   12537.39 |   12541.23 |   20001.00 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   23264.70 |   12788.85 |   25427.88 |    9352.36 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Medium complex object                                                         |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   10519.38 |    5021.29 |    3686.86 |    4643.47 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |    9676.53 |    5326.79 |    8515.77 |    3017.30 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 True values                                                    |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |  105998.03 |  102067.28 |   44758.51 |   60424.80 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |  163869.96 |   78341.57 |  110859.36 |  115013.90 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 dict{string, int} pairs                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   13471.32 |   12109.09 |    3876.40 |    8833.92 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   16890.63 |    8946.07 |   12218.55 |    3350.72 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Dict with 256 arrays with 256 dict{string, int} pairs                         |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |      50.25 |      46.45 |      13.82 |      29.28 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |      33.27 |      22.10 |      27.91 |      10.43 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Dict with 256 arrays with 256 dict{string, int} pairs, outputting sorted keys |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |      27.19 |            |       7.75 |       2.39 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Complex object                                                                |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |     577.98 |            |     387.81 |     470.02 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |     496.73 |     234.44 |     151.00 |     145.16 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+

~~~~~~~~~
Versions:
~~~~~~~~~

- CPython 3.4.3 (default, Oct 14 2015, 20:28:29) [GCC 4.8.4]
- blist     : 1.3.6
- simplejson: 3.8.1
- ujson     : 1.34 (0c52200eb4e2d97e548a765d5f089858c41967b0)
- yajl      : 0.3.5

+-------------------------------------------------------------------------------+------------+------------+------------+------------+
|                                                                               | ujson      | yajl       | simplejson | json       |
+===============================================================================+============+============+============+============+
| Array with 256 doubles                                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |    3477.15 |    5732.24 |    3016.76 |    3071.99 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   23625.20 |    9731.45 |    9501.57 |    9901.92 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 UTF-8 strings                                                  |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |    1995.89 |    2151.61 |    1771.98 |    1817.20 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |    1425.04 |     625.38 |     327.14 |     305.95 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 strings                                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   25461.75 |   12188.64 |   13054.76 |   14429.81 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   21981.31 |   17014.22 |   23869.48 |   22483.58 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Medium complex object                                                         |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   10821.46 |    4837.04 |    3114.04 |    4254.46 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |    7887.77 |    5126.67 |    4934.60 |    6204.97 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 True values                                                    |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |  100452.86 |   94639.42 |   46657.63 |   60358.63 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |  148312.69 |   75485.90 |   88434.91 |  116395.51 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Array with 256 dict{string, int} pairs                                        |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |   11698.13 |    8886.96 |    3043.69 |    6302.35 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |   10686.40 |    7061.77 |    5646.80 |    7702.29 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Dict with 256 arrays with 256 dict{string, int} pairs                         |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |      44.26 |      34.43 |      10.40 |      21.97 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |      28.46 |      23.95 |      18.70 |      22.83 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Dict with 256 arrays with 256 dict{string, int} pairs, outputting sorted keys |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |      33.60 |            |       6.94 |      22.34 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| Complex object                                                                |            |            |            |            |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| encode                                                                        |     432.30 |            |     351.47 |     379.34 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
| decode                                                                        |     434.40 |     221.97 |     149.57 |     147.79 |
+-------------------------------------------------------------------------------+------------+------------+------------+------------+
  * [umap-learn-0.3.9](http://github.com/lmcinnes/umap) .. image:: https://img.shields.io/pypi/v/umap-learn.svg
    :target: https://pypi.python.org/pypi/umap-learn/
    :alt: PyPI Version
.. image:: https://anaconda.org/conda-forge/umap-learn/badges/version.svg
    :target: https://anaconda.org/conda-forge/umap-learn
    :alt: Conda-forge Version
.. image:: https://anaconda.org/conda-forge/umap-learn/badges/downloads.svg
    :target: https://anaconda.org/conda-forge/umap-learn
    :alt: Downloads from conda-forge
.. image:: https://img.shields.io/pypi/l/umap-learn.svg
    :target: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt
    :alt: License
.. image:: https://travis-ci.org/lmcinnes/umap.svg
    :target: https://travis-ci.org/lmcinnes/umap
    :alt: Travis Build Status
.. image:: https://ci.appveyor.com/api/projects/status/github/lmcinnes/umap?branch=master&svg=true
    :target: https://ci.appveyor.com/project/lmcinnes/umap
    :alt: AppVeyor Build Status
.. image:: https://coveralls.io/repos/github/lmcinnes/umap/badge.svg
    :target: https://coveralls.io/github/lmcinnes/umap
    :alt: Test Coverage Status
.. image:: https://readthedocs.org/projects/umap-learn/badge/?version=latest
    :target: https://umap-learn.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status
.. image:: http://joss.theoj.org/papers/10.21105/joss.00861/status.svg
    :target: https://doi.org/10.21105/joss.00861
    :alt: JOSS article for this repository

====
UMAP
====

Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction
technique that can be used for visualisation similarly to t-SNE, but also for
general non-linear dimension reduction. The algorithm is founded on three
assumptions about the data

1. The data is uniformly distributed on a Riemannian manifold;
2. The Riemannian metric is locally constant (or can be approximated as such);
3. The manifold is locally connected.

From these assumptions it is possible to model the manifold with a fuzzy
topological structure. The embedding is found by searching for a low dimensional
projection of the data that has the closest possible equivalent fuzzy
topological structure.

The details for the underlying mathematics can be found in
`our paper on ArXiv <https://arxiv.org/abs/1802.03426>`_:

McInnes, L, Healy, J, *UMAP: Uniform Manifold Approximation and Projection
for Dimension Reduction*, ArXiv e-prints 1802.03426, 2018

The important thing is that you don't need to worry about that -- you can use
UMAP right now for dimension reduction and visualisation as easily as a drop
in replacement for scikit-learn's t-SNE.

Documentation is `available via ReadTheDocs <https://umap-learn.readthedocs.io/>`_.

---------------
How to use UMAP
---------------

The umap package inherits from sklearn classes, and thus drops in neatly
next to other sklearn transformers with an identical calling API.

.. code:: python

    import umap
    from sklearn.datasets import load_digits

    digits = load_digits()

    embedding = umap.UMAP().fit_transform(digits.data)

There are a number of parameters that can be set for the UMAP class; the
major ones are as follows:

 -  ``n_neighbors``: This determines the number of neighboring points used in
    local approximations of manifold structure. Larger values will result in
    more global structure being preserved at the loss of detailed local
    structure. In general this parameter should often be in the range 5 to
    50, with a choice of 10 to 15 being a sensible default.

 -  ``min_dist``: This controls how tightly the embedding is allowed compress
    points together. Larger values ensure embedded points are more evenly
    distributed, while smaller values allow the algorithm to optimise more
    accurately with regard to local structure. Sensible values are in the
    range 0.001 to 0.5, with 0.1 being a reasonable default.

 -  ``metric``: This determines the choice of metric used to measure distance
    in the input space. A wide variety of metrics are already coded, and a user
    defined function can be passed as long as it has been JITd by numba.

An example of making use of these options:

.. code:: python

    import umap
    from sklearn.datasets import load_digits

    digits = load_digits()

    embedding = umap.UMAP(n_neighbors=5,
                          min_dist=0.3,
                          metric='correlation').fit_transform(digits.data)

UMAP also supports fitting to sparse matrix data. For more details
please see `the UMAP documentation <https://umap-learn.readthedocs.io/>`_

----------------
Benefits of UMAP
----------------

UMAP has a few signficant wins in its current incarnation.

First of all UMAP is *fast*. It can handle large datasets and high
dimensional data without too much difficulty, scaling beyond what most t-SNE
packages can manage.

Second, UMAP scales well in embedding dimension -- it isn't just for
visualisation! You can use UMAP as a general purpose dimension reduction
technique as a preliminary step to other machine learning tasks. With a
little care it partners well with the `hdbscan
<https://github.com/scikit-learn-contrib/hdbscan>`_ clustering library (for
more details please see `Using UMAP for Clustering
<https://umap-learn.readthedocs.io/en/latest/clustering.html>`_).

Third, UMAP often performs better at preserving aspects of global structure of
the data than t-SNE. This means that it can often provide a better "big
picture" view of your data as well as preserving local neighbor relations.

Fourth, UMAP supports a wide variety of distance functions, including
non-metric distance functions such as *cosine distance* and *correlation
distance*. You can finally embed word vectors properly using cosine distance!

Fifth, UMAP supports adding new points to an existing embedding via
the standard sklearn ``transform`` method. This means that UMAP can be
used as a preprocessing transformer in sklearn pipelines.

Sixth, UMAP supports supervised and semi-supervised dimension reduction.
This means that if you have label information that you wish to use as
extra information for dimension reduction (even if it is just partial
labelling) you can do that -- as simply as providing it as the ``y``
parameter in the fit method.

Finally UMAP has solid theoretical foundations in manifold learning
(see `our paper on ArXiv <https://arxiv.org/abs/1802.03426>`_).
This both justifies the approach and allows for further
extensions that will soon be added to the library
(embedding dataframes etc.).

------------------------
Performance and Examples
------------------------

UMAP is very efficient at embedding large high dimensional datasets. In
particular it scales well with both input dimension and embedding dimension.
Thus, for a problem such as the 784-dimensional MNIST digits dataset with
70000 data samples, UMAP can complete the embedding in around 2.5 minutes (as
compared with around 45 minutes for most t-SNE implementations). Despite this
runtime efficiency UMAP still produces high quality embeddings.

The obligatory MNIST digits dataset, embedded in 2 minutes  and 22
seconds using a 3.1 GHz Intel Core i7 processor (n_neighbors=10, min_dist=0
.001):

.. image:: images/umap_example_mnist1.png
    :alt: UMAP embedding of MNIST digits

The MNIST digits dataset is fairly straightforward however. A better test is
the more recent "Fashion MNIST" dataset of images of fashion items (again
70000 data sample in 784 dimensions). UMAP
produced this embedding in 2 minutes exactly (n_neighbors=5, min_dist=0.1):

.. image:: images/umap_example_fashion_mnist1.png
    :alt: UMAP embedding of "Fashion MNIST"

The UCI shuttle dataset (43500 sample in 8 dimensions) embeds well under
*correlation* distance in 2 minutes and 39 seconds (note the longer time
required for correlation distance computations):

.. image:: images/umap_example_shuttle.png
    :alt: UMAP embedding the UCI Shuttle dataset

----------
Installing
----------

UMAP depends upon ``scikit-learn``, and thus ``scikit-learn``'s dependencies
such as ``numpy`` and ``scipy``. UMAP adds a requirement for ``numba`` for
performance reasons. The original version used Cython, but the improved code
clarity, simplicity and performance of Numba made the transition necessary.

Requirements:

* numpy
* scipy
* scikit-learn
* numba

**Install Options**

Conda install, via the excellent work of the conda-forge team:

.. code:: bash

    conda install -c conda-forge umap-learn

The conda-forge packages are available for linux, OS X, and Windows 64 bit.

PyPI install, presuming you have numba and sklearn and all its requirements
(numpy and scipy) installed:

.. code:: bash

    pip install umap-learn

If pip is having difficulties pulling the dependencies then we'd suggest installing
the dependencies manually using anaconda followed by pulling umap from pip:

.. code:: bash

    conda install numpy scipy
    conda install scikit-learn
    conda install numba
    pip install umap-learn

For a manual install get this package:

.. code:: bash

    wget https://github.com/lmcinnes/umap/archive/master.zip
    unzip master.zip
    rm master.zip
    cd umap-master

Install the requirements

.. code:: bash

    sudo pip install -r requirements.txt

or

.. code:: bash

    conda install scikit-learn numba

Install the package

.. code:: bash

    python setup.py install

----------------
Help and Support
----------------

Documentation is at `ReadTheDocs <https://umap-learn.readthedocs.io/>`_.
The documentation `includes a FAQ <https://umap-learn.readthedocs.io/en/latest/faq.html>`_ that
may answer your questions. If you still have questions then please
`open an issue <https://github.com/lmcinnes/umap/issues/new>`_
and I will try to provide any help and guidance that I can.

--------
Citation
--------

If you make use of this software for your work we would appreciate it if you
would cite the paper from the Journal of Open Source Software:

.. code:: bibtex

    @article{mcinnes2018umap-software,
      title={UMAP: Uniform Manifold Approximation and Projection},
      author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
      journal={The Journal of Open Source Software},
      volume={3},
      number={29},
      pages={861},
      year={2018}
    }

If you would like to cite this algorithm in your work the ArXiv paper is the
current reference:

.. code:: bibtex

   @article{2018arXivUMAP,
        author = {{McInnes}, L. and {Healy}, J. and {Melville}, J.},
        title = "{UMAP: Uniform Manifold Approximation
        and Projection for Dimension Reduction}",
        journal = {ArXiv e-prints},
        archivePrefix = "arXiv",
        eprint = {1802.03426},
        primaryClass = "stat.ML",
        keywords = {Statistics - Machine Learning,
                    Computer Science - Computational Geometry,
                    Computer Science - Learning},
        year = 2018,
        month = feb,
   }

-------
License
-------

The umap package is 3-clause BSD licensed.

We would like to note that the umap package makes heavy use of
NumFOCUS sponsored projects, and would not be possible without
their support of those projects, so please `consider contributing to NumFOCUS <https://www.numfocus.org/membership>`_.

------------
Contributing
------------

Contributions are more than welcome! There are lots of opportunities
for potential projects, so please get in touch if you would like to
help out. Everything from code to notebooks to
examples and documentation are all *equally valuable* so please don't feel
you can't contribute. To contribute please
`fork the project <https://github.com/lmcinnes/umap/issues#fork-destination-box>`_
make your changes and
submit a pull request. We will do our best to work through any issues with
you and get your code merged into the main branch.



  * [umi_tools-1.0.0](https://github.com/CGATOxford/UMI-tools) umi_tools: Tools for UMI analyses



  * [unicodecsv-0.14.1](https://github.com/jdunck/python-unicodecsv) unicodecsv
==========

The unicodecsv is a drop-in replacement for Python 2.7's csv module which supports unicode strings without a hassle.  Supported versions are python 2.7, 3.3, 3.4, 3.5, and pypy 2.4.0.

More fully
----------

Python 2's csv module doesn't easily deal with unicode strings, leading to the dreaded "'ascii' codec can't encode characters in position ..." exception.

You can work around it by encoding everything just before calling write (or just after read), but why not add support to the serializer?

.. code-block:: pycon

   >>> import unicodecsv as csv
   >>> from io import BytesIO
   >>> f = BytesIO()
   >>> w = csv.writer(f, encoding='utf-8')
   >>> _ = w.writerow((u'é', u'ñ'))
   >>> _ = f.seek(0)
   >>> r = csv.reader(f, encoding='utf-8')
   >>> next(r) == [u'é', u'ñ']
   True

Note that unicodecsv expects a bytestream, not unicode -- so there's no need to use `codecs.open` or similar wrappers.  Plain `open(..., 'rb')` will do.

(Version 0.14.0 dropped support for python 2.6, but 0.14.1 added it back.  See c0b7655248c4249 for the mistaken, breaking change.)
  * [uritemplate-3.0.0](https://uritemplate.readthedocs.org) uritemplate
===========

Documentation_ -- GitHub_ -- BitBucket_ -- Travis-CI_

Simple python library to deal with `URI Templates`_. The API looks like

.. code-block:: python

    from uritemplate import URITemplate, expand

    # NOTE: URI params must be strings not integers

    gist_uri = 'https://api.github.com/users/sigmavirus24/gists{/gist_id}'
    t = URITemplate(gist_uri)
    print(t.expand(gist_id='123456'))
    # => https://api.github.com/users/sigmavirus24/gists/123456

    # or
    print(expand(gist_uri, gist_id='123456'))

    # also
    t.expand({'gist_id': '123456'})
    print(expand(gist_uri, {'gist_id': '123456'}))

Where it might be useful to have a class

.. code-block:: python

    import requests

    class GitHubUser(object):
        url = URITemplate('https://api.github.com/user{/login}')
        def __init__(self, name):
            self.api_url = url.expand(login=name)
            response = requests.get(self.api_url)
            if response.status_code == 200:
                self.__dict__.update(response.json())

When the module containing this class is loaded, ``GitHubUser.url`` is 
evaluated and so the template is created once. It's often hard to notice in 
Python, but object creation can consume a great deal of time and so can the 
``re`` module which uritemplate relies on. Constructing the object once should 
reduce the amount of time your code takes to run.

Installing
----------

::

    pip install uritemplate.py

License
-------

Modified BSD license_


.. _Documentation: http://uritemplate.rtfd.org/
.. _GitHub: https://github.com/sigmavirus24/uritemplate
.. _BitBucket: https://bitbucket.org/icordasc/uritemplate
.. _Travis-CI: https://travis-ci.org/sigmavirus24/uritemplate
.. _URI Templates: http://tools.ietf.org/html/rfc6570
.. _license: https://github.com/sigmavirus24/uritemplate/blob/master/LICENSE


Changelog - uritemplate
=======================

2.0.0 - 2016-08-29
------------------

- Merge uritemplate.py into uritemplate


Changelog - uritemplate.py
==========================

2.0.0 - 2016-08-20
------------------

- Relicense uritemplate.py as Apache 2 and BSD (See
  https://github.com/sigmavirus24/uritemplate/pull/23)

1.0.1 - 2016-08-18
------------------

- Fix some minor packaging problems.

1.0.0 - 2016-08-17
------------------

- Fix handling of Unicode values on Python 2.6 and 2.7 for urllib.quote.

- Confirm public stable API via version number.

0.3.0 - 2013-10-22
------------------

- Add ``#partial`` to partially expand templates and return new instances of 
  ``URITemplate``.

0.2.0 - 2013-07-26
------------------

- Refactor the library a bit and add more tests.

- Backwards incompatible with 0.1.x if using ``URIVariable`` directly from
  ``uritemplate.template``

0.1.1 - 2013-05-19
------------------

- Add ability to get set of variable names in the current URI

- If there is no value or default given, simply return an empty string

- Fix sdist

0.1.0 - 2013-05-14
------------------

- Initial Release



  * [urllib3-1.23](https://urllib3.readthedocs.io/) urllib3
=======

.. image:: https://travis-ci.org/urllib3/urllib3.svg?branch=master
        :alt: Build status on Travis
        :target: https://travis-ci.org/urllib3/urllib3

.. image:: https://img.shields.io/appveyor/ci/urllib3/urllib3/master.svg
        :alt: Build status on AppVeyor
        :target: https://ci.appveyor.com/project/urllib3/urllib3

.. image:: https://readthedocs.org/projects/urllib3/badge/?version=latest
        :alt: Documentation Status
        :target: https://urllib3.readthedocs.io/en/latest/

.. image:: https://img.shields.io/codecov/c/github/urllib3/urllib3.svg
        :alt: Coverage Status
        :target: https://codecov.io/gh/urllib3/urllib3

.. image:: https://img.shields.io/pypi/v/urllib3.svg?maxAge=86400
        :alt: PyPI version
        :target: https://pypi.org/project/urllib3/

.. image:: https://badges.gitter.im/python-urllib3/Lobby.svg
        :alt: Gitter
        :target: https://gitter.im/python-urllib3/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. image:: https://tidelift.com/badges/github/urllib3/urllib3
        :alt: Tidelift Dependencies
        :target: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=docs

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black

urllib3 is a powerful, *sanity-friendly* HTTP client for Python. Much of the
Python ecosystem already uses urllib3 and you should too.
urllib3 brings many critical features that are missing from the Python
standard libraries:

- Thread safety.
- Connection pooling.
- Client-side SSL/TLS verification.
- File uploads with multipart encoding.
- Helpers for retrying requests and dealing with HTTP redirects.
- Support for gzip, deflate, and brotli encoding.
- Proxy support for HTTP and SOCKS.
- 100% test coverage.

urllib3 is powerful and easy to use::

    >>> import urllib3
    >>> http = urllib3.PoolManager()
    >>> r = http.request('GET', 'http://httpbin.org/robots.txt')
    >>> r.status
    200
    >>> r.data
    'User-agent: *\nDisallow: /deny\n'


Installing
----------

urllib3 can be installed with `pip <https://pip.pypa.io>`_::

    $ pip install urllib3

Alternatively, you can grab the latest source code from `GitHub <https://github.com/urllib3/urllib3>`_::

    $ git clone git://github.com/urllib3/urllib3.git
    $ python setup.py install


Documentation
-------------

urllib3 has usage and reference documentation at `urllib3.readthedocs.io <https://urllib3.readthedocs.io>`_.


Contributing
------------

urllib3 happily accepts contributions. Please see our
`contributing documentation <https://urllib3.readthedocs.io/en/latest/contributing.html>`_
for some tips on getting started.


Security Disclosures
--------------------

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure with maintainers.

Maintainers
-----------

- `@sethmlarson <https://github.com/sethmlarson>`_ (Seth M. Larson)
- `@theacodes <https://github.com/theacodes>`_ (Thea Flowers)
- `@haikuginger <https://github.com/haikuginger>`_ (Jess Shapiro)
- `@lukasa <https://github.com/lukasa>`_ (Cory Benfield)
- `@sigmavirus24 <https://github.com/sigmavirus24>`_ (Ian Cordasco)
- `@shazow <https://github.com/shazow>`_ (Andrey Petrov)

👋


Sponsorship
-----------

.. |tideliftlogo| image:: https://nedbatchelder.com/pix/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White_small.png
   :width: 75
   :alt: Tidelift

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for urllib3 is available as part of the `Tidelift
       Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=readme

If your company benefits from this library, please consider `sponsoring its
development <https://urllib3.readthedocs.io/en/latest/contributing.html#sponsorship-project-grants>`_.

Sponsors include:

- Abbott (2018-2019), sponsored `@sethmlarson <https://github.com/sethmlarson>`_'s work on urllib3.
- Google Cloud Platform (2018-2019), sponsored `@theacodes <https://github.com/theacodes>`_'s work on urllib3.
- Akamai (2017-2018), sponsored `@haikuginger <https://github.com/haikuginger>`_'s work on urllib3
- Hewlett Packard Enterprise (2016-2017), sponsored `@Lukasa’s <https://github.com/Lukasa>`_ work on urllib3.


Changes
=======

1.25.6 (2019-09-24)
-------------------

* Fix issue where tilde (``~``) characters were incorrectly
  percent-encoded in the path. (Pull #1692)


1.25.5 (2019-09-19)
-------------------

* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which
  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.
  (Issue #1682)


1.25.4 (2019-09-19)
-------------------

* Propagate Retry-After header settings to subsequent retries. (Pull #1607)

* Fix edge case where Retry-After header was still respected even when
  explicitly opted out of. (Pull #1607)

* Remove dependency on ``rfc3986`` for URL parsing.

* Fix issue where URLs containing invalid characters within ``Url.auth`` would
  raise an exception instead of percent-encoding those characters.

* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses
  work well with BufferedReaders and other ``io`` module features. (Pull #1652)

* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)


1.25.3 (2019-05-23)
-------------------

* Change ``HTTPSConnection`` to load system CA certificates
  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are
  unspecified. (Pull #1608, Issue #1603)

* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)


1.25.2 (2019-04-28)
-------------------

* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)

* Change ``parse_url`` to percent-encode invalid characters within the
  path, query, and target components. (Pull #1586)


1.25.1 (2019-04-24)
-------------------

* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)

* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)


1.25 (2019-04-22)
-----------------

* Require and validate certificates by default when using HTTPS (Pull #1507)

* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)

* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use
  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)

* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``
  implementations. (Pull #1496)

* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, PR #1492)

* Fixed issue where OpenSSL would block if an encrypted client private key was
  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)

* Added support for Brotli content encoding. It is enabled automatically if
  ``brotlipy`` package is installed which can be requested with
  ``urllib3[brotli]`` extra. (Pull #1532)

* Drop ciphers using DSS key exchange from default TLS cipher suites.
  Improve default ciphers when using SecureTransport. (Pull #1496)

* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)

1.24.3 (2019-05-01)
-------------------

* Apply fix for CVE-2019-9740. (Pull #1591)

1.24.2 (2019-04-17)
-------------------

* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or
  ``ssl_context`` parameters are specified.

* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)

* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)


1.24.1 (2018-11-02)
-------------------

* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)

* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)


1.24 (2018-10-16)
-----------------

* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)

* Test against Python 3.7 on AppVeyor. (Pull #1453)

* Early-out ipv6 checks when running on App Engine. (Pull #1450)

* Change ambiguous description of backoff_factor (Pull #1436)

* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)

* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).

* Add a server_hostname parameter to HTTPSConnection which allows for
  overriding the SNI hostname sent in the handshake. (Pull #1397)

* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)

* Fixed bug where responses with header Content-Type: message/* erroneously
  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)

* Move urllib3 to src/urllib3 (Pull #1409)


1.23 (2018-06-04)
-----------------

* Allow providing a list of headers to strip from requests when redirecting
  to a different host. Defaults to the ``Authorization`` header. Different
  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)

* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).

* Dropped Python 3.3 support. (Pull #1242)

* Put the connection back in the pool when calling stream() or read_chunked() on
  a chunked HEAD response. (Issue #1234)

* Fixed pyOpenSSL-specific ssl client authentication issue when clients
  attempted to auth via certificate + chain (Issue #1060)

* Add the port to the connectionpool connect print (Pull #1251)

* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)

* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)

* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)

* Added support for auth info in url for SOCKS proxy (Pull #1363)


1.22 (2017-07-20)
-----------------

* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via
  IPv6 proxy. (Issue #1222)

* Made the connection pool retry on ``SSLError``.  The original ``SSLError``
  is available on ``MaxRetryError.reason``. (Issue #1112)

* Drain and release connection before recursing on retry/redirect.  Fixes
  deadlocks with a blocking connectionpool. (Issue #1167)

* Fixed compatibility for cookiejar. (Issue #1229)

* pyopenssl: Use vendored version of ``six``. (Issue #1231)


1.21.1 (2017-05-02)
-------------------

* Fixed SecureTransport issue that would cause long delays in response body
  delivery. (Pull #1154)

* Fixed regression in 1.21 that threw exceptions when users passed the
  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)

* Fixed regression in 1.21 that threw exceptions when users passed the
  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.
  (Pull #1157)


1.21 (2017-04-25)
-----------------

* Improved performance of certain selector system calls on Python 3.5 and
  later. (Pull #1095)

* Resolved issue where the PyOpenSSL backend would not wrap SysCallError
  exceptions appropriately when sending data. (Pull #1125)

* Selectors now detects a monkey-patched select module after import for modules
  that patch the select module like eventlet, greenlet. (Pull #1128)

* Reduced memory consumption when streaming zlib-compressed responses
  (as opposed to raw deflate streams). (Pull #1129)

* Connection pools now use the entire request context when constructing the
  pool key. (Pull #1016)

* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,
  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.
  (Pull #1016)

* Add retry counter for ``status_forcelist``. (Issue #1147)

* Added ``contrib`` module for using SecureTransport on macOS:
  ``urllib3.contrib.securetransport``.  (Pull #1122)

* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:
  for schemes it does not recognise, it assumes they are case-sensitive and
  leaves them unchanged.
  (Issue #1080)


1.20 (2017-01-19)
-----------------

* Added support for waiting for I/O using selectors other than select,
  improving urllib3's behaviour with large numbers of concurrent connections.
  (Pull #1001)

* Updated the date for the system clock check. (Issue #1005)

* ConnectionPools now correctly consider hostnames to be case-insensitive.
  (Issue #1032)

* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module
  to fail when it is injected, rather than at first use. (Pull #1063)

* Outdated versions of cryptography now cause the PyOpenSSL contrib module
  to fail when it is injected, rather than at first use. (Issue #1044)

* Automatically attempt to rewind a file-like body object when a request is
  retried or redirected. (Pull #1039)

* Fix some bugs that occur when modules incautiously patch the queue module.
  (Pull #1061)

* Prevent retries from occurring on read timeouts for which the request method
  was not in the method whitelist. (Issue #1059)

* Changed the PyOpenSSL contrib module to lazily load idna to avoid
  unnecessarily bloating the memory of programs that don't need it. (Pull
  #1076)

* Add support for IPv6 literals with zone identifiers. (Pull #1013)

* Added support for socks5h:// and socks4a:// schemes when working with SOCKS
  proxies, and controlled remote DNS appropriately. (Issue #1035)


1.19.1 (2016-11-16)
-------------------

* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)


1.19 (2016-11-03)
-----------------

* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when
  using the default retry logic. (Pull #955)

* Remove markers from setup.py to assist ancient setuptools versions. (Issue
  #986)

* Disallow superscripts and other integerish things in URL ports. (Issue #989)

* Allow urllib3's HTTPResponse.stream() method to continue to work with
  non-httplib underlying FPs. (Pull #990)

* Empty filenames in multipart headers are now emitted as such, rather than
  being suppressed. (Issue #1015)

* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)


1.18.1 (2016-10-27)
-------------------

* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with
  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This
  release fixes a vulnerability whereby urllib3 in the above configuration
  would silently fail to validate TLS certificates due to erroneously setting
  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous
  flags do not cause a problem in OpenSSL versions before 1.1.0, which
  interprets the presence of any flag as requesting certificate validation.

  There is no PR for this patch, as it was prepared for simultaneous disclosure
  and release. The master branch received the same fix in PR #1010.


1.18 (2016-09-26)
-----------------

* Fixed incorrect message for IncompleteRead exception. (PR #973)

* Accept ``iPAddress`` subject alternative name fields in TLS certificates.
  (Issue #258)

* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.
  (Issue #977)

* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)


1.17 (2016-09-06)
-----------------

* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)

* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)

* Substantially refactored documentation. (Issue #887)

* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.
  (Issue #858)

* Normalize the scheme and host in the URL parser (Issue #833)

* ``HTTPResponse`` contains the last ``Retry`` object, which now also
  contains retries history. (Issue #848)

* Timeout can no longer be set as boolean, and must be greater than zero.
  (PR #924)

* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We
  now use cryptography and idna, both of which are already dependencies of
  PyOpenSSL. (PR #930)

* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)

* Try to use the operating system's certificates when we are using an
  ``SSLContext``. (PR #941)

* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to
  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)

* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)

* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)

* Implemented ``length_remaining`` to determine remaining content
  to be read. (PR #949)

* Implemented ``enforce_content_length`` to enable exceptions when
  incomplete data chunks are received. (PR #949)

* Dropped connection start, dropped connection reset, redirect, forced retry,
  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)


1.16 (2016-06-11)
-----------------

* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)

* Provide ``key_fn_by_scheme`` pool keying mechanism that can be
  overridden. (Issue #830)

* Normalize scheme and host to lowercase for pool keys, and include
  ``source_address``. (Issue #830)

* Cleaner exception chain in Python 3 for ``_make_request``.
  (Issue #861)

* Fixed installing ``urllib3[socks]`` extra. (Issue #864)

* Fixed signature of ``ConnectionPool.close`` so it can actually safely be
  called by subclasses. (Issue #873)

* Retain ``release_conn`` state across retries. (Issues #651, #866)

* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to
  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)


1.15.1 (2016-04-11)
-------------------

* Fix packaging to include backports module. (Issue #841)


1.15 (2016-04-06)
-----------------

* Added Retry(raise_on_status=False). (Issue #720)

* Always use setuptools, no more distutils fallback. (Issue #785)

* Dropped support for Python 3.2. (Issue #786)

* Chunked transfer encoding when requesting with ``chunked=True``.
  (Issue #790)

* Fixed regression with IPv6 port parsing. (Issue #801)

* Append SNIMissingWarning messages to allow users to specify it in
  the PYTHONWARNINGS environment variable. (Issue #816)

* Handle unicode headers in Py2. (Issue #818)

* Log certificate when there is a hostname mismatch. (Issue #820)

* Preserve order of request/response headers. (Issue #821)


1.14 (2015-12-29)
-----------------

* contrib: SOCKS proxy support! (Issue #762)

* Fixed AppEngine handling of transfer-encoding header and bug
  in Timeout defaults checking. (Issue #763)


1.13.1 (2015-12-18)
-------------------

* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)


1.13 (2015-12-14)
-----------------

* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)

* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)

* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)

* Close connections more defensively on exception. (Issue #734)

* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without
  repeatedly flushing the decoder, to function better on Jython. (Issue #743)

* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)


1.12 (2015-09-03)
-----------------

* Rely on ``six`` for importing ``httplib`` to work around
  conflicts with other Python 3 shims. (Issue #688)

* Add support for directories of certificate authorities, as supported by
  OpenSSL. (Issue #701)

* New exception: ``NewConnectionError``, raised when we fail to establish
  a new connection, usually ``ECONNREFUSED`` socket error.


1.11 (2015-07-21)
-----------------

* When ``ca_certs`` is given, ``cert_reqs`` defaults to
  ``'CERT_REQUIRED'``. (Issue #650)

* ``pip install urllib3[secure]`` will install Certifi and
  PyOpenSSL as dependencies. (Issue #678)

* Made ``HTTPHeaderDict`` usable as a ``headers`` input value
  (Issues #632, #679)

* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_
  which has an ``AppEngineManager`` for using ``URLFetch`` in a
  Google AppEngine environment. (Issue #664)

* Dev: Added test suite for AppEngine. (Issue #631)

* Fix performance regression when using PyOpenSSL. (Issue #626)

* Passing incorrect scheme (e.g. ``foo://``) will raise
  ``ValueError`` instead of ``AssertionError`` (backwards
  compatible for now, but please migrate). (Issue #640)

* Fix pools not getting replenished when an error occurs during a
  request using ``release_conn=False``. (Issue #644)

* Fix pool-default headers not applying for url-encoded requests
  like GET. (Issue #657)

* log.warning in Python 3 when headers are skipped due to parsing
  errors. (Issue #642)

* Close and discard connections if an error occurs during read.
  (Issue #660)

* Fix host parsing for IPv6 proxies. (Issue #668)

* Separate warning type SubjectAltNameWarning, now issued once
  per host. (Issue #671)

* Fix ``httplib.IncompleteRead`` not getting converted to
  ``ProtocolError`` when using ``HTTPResponse.stream()``
  (Issue #674)

1.10.4 (2015-05-03)
-------------------

* Migrate tests to Tornado 4. (Issue #594)

* Append default warning configuration rather than overwrite.
  (Issue #603)

* Fix streaming decoding regression. (Issue #595)

* Fix chunked requests losing state across keep-alive connections.
  (Issue #599)

* Fix hanging when chunked HEAD response has no body. (Issue #605)


1.10.3 (2015-04-21)
-------------------

* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.
  (Issue #558)

* Fix regression of duplicate header keys being discarded.
  (Issue #563)

* ``Response.stream()`` returns a generator for chunked responses.
  (Issue #560)

* Set upper-bound timeout when waiting for a socket in PyOpenSSL.
  (Issue #585)

* Work on platforms without `ssl` module for plain HTTP requests.
  (Issue #587)

* Stop relying on the stdlib's default cipher list. (Issue #588)


1.10.2 (2015-02-25)
-------------------

* Fix file descriptor leakage on retries. (Issue #548)

* Removed RC4 from default cipher list. (Issue #551)

* Header performance improvements. (Issue #544)

* Fix PoolManager not obeying redirect retry settings. (Issue #553)


1.10.1 (2015-02-10)
-------------------

* Pools can be used as context managers. (Issue #545)

* Don't re-use connections which experienced an SSLError. (Issue #529)

* Don't fail when gzip decoding an empty stream. (Issue #535)

* Add sha256 support for fingerprint verification. (Issue #540)

* Fixed handling of header values containing commas. (Issue #533)


1.10 (2014-12-14)
-----------------

* Disabled SSLv3. (Issue #473)

* Add ``Url.url`` property to return the composed url string. (Issue #394)

* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)

* ``MaxRetryError.reason`` will always be an exception, not string.
  (Issue #481)

* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)

* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)

* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.
  (Issue #496)

* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.
  (Issue #499)

* Close and discard sockets which experienced SSL-related errors.
  (Issue #501)

* Handle ``body`` param in ``.request(...)``. (Issue #513)

* Respect timeout with HTTPS proxy. (Issue #505)

* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)


1.9.1 (2014-09-13)
------------------

* Apply socket arguments before binding. (Issue #427)

* More careful checks if fp-like object is closed. (Issue #435)

* Fixed packaging issues of some development-related files not
  getting included. (Issue #440)

* Allow performing *only* fingerprint verification. (Issue #444)

* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)

* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)

* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.
  (Issue #443)



1.9 (2014-07-04)
----------------

* Shuffled around development-related files. If you're maintaining a distro
  package of urllib3, you may need to tweak things. (Issue #415)

* Unverified HTTPS requests will trigger a warning on the first request. See
  our new `security documentation
  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.
  (Issue #426)

* New retry logic and ``urllib3.util.retry.Retry`` configuration object.
  (Issue #326)

* All raised exceptions should now wrapped in a
  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)

* All errors during a retry-enabled request should be wrapped in
  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions
  which were previously exempt. Underlying error is accessible from the
  ``.reason`` property. (Issue #326)

* ``urllib3.exceptions.ConnectionError`` renamed to
  ``urllib3.exceptions.ProtocolError``. (Issue #326)

* Errors during response read (such as IncompleteRead) are now wrapped in
  ``urllib3.exceptions.ProtocolError``. (Issue #418)

* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.
  (Issue #417)

* Catch read timeouts over SSL connections as
  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)

* Apply socket arguments before connecting. (Issue #427)


1.8.3 (2014-06-23)
------------------

* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)

* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)

* Wrap ``socket.timeout`` exception with
  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)

* Fixed proxy-related bug where connections were being reused incorrectly.
  (Issues #366, #369)

* Added ``socket_options`` keyword parameter which allows to define
  ``setsockopt`` configuration of new sockets. (Issue #397)

* Removed ``HTTPConnection.tcp_nodelay`` in favor of
  ``HTTPConnection.default_socket_options``. (Issue #397)

* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)


1.8.2 (2014-04-17)
------------------

* Fix ``urllib3.util`` not being included in the package.


1.8.1 (2014-04-17)
------------------

* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)

* Don't install ``dummyserver`` into ``site-packages`` as it's only needed
  for the test suite. (Issue #362)

* Added support for specifying ``source_address``. (Issue #352)


1.8 (2014-03-04)
----------------

* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in
  username, and blank ports like 'hostname:').

* New ``urllib3.connection`` module which contains all the HTTPConnection
  objects.

* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor
  signature to a more sensible order. [Backwards incompatible]
  (Issues #252, #262, #263)

* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)

* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which
  returns the number of bytes read so far. (Issue #277)

* Support for platforms without threading. (Issue #289)

* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``
  to allow a pool with no specified port to be considered equal to to an
  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)

* Improved default SSL/TLS settings to avoid vulnerabilities.
  (Issue #309)

* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.
  (Issue #310)

* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests
  will send the entire HTTP request ~200 milliseconds faster; however, some of
  the resulting TCP packets will be smaller. (Issue #254)

* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``
  from the default 64 to 1024 in a single certificate. (Issue #318)

* Headers are now passed and stored as a custom
  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.
  (Issue #329, #333)

* Headers no longer lose their case on Python 3. (Issue #236)

* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA
  certificates on inject. (Issue #332)

* Requests with ``retries=False`` will immediately raise any exceptions without
  wrapping them in ``MaxRetryError``. (Issue #348)

* Fixed open socket leak with SSL-related failures. (Issue #344, #348)


1.7.1 (2013-09-25)
------------------

* Added granular timeout support with new ``urllib3.util.Timeout`` class.
  (Issue #231)

* Fixed Python 3.4 support. (Issue #238)


1.7 (2013-08-14)
----------------

* More exceptions are now pickle-able, with tests. (Issue #174)

* Fixed redirecting with relative URLs in Location header. (Issue #178)

* Support for relative urls in ``Location: ...`` header. (Issue #179)

* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus
  file-like functionality. (Issue #187)

* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will
  skip hostname verification for SSL connections. (Issue #194)

* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a
  generator wrapped around ``.read(...)``. (Issue #198)

* IPv6 url parsing enforces brackets around the hostname. (Issue #199)

* Fixed thread race condition in
  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)

* ``ProxyManager`` requests now include non-default port in ``Host: ...``
  header. (Issue #217)

* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)

* New ``RequestField`` object can be passed to the ``fields=...`` param which
  can specify headers. (Issue #220)

* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.
  (Issue #221)

* Use international headers when posting file names. (Issue #119)

* Improved IPv6 support. (Issue #203)


1.6 (2013-04-25)
----------------

* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)

* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.

* Improved SSL-related code. ``cert_req`` now optionally takes a string like
  "REQUIRED" or "NONE". Same with ``ssl_version`` takes strings like "SSLv23"
  The string values reflect the suffix of the respective constant variable.
  (Issue #130)

* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly
  closed proxy connections and larger read buffers. (Issue #135)

* Ensure the connection is closed if no data is received, fixes connection leak
  on some platforms. (Issue #133)

* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)

* Tests fixed to be compatible with Py26 again. (Issue #125)

* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant
  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)

* Allow an explicit content type to be specified when encoding file fields.
  (Issue #126)

* Exceptions are now pickleable, with tests. (Issue #101)

* Fixed default headers not getting passed in some cases. (Issue #99)

* Treat "content-encoding" header value as case-insensitive, per RFC 2616
  Section 3.5. (Issue #110)

* "Connection Refused" SocketErrors will get retried rather than raised.
  (Issue #92)

* Updated vendored ``six``, no longer overrides the global ``six`` module
  namespace. (Issue #113)

* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding
  the exception that prompted the final retry. If ``reason is None`` then it
  was due to a redirect. (Issue #92, #114)

* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.
  (Issue #149)

* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters
  that are not files. (Issue #111)

* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)

* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or
  against an arbitrary hostname (when connecting by IP or for misconfigured
  servers). (Issue #140)

* Streaming decompression support. (Issue #159)


1.5 (2012-08-02)
----------------

* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug
  logging in urllib3.

* Native full URL parsing (including auth, path, query, fragment) available in
  ``urllib3.util.parse_url(url)``.

* Built-in redirect will switch method to 'GET' if status code is 303.
  (Issue #11)

* ``urllib3.PoolManager`` strips the scheme and host before sending the request
  uri. (Issue #8)

* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,
  based on the Content-Type header, fails.

* Fixed bug with pool depletion and leaking connections (Issue #76). Added
  explicit connection closing on pool eviction. Added
  ``urllib3.PoolManager.clear()``.

* 99% -> 100% unit test coverage.


1.4 (2012-06-16)
----------------

* Minor AppEngine-related fixes.

* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.

* Improved url parsing. (Issue #73)

* IPv6 url support. (Issue #72)


1.3 (2012-03-25)
----------------

* Removed pre-1.0 deprecated API.

* Refactored helpers into a ``urllib3.util`` submodule.

* Fixed multipart encoding to support list-of-tuples for keys with multiple
  values. (Issue #48)

* Fixed multiple Set-Cookie headers in response not getting merged properly in
  Python 3. (Issue #53)

* AppEngine support with Py27. (Issue #61)

* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs
  bytes.


1.2.2 (2012-02-06)
------------------

* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)


1.2.1 (2012-02-05)
------------------

* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)

* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``
  which inherits from ``ValueError``.


1.2 (2012-01-29)
----------------

* Added Python 3 support (tested on 3.2.2)

* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)

* Use ``select.poll`` instead of ``select.select`` for platforms that support
  it.

* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive
  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.

* Fixed ``ImportError`` during install when ``ssl`` module is not available.
  (Issue #41)

* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not
  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)

* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +
  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.
  Added socket-level tests.

* More tests. Achievement Unlocked: 99% Coverage.


1.1 (2012-01-07)
----------------

* Refactored ``dummyserver`` to its own root namespace module (used for
  testing).

* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in
  Py32's ``ssl_match_hostname``. (Issue #25)

* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)

* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue
  #27)

* Fixed timeout-related bugs. (Issues #17, #23)


1.0.2 (2011-11-04)
------------------

* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if
  you're using the object manually. (Thanks pyos)

* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by
  wrapping the access log in a mutex. (Thanks @christer)

* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and
  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.


1.0.1 (2011-10-10)
------------------

* Fixed a bug where the same connection would get returned into the pool twice,
  causing extraneous "HttpConnectionPool is full" log warnings.


1.0 (2011-10-08)
----------------

* Added ``PoolManager`` with LRU expiration of connections (tested and
  documented).
* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works
  with HTTPS proxies).
* Added optional partial-read support for responses when
  ``preload_content=False``. You can now make requests and just read the headers
  without loading the content.
* Made response decoding optional (default on, same as before).
* Added optional explicit boundary string for ``encode_multipart_formdata``.
* Convenience request methods are now inherited from ``RequestMethods``. Old
  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of
  the new ``request(method, url, ...)``.
* Refactored code to be even more decoupled, reusable, and extendable.
* License header added to ``.py`` files.
* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code
  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.
* Embettered all the things!
* Started writing this file.


0.4.1 (2011-07-17)
------------------

* Minor bug fixes, code cleanup.


0.4 (2011-03-01)
----------------

* Better unicode support.
* Added ``VerifiedHTTPSConnection``.
* Added ``NTLMConnectionPool`` in contrib.
* Minor improvements.


0.3.1 (2010-07-13)
------------------

* Added ``assert_host_name`` optional parameter. Now compatible with proxies.


0.3 (2009-12-10)
----------------

* Added HTTPS support.
* Minor bug fixes.
* Refactored, broken backwards compatibility with 0.2.
* API to be treated as stable from this version forward.


0.2 (2008-11-17)
----------------

* Added unit tests.
* Bug fixes.


0.1 (2008-11-16)
----------------

* First release.



  * [urwid-2.0.1](http://urwid.org/) 
About
=====

Urwid is a console user interface library for Python.
It includes many features useful for text console application developers including:

- Applications resize quickly and smoothly
- Automatic, programmable text alignment and wrapping
- Simple markup for setting text attributes within blocks of text
- Powerful list box with programmable content for scrolling all widget types
- Your choice of event loops: Twisted, Glib, Tornado or select-based loop
- Pre-built widgets include edit boxes, buttons, check boxes and radio buttons
- Display modules include raw, curses, and experimental LCD and web displays
- Support for UTF-8, simple 8-bit and CJK encodings
- 256 and 88 color mode support
- Compatible with Python 2.6, 2.7, 3.2+ and PyPy

Home Page:
  http://urwid.org/

Testing
=======

To run tests locally, install & run `tox`. You must have
appropriate Python versions installed to run `tox` for
each of them.

To test code in all Python versions:

.. code:: bash

    tox                    # Test all versions specified in tox.ini:
    tox -e py36            # Test Python 3.6 only
    tox -e py27,py36,pypy  # Test Python 2.7, Python 3.6 & pypy

Contributors
============

- `wardi <//github.com/wardi>`_
- `aszlig <//github.com/aszlig>`_
- `mgiusti <//github.com/mgiusti>`_
- `and3rson <//github.com/and3rson>`_
- `pazz <//github.com/pazz>`_
- `wackywendell <//github.com/wackywendell>`_
- `eevee <//github.com/eevee>`_
- `marienz <//github.com/marienz>`_
- `rndusr <//github.com/rndusr>`_
- `matthijskooijman <//github.com/matthijskooijman>`_
- `Julian <//github.com/Julian>`_
- `techtonik <//github.com/techtonik>`_
- `garrison <//github.com/garrison>`_
- `ivanov <//github.com/ivanov>`_
- `abadger <//github.com/abadger>`_
- `aglyzov <//github.com/aglyzov>`_
- `ismail-s <//github.com/ismail-s>`_
- `horazont <//github.com/horazont>`_
- `robla <//github.com/robla>`_
- `usrlocalben <//github.com/usrlocalben>`_
- `geier <//github.com/geier>`_
- `federicotdn <//github.com/federicotdn>`_
- `jwilk <//github.com/jwilk>`_
- `rr- <//github.com/rr->`_
- `tonycpsu <//github.com/tonycpsu>`_
- `westurner <//github.com/westurner>`_
- `grugq <//github.com/grugq>`_
- `inducer <//github.com/inducer>`_
- `winbornejw <//github.com/winbornejw>`_

  * [vctools-0.1.6.1](https://github.com/bmcollier/vctools/) vctools
=======

Functions for interfacing with VMware vCloud (both private cloud and vCloud Air).

This system is rather rudimentary at the moment, but does provide workable functionality for making remote changes to VApps from within Python. The system also includes a server module, which will launch a Tornado instance and respond to API requests from a web application.

More documentation to follow in later releases.

Contributors
------------

Ben Collier

Legal Notes
-----------

vCloud Suite, vCloud Air and vCloud Director are registered trademarks of VMware, Inc.
  * [vega-2.5.0](http://github.com/vega/ipyvega) 
IPython Vega
============

IPython/Jupyter notebook module for `Vega <https://github.com/vega/vega/>`_
and `Vega-Lite <https://github.com/vega/vega-lite/>`_.
Notebooks with embedded visualizations can be viewed on github and nbviewer.

For more information, see https://github.com/vega/ipyvega.

Installation Notes
------------------
When installing from PyPI, extra steps may be required to enable the Jupyter
notebook extension. For more information, see the
`github page <https://github.com/vega/ipyvega>`_.



  * [vega3-0.13.0](http://github.com/vega/ipyvega/tree/vega3) 
IPython Vega
============

Deprecated. Please use vega instead.

See https://github.com/vega/ipyvega

  * [vine-1.3.0](http://github.com/celery/vine) =====================================================================
 vine - Python Promises
=====================================================================

|build-status| |coverage| |license| |wheel| |pyversion| |pyimp|

:Version: 1.3.0
:Web: https://vine.readthedocs.io/
:Download: https://pypi.org/project/vine/
:Source: http://github.com/celery/vine/
:Keywords: promise, async, future

About
=====


.. |build-status| image:: https://secure.travis-ci.org/celery/vine.png?branch=master
    :alt: Build status
    :target: https://travis-ci.org/celery/vine

.. |coverage| image:: https://codecov.io/github/celery/vine/coverage.svg?branch=master
    :target: https://codecov.io/github/celery/vine?branch=master

.. |license| image:: https://img.shields.io/pypi/l/vine.svg
    :alt: BSD License
    :target: https://opensource.org/licenses/BSD-3-Clause

.. |wheel| image:: https://img.shields.io/pypi/wheel/vine.svg
    :alt: Vine can be installed via wheel
    :target: https://pypi.org/project/vine/

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/vine.svg
    :alt: Supported Python versions.
    :target: https://pypi.org/project/vine/

.. |pyimp| image:: https://img.shields.io/pypi/implementation/vine.svg
    :alt: Support Python implementations.
    :target: https://pypi.org/project/vine/




  * [virtualenv-16.0.0](https://virtualenv.pypa.io/) virtualenv
==========

A tool for creating isolated 'virtual' python environments.

.. image:: https://img.shields.io/pypi/v/virtualenv.svg
  :target: https://pypi.org/project/virtualenv
  :alt: Latest version on PyPi
.. image:: https://img.shields.io/pypi/pyversions/virtualenv.svg
  :target: https://pypi.org/project/virtualenv/
  :alt: Supported Python versions
.. image:: https://dev.azure.com/pypa/virtualenv/_apis/build/status/pypa.virtualenv?branchName=master
  :target: https://dev.azure.com/pypa/virtualenv/_build/latest?definitionId=11&branchName=master
  :alt: Azure Pipelines build status
.. image:: https://readthedocs.org/projects/virtualenv/badge/?version=latest&style=flat-square
  :target: https://virtualenv.readthedocs.io/en/latest/?badge=latest
  :alt: Documentation status
.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
  :target: https://github.com/ambv/black
  :alt: Code style: black

* `Installation <https://virtualenv.pypa.io/en/latest/installation.html>`_
* `Documentation <https://virtualenv.pypa.io/>`_
* `Changelog <https://virtualenv.pypa.io/en/latest/changes.html>`_
* `Issues <https://github.com/pypa/virtualenv/issues>`_
* `PyPI <https://pypi.org/project/virtualenv/>`_
* `Github <https://github.com/pypa/virtualenv>`_
* `User mailing list <http://groups.google.com/group/python-virtualenv>`_
* `Dev mailing list <http://groups.google.com/group/pypa-dev>`_
* User IRC: `#pypa on Freenode <https://webchat.freenode.net/?channels=%23pypa>`_
* Dev IRC: `#pypa-dev on Freenode <https://webchat.freenode.net/?channels=%23pypa-dev>`_


Code of Conduct
---------------

Everyone interacting in the virtualenv project's codebases, issue trackers,
chat rooms, and mailing lists is expected to follow the
`PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/



  * [virtualenv-clone-0.5.3](https://github.com/edwardgeorge/virtualenv-clone) virtualenv cloning script.

[![Build Status](https://travis-ci.org/edwardgeorge/virtualenv-clone.svg?branch=master)](https://travis-ci.org/edwardgeorge/virtualenv-clone)

A script for cloning a non-relocatable virtualenv.

Virtualenv provides a way to make virtualenv's relocatable which could then be
copied as we wanted. However making a virtualenv relocatable this way breaks
the no-site-packages isolation of the virtualenv as well as other aspects that
come with relative paths and `/usr/bin/env` shebangs that may be undesirable.

Also, the .pth and .egg-link rewriting doesn't seem to work as intended. This
attempts to overcome these issues and provide a way to easily clone an
existing virtualenv.

It performs the following:

- copies `sys.argv[1]` dir to `sys.argv[2]`
- updates the hardcoded `VIRTUAL_ENV` variable in the activate script to the
  new repo location. (`--relocatable` doesn't touch this)
- updates the shebangs of the various scripts in bin to the new Python if
  they pointed to the old Python. (version numbering is retained.)

    it can also change `/usr/bin/env python` shebangs to be absolute too,
    though this functionality is not exposed at present.

- checks `sys.path` of the cloned virtualenv and if any of the paths are from
  the old environment it finds any `.pth` or `.egg` link files within sys.path
  located in the new environment and makes sure any absolute paths to the
  old environment are updated to the new environment.

- finally it double checks `sys.path` again and will fail if there are still
  paths from the old environment present.

NOTE: This script requires Python 2.7 or 3.4+



  * [visdom-0.1.8.8](https://github.com/facebookresearch/visdom) # **Visdom**

![visdom_big](https://lh3.googleusercontent.com/-bqH9UXCw-BE/WL2UsdrrbAI/AAAAAAAAnYc/emrxwCmnrW4_CLTyyUttB0SYRJ-i4CCiQCLcB/s0/Screen+Shot+2017-03-06+at+10.51.02+AM.png"visdom_big")

A flexible tool for creating, organizing, and sharing visualizations of live, rich data. Supports Torch and Numpy.

* [Overview](#overview)
* [Concepts](#concepts)
* [Setup](#setup)
* [Usage](#usage)
* [API](#api)
* [To Do](#to-do)
* [Contributing](#contributing)


## Overview

Visdom aims to facilitate visualization of (remote) data with an emphasis on supporting scientific experimentation.

<p align="center"><img src="https://lh3.googleusercontent.com/-h3HuvbU2V0SfgqgXGiK3LPghE5vqvS0pzpObS0YgG_LABMFk62JCa3KVu_2NV_4LJKaAa5-tg=s0" width="500"  /></p>

Broadcast visualizations of plots, images, and text for yourself and your collaborators.

<p align="center"><img src="https://thumbs.gfycat.com/SlipperySecondhandGemsbuck-size_restricted.gif" width="500" /></p>

Organize your visualization space programmatically or through the UI to create dashboards for live data, inspect results of experiments, or debug experimental code.

<p align="center"><img align="center" src="https://lh3.googleusercontent.com/-IHexvZ-FMtk/WLTXBgQlijI/AAAAAAAAm_s/514LM8R1XFgyNKPVMf4tNwYluZsHsC63wCLcB/s0/Screen+Shot+2017-02-27+at+3.15.27+PM.png" width="500" /></p>



 <br/>

## Concepts
Visdom has a simple set of features that can be composed for various use-cases.

### Windows
<p align="center"><img align="center" src="https://lh3.googleusercontent.com/-kLnogsg9RCs/WLx34PEsGWI/AAAAAAAAnSs/7t_62pbfmfoEBnkcbKTXIqz0WM8pQJHVQCLcB/s0/Screen+Shot+2017-03-05+at+3.34.43+PM.png" width="500" /></p>


The UI begins as a blank slate -- you can populate it with plots, images, and text. These appear in windows that you can drag, drop, resize, and destroy. The windows live in `envs` and the state of `envs` is stored across sessions. You can download the content of windows -- including your plots in `svg`.



> **Tip**: You can use the zoom of your browser to adjust the scale of the UI.

##### Callbacks

The python Visdom implementation supports callbacks on a window. The demo shows an example of this in the form of an editable text pad. The functionality of these callbacks allows the Visdom object to receive and react to events that happen in the frontend.

You can subscribe a window to events by adding a function to the event handlers dict for the window id you want to subscribe by calling `viz.register_event_handler(handler, win_id)` with your handler and the window id. Multiple handlers can be registered to the same window. You can remove all event handlers from a window using `viz.clear_event_handlers(win_id)`. When an event occurs to that window, your callbacks will be called on a dict containing:

 - `event_type`: one of the below event types
 - `pane_data`: all of the stored contents for that window including layout and content.
 - `eid`: the current environment id
 - `target`: the window id the event is called on

Additional parameters are defined below.

Right now three callback events are supported:

1. `Close` - Triggers when a window is closed. Returns a dict with only the aforementioned fields.
2. `KeyPress` - Triggers when a key is pressed. Contains additional parameters:
    - `key` - A string representation of the key pressed (applying state modifiers such as SHIFT)
    - `key_code` - The javascript event keycode for the pressed key (no modifiers)
3. `PropertyUpdate` - Triggers when a property is updated in Property pane
    - `propertyId` - Position in properties list
    - `value` - New property value

### Environments
<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34618198-fc63976c-f20b-11e7-9c0d-060132fdb37e.png" width="300" /></p>

You can partition your visualization space with `envs`. By default, every user will have an env called `main`. New envs can be created in the UI or programmatically. The state of envs is chronically saved. Environments are able to keep entirely different pools of plots.

You can access a specific env via url: `http://localhost.com:8097/env/main`. If your server is hosted, you can share this url so others can see your visualizations too.

Environments are automatically hierarchically organized by the first `_`.

#### Selecting Environments
<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34618242-261d55d4-f20c-11e7-820d-c16731248b26.png" width="300" /></p>

From the main page it is possible to toggle between different environments using the environment selector. Selecting a new environment will query the server for the plots that exist in that environment. The environment selector allows for searching and filtering for the new enironment.

#### Comparing Environments

From the main page it is possible to compare different environments using the environment selector. Selecting multiple environments in the check box will query the server for the plots with the same titles in all environments and plot them in a single plot. An additional compare legend pane is created with a number corresponding to each selected environment. Individual plots are updated with legends corresponding to "x_name" where `x` is a number corresponding with the compare legend pane and `name` is the original name in the legend.

> **Note**: The compare envs view is not robust to high throughput data, as the server is responsible for generating the compared content. Do not compare an environment that is receiving a high quantity of updates on any plot, as every update will request regenerating the comparison. If you need to compare two plots that are receiving high quantities of data, have them share the same window on a singular env.

#### Clearing Environments
You can use the eraser button to remove all of the current contents of an environment. This closes the plot windows for that environment but keeps the empty environment for new plots.

#### Managing Environments
<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34618262-3bb635c8-f20c-11e7-9370-9facfde0cfb7.png" width="400" /></p>

Pressing the folder icon opens a dialog that allows you to fork or force save the current environment, or delete any of your existing environments. Use of this feature is fully described in the **State** section.

>**Env Files:**
>Your envs are loaded at initialization of the server, by default from `$HOME/.visdom/`. Custom paths can be passed as a cmd-line argument. Envs are removed by using the delete button or by deleting the corresponding `.json` file from the env dir.

### State
Once you've created a few visualizations, state is maintained. The server automatically caches your visualizations -- if you reload the page, your visualizations reappear.

<p align="center"><img align="center" src="https://lh3.googleusercontent.com/-ZKeFJfMe5S4/WLXebiNgFwI/AAAAAAAAnFI/AH2cGsf40hEWbH6UeclYQcZPS0YZbcayQCLcB/s0/env_fork_2.gif" width="400" /></p>

* **Save:** You can manually do so with the `save` button. This will serialize the env's state (to disk, in JSON), including window positions. You can save an `env` programmatically.
<br/>This is helpful for more sophisticated visualizations in which configuration is meaningful, e.g. a data-rich demo, a model training dashboard, or systematic experimentation. This also makes them easy to share and reuse.


* **Fork:** If you enter a new env name, saving will create a new env -- effectively **forking** the previous env.

> **Tip**: Fork an environment before you begin to make edits to ensure that your changes are saved seperately.

### Filter
You can use the `filter` to dynamically sift through windows present in an env -- just provide a regular expression with which to match titles of window you want to show. This can be helpful in use cases involving an env with many windows e.g. when systematically checking experimental results.

<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34618118-b86cb138-f20b-11e7-834d-b7d7039313f0.png" width="300" /></p>

> **Note**: If you have saved your current view, the view will be restored after clearing the filter.
> <p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34849912-f0693f30-f6f1-11e7-90b6-2a39f83280e8.gif" width="500" /></p>

### Views
<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34618173-e2546f40-f20b-11e7-9969-16267891fb53.png" width="300" /></p>

It is possible to manage the views simply by dragging the tops of windows around, however additional features exist to keep views organized and save common views. View management can be useful for saving and switching between multiple common organizations of your windows.

#### Saving/Deleting Views
Using the folder icon, a dialog window opens where views can be forked in the same way that envs can be. Saving a view will retain the position and sizes of all of the windows in a given environment. Views are saved in `$HOME/.visdom/view/layouts.json` in the visdom filepath.

> **Note**: Saved views are static, and editing a saved view copies that view over to the `current` view where editing can occur.

#### Re-Packing
Using the repack icon (9 boxes), visdom will attempt to pack your windows in a way that they best fit while retaining row/column ordering.

> **Note**: Due to the reliance on row/column ordering and `ReactGridLayout` the final layout might be slightly different than what might be expected. We're working on improving that experience or providing alternatives that give more fine-tuned control.

#### Reloading Views
<p align="center"><img align="center" src="https://user-images.githubusercontent.com/1276867/34621042-9c6c05f6-f215-11e7-92c7-60afe2bf7e1e.gif" width="600" /></p>

Using the view dropdown it is possible to select previously saved views, restoring the locations and sizes of all of the windows within the current environment to the places they were when that view was saved last.

## Setup

Requires Python 2.7/3 (and optionally Torch7)

```bash
# Install Python server and client from pip
# (STABLE VERSION, NOT ALL CURRENT FEATURES ARE SUPPORTED)
pip install visdom

# Install Torch client
# (STABLE VERSION, NOT ALL CURRENT FEATURES ARE SUPPORTED)
luarocks install visdom

```

```bash
# Install visdom from source
pip install -e .
# If the above runs into issues, you can try the below
easy_install .

# Install Torch client from source (from th directory)
luarocks make

```

## Usage

Start the server (probably in a  `screen` or `tmux`) from the command line:

```bash
> visdom
```

Visdom now can be accessed by going to `http://localhost:8097` in your browser, or your own host address if specified.

> The `visdom` command is equivalent to running `python -m visdom.server`.

>If the above does not work, try using an SSH tunnel to your server by adding the following line to your local  `~/.ssh/config`:
```LocalForward 127.0.0.1:8097 127.0.0.1:8097```.

#### Command Line Options

The following options can be provided to the server:

1. `-port` : The port to run the server on.
2. `-hostname` : The hostname to run the server on.
3. `-base_url` : The base server url (default = /).
4. `-env_path` : The path to the serialized session to reload.
5. `-logging_level` : Logging level (default = INFO). Accepts both standard text and numeric logging values.
6. `-readonly` : Flag to start server in readonly mode.
7. `-enable_login` : Flag to setup authentication for the sever, requiring a username and password to login.
8. `-force_new_cookie` : Flag to reset the secure cookie used by the server, invalidating current login cookies.
Requires `-enable_login`.

When `-enable_login` flag is provided, the server asks user to input credentials using terminal prompt. Alternatevily,
you can setup `VISDOM_USE_ENV_CREDENTIALS` env variable, and then provide your username and password via
`VISDOM_USERNAME` and `VISDOM_PASSWORD` env variables without manually interacting with the terminal. This setup
is useful in case if you would like to launch `visdom` server from bash script, or from Jupyter notebook.
```bash
VISDOM_USERNAME=username
VISDOM_PASSWORD=password
VISDOM_USE_ENV_CREDENTIALS=1 visdom -enable_login
```
You can also use `VISDOM_COOKIE` variable to provide cookies value if the cookie file wasn't generated, or the 
flag `-force_new_cookie` was set.

#### Python example
```python
import visdom
import numpy as np
vis = visdom.Visdom()
vis.text('Hello, world!')
vis.image(np.ones((3, 10, 10)))
```

#### Torch example
```lua
require 'image'
vis = require 'visdom'()
vis:text{text = 'Hello, world!'}
vis:image{img = image.fabio()}
```

Some users have reported issues when connecting Lua clients to the Visdom server.
A potential work-around may be to switch off IPv6:
```
vis = require 'visdom'()
vis.ipv6 = false  -- switches off IPv6
vis:text{text = 'Hello, world!'}
```


### Demos

```bash
python example/demo.py
th example/demo1.lua
th example/demo2.lua
```


## API
For a quick introduction into the capabilities of `visdom`, have a look at the `example` directory, or read the details below.

### Visdom Arguments (Python only)
The python visdom client takes a few options:
- `server`: the hostname of your visdom server (default: `'http://localhost'`)
- `port`: the port for your visdom server (default: `8097`)
- `base_url`: the base visdom server url (default: `/`)
- `env`: Default environment to plot to when no `env` is provided (default: `main`)
- `raise_exceptions`: Raise exceptions upon failure rather than printing them (default: `True` (soon))
- `log_to_filename`: If not none, log all plotting and updating events to the given file (append mode) so that they can be replayed later using `replay_log` (default: `None`)
- `use_incoming_socket`: enable use of the socket for receiving events from the web client, allowing user to register callbacks (default: `True`)
- `http_proxy_host`: Deprecated. Use Proxies argument for complete proxy support.
- `http_proxy_port`: Deprecated. Use Proxies argument for complete proxy support.
- `username`: username to use for authentication, if server started with `-enable_login` (default: `None`)
- `password`: password to use for authentication, if server started with `-enable_login` (default: `None`)
- `proxies`: Dictionary mapping protocol to the URL of the proxy (e.g. {`http`: `foo.bar:3128`}) to be used on each Request. (default: `None`)
- `offline`: Flag to run visdom in offline mode, where all requests are logged to file rather than to the server. Requires `log_to_filename` is set. In offline mode, all visdom commands that don't create or update plots will simply return `True`. (default: `False`)

Other options are either currently unused (endpoint, ipv6) or used for internal functionality (send allows the visdom server to replicate events for the lua client).

### Basics
Visdom offers the following basic visualization functions:
- [`vis.image`](#visimage)    : image
- [`vis.images`](#visimages)   : list of images
- [`vis.text`](#vistext)     : arbitrary HTML
- [`vis.properties`](#visproperties)     : properties grid
- [`vis.audio`](#visaudio)    : audio
- [`vis.video`](#visvideo)    : videos
- [`vis.svg`](#vissvg)      : SVG object
- [`vis.matplot`](#vismatplot)  : matplotlib plot
- [`vis.save`](#vissave)     : serialize state server-side

### Plotting
We have wrapped several common plot types to make creating basic visualizations easily. These visualizations are powered by [Plotly](https://plot.ly/).

The following API is currently supported:
- [`vis.scatter`](#visscatter)  : 2D or 3D scatter plots
- [`vis.line`](#visline)     : line plots
- [`vis.stem`](#visstem)     : stem plots
- [`vis.heatmap`](#visheatmap)  : heatmap plots
- [`vis.bar`](#visbar)  : bar graphs
- [`vis.histogram`](#vishistogram) : histograms
- [`vis.boxplot`](#visboxplot)  : boxplots
- [`vis.surf`](#vissurf)     : surface plots
- [`vis.contour`](#viscontour)  : contour plots
- [`vis.quiver`](#visquiver)   : quiver plots
- [`vis.mesh`](#vismesh)     : mesh plots

### Generic Plots
Note that the server API adheres to the Plotly convention of `data` and `layout` objects, such that you can produce your own arbitrary `Plotly` visualizations:

```python
import visdom
vis = visdom.Visdom()

trace = dict(x=[1, 2, 3], y=[4, 5, 6], mode="markers+lines", type='custom',
             marker={'color': 'red', 'symbol': 104, 'size': "10"},
             text=["one", "two", "three"], name='1st Trace')
layout = dict(title="First Plot", xaxis={'title': 'x1'}, yaxis={'title': 'x2'})

vis._send({'data': [trace], 'layout': layout, 'win': 'mywin'})
```

### Others
- [`vis.close`](#visclose)    : close a window by id
- [`vis.delete_env`](#visdelete_env) : delete an environment by env_id
- [`vis.win_exists`](#viswin_exists) : check if a window already exists by id
- [`vis.get_env_list`](#visget_env_list) : get a list of all of the environments on your server
- [`vis.win_hash`](#viswin_hash): get md5 hash of window's contents
- [`vis.get_window_data`](#visget_window_data): get current data for a window
- [`vis.check_connection`](#vischeck_connection): check if the server is connected
- [`vis.replay_log`](#visreplay_log): replay the actions from the provided log file


## Details
![visdom_big](https://lh3.googleusercontent.com/-bqH9UXCw-BE/WL2UsdrrbAI/AAAAAAAAnYc/emrxwCmnrW4_CLTyyUttB0SYRJ-i4CCiQCLcB/s0/Screen+Shot+2017-03-06+at+10.51.02+AM.png"visdom_big")

### Basics

#### vis.image
This function draws an `img`. It takes as input an `CxHxW` tensor `img`
that contains the image.

The following `opts` are supported:

- `jpgquality`: JPG quality (`number` 0-100). If defined image will be saved as JPG to reduce file size. If not defined image will be saved as PNG.
- `caption`: Caption for the image
- `store_history`: Keep all images stored to the same window and attach a slider to the bottom that will let you select the image to view. You must always provide this opt when sending new images to an image with history.

> **Note** You can use alt on an image pane to view the x/y coordinates of the cursor. You can also ctrl-scroll to zoom, alt scroll to pan vertically, and alt-shift scroll to pan horizontally. Double click inside the pane to restore the image to default.


#### vis.images

This function draws a list of `images`. It takes an input `B x C x H x W` tensor or a `list of images` all of the same size. It makes a grid of images of size (B / nrow, nrow).

The following arguments and `opts` are supported:

- `nrow`: Number of images in a row
- `padding`: Padding around the image, equal padding around all 4 sides
- `opts.jpgquality`: JPG quality (`number` 0-100). If defined image will be saved as JPG to reduce file size. If not defined image will be saved as PNG.
- `opts.caption`: Caption for the image

#### vis.text
This function prints text in a  box. You can use this to embed arbitrary HTML.
It takes as input a `text` string.
No specific `opts` are currently supported.

#### vis.properties
This function shows editable properties in a pane. Properties are expected to be a List of Dicts e.g.:
```
    properties = [
        {'type': 'text', 'name': 'Text input', 'value': 'initial'},
        {'type': 'number', 'name': 'Number input', 'value': '12'},
        {'type': 'button', 'name': 'Button', 'value': 'Start'},
        {'type': 'checkbox', 'name': 'Checkbox', 'value': True},
        {'type': 'select', 'name': 'Select', 'value': 1, 'values': ['Red', 'Green', 'Blue']},
    ]
```
Supported types:
 - text: string
 - number: decimal number
 - button: button labeled with "value"
 - checkbox: boolean value rendered as a checkbox
 - select: multiple values select box
    - `value`: id of selected value (zero based)
    - `values`: list of possible values

Callback are called on property value update:
 - `event_type`: `"PropertyUpdate"`
 - `propertyId`: position in the `properties` list
 - `value`: new value

No specific `opts` are currently supported.

#### vis.audio
This function plays audio. It takes as input the filename of the audio
file or an `N` tensor containing the waveform (use an `Nx2` matrix for stereo
audio). The function does not support any plot-specific `opts`.

The following `opts` are supported:

- `opts.sample_frequency`: sample frequency (`integer` > 0; default = 44100)

Known issue: Visdom uses scipy to convert tensor inputs to wave files. Some
versions of Chrome are known not to play these wave files (Firefox and Safari work fine).

#### vis.video
This function plays a video. It takes as input the filename of the video
`videofile` or a `LxHxWxC`-sized
`tensor` containing all the frames of the video as input. The
function does not support any plot-specific `opts`.

The following `opts` are supported:

- `opts.fps`: FPS for the video (`integer` > 0; default = 25)

Note: Using `tensor` input requires that ffmpeg is installed and working.
Your ability to play video may depend on the browser you use: your browser has
to support the Theano codec in an OGG container (Chrome supports this).

#### vis.svg
This function draws an SVG object. It takes as input a SVG string `svgstr` or
the name of an SVG file `svgfile`. The function does not support any specific
`opts`.

#### vis.matplot
This function draws a Matplotlib `plot`. The function supports
one plot-specific option: `resizable`.

> **Note** When set to `True` the plot is resized with the
pane. You need `beautifulsoup4` and `lxml`
packages installed to use this option.

> **Note**: `matplot` is not rendered using the same backend as plotly plots, and is somewhat less efficient. Using too many matplot windows may degrade visdom performance.

#### vis.plotlyplot

This function draws a Plotly `Figure` object. It does not explicitly take options as it assumes you have already explicitly configured the figure's `layout`.

> **Note** You must have the `plotly` Python package installed to use this function. It can typically be installed by running `pip install plotly`.

#### vis.embeddings

This function visualizes a collection of features using the [Barnes-Hut t-SNE algorithm](https://github.com/lvdmaaten/bhtsne).

The function accepts the following arguments:
- `features`: a list of tensors
- `labels`: a list of corresponding labels for the tensors provided for `features`
- `data_getter=fn`: (optional) a function that takes as a parameter an index into the features array and returns a summary representation of the tensor. If this is set, `data_type` must also be set.
- `data_type=str`: (optional) currently the only acceptable value here is `"html"`

We currently assume that there are no more than 10 unique labels, in the future we hope to provide a colormap in opts for other cases.

From the UI you can also draw a lasso around a subset of features. This will rerun the t-SNE visualization on the selected subset.

#### vis.save
This function saves the `envs` that are alive on the visdom server. It takes input a list (in python) or table (in lua) of env ids to be saved.

### Plotting
Further details on the wrapped plotting functions are given below.

The exact inputs into the plotting functions vary, although most of them take as input a tensor `X` than contains the data and an (optional) tensor `Y` that contains optional data variables (such as labels or timestamps). All plotting functions take as input an optional `win` that can be used to plot into a specific window; each plotting function also returns the `win` of the window it plotted in. One can also specify the `env`  to which the visualization should be added.

#### vis.scatter

This function draws a 2D or 3D scatter plot. It takes as input an `Nx2` or
`Nx3` tensor `X` that specifies the locations of the `N` points in the
scatter plot. An optional `N` tensor `Y` containing discrete labels that
range between `1` and `K` can be specified as well -- the labels will be
reflected in the colors of the markers.

`update` can be used to efficiently update the data of an existing plot. Use `'append'` to append data, `'replace'` to use new data, or `'remove'` to remove the trace specified by `name`.
Using `update='append'` will create a plot if it doesn't exist and append to the existing plot otherwise.
If updating a single trace, use `name` to specify the name of the trace to be updated. Update data that is all NaN is ignored (can be used for masking update).


The following `opts` are supported:

- `opts.markersymbol`: marker symbol (`string`; default = `'dot'`)
- `opts.markersize`  : marker size (`number`; default = `'10'`)
- `opts.markercolor` : color per marker. (`torch.*Tensor`; default = `nil`)
- `opts.legend`      : `table` containing legend names
- `opts.textlabels`  : text label for each point (`list`: default = `None`)
- `opts.layoutopts`  : dict of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.
- `opts.traceopts`   : dict mapping trace names or indices to dicts of additional options that the graph backend accepts. For example `traceopts = {'plotly': {'myTrace': {'mode': 'markers'}}}`.
- `opts.webgl`       : use WebGL for plotting (`boolean`; default = `false`). It is faster if a plot contains too many points. Use sparingly as browsers won't allow more than a couple of WebGL contexts on a single page.

`opts.markercolor` is a Tensor with Integer values. The tensor can be of size `N` or `N x 3` or `K` or `K x 3`.

- Tensor of size `N`: Single intensity value per data point. 0 = black, 255 = red
- Tensor of size `N x 3`: Red, Green and Blue intensities per data point. 0,0,0 = black, 255,255,255 = white
- Tensor of size `K` and `K x 3`: Instead of having a unique color per data point, the same color is shared for all points of a particular label.


#### vis.line
This function draws a line plot. It takes as input an `N` or `NxM` tensor
`Y` that specifies the values of the `M` lines (that connect `N` points)
to plot. It also takes an optional `X` tensor that specifies the
corresponding x-axis values; `X` can be an `N` tensor (in which case all
lines will share the same x-axis values) or have the same size as `Y`.

`update` can be used to efficiently update the data of an existing plot. Use 'append' to append data, 'replace' to use new data, or 'remove' to remove the trace specified by `name`. If updating a single trace, use `name` to specify the name of the trace to be updated. Update data that is all NaN is ignored (can be used for masking update).

The following `opts` are supported:

- `opts.fillarea`    : fill area below line (`boolean`)
- `opts.markers`     : show markers (`boolean`; default = `false`)
- `opts.markersymbol`: marker symbol (`string`; default = `'dot'`)
- `opts.markersize`  : marker size (`number`; default = `'10'`)
- `opts.linecolor`   : line colors (`np.array`; default = None)
- `opts.dash`        : line dash type for each line (`np.array`; default = 'solid'), one of `solid`, `dash`, `dashdot` or `dash`, size should match number of lines being drawn
- `opts.legend`      : `table` containing legend names
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.
- `opts.traceopts`   : `dict` mapping trace names or indices to `dict`s of additional options that plot.ly accepts for a trace.
- `opts.webgl`       : use WebGL for plotting (`boolean`; default = `false`). It is faster if a plot contains too many points. Use sparingly as browsers won't allow more than a couple of WebGL contexts on a single page.


#### vis.stem
This function draws a stem plot. It takes as input an `N` or `NxM` tensor
`X` that specifies the values of the `N` points in the `M` time series.
An optional `N` or `NxM` tensor `Y` containing timestamps can be specified
as well; if `Y` is an `N` tensor then all `M` time series are assumed to
have the same timestamps.

The following `opts` are supported:

- `opts.colormap`: colormap (`string`; default = `'Viridis'`)
- `opts.legend`  : `table` containing legend names
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.heatmap
This function draws a heatmap. It takes as input an `NxM` tensor `X` that
specifies the value at each location in the heatmap.

The following `opts` are supported:

- `opts.colormap`   : colormap (`string`; default = `'Viridis'`)
- `opts.xmin`       : clip minimum value (`number`; default = `X:min()`)
- `opts.xmax`       : clip maximum value (`number`; default = `X:max()`)
- `opts.columnnames`: `table` containing x-axis labels
- `opts.rownames`   : `table` containing y-axis labels
- `opts.layoutopts` : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.
- `opts.nancolor`   : color for plotting `NaN`s. If this is `None`, `NaN`s will be plotted as transparent. (`string`; default = `None`)

#### vis.bar
This function draws a regular, stacked, or grouped bar plot. It takes as
input an `N` or `NxM` tensor `X` that specifies the height of each of the
bars. If `X` contains `M` columns, the values corresponding to each row
are either stacked or grouped (depending on how `opts.stacked` is
set). In addition to `X`, an (optional) `N` tensor `Y` can be specified
that contains the corresponding x-axis values.

The following plot-specific `opts` are currently supported:

- `opts.rownames`: `table` containing x-axis labels
- `opts.stacked`    : stack multiple columns in `X`
- `opts.legend`     : `table` containing legend labels
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.histogram
This function draws a histogram of the specified data. It takes as input
an `N` tensor `X` that specifies the data of which to construct the
histogram.

The following plot-specific `opts` are currently supported:

- `opts.numbins`: number of bins (`number`; default = 30)
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.boxplot
This function draws boxplots of the specified data. It takes as input
an `N` or an `NxM` tensor `X` that specifies the `N` data values of which
to construct the `M` boxplots.

The following plot-specific `opts` are currently supported:

- `opts.legend`: labels for each of the columns in `X`
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.surf
This function draws a surface plot. It takes as input an `NxM` tensor `X`
that specifies the value at each location in the surface plot.

The following `opts` are supported:

- `opts.colormap`: colormap (`string`; default = `'Viridis'`)
- `opts.xmin`    : clip minimum value (`number`; default = `X:min()`)
- `opts.xmax`    : clip maximum value (`number`; default = `X:max()`)
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.contour
This function draws a contour plot. It takes as input an `NxM` tensor `X`
that specifies the value at each location in the contour plot.

The following `opts` are supported:

- `opts.colormap`: colormap (`string`; default = `'Viridis'`)
- `opts.xmin`    : clip minimum value (`number`; default = `X:min()`)
- `opts.xmax`    : clip maximum value (`number`; default = `X:max()`)
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.quiver
This function draws a quiver plot in which the direction and length of the
arrows is determined by the `NxM` tensors `X` and `Y`. Two optional `NxM`
tensors `gridX` and `gridY` can be provided that specify the offsets of
the arrows; by default, the arrows will be done on a regular grid.

The following `opts` are supported:

- `opts.normalize`:  length of longest arrows (`number`)
- `opts.arrowheads`: show arrow heads (`boolean`; default = `true`)
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

#### vis.mesh
This function draws a mesh plot from a set of vertices defined in an
`Nx2` or `Nx3` matrix `X`, and polygons defined in an optional `Mx2` or
`Mx3` matrix `Y`.

The following `opts` are supported:

- `opts.color`: color (`string`)
- `opts.opacity`: opacity of polygons (`number` between 0 and 1)
- `opts.layoutopts`  : `dict` of any additional options that the graph backend accepts for a layout. For example `layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}`.

### Customizing plots

The plotting functions take an optional `opts` table as input that can be used to change (generic or plot-specific) properties of the plots. All input arguments are specified in a single table; the input arguments are matches based on the keys they have in the input table.

The following `opts` are generic in the sense that they are the same for all visualizations (except `plot.image`, `plot.text`, `plot.video`, and `plot.audio`):

- `opts.title`       : figure title
- `opts.width`       : figure width
- `opts.height`      : figure height
- `opts.showlegend`  : show legend (`true` or `false`)
- `opts.xtype`       : type of x-axis (`'linear'` or `'log'`)
- `opts.xlabel`      : label of x-axis
- `opts.xtick`       : show ticks on x-axis (`boolean`)
- `opts.xtickmin`    : first tick on x-axis (`number`)
- `opts.xtickmax`    : last tick on x-axis (`number`)
- `opts.xtickvals`   : locations of ticks on x-axis (`table` of `number`s)
- `opts.xticklabels` : ticks labels on x-axis (`table` of `string`s)
- `opts.xtickstep`   : distances between ticks on x-axis (`number`)
- `opts.xtickfont`   : font for x-axis labels (dict of [font information](https://plot.ly/javascript/reference/#layout-font))
- `opts.ytype`       : type of y-axis (`'linear'` or `'log'`)
- `opts.ylabel`      : label of y-axis
- `opts.ytick`       : show ticks on y-axis (`boolean`)
- `opts.ytickmin`    : first tick on y-axis (`number`)
- `opts.ytickmax`    : last tick on y-axis (`number`)
- `opts.ytickvals`   : locations of ticks on y-axis (`table` of `number`s)
- `opts.yticklabels` : ticks labels on y-axis (`table` of `string`s)
- `opts.ytickstep`   : distances between ticks on y-axis (`number`)
- `opts.ytickfont`   : font for y-axis labels (dict of [font information](https://plot.ly/javascript/reference/#layout-font))
- `opts.marginleft`  : left margin (in pixels)
- `opts.marginright` : right margin (in pixels)
- `opts.margintop`   : top margin (in pixels)
- `opts.marginbottom`: bottom margin (in pixels)

The other options are visualization-specific, and are described in the
documentation of the functions.

### Others

#### vis.close

This function closes a specific window. It takes input window id `win` and environment id `eid`. Use `win` as `None` to close all windows in an environment.

#### vis.delete_env

This function deletes a specified env entirely. It takes env id `eid` as input.

> **Note**: `delete_env` is deletes all data for an environment and is IRREVERSIBLE. Do not use unless you absolutely want to remove an environment.


#### vis.fork_env

This function forks an environment, similiar to the UI feature.

Arguments:
- `prev_eid`: Environment ID that we want to fork.
- `eid`: New Environment ID that will be created with the fork.

> **Note**: `fork_env` an exception will occur if an env that doesn't exist is forked.

#### vis.win_exists

This function returns a bool indicating whether or not a window `win` exists on the server already. Returns None if something went wrong.

Optional arguments:
- `env`: Environment to search for the window in. Default is `None`.

#### vis.get_env_list

This function returns a list of all of the environments on the server at the time of calling. It takes no arguments.

#### vis.win_hash

This function returns md5 hash of the contents of a window `win` if it exists on the server. Returns None otherwise.

Optional arguments:
- `env` : Environment to search for the window in. Default is `None`.


#### vis.get_window_data
This function returns the window data for the given window. Returns data for all windows in an env if win is None.

Arguments:
- `env`: Environment to search for the window in.
- `win`: Window to return data for. Set to `None` to retrieve all the windows in an environment.

#### vis.check_connection

This function returns a bool indicating whether or not the server is connected. It accepts an optional argument `timeout_seconds` for a number of seconds to wait for the server to come up.

#### vis.replay_log
This function takes the contents of a visdom log and replays them to the current server to restore a state or handle any missing entries.

Arguments:
- `log_filename`: log file to replay the contents of.

## License
visdom is Creative Commons Attribution-NonCommercial 4.0 International Public licensed, as found in the LICENSE file.

## Contributing
See guidelines for contributing [here.](./CONTRIBUTING.md)

## Acknowledgments
Visdom was inspired by tools like [display](https://github.com/szym/display) and relies on [Plotly](https://plot.ly/) as a plotting front-end.
  * [visitor-0.1.3](http://github.com/mbr/visitor) visitor
=======

A tiny library to facilitate `visitor
<https://en.wikipedia.org/wiki/Visitor_pattern>`_ implementation in Python
(which are slightly peculiar due to dynamic typing). In fact, it is so small,
you may just be better off copy & pasting the source straight into your
project...


Example use
-----------

A simple JSON-encoder:

.. code-block:: python

    from visitor import Visitor


    class JSONEncoder(Visitor):
        def __init__(self):
            self.indent = 0

        def escape_str(self, s):
            # note: this is not a good escape function, do not use this in
            # production!
            s = s.replace('\\', '\\\\')
            s = s.replace('"', '\\"')
            return '"' + s + '"'

        def visit_list(self, node):
            self.indent += 1
            s = '[\n' + '  ' * self.indent
            s += (',\n' + '  ' * self.indent).join(self.visit(item)
                                                   for item in node)
            self.indent -= 1
            s += '\n' + '  ' * self.indent + ']'
            return s

        def visit_str(self, node):
            return self.escape_str(node)

        def visit_int(self, node):
            return str(node)

        def visit_bool(self, node):
            return 'true' if node else 'false'

        def visit_dict(self, node):
            self.indent += 1
            s = '{\n' + '  ' * self.indent
            s += (',\n' + '  ' * self.indent).join(
                '{}: {}'.format(self.escape_str(key), self.visit(value))
                for key, value in sorted(node.items())
            )
            self.indent -= 1
            s += '\n' + '  ' * self.indent + '}'
            return s


    data = [
        'List', 'of', 42, 'items', True, {
            'sub1': 'some string',
            'sub2': {
                'sub2sub1': False,
                'sub2sub2': 123,
            }
        }
    ]

    print(JSONEncoder().visit(data))



Output::

    [
      "List",
      "of",
      42,
      "items",
      true,
      {
        "sub1": "some string",
        "sub2": {
          "sub2sub1": false,
          "sub2sub2": 123
        }
      }
    ]
  * [vistir-0.4.3](https://github.com/sarugaku/vistir) ===============================================================================
vistir: Setup / utilities which most projects eventually need
===============================================================================

.. image:: https://img.shields.io/pypi/v/vistir.svg
    :target: https://pypi.python.org/pypi/vistir

.. image:: https://img.shields.io/pypi/l/vistir.svg
    :target: https://pypi.python.org/pypi/vistir

.. image:: https://travis-ci.com/sarugaku/vistir.svg?branch=master
    :target: https://travis-ci.com/sarugaku/vistir

.. image:: https://img.shields.io/pypi/pyversions/vistir.svg
    :target: https://pypi.python.org/pypi/vistir

.. image:: https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg
    :target: https://saythanks.io/to/techalchemy

.. image:: https://readthedocs.org/projects/vistir/badge/?version=latest
    :target: https://vistir.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://dev.azure.com/sarugaku/vistir/_apis/build/status/Vistir%20Build%20Pipeline?branchName=master
    :target: https://dev.azure.com/sarugaku/vistir/_build/latest?definitionId=2&branchName=master


🐉 Installation
=================

Install from `PyPI`_:

  ::

    $ pipenv install vistir

Install from `Github`_:

  ::

    $ pipenv install -e git+https://github.com/sarugaku/vistir.git#egg=vistir


.. _PyPI: https://www.pypi.org/project/vistir
.. _Github: https://github.com/sarugaku/vistir


.. _`Summary`:

🐉 Summary
===========

**vistir** is a library full of utility functions designed to make life easier. Here are
some of the places where these functions are used:

  * `pipenv`_
  * `requirementslib`_
  * `pip-tools`_
  * `passa`_
  * `pythonfinder`_

.. _passa: https://github.com/sarugaku/passa
.. _pipenv: https://github.com/pypa/pipenv
.. _pip-tools: https://github.com/jazzband/pip-tools
.. _requirementslib: https://github.com/sarugaku/requirementslib
.. _pythonfinder: https://github.com/sarugaku/pythonfinder


.. _`Usage`:

🐉 Usage
==========

Importing a utility
--------------------

You can import utilities directly from **vistir**:

.. code:: python

    from vistir import cd
    with cd('/path/to/somedir'):
        do_stuff_in('somedir')


.. _`Functionality`:

🐉 Functionality
==================

**vistir** provides several categories of functionality, including:

    * Backports
    * Compatibility Shims
    * Context Managers
    * Miscellaneous Utilities
    * Path Utilities

.. note::

   The backports should be imported via ``vistir.compat`` which will provide the
   native versions of the backported items if possible.


🐉 Compatibility Shims
-----------------------

Shims are provided for full API compatibility from python 2.7 through 3.7 for the following:

    * ``weakref.finalize``
    * ``functools.partialmethod`` (via ``vistir.backports.functools.partialmethod``)
    * ``tempfile.TemporaryDirectory`` (via ``vistir.backports.tempfile.TemporaryDirectory``)
    * ``tempfile.NamedTemporaryFile`` (via ``vistir.backports.tempfile.NamedTemporaryFile``)
    * ``vistir.compat.Path``
    * ``vistir.compat.get_terminal_size``
    * ``vistir.compat.JSONDecodeError``
    * ``vistir.compat.ResourceWarning``
    * ``vistir.compat.FileNotFoundError``
    * ``vistir.compat.PermissionError``
    * ``vistir.compat.IsADirectoryError``

The following additional functions are provided for encoding strings to the filesystem
default encoding:

    * ``vistir.compat.fs_str``
    * ``vistir.compat.to_native_string``
    * ``vistir.compat.fs_encode``
    * ``vistir.compat.fs_decode``


🐉 Context Managers
--------------------

**vistir** provides the following context managers as utility contexts:

    * ``vistir.contextmanagers.atomic_open_for_write``
    * ``vistir.contextmanagers.cd``
    * ``vistir.contextmanagers.open_file``
    * ``vistir.contextmanagers.replaced_stream``
    * ``vistir.contextmanagers.replaced_streams``
    * ``vistir.contextmanagers.spinner``
    * ``vistir.contextmanagers.temp_environ``
    * ``vistir.contextmanagers.temp_path``


.. _`atomic_open_for_write`:

**atomic_open_for_write**
///////////////////////////

This context manager ensures that a file only gets overwritten if the contents can be
successfully written in its place.  If you open a file for writing and then fail in the
middle under normal circumstances, your original file is already gone.

.. code:: python

    >>> fn = "test_file.txt"
    >>> with open(fn, "w") as fh:
            fh.write("this is some test text")
    >>> read_test_file()
    this is some test text
    >>> def raise_exception_while_writing(filename):
            with vistir.contextmanagers.atomic_open_for_write(filename) as fh:
                fh.write("Overwriting all the text from before with even newer text")
                raise RuntimeError("But did it get overwritten now?")
    >>> raise_exception_while_writing(fn)
        Traceback (most recent call last):
            ...
        RuntimeError: But did it get overwritten now?
    >>> read_test_file()
    this is some test text


.. _`cd`:

**cd**
///////

A context manager for temporarily changing the working directory.


.. code:: python

    >>> os.path.abspath(os.curdir)
    '/tmp/test'
    >>> with vistir.contextmanagers.cd('/tmp/vistir_test'):
            print(os.path.abspath(os.curdir))
    /tmp/vistir_test


.. _`open_file`:

**open_file**
///////////////

A context manager for streaming file contents, either local or remote. It is recommended
to pair this with an iterator which employs a sensible chunk size.


.. code:: python

    >>> filecontents = b""
        with vistir.contextmanagers.open_file("https://norvig.com/big.txt") as fp:
            for chunk in iter(lambda: fp.read(16384), b""):
                filecontents.append(chunk)
    >>> import io
    >>> import shutil
    >>> filecontents = io.BytesIO(b"")
    >>> with vistir.contextmanagers.open_file("https://norvig.com/big.txt") as fp:
            shutil.copyfileobj(fp, filecontents)


**replaced_stream**
////////////////////

.. _`replaced_stream`:

A context manager to temporarily swap out *stream_name* with a stream wrapper.  This will
capture the stream output and prevent it from being written as normal.

.. code-block:: python

    >>> orig_stdout = sys.stdout
    >>> with replaced_stream("stdout") as stdout:
    ...     sys.stdout.write("hello")
    ...     assert stdout.getvalue() == "hello"
    ...     assert orig_stdout.getvalue() != "hello"

    >>> sys.stdout.write("hello")
    'hello'


.. _`replaced_streams`:

**replaced_streams**
/////////////////////


Temporarily replaces both *sys.stdout* and *sys.stderr* and captures anything written
to these respective targets.


.. code-block:: python

    >>> import sys
    >>> with vistir.contextmanagers.replaced_streams() as streams:
    >>>     stdout, stderr = streams
    >>>     sys.stderr.write("test")
    >>>     sys.stdout.write("hello")
    >>>     assert stdout.getvalue() == "hello"
    >>>     assert stderr.getvalue() == "test"

    >>> stdout.getvalue()
    'hello'

    >>> stderr.getvalue()
    'test'


.. _`spinner`:

**spinner**
////////////

A context manager for wrapping some actions with a threaded, interrupt-safe spinner. The
spinner is fully compatible with all terminals (you can use ``bouncingBar`` on non-utf8
terminals) and will allow you to update the text of the spinner itself by simply setting
``spinner.text`` or write lines to the screen above the spinner by using
``spinner.write(line)``. Success text can be indicated using ``spinner.ok("Text")`` and
failure text can be indicated with ``spinner.fail("Fail text")``.

.. code:: python

    >>> lines = ["a", "b"]
    >>> with vistir.contextmanagers.spinner(spinner_name="dots", text="Running...", handler_map={}, nospin=False) as sp:
            for line in lines:
            sp.write(line + "\n")
            while some_variable = some_queue.pop():
                sp.text = "Consuming item: %s" % some_variable
            if success_condition:
                sp.ok("Succeeded!")
            else:
                sp.fail("Failed!")


.. _`temp_environ`:

**temp_environ**
/////////////////

Sets a temporary environment context to freely manipulate ``os.environ`` which will
be reset upon exiting the context.


.. code:: python

    >>> os.environ['MY_KEY'] = "test"
    >>> os.environ['MY_KEY']
    'test'
    >>> with vistir.contextmanagers.temp_environ():
            os.environ['MY_KEY'] = "another thing"
            print("New key: %s" % os.environ['MY_KEY'])
    New key: another thing
    >>> os.environ['MY_KEY']
    'test'


.. _`temp_path`:

**temp_path**
//////////////

Sets a temporary environment context to freely manipulate ``sys.path`` which will
be reset upon exiting the context.


.. code:: python

    >>> path_from_virtualenv = load_path("/path/to/venv/bin/python")
    >>> print(sys.path)
    ['/home/user/.pyenv/versions/3.7.0/bin', '/home/user/.pyenv/versions/3.7.0/lib/python37.zip', '/home/user/.pyenv/versions/3.7.0/lib/python3.7', '/home/user/.pyenv/versions/3.7.0/lib/python3.7/lib-dynload', '/home/user/.pyenv/versions/3.7.0/lib/python3.7/site-packages']
    >>> with temp_path():
            sys.path = path_from_virtualenv
            # Running in the context of the path above
            run(["pip", "install", "stuff"])
    >>> print(sys.path)
    ['/home/user/.pyenv/versions/3.7.0/bin', '/home/user/.pyenv/versions/3.7.0/lib/python37.zip', '/home/user/.pyenv/versions/3.7.0/lib/python3.7', '/home/user/.pyenv/versions/3.7.0/lib/python3.7/lib-dynload', '/home/user/.pyenv/versions/3.7.0/lib/python3.7/site-packages']


🐉 Cursor Utilities
--------------------------

The following Cursor utilities are available to manipulate the console cursor:

    * ``vistir.cursor.hide_cursor``
    * ``vistir.cursor.show_cursor``


.. _`hide_cursor`:

**hide_cursor**
/////////////////

Hide the console cursor in the given stream.

.. code:: python

    >>> vistir.cursor.hide_cursor(stream=sys.stdout)


.. _`show_cursor`:

**show_cursor**
/////////////////

Show the console cursor in the given stream.

.. code:: python

    >>> vistir.cursor.show_cursor(stream=sys.stdout)


🐉 Miscellaneous Utilities
--------------------------

The following Miscellaneous utilities are available as helper methods:

    * ``vistir.misc.shell_escape``
    * ``vistir.misc.unnest``
    * ``vistir.misc.dedup``
    * ``vistir.misc.run``
    * ``vistir.misc.load_path``
    * ``vistir.misc.partialclass``
    * ``vistir.misc.to_text``
    * ``vistir.misc.to_bytes``
    * ``vistir.misc.divide``
    * ``vistir.misc.take``
    * ``vistir.misc.chunked``
    * ``vistir.misc.decode_for_output``
    * ``vistir.misc.get_canonical_encoding_name``
    * ``vistir.misc.get_wrapped_stream``
    * ``vistir.misc.StreamWrapper``
    * ``vistir.misc.get_text_stream``
    * ``vistir.misc.replace_with_text_stream``
    * ``vistir.misc.get_text_stdin``
    * ``vistir.misc.get_text_stdout``
    * ``vistir.misc.get_text_stderr``
    * ``vistir.misc.echo``


.. _`shell_escape`:

**shell_escape**
/////////////////

Escapes a string for use as shell input when passing *shell=True* to ``os.Popen``.

.. code:: python

    >>> vistir.misc.shell_escape("/tmp/test/test script.py hello")
    '/tmp/test/test script.py hello'


.. _`unnest`:

**unnest**
///////////

Unnests nested iterables into a flattened one.

.. code:: python

    >>> nested_iterable = (1234, (3456, 4398345, (234234)), (2396, (23895750, 9283798, 29384, (289375983275, 293759, 2347, (2098, 7987, 27599)))))
    >>> list(vistir.misc.unnest(nested_iterable))
    [1234, 3456, 4398345, 234234, 2396, 23895750, 9283798, 29384, 289375983275, 293759, 2347, 2098, 7987, 27599]


.. _`dedup`:

**dedup**
//////////

Deduplicates an iterable (like a ``set``, but preserving order).

.. code:: python

    >>> iterable = ["repeatedval", "uniqueval", "repeatedval", "anotherval", "somethingelse"]
    >>> list(vistir.misc.dedup(iterable))
    ['repeatedval', 'uniqueval', 'anotherval', 'somethingelse']

.. _`run`:

**run**
////////

Runs the given command using ``subprocess.Popen`` and passing sane defaults.

.. code:: python

    >>> out, err = vistir.run(["cat", "/proc/version"])
    >>> out
    'Linux version 4.15.0-27-generic (buildd@lgw01-amd64-044) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) #29-Ubuntu SMP Wed Jul 11 08:21:57 UTC 2018'


.. _`load_path`:

**load_path**
//////////////

Load the ``sys.path`` from the given python executable's environment as json.

.. code:: python

    >>> load_path("/home/user/.virtualenvs/requirementslib-5MhGuG3C/bin/python")
    ['', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python37.zip', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/lib-dynload', '/home/user/.pyenv/versions/3.7.0/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/site-packages', '/home/user/git/requirementslib/src']


.. _`partialclass`:

**partialclass**
/////////////////

Create a partially instantiated class.

.. code:: python

    >>> source = partialclass(Source, url="https://pypi.org/simple")
    >>> new_source = source(name="pypi")
    >>> new_source
    <__main__.Source object at 0x7f23af189b38>
    >>> new_source.__dict__
    {'url': 'https://pypi.org/simple', 'verify_ssl': True, 'name': 'pypi'}


.. _`to_text`:

**to_text**
////////////

Convert arbitrary text-formattable input to text while handling errors.

.. code:: python

    >>> vistir.misc.to_text(b"these are bytes")
    'these are bytes'


.. _`to_bytes`:

**to_bytes**
/////////////

Converts arbitrary byte-convertable input to bytes while handling errors.

.. code:: python

    >>> vistir.misc.to_bytes("this is some text")
    b'this is some text'
    >>> vistir.misc.to_bytes(u"this is some text")
    b'this is some text'


.. _`chunked`:

**chunked**
////////////

Splits an iterable up into groups *of the specified length*, per `more itertools`_.  Returns an iterable.

This example will create groups of chunk size **5**, which means there will be *6 groups*.

.. code-block:: python

    >>> chunked_iterable = vistir.misc.chunked(5, range(30))
    >>> for chunk in chunked_iterable:
    ...     add_to_some_queue(chunk)

.. _more itertools: https://more-itertools.readthedocs.io/en/latest/api.html#grouping


.. _`take`:

**take**
/////////

Take elements from the supplied iterable without consuming it.

.. code-block:: python

    >>> iterable = range(30)
    >>> first_10 = take(10, iterable)
    >>> [i for i in first_10]
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

    >>> [i for i in iterable]
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]


.. _`divide`:

**divide**
////////////

Splits an iterable up into the *specified number of groups*, per `more itertools`_.  Returns an iterable.

.. code-block:: python

    >>> iterable = range(30)
    >>> groups = []
    >>> for grp in vistir.misc.divide(3, iterable):
    ...     groups.append(grp)
    >>> groups
    [<tuple_iterator object at 0x7fb7966006a0>, <tuple_iterator object at 0x7fb796652780>, <tuple_iterator object at 0x7fb79650a2b0>]


.. _more itertools: https://more-itertools.readthedocs.io/en/latest/api.html#grouping


.. _`decode_for_output`:

**decode_for_output**
//////////////////////

Converts an arbitrary text input to output which is encoded for printing to terminal
outputs using the system preferred locale using ``locale.getpreferredencoding(False)``
with some additional hackery on linux systems.


.. _`get_canonical_encoding_name`:

**get_canonical_encoding_name**
////////////////////////////////

Given an encoding name, get the canonical name from a codec lookup.

.. code-block:: python

    >>> vistir.misc.get_canonical_encoding_name("utf8")
    "utf-8"


.. _`get_wrapped_stream`:

**get_wrapped_stream**
//////////////////////

Given a stream, wrap it in a `StreamWrapper` instance and return the wrapped stream.

.. code-block:: python

    >>> stream = sys.stdout
    >>> wrapped_stream = vistir.misc.get_wrapped_stream(sys.stdout)
    >>> wrapped_stream.write("unicode\u0141")
    >>> wrapped_stream.seek(0)
    >>> wrapped_stream.read()
    "unicode\u0141"


.. _`StreamWrapper`:

**StreamWrapper**
//////////////////

A stream wrapper and compatibility class for handling wrapping file-like stream objects
which may be used in place of ``sys.stdout`` and other streams.

.. code-block:: python

    >>> wrapped_stream = vistir.misc.StreamWrapper(sys.stdout, encoding="utf-8", errors="replace", line_buffering=True)
    >>> wrapped_stream = vistir.misc.StreamWrapper(io.StringIO(), encoding="utf-8", errors="replace", line_buffering=True)


.. _`get_text_stream`:

**get_text_stream**
////////////////////

An implementation of the **StreamWrapper** for the purpose of wrapping **sys.stdin** or **sys.stdout**.

On Windows, this returns the appropriate handle to the requested output stream.

.. code-block:: python

    >>> text_stream = vistir.misc.get_text_stream("stdout")
    >>> sys.stdout = text_stream
    >>> sys.stdin = vistir.misc.get_text_stream("stdin")
    >>> vistir.misc.echo(u"\0499", fg="green")
    ҙ


.. _`replace_with_text_stream`:

**replace_with_text_stream**
/////////////////////////////

Given a text stream name, replaces the text stream with a **StreamWrapper** instance.


.. code-block:: python

    >>> vistir.misc.replace_with_text_stream("stdout")

Once invoked, the standard stream in question is replaced with the required wrapper,
turning it into a ``TextIOWrapper`` compatible stream (which ensures that unicode
characters can be written to it).


.. _`get_text_stdin`:

**get_text_stdin**
///////////////////

A helper function for calling **get_text_stream("stdin")**.


.. _`get_text_stdout`:

**get_text_stdout**
////////////////////

A helper function for calling **get_text_stream("stdout")**.


.. _`get_text_stderr`:

**get_text_stderr**
////////////////////

A helper function for calling **get_text_stream("stderr")**.


.. _`echo`:

**echo**
/////////

Writes colored, stream-compatible output to the desired handle (``sys.stdout`` by default).

.. code-block:: python

    >>> vistir.misc.echo("some text", fg="green", bg="black", style="bold", err=True)  # write to stderr
    some text
    >>> vistir.misc.echo("some other text", fg="cyan", bg="white", style="underline")  # write to stdout
    some other text


🐉 Path Utilities
------------------

**vistir** provides utilities for interacting with filesystem paths:

    * ``vistir.path.get_converted_relative_path``
    * ``vistir.path.normalize_path``
    * ``vistir.path.is_in_path``
    * ``vistir.path.handle_remove_readonly``
    * ``vistir.path.is_file_url``
    * ``vistir.path.is_readonly_path``
    * ``vistir.path.is_valid_url``
    * ``vistir.path.mkdir_p``
    * ``vistir.path.ensure_mkdir_p``
    * ``vistir.path.create_tracked_tempdir``
    * ``vistir.path.create_tracked_tempfile``
    * ``vistir.path.path_to_url``
    * ``vistir.path.rmtree``
    * ``vistir.path.safe_expandvars``
    * ``vistir.path.set_write_bit``
    * ``vistir.path.url_to_path``
    * ``vistir.path.walk_up``


.. _`normalize_path`:

**normalize_path**
//////////////////

Return a case-normalized absolute variable-expanded path.


.. code:: python

    >>> vistir.path.normalize_path("~/${USER}")
    /home/user/user


.. _`is_in_path`:

**is_in_path**
//////////////

Determine if the provided full path is in the given parent root.


.. code:: python

    >>> vistir.path.is_in_path("~/.pyenv/versions/3.7.1/bin/python", "${PYENV_ROOT}/versions")
    True


.. _`get_converted_relative_path`:

**get_converted_relative_path**
////////////////////////////////

Convert the supplied path to a relative path (relative to ``os.curdir``)


.. code:: python

    >>> os.chdir('/home/user/code/myrepo/myfolder')
    >>> vistir.path.get_converted_relative_path('/home/user/code/file.zip')
    './../../file.zip'
    >>> vistir.path.get_converted_relative_path('/home/user/code/myrepo/myfolder/mysubfolder')
    './mysubfolder'
    >>> vistir.path.get_converted_relative_path('/home/user/code/myrepo/myfolder')
    '.'


.. _`handle_remove_readonly`:

**handle_remove_readonly**
///////////////////////////

Error handler for shutil.rmtree.

Windows source repo folders are read-only by default, so this error handler attempts to
set them as writeable and then proceed with deletion.

This function will call check ``vistir.path.is_readonly_path`` before attempting to
call ``vistir.path.set_write_bit`` on the target path and try again.


.. _`is_file_url`:

**is_file_url**
////////////////

Checks whether the given url is a properly formatted ``file://`` uri.

.. code:: python

    >>> vistir.path.is_file_url('file:///home/user/somefile.zip')
    True
    >>> vistir.path.is_file_url('/home/user/somefile.zip')
    False


.. _`is_readonly_path`:

**is_readonly_path**
/////////////////////

Check if a provided path exists and is readonly by checking for ``bool(path.stat & stat.S_IREAD) and not os.access(path, os.W_OK)``

.. code:: python

    >>> vistir.path.is_readonly_path('/etc/passwd')
    True
    >>> vistir.path.is_readonly_path('/home/user/.bashrc')
    False


.. _`is_valid_url`:

**is_valid_url**
/////////////////

Checks whether a URL is valid and parseable by checking for the presence of a scheme and
a netloc.

.. code:: python

    >>> vistir.path.is_valid_url("https://google.com")
    True
    >>> vistir.path.is_valid_url("/home/user/somefile")
    False


.. _`mkdir_p`:

**mkdir_p**
/////////////

Recursively creates the target directory and all of its parents if they do not
already exist.  Fails silently if they do.

.. code:: python

    >>> os.mkdir('/tmp/test_dir')
    >>> os.listdir('/tmp/test_dir')
    []
    >>> vistir.path.mkdir_p('/tmp/test_dir/child/subchild/subsubchild')
    >>> os.listdir('/tmp/test_dir/child/subchild')
    ['subsubchild']


.. _`ensure_mkdir_p`:

**ensure_mkdir_p**
///////////////////

A decorator which ensures that the caller function's return value is created as a
directory on the filesystem.

.. code:: python

    >>> @ensure_mkdir_p
    def return_fake_value(path):
        return path
    >>> return_fake_value('/tmp/test_dir')
    >>> os.listdir('/tmp/test_dir')
    []
    >>> return_fake_value('/tmp/test_dir/child/subchild/subsubchild')
    >>> os.listdir('/tmp/test_dir/child/subchild')
    ['subsubchild']


.. _`create_tracked_tempdir`:

**create_tracked_tempdir**
////////////////////////////

Creates a tracked temporary directory using ``vistir.path.TemporaryDirectory``, but does
not remove the directory when the return value goes out of scope, instead registers a
handler to cleanup on program exit.

.. code:: python

    >>> temp_dir = vistir.path.create_tracked_tempdir(prefix="test_dir")
    >>> assert temp_dir.startswith("test_dir")
    True
    >>> with vistir.path.create_tracked_tempdir(prefix="test_dir") as temp_dir:
        with io.open(os.path.join(temp_dir, "test_file.txt"), "w") as fh:
            fh.write("this is a test")
    >>> os.listdir(temp_dir)


.. _`create_tracked_tempfile`:

**create_tracked_tempfile**
////////////////////////////

Creates a tracked temporary file using ``vistir.compat.NamedTemporaryFile``, but creates
a ``weakref.finalize`` call which will detach on garbage collection to close and delete
the file.

.. code:: python

    >>> temp_file = vistir.path.create_tracked_tempfile(prefix="requirements", suffix="txt")
    >>> temp_file.write("some\nstuff")
    >>> exit()


.. _`path_to_url`:

**path_to_url**
////////////////

Convert the supplied local path to a file uri.

.. code:: python

    >>> path_to_url("/home/user/code/myrepo/myfile.zip")
    'file:///home/user/code/myrepo/myfile.zip'


.. _`rmtree`:

**rmtree**
///////////

Stand-in for ``shutil.rmtree`` with additional error-handling.

This version of `rmtree` handles read-only paths, especially in the case of index files
written by certain source control systems.

.. code:: python

    >>> vistir.path.rmtree('/tmp/test_dir')
    >>> [d for d in os.listdir('/tmp') if 'test_dir' in d]
    []

.. note::

    Setting `ignore_errors=True` may cause this to silently fail to delete the path


.. _`safe_expandvars`:

**safe_expandvars**
////////////////////

Call ``os.path.expandvars`` if value is a string, otherwise do nothing.

.. code:: python

    >>> os.environ['TEST_VAR'] = "MY_TEST_VALUE"
    >>> vistir.path.safe_expandvars("https://myuser:${TEST_VAR}@myfakewebsite.com")
    'https://myuser:MY_TEST_VALUE@myfakewebsite.com'


.. _`set_write_bit`:

**set_write_bit**
//////////////////

Set read-write permissions for the current user on the target path.  Fail silently
if the path doesn't exist.

.. code:: python

    >>> vistir.path.set_write_bit('/path/to/some/file')
    >>> with open('/path/to/some/file', 'w') as fh:
            fh.write("test text!")


.. _`url_to_path`:

**url_to_path**
////////////////

Convert a valid file url to a local filesystem path. Follows logic taken from pip.

.. code:: python

    >>> vistir.path.url_to_path("file:///home/user/somefile.zip")
    '/home/user/somefile.zip'



  * [voluptuous-0.11.5](https://github.com/alecthomas/voluptuous) # Voluptuous is a Python data validation library

[![Build Status](https://travis-ci.org/alecthomas/voluptuous.svg)](https://travis-ci.org/alecthomas/voluptuous)
[![Coverage Status](https://coveralls.io/repos/github/alecthomas/voluptuous/badge.svg?branch=master)](https://coveralls.io/github/alecthomas/voluptuous?branch=master) [![Gitter chat](https://badges.gitter.im/alecthomas.svg)](https://gitter.im/alecthomas/Lobby)

Voluptuous, *despite* the name, is a Python data validation library. It
is primarily intended for validating data coming into Python as JSON,
YAML, etc.

It has three goals:

1.  Simplicity.
2.  Support for complex data structures.
3.  Provide useful error messages.

## Contact

Voluptuous now has a mailing list! Send a mail to
[<voluptuous@librelist.com>](mailto:voluptuous@librelist.com) to subscribe. Instructions
will follow.

You can also contact me directly via [email](mailto:alec@swapoff.org) or
[Twitter](https://twitter.com/alecthomas).

To file a bug, create a [new issue](https://github.com/alecthomas/voluptuous/issues/new) on GitHub with a short example of how to replicate the issue.

## Documentation

The documentation is provided [here](http://alecthomas.github.io/voluptuous/).

## Changelog

See [CHANGELOG.md](https://github.com/alecthomas/voluptuous/blob/master/CHANGELOG.md).

## Show me an example

Twitter's [user search API](https://dev.twitter.com/rest/reference/get/users/search) accepts
query URLs like:

```
$ curl 'https://api.twitter.com/1.1/users/search.json?q=python&per_page=20&page=1'
```

To validate this we might use a schema like:

```pycon
>>> from voluptuous import Schema
>>> schema = Schema({
...   'q': str,
...   'per_page': int,
...   'page': int,
... })

```

This schema very succinctly and roughly describes the data required by
the API, and will work fine. But it has a few problems. Firstly, it
doesn't fully express the constraints of the API. According to the API,
`per_page` should be restricted to at most 20, defaulting to 5, for
example. To describe the semantics of the API more accurately, our
schema will need to be more thoroughly defined:

```pycon
>>> from voluptuous import Required, All, Length, Range
>>> schema = Schema({
...   Required('q'): All(str, Length(min=1)),
...   Required('per_page', default=5): All(int, Range(min=1, max=20)),
...   'page': All(int, Range(min=0)),
... })

```

This schema fully enforces the interface defined in Twitter's
documentation, and goes a little further for completeness.

"q" is required:

```pycon
>>> from voluptuous import MultipleInvalid, Invalid
>>> try:
...   schema({})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "required key not provided @ data['q']"
True

```

...must be a string:

```pycon
>>> try:
...   schema({'q': 123})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "expected str for dictionary value @ data['q']"
True

```

...and must be at least one character in length:

```pycon
>>> try:
...   schema({'q': ''})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "length of value must be at least 1 for dictionary value @ data['q']"
True
>>> schema({'q': '#topic'}) == {'q': '#topic', 'per_page': 5}
True

```

"per\_page" is a positive integer no greater than 20:

```pycon
>>> try:
...   schema({'q': '#topic', 'per_page': 900})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "value must be at most 20 for dictionary value @ data['per_page']"
True
>>> try:
...   schema({'q': '#topic', 'per_page': -10})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "value must be at least 1 for dictionary value @ data['per_page']"
True

```

"page" is an integer \>= 0:

```pycon
>>> try:
...   schema({'q': '#topic', 'per_page': 'one'})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc)
"expected int for dictionary value @ data['per_page']"
>>> schema({'q': '#topic', 'page': 1}) == {'q': '#topic', 'page': 1, 'per_page': 5}
True

```

## Defining schemas

Schemas are nested data structures consisting of dictionaries, lists,
scalars and *validators*. Each node in the input schema is pattern
matched against corresponding nodes in the input data.

### Literals

Literals in the schema are matched using normal equality checks:

```pycon
>>> schema = Schema(1)
>>> schema(1)
1
>>> schema = Schema('a string')
>>> schema('a string')
'a string'

```

### Types

Types in the schema are matched by checking if the corresponding value
is an instance of the type:

```pycon
>>> schema = Schema(int)
>>> schema(1)
1
>>> try:
...   schema('one')
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "expected int"
True

```

### URLs

URLs in the schema are matched by using `urlparse` library.

```pycon
>>> from voluptuous import Url
>>> schema = Schema(Url())
>>> schema('http://w3.org')
'http://w3.org'
>>> try:
...   schema('one')
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "expected a URL"
True

```

### Lists

Lists in the schema are treated as a set of valid values. Each element
in the schema list is compared to each value in the input data:

```pycon
>>> schema = Schema([1, 'a', 'string'])
>>> schema([1])
[1]
>>> schema([1, 1, 1])
[1, 1, 1]
>>> schema(['a', 1, 'string', 1, 'string'])
['a', 1, 'string', 1, 'string']

```

However, an empty list (`[]`) is treated as is. If you want to specify a list that can
contain anything, specify it as `list`:

```pycon
>>> schema = Schema([])
>>> try:
...   schema([1])
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "not a valid value @ data[1]"
True
>>> schema([])
[]
>>> schema = Schema(list)
>>> schema([])
[]
>>> schema([1, 2])
[1, 2]

```

### Sets and frozensets

Sets and frozensets are treated as a set of valid values. Each element
in the schema set is compared to each value in the input data:

```pycon
>>> schema = Schema({42})
>>> schema({42}) == {42}
True
>>> try:
...   schema({43})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "invalid value in set"
True
>>> schema = Schema({int})
>>> schema({1, 2, 3}) == {1, 2, 3}
True
>>> schema = Schema({int, str})
>>> schema({1, 2, 'abc'}) == {1, 2, 'abc'}
True
>>> schema = Schema(frozenset([int]))
>>> try:
...   schema({3})
...   raise AssertionError('Invalid not raised')
... except Invalid as e:
...   exc = e
>>> str(exc) == 'expected a frozenset'
True

```

However, an empty set (`set()`) is treated as is. If you want to specify a set
that can contain anything, specify it as `set`:

```pycon
>>> schema = Schema(set())
>>> try:
...   schema({1})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "invalid value in set"
True
>>> schema(set()) == set()
True
>>> schema = Schema(set)
>>> schema({1, 2}) == {1, 2}
True

```

### Validation functions

Validators are simple callables that raise an `Invalid` exception when
they encounter invalid data. The criteria for determining validity is
entirely up to the implementation; it may check that a value is a valid
username with `pwd.getpwnam()`, it may check that a value is of a
specific type, and so on.

The simplest kind of validator is a Python function that raises
ValueError when its argument is invalid. Conveniently, many builtin
Python functions have this property. Here's an example of a date
validator:

```pycon
>>> from datetime import datetime
>>> def Date(fmt='%Y-%m-%d'):
...   return lambda v: datetime.strptime(v, fmt)

```

```pycon
>>> schema = Schema(Date())
>>> schema('2013-03-03')
datetime.datetime(2013, 3, 3, 0, 0)
>>> try:
...   schema('2013-03')
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "not a valid value"
True

```

In addition to simply determining if a value is valid, validators may
mutate the value into a valid form. An example of this is the
`Coerce(type)` function, which returns a function that coerces its
argument to the given type:

```python
def Coerce(type, msg=None):
    """Coerce a value to a type.

    If the type constructor throws a ValueError, the value will be marked as
    Invalid.
    """
    def f(v):
        try:
            return type(v)
        except ValueError:
            raise Invalid(msg or ('expected %s' % type.__name__))
    return f

```

This example also shows a common idiom where an optional human-readable
message can be provided. This can vastly improve the usefulness of the
resulting error messages.

### Dictionaries

Each key-value pair in a schema dictionary is validated against each
key-value pair in the corresponding data dictionary:

```pycon
>>> schema = Schema({1: 'one', 2: 'two'})
>>> schema({1: 'one'})
{1: 'one'}

```

#### Extra dictionary keys

By default any additional keys in the data, not in the schema will
trigger exceptions:

```pycon
>>> schema = Schema({2: 3})
>>> try:
...   schema({1: 2, 2: 3})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "extra keys not allowed @ data[1]"
True

```

This behaviour can be altered on a per-schema basis. To allow
additional keys use
`Schema(..., extra=ALLOW_EXTRA)`:

```pycon
>>> from voluptuous import ALLOW_EXTRA
>>> schema = Schema({2: 3}, extra=ALLOW_EXTRA)
>>> schema({1: 2, 2: 3})
{1: 2, 2: 3}

```

To remove additional keys use
`Schema(..., extra=REMOVE_EXTRA)`:

```pycon
>>> from voluptuous import REMOVE_EXTRA
>>> schema = Schema({2: 3}, extra=REMOVE_EXTRA)
>>> schema({1: 2, 2: 3})
{2: 3}

```

It can also be overridden per-dictionary by using the catch-all marker
token `extra` as a key:

```pycon
>>> from voluptuous import Extra
>>> schema = Schema({1: {Extra: object}})
>>> schema({1: {'foo': 'bar'}})
{1: {'foo': 'bar'}}

```

#### Required dictionary keys

By default, keys in the schema are not required to be in the data:

```pycon
>>> schema = Schema({1: 2, 3: 4})
>>> schema({3: 4})
{3: 4}

```

Similarly to how extra\_ keys work, this behaviour can be overridden
per-schema:

```pycon
>>> schema = Schema({1: 2, 3: 4}, required=True)
>>> try:
...   schema({3: 4})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "required key not provided @ data[1]"
True

```

And per-key, with the marker token `Required(key)`:

```pycon
>>> schema = Schema({Required(1): 2, 3: 4})
>>> try:
...   schema({3: 4})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "required key not provided @ data[1]"
True
>>> schema({1: 2})
{1: 2}

```

#### Optional dictionary keys

If a schema has `required=True`, keys may be individually marked as
optional using the marker token `Optional(key)`:

```pycon
>>> from voluptuous import Optional
>>> schema = Schema({1: 2, Optional(3): 4}, required=True)
>>> try:
...   schema({})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "required key not provided @ data[1]"
True
>>> schema({1: 2})
{1: 2}
>>> try:
...   schema({1: 2, 4: 5})
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "extra keys not allowed @ data[4]"
True

```

```pycon
>>> schema({1: 2, 3: 4})
{1: 2, 3: 4}

```

### Recursive / nested schema

You can use `voluptuous.Self` to define a nested schema:

```pycon
>>> from voluptuous import Schema, Self
>>> recursive = Schema({"more": Self, "value": int})
>>> recursive({"more": {"value": 42}, "value": 41}) == {'more': {'value': 42}, 'value': 41}
True

```

### Extending an existing Schema

Often it comes handy to have a base `Schema` that is extended with more
requirements. In that case you can use `Schema.extend` to create a new
`Schema`:

```pycon
>>> from voluptuous import Schema
>>> person = Schema({'name': str})
>>> person_with_age = person.extend({'age': int})
>>> sorted(list(person_with_age.schema.keys()))
['age', 'name']

```

The original `Schema` remains unchanged.

### Objects

Each key-value pair in a schema dictionary is validated against each
attribute-value pair in the corresponding object:

```pycon
>>> from voluptuous import Object
>>> class Structure(object):
...     def __init__(self, q=None):
...         self.q = q
...     def __repr__(self):
...         return '<Structure(q={0.q!r})>'.format(self)
...
>>> schema = Schema(Object({'q': 'one'}, cls=Structure))
>>> schema(Structure(q='one'))
<Structure(q='one')>

```

### Allow None values

To allow value to be None as well, use Any:

```pycon
>>> from voluptuous import Any

>>> schema = Schema(Any(None, int))
>>> schema(None)
>>> schema(5)
5

```

## Error reporting

Validators must throw an `Invalid` exception if invalid data is passed
to them. All other exceptions are treated as errors in the validator and
will not be caught.

Each `Invalid` exception has an associated `path` attribute representing
the path in the data structure to our currently validating value, as well
as an `error_message` attribute that contains the message of the original
exception. This is especially useful when you want to catch `Invalid`
exceptions and give some feedback to the user, for instance in the context of
an HTTP API.


```pycon
>>> def validate_email(email):
...     """Validate email."""
...     if not "@" in email:
...         raise Invalid("This email is invalid.")
...     return email
>>> schema = Schema({"email": validate_email})
>>> exc = None
>>> try:
...     schema({"email": "whatever"})
... except MultipleInvalid as e:
...     exc = e
>>> str(exc)
"This email is invalid. for dictionary value @ data['email']"
>>> exc.path
['email']
>>> exc.msg
'This email is invalid.'
>>> exc.error_message
'This email is invalid.'

```

The `path` attribute is used during error reporting, but also during matching
to determine whether an error should be reported to the user or if the next
match should be attempted. This is determined by comparing the depth of the
path where the check is, to the depth of the path where the error occurred. If
the error is more than one level deeper, it is reported.

The upshot of this is that *matching is depth-first and fail-fast*.

To illustrate this, here is an example schema:

```pycon
>>> schema = Schema([[2, 3], 6])

```

Each value in the top-level list is matched depth-first in-order. Given
input data of `[[6]]`, the inner list will match the first element of
the schema, but the literal `6` will not match any of the elements of
that list. This error will be reported back to the user immediately. No
backtracking is attempted:

```pycon
>>> try:
...   schema([[6]])
...   raise AssertionError('MultipleInvalid not raised')
... except MultipleInvalid as e:
...   exc = e
>>> str(exc) == "not a valid value @ data[0][0]"
True

```

If we pass the data `[6]`, the `6` is not a list type and so will not
recurse into the first element of the schema. Matching will continue on
to the second element in the schema, and succeed:

```pycon
>>> schema([6])
[6]

```

## Multi-field validation

Validation rules that involve multiple fields can be implemented as
custom validators. It's recommended to use `All()` to do a two-pass
validation - the first pass checking the basic structure of the data,
and only after that, the second pass applying your cross-field
validator:

```python
def passwords_must_match(passwords):
    if passwords['password'] != passwords['password_again']:
        raise Invalid('passwords must match')
    return passwords

s=Schema(All(
    # First "pass" for field types
    {'password':str, 'password_again':str},
    # Follow up the first "pass" with your multi-field rules
    passwords_must_match
))

# valid
s({'password':'123', 'password_again':'123'})

# raises MultipleInvalid: passwords must match
s({'password':'123', 'password_again':'and now for something completely different'})

```

With this structure, your multi-field validator will run with
pre-validated data from the first "pass" and so will not have to do
its own type checking on its inputs.

The flipside is that if the first "pass" of validation fails, your
cross-field validator will not run:

```
# raises Invalid because password_again is not a string
# passwords_must_match() will not run because first-pass validation already failed
s({'password':'123', 'password_again': 1337})
```

## Running tests.

Voluptuous is using nosetests:

    $ nosetests


## Why use Voluptuous over another validation library?

**Validators are simple callables**
:   No need to subclass anything, just use a function.

**Errors are simple exceptions.**
:   A validator can just `raise Invalid(msg)` and expect the user to get
useful messages.

**Schemas are basic Python data structures.**
:   Should your data be a dictionary of integer keys to strings?
`{int: str}` does what you expect. List of integers, floats or
strings? `[int, float, str]`.

**Designed from the ground up for validating more than just forms.**
:   Nested data structures are treated in the same way as any other
type. Need a list of dictionaries? `[{}]`

**Consistency.**
:   Types in the schema are checked as types. Values are compared as
values. Callables are called to validate. Simple.

## Other libraries and inspirations

Voluptuous is heavily inspired by
[Validino](http://code.google.com/p/validino/), and to a lesser extent,
[jsonvalidator](http://code.google.com/p/jsonvalidator/) and
[json\_schema](http://blog.sendapatch.se/category/json_schema.html).

[pytest-voluptuous](https://github.com/F-Secure/pytest-voluptuous) is a
[pytest](https://github.com/pytest-dev/pytest) plugin that helps in
using voluptuous validators in `assert`s.

I greatly prefer the light-weight style promoted by these libraries to
the complexity of libraries like FormEncode.
  * [wadllib-1.3.3](https://launchpad.net/wadllib) ..
   Copyright (C) 2008-2013 Canonical Ltd.

   This file is part of wadllib.

   wadllib is free software: you can redistribute it and/or modify it under
   the terms of the GNU Lesser General Public License as published by the
   Free Software Foundation, version 3 of the License.

   wadllib is distributed in the hope that it will be useful, but WITHOUT ANY
   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
   FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for
   more details.

   You should have received a copy of the GNU Lesser General Public License
   along with wadllib. If not, see <http://www.gnu.org/licenses/>.

wadllib
*******

An Application object represents a web service described by a WADL
file.

   >>> import os
   >>> import sys
   >>> import pkg_resources
   >>> from wadllib.application import Application

The first argument to the Application constructor is the URL at which
the WADL file was found. The second argument may be raw WADL markup.

   >>> wadl_string = pkg_resources.resource_string(
   ...     'wadllib.tests.data', 'launchpad-wadl.xml')
   >>> wadl = Application("http://api.launchpad.dev/beta/", wadl_string)

Or the second argument may be an open filehandle containing the markup.

   >>> cleanups = []
   >>> def application_for(filename, url="http://www.example.com/"):
   ...    wadl_stream = pkg_resources.resource_stream(
   ...    'wadllib.tests.data', filename)
   ...    cleanups.append(wadl_stream)
   ...    return Application(url, wadl_stream)
   >>> wadl = application_for("launchpad-wadl.xml",
   ...                        "http://api.launchpad.dev/beta/")


Link navigation
===============

The preferred technique for finding a resource is to start at one of
the resources defined in the WADL file, and follow links. This code
retrieves the definition of the root resource.

   >>> service_root = wadl.get_resource_by_path('')
   >>> service_root.url
   'http://api.launchpad.dev/beta/'
   >>> service_root.type_url
   '#service-root'

The service root resource supports GET.

   >>> get_method = service_root.get_method('get')
   >>> get_method.id
   'service-root-get'

   >>> get_method = service_root.get_method('GET')
   >>> get_method.id
   'service-root-get'

If we want to invoke this method, we send a GET request to the service
root URL.

   >>> get_method.name
   'get'
   >>> get_method.build_request_url()
   'http://api.launchpad.dev/beta/'

The WADL description of a resource knows which representations are
available for that resource. In this case, the server root resource
has a a JSON representation, and it defines parameters like
'people_collection_link', a link to a list of people in Launchpad. We
should be able to use the get_parameter() method to get the WADL
definition of the 'people_collection_link' parameter and find out more
about it--for instance, is it a link to another resource?

   >>> def test_raises(exc_class, method, *args, **kwargs):
   ...     try:
   ...         method(*args, **kwargs)
   ...     except Exception:
   ...         # Contortion to support Python < 2.6 and >= 3 simultaneously.
   ...         e = sys.exc_info()[1]
   ...         if isinstance(e, exc_class):
   ...             print(e)
   ...             return
   ...         raise
   ...     raise Exception("Expected exception %s not raised" % exc_class)

   >>> from wadllib.application import NoBoundRepresentationError
   >>> link_name = 'people_collection_link'
   >>> test_raises(
   ...     NoBoundRepresentationError, service_root.get_parameter, link_name)
   Resource is not bound to any representation, and no media media type was specified.

Oops. The code has no way to know whether 'people_collection_link' is
a parameter of the JSON representation or some other kind of
representation. We can pass a media type to get_parameter and let it
know which representation the parameter lives in.

   >>> link_parameter = service_root.get_parameter(
   ...     link_name, 'application/json')
   >>> test_raises(NoBoundRepresentationError, link_parameter.get_value)
   Resource is not bound to any representation.

Oops again. The parameter is available, but it has no value, because
there's no actual data associated with the resource. The browser can
look up the description of the GET method to make an actual GET
request to the service root, and bind the resulting representation to
the WADL description of the service root.

You can't bind just any representation to a WADL resource description.
It has to be of a media type understood by the WADL description.

   >>> from wadllib.application import UnsupportedMediaTypeError
   >>> test_raises(
   ...     UnsupportedMediaTypeError, service_root.bind,
   ...     '<html>Some HTML</html>', 'text/html')
   This resource doesn't define a representation for media type text/html

The WADL description of the service root resource has a JSON
representation. Here it is.

   >>> json_representation = service_root.get_representation_definition(
   ...     'application/json')
   >>> json_representation.media_type
   'application/json'

We already have a WADL representation of the service root resource, so
let's try binding it to that JSON representation. We use test JSON
data from a file to simulate the result of a GET request to the
service root.

   >>> def get_testdata(filename):
   ...     return pkg_resources.resource_string(
   ...         'wadllib.tests.data', filename + '.json')

   >>> def bind_to_testdata(resource, filename):
   ...     return resource.bind(get_testdata(filename), 'application/json')

The return value is a new Resource object that's "bound" to that JSON
test data.

   >>> bound_service_root = bind_to_testdata(service_root, 'root')
   >>> sorted([param.name for param in bound_service_root.parameters()])
   ['bugs_collection_link', 'people_collection_link']
   >>> sorted(bound_service_root.parameter_names())
   ['bugs_collection_link', 'people_collection_link']
   >>> [method.id for method in bound_service_root.method_iter]
   ['service-root-get']

Now the bound resource object has a JSON representation, and now
'people_collection_link' makes sense. We can follow the
'people_collection_link' to a new Resource object.

   >>> link_parameter = bound_service_root.get_parameter(link_name)
   >>> link_parameter.style
   'plain'
   >>> print(link_parameter.get_value())
   http://api.launchpad.dev/beta/people
   >>> personset_resource = link_parameter.linked_resource
   >>> personset_resource.__class__
   <class 'wadllib.application.Resource'>
   >>> print(personset_resource.url)
   http://api.launchpad.dev/beta/people
   >>> personset_resource.type_url
   'http://api.launchpad.dev/beta/#people'

This new resource is a collection of people.

   >>> personset_resource.id
   'people'

The "collection of people" resource supports a standard GET request as
well as a special GET and an overloaded POST. The get_method() method
is used to retrieve WADL definitions of the possible HTTP requests you
might make. Here's how to get the WADL definition of the standard GET
request.

   >>> get_method = personset_resource.get_method('get')
   >>> get_method.id
   'people-get'

The method name passed into get_method() is treated case-insensitively.

   >>> personset_resource.get_method('GET').id
   'people-get'

To invoke the special GET request, the client sets the 'ws.op' query
parameter to the fixed string 'findPerson'.

   >>> find_method = personset_resource.get_method(
   ...     query_params={'ws.op' : 'findPerson'})
   >>> find_method.id
   'people-findPerson'

Given an end-user's values for the non-fixed parameters, it's possible
to get the URL that should be used to invoke the method.

   >>> print(find_method.build_request_url(text='foo'))
   http://api.launchpad.dev/beta/people?text=foo&ws.op=findPerson

   >>> print(find_method.build_request_url(
   ...     {'ws.op' : 'findPerson', 'text' : 'bar'}))
   http://api.launchpad.dev/beta/people?text=bar&ws.op=findPerson

An error occurs if the end-user gives an incorrect value for a fixed
parameter value, or omits a required parameter.

   >>> find_method.build_request_url()
   Traceback (most recent call last):
   ...
   ValueError: No value for required parameter 'text'

   >>> find_method.build_request_url(
   ...     {'ws.op' : 'findAPerson', 'text' : 'foo'})
   ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
   Traceback (most recent call last):
   ...
   ValueError: Value 'findAPerson' for parameter 'ws.op' conflicts
   with fixed value 'findPerson'

To invoke the overloaded POST request, the client sets the 'ws.op'
query variable to the fixed string 'newTeam':

   >>> create_team_method = personset_resource.get_method(
   ...     'post', representation_params={'ws.op' : 'newTeam'})
   >>> create_team_method.id
   'people-newTeam'

findMethod() returns None when there's no WADL method matching the
name or the fixed parameters.

   >>> print(personset_resource.get_method('nosuchmethod'))
   None

   >>> print(personset_resource.get_method(
   ...     'post', query_params={'ws_op' : 'nosuchparam'}))
   None

Let's say the browser makes a GET request to the person set resource
and gets back a representation. We can bind that representation to our
description of the person set resource.

   >>> bound_personset = bind_to_testdata(personset_resource, 'personset')
   >>> bound_personset.get_parameter("start").get_value()
   0
   >>> bound_personset.get_parameter("total_size").get_value()
   63

We can keep following links indefinitely, so long as we bind to a
representation to each resource as we get it, and use the
representation to find the next link.

   >>> next_page_link = bound_personset.get_parameter("next_collection_link")
   >>> print(next_page_link.get_value())
   http://api.launchpad.dev/beta/people?ws.start=5&ws.size=5
   >>> page_two = next_page_link.linked_resource
   >>> bound_page_two = bind_to_testdata(page_two, 'personset-page2')
   >>> print(bound_page_two.url)
   http://api.launchpad.dev/beta/people?ws.start=5&ws.size=5
   >>> bound_page_two.get_parameter("start").get_value()
   5
   >>> print(bound_page_two.get_parameter("next_collection_link").get_value())
   http://api.launchpad.dev/beta/people?ws.start=10&ws.size=5

Let's say the browser makes a POST request that invokes the 'newTeam'
named operation. The response will include a number of HTTP headers,
including 'Location', which points the way to the newly created team.

   >>> headers = { 'Location' : 'http://api.launchpad.dev/~newteam' }
   >>> response = create_team_method.response.bind(headers)
   >>> location_parameter = response.get_parameter('Location')
   >>> location_parameter.get_value()
   'http://api.launchpad.dev/~newteam'
   >>> new_team = location_parameter.linked_resource
   >>> new_team.url
   'http://api.launchpad.dev/~newteam'
   >>> new_team.type_url
   'http://api.launchpad.dev/beta/#team'

Examining links
---------------

The 'linked_resource' property of a parameter lets you follow a link
to another object. The 'link' property of a parameter lets you examine
links before following them.

    >>> import json
    >>> links_wadl = application_for('links-wadl.xml')
    >>> service_root = links_wadl.get_resource_by_path('')
    >>> representation = json.dumps(
    ...     {'scalar_value': 'foo',
    ...      'known_link': 'http://known/',
    ...      'unknown_link': 'http://unknown/'})
    >>> bound_root = service_root.bind(representation)

    >>> print(bound_root.get_parameter("scalar_value").link)
    None

    >>> known_resource = bound_root.get_parameter("known_link")
    >>> unknown_resource = bound_root.get_parameter("unknown_link")

    >>> print(known_resource.link.can_follow)
    True
    >>> print(unknown_resource.link.can_follow)
    False

A link whose type is unknown is a link to a resource not described by
WADL. Following this link using .linked_resource or .link.follow will
cause a wadllib error. You'll need to follow the link using a general
HTTP library or some other tool.

    >>> known_resource.link.follow
    <wadllib.application.Resource object ...>
    >>> known_resource.linked_resource
    <wadllib.application.Resource object ...>

    >>> from wadllib.application import WADLError
    >>> test_raises(WADLError, getattr, unknown_resource.link, 'follow')
    Cannot follow a link when the target has no WADL
    description. Try using a general HTTP client instead.

    >>> test_raises(WADLError, getattr, unknown_resource, 'linked_resource')
    Cannot follow a link when the target has no WADL
    description. Try using a general HTTP client instead.

Creating a Resource from a representation definition
====================================================

Although every representation is a representation of some HTTP
resource, an HTTP resource doesn't necessarily correspond directly to
a WADL <resource> or <resource_type> tag. Sometimes a representation
is defined within a WADL <method> tag.

   >>> find_method = personset_resource.get_method(
   ...     query_params={'ws.op' : 'find'})
   >>> find_method.id
   'people-find'

   >>> representation_definition = (
   ...     find_method.response.get_representation_definition(
   ...     'application/json'))

There may be no WADL <resource> or <resource_type> tag for the
representation defined here. That's why wadllib makes it possible to
instantiate an anonymous Resource object using only the representation
definition.

   >>> from wadllib.application import Resource
   >>> anonymous_resource = Resource(
   ...     wadl, "http://foo/", representation_definition.tag)

We can bind this resource to a representation, as long as we
explicitly pass in the representation definition.

   >>> anonymous_resource = anonymous_resource.bind(
   ...     get_testdata('personset'), 'application/json',
   ...     representation_definition=representation_definition)

Once the resource is bound to a representation, we can get its
parameter values.

   >>> print(anonymous_resource.get_parameter(
   ...     'total_size', 'application/json').get_value())
   63

Resource instantiation
======================

If you happen to have the URL to an object lying around, and you know
its type, you can construct a Resource object directly instead of
by following links.

   >>> from wadllib.application import Resource
   >>> limi_person = Resource(wadl, "http://api.launchpad.dev/beta/~limi",
   ...     "http://api.launchpad.dev/beta/#person")
   >>> sorted([method.id for method in limi_person.method_iter])[:3]
   ['person-acceptInvitationToBeMemberOf', 'person-addMember', 'person-declineInvitationToBeMemberOf']

   >>> bound_limi = bind_to_testdata(limi_person, 'person-limi')
   >>> sorted(bound_limi.parameter_names())[:3]
   ['admins_collection_link', 'confirmed_email_addresses_collection_link',
    'date_created']
   >>> languages_link = bound_limi.get_parameter("languages_collection_link")
   >>> print(languages_link.get_value())
   http://api.launchpad.dev/beta/~limi/languages

You can bind a Resource to a representation when you create it.

   >>> limi_data = get_testdata('person-limi')
   >>> bound_limi = Resource(
   ...     wadl, "http://api.launchpad.dev/beta/~limi",
   ...     "http://api.launchpad.dev/beta/#person", limi_data,
   ...     "application/json")
   >>> print(bound_limi.get_parameter(
   ...     "languages_collection_link").get_value())
   http://api.launchpad.dev/beta/~limi/languages

By default the representation is treated as a string and processed
according to the media type you pass into the Resource constructor. If
you've already processed the representation, pass in False for the
'representation_needs_processing' argument.

   >>> from wadllib import _make_unicode
   >>> processed_limi_data = json.loads(_make_unicode(limi_data))
   >>> bound_limi = Resource(wadl, "http://api.launchpad.dev/beta/~limi",
   ...     "http://api.launchpad.dev/beta/#person", processed_limi_data,
   ...     "application/json", False)
   >>> print(bound_limi.get_parameter(
   ...     "languages_collection_link").get_value())
   http://api.launchpad.dev/beta/~limi/languages

Most of the time, the representation of a resource is of the type
you'd get by sending a standard GET to that resource. If that's not
the case, you can specify a RepresentationDefinition as the
'representation_definition' argument to bind() or the Resource
constructor, to show what the representation really looks like. Here's
an example.

There's a method on a person resource such as bound_limi that's
identified by a distinctive query argument: ws.op=getMembersByStatus.

   >>> method = bound_limi.get_method(
   ...     query_params={'ws.op' : 'findPathToTeam'})

Invoke this method with a GET request and you'll get back a page from
a list of people.

   >>> people_page_repr_definition = (
   ...     method.response.get_representation_definition('application/json'))
   >>> people_page_repr_definition.tag.attrib['href']
   'http://api.launchpad.dev/beta/#person-page'

As it happens, we have a page from a list of people to use as test data.

   >>> people_page_repr = get_testdata('personset')

If we bind the resource to the result of the method invocation as
happened above, we don't be able to access any of the parameters we'd
expect. wadllib will think the representation is of type
'person-full', the default GET type for bound_limi.

   >>> bad_people_page = bound_limi.bind(people_page_repr)
   >>> print(bad_people_page.get_parameter('total_size'))
   None

Since we don't actually have a 'person-full' representation, we won't
be able to get values for the parameters of that kind of
representation.

   >>> bad_people_page.get_parameter('name').get_value()
   Traceback (most recent call last):
   ...
   KeyError: 'name'

So that's a dead end. *But*, if we pass the correct representation
type into bind(), we can access the parameters associated with a
'person-page' representation.

   >>> people_page = bound_limi.bind(
   ...     people_page_repr,
   ...     representation_definition=people_page_repr_definition)
   >>> people_page.get_parameter('total_size').get_value()
   63

If you invoke the method and ask for a media type other than JSON, you
won't get anything.

   >>> print(method.response.get_representation_definition('text/html'))
   None

Data type conversion
--------------------

The values of date and dateTime parameters are automatically converted to
Python datetime objects.

   >>> data_type_wadl = application_for('data-types-wadl.xml')
   >>> service_root = data_type_wadl.get_resource_by_path('')

   >>> representation = json.dumps(
   ...     {'a_date': '2007-10-20',
   ...      'a_datetime': '2005-06-06T08:59:51.619713+00:00'})
   >>> bound_root = service_root.bind(representation, 'application/json')

   >>> bound_root.get_parameter('a_date').get_value()
   datetime.datetime(2007, 10, 20, 0, 0)
   >>> bound_root.get_parameter('a_datetime').get_value()
   datetime.datetime(2005, 6, 6, 8, ...)

A 'date' field can include a timestamp, and a 'datetime' field can
omit one. wadllib will turn both into datetime objects.

   >>> representation = json.dumps(
   ...     {'a_date': '2005-06-06T08:59:51.619713+00:00',
   ...      'a_datetime': '2007-10-20'})
   >>> bound_root = service_root.bind(representation, 'application/json')

   >>> bound_root.get_parameter('a_datetime').get_value()
   datetime.datetime(2007, 10, 20, 0, 0)
   >>> bound_root.get_parameter('a_date').get_value()
   datetime.datetime(2005, 6, 6, 8, ...)

If a date or dateTime parameter has a null value, you get None. If the
value is a string that can't be parsed to a datetime object, you get a
ValueError.

   >>> representation = json.dumps(
   ...     {'a_date': 'foo', 'a_datetime': None})
   >>> bound_root = service_root.bind(representation, 'application/json')
   >>> bound_root.get_parameter('a_date').get_value()
   Traceback (most recent call last):
   ...
   ValueError: foo
   >>> print(bound_root.get_parameter('a_datetime').get_value())
   None

Representation creation
=======================

You must provide a representation when invoking certain methods. The
representation() method helps you build one without knowing the
details of how a representation is put together.

   >>> create_team_method.build_representation(
   ...     display_name='Joe Bloggs', name='joebloggs')
   ('application/x-www-form-urlencoded', 'display_name=Joe+Bloggs&name=joebloggs&ws.op=newTeam')

The return value of build_representation is a 2-tuple containing the
media type of the built representation, and the string representation
itself. Along with the resource's URL, this is all you need to send
the representation to a web server.

   >>> bound_limi.get_method('patch').build_representation(name='limi2')
   ('application/json', '{"name": "limi2"}')

Representations may require values for certain parameters.

   >>> create_team_method.build_representation()
   Traceback (most recent call last):
   ...
   ValueError: No value for required parameter 'display_name'

   >>> bound_limi.get_method('put').build_representation(name='limi2')
   Traceback (most recent call last):
   ...
   ValueError: No value for required parameter 'mugshot_link'

Some representations may safely include binary data.

   >>> binary_stream = pkg_resources.resource_stream(
   ...     'wadllib.tests.data', 'multipart-binary-wadl.xml')
   >>> cleanups.append(binary_stream)
   >>> binary_wadl = Application(
   ...     "http://www.example.com/", binary_stream)
   >>> service_root = binary_wadl.get_resource_by_path('')

Define a helper that processes the representation the same way
zope.publisher would.

   >>> import cgi
   >>> import io
   >>> def assert_message_parts(media_type, doc, expected):
   ...     if sys.version_info[0] == 3 and sys.version_info[1] < 3:
   ...         # We can't do much due to https://bugs.python.org/issue18013.
   ...         for value in expected:
   ...             if not isinstance(value, bytes):
   ...                 value = value.encode('UTF-8')
   ...             assert value in doc
   ...         return
   ...     environ = {
   ...         'REQUEST_METHOD': 'POST',
   ...         'CONTENT_TYPE': media_type,
   ...         'CONTENT_LENGTH': str(len(doc)),
   ...         }
   ...     kwargs = (
   ...         {'encoding': 'UTF-8'} if sys.version_info[0] >= 3 else {})
   ...     fs = cgi.FieldStorage(
   ...         fp=io.BytesIO(doc), environ=environ, keep_blank_values=1,
   ...         **kwargs)
   ...     values = []
   ...     def append_values(fields):
   ...         for field in fields:
   ...             if field.list:
   ...                 append_values(field.list)
   ...             else:
   ...                 values.append(field.value)
   ...     append_values(fs.list)
   ...     assert values == expected, (
   ...         'Expected %s, got %s' % (expected, values))

   >>> method = service_root.get_method('post', 'multipart/form-data')
   >>> media_type, doc = method.build_representation(
   ...     text_field="text", binary_field=b"\x01\x02\r\x81\r")
   >>> print(media_type)
   multipart/form-data; boundary=...
   >>> assert_message_parts(media_type, doc, ['text', b'\x01\x02\r\x81\r'])

   >>> method = service_root.get_method('post', 'multipart/form-data')
   >>> media_type, doc = method.build_representation(
   ...     text_field="text\n", binary_field=b"\x01\x02\r\x81\n\r")
   >>> print(media_type)
   multipart/form-data; boundary=...
   >>> assert_message_parts(
   ...     media_type, doc, ['text\r\n', b'\x01\x02\r\x81\n\r'])

   >>> method = service_root.get_method('post', 'multipart/form-data')
   >>> media_type, doc = method.build_representation(
   ...     text_field="text\r\nmore\r\n",
   ...     binary_field=b"\x01\x02\r\n\x81\r\x82\n")
   >>> print(media_type)
   multipart/form-data; boundary=...
   >>> assert_message_parts(
   ...     media_type, doc, ['text\r\nmore\r\n', b'\x01\x02\r\n\x81\r\x82\n'])

   >>> method = service_root.get_method('post', 'text/unknown')
   >>> method.build_representation(field="value")
   Traceback (most recent call last):
   ...
   ValueError: Unsupported media type: 'text/unknown'

Options
=======

Some parameters take values from a predefined list of options.

   >>> option_wadl = application_for('options-wadl.xml')
   >>> definitions = option_wadl.representation_definitions
   >>> service_root = option_wadl.get_resource_by_path('')
   >>> definition = definitions['service-root-json']
   >>> param = definition.params(service_root)[0]
   >>> print(param.name)
   has_options
   >>> sorted([option.value for option in param.options])
   ['Value 1', 'Value 2']

Such parameters cannot take values that are not in the list.

   >>> definition.validate_param_values(
   ...     [param], {'has_options': 'Value 1'})
   {'has_options': 'Value 1'}

   >>> definition.validate_param_values(
   ...     [param], {'has_options': 'Invalid value'})
   Traceback (most recent call last):
   ...
   ValueError: Invalid value 'Invalid value' for parameter
   'has_options': valid values are: "Value 1", "Value 2"


Error conditions
================

You'll get None if you try to look up a nonexistent resource.

   >>> print(wadl.get_resource_by_path('nosuchresource'))
   None

You'll get an exception if you try to look up a nonexistent resource
type.

   >>> print(wadl.get_resource_type('#nosuchtype'))
   Traceback (most recent call last):
   KeyError: 'No such XML ID: "#nosuchtype"'

You'll get None if you try to look up a method whose parameters don't
match any defined method.

   >>> print(bound_limi.get_method(
   ...     'post', representation_params={ 'foo' : 'bar' }))
   None

.. cleanup
   >>> for stream in cleanups:
   ...    stream.close()


.. toctree::
   :glob:

   *
   docs/*

================
NEWS for wadllib
================

1.3.3 (2018-07-20)
==================

- Drop support for Python < 2.6.
- Add tox testing support.
- Implement a subset of MIME multipart/form-data encoding locally rather
  than using the standard library's email module, which doesn't have good
  handling of binary parts and corrupts bytes in them that look like line
  endings in various ways depending on the Python version.  [bug=1729754]

1.3.2 (2013-02-25)
==================

- Impose sort order to avoid test failures due to hash randomization.
  LP: #1132125
- Be sure to close streams opened by pkg_resources.resource_stream() to avoid
  test suite complaints.


1.3.1 (2012-03-22)
==================

- Correct the double pass through _from_string causing datetime issues


1.3.0 (2012-01-27)
==================

- Add Python 3 compatibility

- Add the ability to inspect links before following them.

- Ensure that the sample data is packaged.

1.2.0 (2011-02-03)
==================

- It's now possible to examine a link before following it, to see
  whether it has a WADL description or whether it needs to be fetched
  with a general HTTP client.

- It's now possible to iterate over a resource's Parameter objects
  with the .parameters() method.

1.1.8 (2010-10-27)
==================

- This revision contains no code changes, but the build system was
  changed (yet again).  This time to include the version.txt file
  used by setup.py.

1.1.7 (2010-10-26)
==================

- This revision contains no code changes, but the build system was
  changed (again) to include the sample data used in tests.

1.1.6 (2010-10-21)
==================

- This revision contains no code changes, but the build system was
  changed to include the sample data used in tests.

1.1.5 (2010-05-04)
==================

- Fixed a bug (Launchpad bug 274074) that prevented the lookup of
  parameter values in resources associated directly with a
  representation definition (rather than a resource type with a
  representation definition). Bug fix provided by James Westby.

1.1.4 (2009-09-15)
==================

- Fixed a bug that crashed wadllib unless all parameters of a
  multipart representation were provided.

1.1.3 (2009-08-26)
==================

- Remove unnecessary build dependencies.

- Add missing dependencies to setup file.

- Remove sys.path hack from setup.py.

1.1.2 (2009-08-20)
==================

- Consistently handle different versions of simplejson.

1.1.1 (2009-07-14)
==================

- Make wadllib aware of the <option> tags that go beneath <param> tags.

1.1 (2009-07-09)
================

- Make wadllib capable of recognizing and generating
  multipart/form-data representations, including representations that
  incorporate binary parameters.


1.0 (2009-03-23)
================

- Initial release on PyPI
  * [warlock-1.3.3](http://github.com/bcwaldon/warlock) # Warlock — self-validating Python objects using JSON schema

[![PyPI](https://img.shields.io/pypi/v/warlock.svg)][warlock]
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/warlock.svg)][warlock]
[![PyPI - Downloads](https://img.shields.io/pypi/dw/warlock.svg)][pypistats]

[![Build Status](https://travis-ci.org/bcwaldon/warlock.svg?branch=master)][ci-builds]
[![Coverage Status](https://coveralls.io/repos/github/bcwaldon/warlock/badge.svg?branch=master)][coveralls]

## Installation

Warlock is [available on PyPI][warlock]:

```shell
pip install warlock
```

## Usage

1) Create your schema

    ```python
    >>> schema = {
        'name': 'Country',
        'properties': {
            'name': {'type': 'string'},
            'abbreviation': {'type': 'string'},
            'population': {'type': 'integer'},
        },
        'additionalProperties': False,
    }
    ```

2) Create a model

    ```python
    >>> import warlock
    >>> Country = warlock.model_factory(schema)
    ```

3) Create an object using your model

    ```python
    >>> sweden = Country(name='Sweden', abbreviation='SE')
    ```

4) Let the object validate itself

    ```python
    >>> sweden.name = 5
    Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "warlock/core.py", line 53, in __setattr__
        raise InvalidOperation(msg)
    warlock.core.InvalidOperation: Unable to set 'name' to '5'

    >>> sweden.overlord = 'Bears'
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "warlock/core.py", line 53, in __setattr__
        raise InvalidOperation(msg)
    warlock.core.InvalidOperation: Unable to set 'overlord' to 'Bears'
    ```

5) Generate a [JSON Patch document](http://tools.ietf.org/html/draft-ietf-appsawg-json-patch) to track changes

    ```python
    >>> sweden.population=9453000
    >>> sweden.patch
    '[{"path": "/population", "value": 9453000, "op": "add"}]'
    ```

[warlock]: https://pypi.org/project/warlock/
[pip]: https://pip.pypa.io/en/stable/
[ci-builds]: https://travis-ci.org/bcwaldon/warlock
[coveralls]: https://coveralls.io/github/bcwaldon/warlock?branch=master
[pypistats]: https://pypistats.org/packages/warlock
  * [watchdog-0.9.0](http://github.com/gorakhargosh/watchdog) Watchdog
========
Python API and shell utilities to monitor file system events.


Example API Usage
-----------------
A simple program that uses watchdog to monitor directories specified
as command-line arguments and logs events generated:
    
.. code-block:: python

    import sys
    import time
    import logging
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler

    if __name__ == "__main__":
        logging.basicConfig(level=logging.INFO,
                            format='%(asctime)s - %(message)s',
                            datefmt='%Y-%m-%d %H:%M:%S')
        path = sys.argv[1] if len(sys.argv) > 1 else '.'
        event_handler = LoggingEventHandler()
        observer = Observer()
        observer.schedule(event_handler, path, recursive=True)
        observer.start()
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
        observer.join()


Shell Utilities
---------------
Watchdog comes with a utility script called ``watchmedo``.
Please type ``watchmedo --help`` at the shell prompt to
know more about this tool.

Here is how you can log the current directory recursively
for events related only to ``*.py`` and ``*.txt`` files while
ignoring all directory events:
    
.. code-block:: bash

    watchmedo log \
        --patterns="*.py;*.txt" \
        --ignore-directories \
        --recursive \
        .

You can use the ``shell-command`` subcommand to execute shell commands in
response to events:
    
.. code-block:: bash

    watchmedo shell-command \
        --patterns="*.py;*.txt" \
        --recursive \
        --command='echo "${watch_src_path}"' \
        .

Please see the help information for these commands by typing:

.. code-block:: bash

    watchmedo [command] --help


About ``watchmedo`` Tricks
~~~~~~~~~~~~~~~~~~~~~~~~~~
``watchmedo`` can read ``tricks.yaml`` files and execute tricks within them in
response to file system events. Tricks are actually event handlers that
subclass ``watchdog.tricks.Trick`` and are written by plugin authors. Trick
classes are augmented with a few additional features that regular event handlers
don't need.

An example ``tricks.yaml`` file:
    
.. code-block:: yaml

    tricks:
    - watchdog.tricks.LoggerTrick:
        patterns: ["*.py", "*.js"]
    - watchmedo_webtricks.GoogleClosureTrick:
        patterns: ['*.js']
        hash_names: true
        mappings_format: json                  # json|yaml|python
        mappings_module: app/javascript_mappings
        suffix: .min.js
        compilation_level: advanced            # simple|advanced
        source_directory: app/static/js/
        destination_directory: app/public/js/
        files:
          index-page:
          - app/static/js/vendor/jquery*.js
          - app/static/js/base.js
          - app/static/js/index-page.js
          about-page:
          - app/static/js/vendor/jquery*.js
          - app/static/js/base.js
          - app/static/js/about-page/**/*.js

The directory containing the ``tricks.yaml`` file will be monitored. Each trick
class is initialized with its corresponding keys in the ``tricks.yaml`` file as
arguments and events are fed to an instance of this class as they arrive.

Tricks will be included in the 0.5.0 release. I need community input about them.
Please file enhancement requests at the `issue tracker`_.


Installation
------------
Installing from PyPI using ``pip``:
    
.. code-block:: bash

    $ pip install watchdog

Installing from PyPI using ``easy_install``:
    
.. code-block:: bash

    $ easy_install watchdog

Installing from source:
    
.. code-block:: bash

    $ python setup.py install


Installation Caveats
~~~~~~~~~~~~~~~~~~~~
The ``watchmedo`` script depends on PyYAML_ which links with LibYAML_,
which brings a performance boost to the PyYAML parser. However, installing
LibYAML_ is optional but recommended. On Mac OS X, you can use homebrew_
to install LibYAML:

.. code-block:: bash

    $ brew install libyaml

On Linux, use your favorite package manager to install LibYAML. Here's how you
do it on Ubuntu:
    
.. code-block:: bash

    $ sudo aptitude install libyaml-dev

On Windows, please install PyYAML_ using the binaries they provide.

Documentation
-------------
You can browse the latest release documentation_ online.

Contribute
----------
Fork the `repository`_ on GitHub and send a pull request, or file an issue
ticket at the `issue tracker`_. For general help and questions use the official
`mailing list`_ or ask on `stackoverflow`_ with tag `python-watchdog`.

Create and activate your virtual environment, then::

    pip install pytest
    pip install -e .
    py.test tests


Supported Platforms
-------------------
* Linux 2.6 (inotify)
* Mac OS X (FSEvents, kqueue)
* FreeBSD/BSD (kqueue)
* Windows (ReadDirectoryChangesW with I/O completion ports;
  ReadDirectoryChangesW worker threads)
* OS-independent (polling the disk for directory snapshots and comparing them
  periodically; slow and not recommended)

Note that when using watchdog with kqueue, you need the
number of file descriptors allowed to be opened by programs
running on your system to be increased to more than the
number of files that you will be monitoring. The easiest way
to do that is to edit your ``~/.profile`` file and add
a line similar to::

    ulimit -n 1024

This is an inherent problem with kqueue because it uses
file descriptors to monitor files. That plus the enormous
amount of bookkeeping that watchdog needs to do in order
to monitor file descriptors just makes this a painful way
to monitor files and directories. In essence, kqueue is
not a very scalable way to monitor a deeply nested
directory of files and directories with a large number of
files.

About using watchdog with editors like Vim
------------------------------------------
Vim does not modify files unless directed to do so.
It creates backup files and then swaps them in to replace
the files you are editing on the disk. This means that
if you use Vim to edit your files, the on-modified events
for those files will not be triggered by watchdog.
You may need to configure Vim to appropriately to disable
this feature.


Dependencies
------------
1. Python 2.6 or above.
2. pathtools_
3. select_backport_ (select.kqueue replacement for 2.6 on BSD/Mac OS X)
4. XCode_ (only on Mac OS X)
5. PyYAML_ (only for ``watchmedo`` script)
6. argh_ (only for ``watchmedo`` script)


Licensing
---------
Watchdog is licensed under the terms of the `Apache License, version 2.0`_.

Copyright 2011 `Yesudeep Mangalapilly`_.

Copyright 2012 Google, Inc.

Project `source code`_ is available at Github. Please report bugs and file
enhancement requests at the `issue tracker`_.

Why Watchdog?
-------------
Too many people tried to do the same thing and none did what I needed Python
to do:

* pnotify_
* `unison fsmonitor`_
* fsmonitor_
* guard_
* pyinotify_
* `inotify-tools`_
* jnotify_
* treewalker_
* `file.monitor`_
* pyfilesystem_

.. links:
.. _Yesudeep Mangalapilly: yesudeep@gmail.com
.. _source code: http://github.com/gorakhargosh/watchdog
.. _issue tracker: http://github.com/gorakhargosh/watchdog/issues
.. _Apache License, version 2.0: http://www.apache.org/licenses/LICENSE-2.0
.. _documentation: http://packages.python.org/watchdog/
.. _stackoverflow: http://stackoverflow.com/questions/tagged/python-watchdog
.. _mailing list: http://groups.google.com/group/watchdog-python
.. _repository: http://github.com/gorakhargosh/watchdog
.. _issue tracker: http://github.com/gorakhargosh/watchdog/issues

.. _homebrew: http://mxcl.github.com/homebrew/
.. _select_backport: http://pypi.python.org/pypi/select_backport
.. _argh: http://pypi.python.org/pypi/argh
.. _PyYAML: http://www.pyyaml.org/
.. _XCode: http://developer.apple.com/technologies/tools/xcode.html
.. _LibYAML: http://pyyaml.org/wiki/LibYAML
.. _pathtools: http://github.com/gorakhargosh/pathtools

.. _pnotify: http://mark.heily.com/pnotify
.. _unison fsmonitor: https://webdav.seas.upenn.edu/viewvc/unison/trunk/src/fsmonitor.py?view=markup&pathrev=471
.. _fsmonitor: http://github.com/shaurz/fsmonitor
.. _guard: http://github.com/guard/guard
.. _pyinotify: http://github.com/seb-m/pyinotify
.. _inotify-tools: http://github.com/rvoicilas/inotify-tools
.. _jnotify: http://jnotify.sourceforge.net/
.. _treewalker: http://github.com/jbd/treewatcher
.. _file.monitor: http://github.com/pke/file.monitor
.. _pyfilesystem: http://code.google.com/p/pyfilesystem


.. :changelog:

API changes
-----------

0.8.2
~~~~~

- Event emitters are no longer started on schedule if ``Observer`` is not
  already running.


0.8.0
~~~~~

- ``DirectorySnapshot``: methods returning internal stat info replaced by
  ``mtime``, ``inode`` and ``path`` methods.
- ``DirectorySnapshot``: ``walker_callback`` parameter deprecated.
  * [wcwidth-0.1.7](https://github.com/jquast/wcwidth) .. image:: https://img.shields.io/travis/jquast/wcwidth.svg
    :target: https://travis-ci.org/jquast/wcwidth
    :alt: Travis Continous Integration

.. image:: https://img.shields.io/coveralls/jquast/wcwidth.svg
    :target: https://coveralls.io/r/jquast/wcwidth
    :alt: Coveralls Code Coverage

.. image:: https://img.shields.io/pypi/v/wcwidth.svg
    :target: https://pypi.python.org/pypi/wcwidth/
    :alt: Latest Version

.. image:: https://img.shields.io/github/license/jquast/wcwidth.svg
    :target: https://pypi.python.org/pypi/wcwidth/
    :alt: License

.. image:: https://img.shields.io/pypi/wheel/wcwidth.svg
    :alt: Wheel Status

.. image:: https://img.shields.io/pypi/dm/wcwidth.svg
    :target: https://pypi.python.org/pypi/wcwidth/
    :alt: Downloads

============
Introduction
============

This Library is mainly for those implementing a Terminal Emulator, or programs
that carefully produce output to be interpreted by one.

**Problem Statement**: When printed to the screen, the length of the string is
usually equal to the number of cells it occupies.  However, there are
categories of characters that occupy 2 cells (full-wide), and others that
occupy 0.


**Solution**: POSIX.1-2001 and POSIX.1-2008 conforming systems provide
`wcwidth(3)`_ and `wcswidth(3)`_ C functions of which this python module's
functions precisely copy.  *These functions return the number of cells a
unicode string is expected to occupy.*

This library aims to be forward-looking, portable, and most correct.  The most
current release of this API is based on the Unicode Standard release files:

``DerivedGeneralCategory-9.0.0.txt``
  *Date: 2016-06-01, 10:34:26 GMT*
  © 2016 Unicode®, Inc.

``EastAsianWidth-9.0.0.txt``
  *Date: 2016-05-27, 17:00:00 GMT [KW, LI]*
  © 2016 Unicode®, Inc.


Installation
------------

The stable version of this package is maintained on pypi, install using pip::

    pip install wcwidth

Example
-------

To Display ``u'コンニチハ'`` right-adjusted on screen of 80 columns::

    >>> from wcwidth import wcswidth
    >>> text = u'コンニチハ'
    >>> text_len = wcswidth(text)
    >>> print(u' ' * (80 - text_len) + text)

wcwidth, wcswidth
-----------------
Use function ``wcwidth()`` to determine the length of a *single unicode
character*, and ``wcswidth()`` to determine the length of a several, or a
*string of unicode characters*.

Briefly, return values of function ``wcwidth()`` are:

``-1``
  Indeterminate (not printable).

``0``
  Does not advance the cursor, such as NULL or Combining.

``2``
  Characters of category East Asian Wide (W) or East Asian
  Full-width (F) which are displayed using two terminal cells.

``1``
  All others.

Function ``wcswidth()`` simply returns the sum of all values for each character
along a string, or ``-1`` when it occurs anywhere along a string.

More documentation is available using pydoc::

    $ pydoc wcwidth

=======
Caveats
=======

This library attempts to determine the printable width by an unknown targeted
terminal emulator.  It does not provide any ability to discern what the target
emulator software, version, of level of support is.  Results may vary!

A `crude method
<http://blessed.readthedocs.org/en/latest/examples.html#detect-multibyte-py>`_
of determining the level of unicode support by the target emulator may be
performed using the VT100 Query Cursor Position sequence.

The libc version of `wcwidth(3)`_ is often several unicode releases behind,
and therefor several levels of support lower than this python library.  You
may determine an exacting list of these discrepancies using the project
file `wcwidth-libc-comparator.py
<https://github.com/jquast/wcwidth/tree/master/bin/wcwidth-libc-comparator.py>`_.


==========
Developing
==========

Install wcwidth in editable mode::

   pip install -e.

Install developer requirements::

   pip install -r requirements-develop.txt

Execute unit tests using tox::

   tox

Updating Tables
---------------

The command ``python setup.py update`` will fetch the following resources:

- http://www.unicode.org/Public/UNIDATA/EastAsianWidth.txt
- http://www.unicode.org/Public/UNIDATA/extracted/DerivedGeneralCategory.txt

And generates the table files:

- `wcwidth/table_wide.py <https://github.com/jquast/wcwidth/tree/master/wcwidth/table_wide.py>`_
- `wcwidth/table_zero.py <https://github.com/jquast/wcwidth/tree/master/wcwidth/table_zero.py>`_

Uses
----

This library is used in:

- `jquast/blessed`_, a simplified wrapper around curses.

- `jonathanslenders/python-prompt-toolkit`_, a Library for building powerful
  interactive command lines in Python.

Additional tools for displaying and testing wcwidth are found in the `bin/
<https://github.com/jquast/wcwidth/tree/master/bin>`_ folder of this project's
source code.  They are not distributed.

=======
History
=======

0.1.7 *2016-07-01*
  * **Updated** tables to Unicode Specification 9.0.0. (`PR #18`_).

0.1.6 *2016-01-08 Production/Stable*
  * ``LICENSE`` file now included with distribution.

0.1.5 *2015-09-13 Alpha*
  * **Bugfix**:
    Resolution of "combining_ character width" issue, most especially
    those that previously returned -1 now often (correctly) return 0.
    resolved by `Philip Craig`_ via `PR #11`_.
  * **Deprecated**:
    The module path ``wcwidth.table_comb`` is no longer available,
    it has been superseded by module path ``wcwidth.table_zero``.

0.1.4 *2014-11-20 Pre-Alpha*
  * **Feature**: ``wcswidth()`` now determines printable length
    for (most) combining_ characters.  The developer's tool
    `bin/wcwidth-browser.py`_ is improved to display combining_
    characters when provided the ``--combining`` option
    (`Thomas Ballinger`_ and `Leta Montopoli`_ `PR #5`_).
  * **Feature**: added static analysis (prospector_) to testing
    framework.

0.1.3 *2014-10-29 Pre-Alpha*
  * **Bugfix**: 2nd parameter of wcswidth was not honored.
    (`Thomas Ballinger`_, `PR #4`_).

0.1.2 *2014-10-28 Pre-Alpha*
  * **Updated** tables to Unicode Specification 7.0.0.
    (`Thomas Ballinger`_, `PR #3`_).

0.1.1 *2014-05-14 Pre-Alpha*
  * Initial release to pypi, Based on Unicode Specification 6.3.0

This code was originally derived directly from C code of the same name,
whose latest version is available at
http://www.cl.cam.ac.uk/~mgk25/ucs/wcwidth.c::

 * Markus Kuhn -- 2007-05-26 (Unicode 5.0)
 *
 * Permission to use, copy, modify, and distribute this software
 * for any purpose and without fee is hereby granted. The author
 * disclaims all warranties with regard to this software.

.. _`prospector`: https://github.com/landscapeio/prospector
.. _`combining`: https://en.wikipedia.org/wiki/Combining_character
.. _`bin/wcwidth-browser.py`: https://github.com/jquast/wcwidth/tree/master/bin/wcwidth-browser.py
.. _`Thomas Ballinger`: https://github.com/thomasballinger
.. _`Leta Montopoli`: https://github.com/lmontopo
.. _`Philip Craig`: https://github.com/philipc
.. _`PR #3`: https://github.com/jquast/wcwidth/pull/3
.. _`PR #4`: https://github.com/jquast/wcwidth/pull/4
.. _`PR #5`: https://github.com/jquast/wcwidth/pull/5
.. _`PR #11`: https://github.com/jquast/wcwidth/pull/11
.. _`PR #18`: https://github.com/jquast/wcwidth/pull/18
.. _`jquast/blessed`: https://github.com/jquast/blessed
.. _`jonathanslenders/python-prompt-toolkit`: https://github.com/jonathanslenders/python-prompt-toolkit
.. _`wcwidth(3)`:  http://man7.org/linux/man-pages/man3/wcwidth.3.html
.. _`wcswidth(3)`: http://man7.org/linux/man-pages/man3/wcswidth.3.html
  * [webcolors-1.9.1](https://github.com/ubernostrum/webcolors) .. -*-restructuredtext-*-

.. image:: https://travis-ci.org/ubernostrum/webcolors.svg?branch=master
    :target: https://travis-ci.org/ubernostrum/webcolors

``webcolors`` is a Python (2.7, 3.5+) module for working with HTML/CSS
color definitions.

Support is included for normalizing and converting between the
following formats (RGB colorspace only; conversion to/from HSL can be
handled by the ``colorsys`` module in the Python standard library):

* Specification-defined color names

* Six-digit hexadecimal

* Three-digit hexadecimal

* Integer ``rgb()`` triplet

* Percentage ``rgb()`` triplet

For example:

.. code-block:: python

    >>> import webcolors
    >>> webcolors.hex_to_name(u'#daa520')
    u'goldenrod'

Implementations are also provided for the HTML5 color parsing and
serialization algorithms. For example, parsing the infamous
"chucknorris" string into an rgb() triplet:

.. code-block:: python

    >>> import webcolors
    >>> webcolors.html5_parse_legacy_color(u'chucknorris')
    HTML5SimpleColor(red=192, green=0, blue=0)

Full documentation is `available online <https://webcolors.readthedocs.io/>`_.



  * [webencodings-0.5.1](https://github.com/SimonSapin/python-webencodings) python-webencodings
===================

This is a Python implementation of the `WHATWG Encoding standard
<http://encoding.spec.whatwg.org/>`_.

* Latest documentation: http://packages.python.org/webencodings/
* Source code and issue tracker:
  https://github.com/gsnedders/python-webencodings
* PyPI releases: http://pypi.python.org/pypi/webencodings
* License: BSD
* Python 2.6+ and 3.3+

In order to be compatible with legacy web content
when interpreting something like ``Content-Type: text/html; charset=latin1``,
tools need to use a particular set of aliases for encoding labels
as well as some overriding rules.
For example, ``US-ASCII`` and ``iso-8859-1`` on the web are actually
aliases for ``windows-1252``, and an UTF-8 or UTF-16 BOM takes precedence
over any other encoding declaration.
The Encoding standard defines all such details so that implementations do
not have to reverse-engineer each other.

This module has encoding labels and BOM detection,
but the actual implementation for encoders and decoders is Python’s.



  * [websocket-0.2.1](http://pypi.python.org/pypi/websocket) UNKNOWN
  * [whatshap-0.18](https://bitbucket.org/whatshap/whatshap/) .. image:: https://img.shields.io/pypi/v/whatshap.svg?branch=master
    :target: https://pypi.python.org/pypi/whatshap

.. image:: https://semaphoreci.com/api/v1/whatshap/whatshap/branches/master/shields_badge.svg
    :target: https://semaphoreci.com/whatshap/whatshap

.. image:: https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg
    :target: http://bioconda.github.io/recipes/whatshap/README.html

|

.. image:: https://bitbucket.org/repo/8AjxBd/images/543323972-whatshap_logo.png

|

WhatsHap
========

WhatsHap is a software for phasing genomic variants using DNA sequencing
reads, also called *read-based phasing* or *haplotype assembly*. It is
especially suitable for long reads, but works also well with short reads.

Please :ref:`cite us if you use WhatsHap <howtocite>`.


Features
========

  * Very accurate results (Martin et al.,
    `WhatsHap: fast and accurate read-based phasing <https://doi.org/10.1101/085050>`_)
  * Works well with Illumina, PacBio, Oxford Nanopore and other types of reads
  * It phases SNVs, indels and even “complex” variants (such as ``TCG`` → ``AGAA``)
  * Pedigree phasing mode uses reads from related individuals (such as trios)
    to improve results and to reduce coverage requirements
    (Garg et al., `Read-Based Phasing of Related Individuals <https://doi.org/10.1093/bioinformatics/btw276>`_).
  * WhatsHap is :ref:`easy to install <installation>`
  * It is :ref:`easy to use <user-guide>`: Pass in a VCF and one or more BAM files, get out a phased VCF.
    Supports multi-sample VCFs.
  * It produces standard-compliant VCF output by default
  * If desired, get output that is compatible with ReadBackedPhasing
  * Open Source (MIT license)


Documentation
-------------

* `Bitbucket page <https://bitbucket.org/whatshap/whatshap/>`_
* `Read the documentation online <https://whatshap.readthedocs.io/>`_.
  Offline documentation is available in the ``doc/`` subdirectory in the
  repository and in the downloaded tar distribution.


Mailing list
------------
We run a `public mailing list <https://lists.cwi.nl/mailman/listinfo/whatshap>`_. Please
don't hesitate to post questions and comments.



  * [wheel-0.33.4](https://github.com/pypa/wheel) wheel
=====

This library is the reference implementation of the Python wheel packaging
standard, as defined in `PEP 427`_.

It has two different roles:

#. A setuptools_ extension for building wheels that provides the
   ``bdist_wheel`` setuptools command
#. A command line tool for working with wheel files

It should be noted that wheel is **not** intended to be used as a library, and
as such there is no stable, public API.

.. _PEP 427: https://www.python.org/dev/peps/pep-0427/
.. _setuptools: https://pypi.org/project/setuptools/


Code of Conduct
---------------

Everyone interacting in the wheel project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/



  * [widgetsnbextension-3.5.1](http://ipython.org) 
.. image:: https://img.shields.io/pypi/v/widgetsnbextension.svg
   :target: https://pypi.python.org/pypi/widgetsnbextension/
   :alt: Version Number

.. image:: https://img.shields.io/pypi/dm/widgetsnbextension.svg
   :target: https://pypi.python.org/pypi/widgetsnbextension/
   :alt: Number of PyPI downloads

Interactive HTML Widgets
========================

Interactive HTML widgets for Jupyter notebooks.

Usage
=====

Install the corresponding package for your kernel.  i.e. Python users would also
install `ipywidgets`.  Refer to that package's documentation for usage
instructions.



  * [wrapt-1.11.2](https://github.com/GrahamDumpleton/wrapt) wrapt
=====

|Travis| |AppVeyor| |Coveralls| |PyPI|

The aim of the **wrapt** module is to provide a transparent object proxy
for Python, which can be used as the basis for the construction of function
wrappers and decorator functions.

The **wrapt** module focuses very much on correctness. It therefore goes
way beyond existing mechanisms such as ``functools.wraps()`` to ensure that
decorators preserve introspectability, signatures, type checking abilities
etc. The decorators that can be constructed using this module will work in
far more scenarios than typical decorators and provide more predictable and
consistent behaviour.

To ensure that the overhead is as minimal as possible, a C extension module
is used for performance critical components. An automatic fallback to a
pure Python implementation is also provided where a target system does not
have a compiler to allow the C extension to be compiled.

Documentation
-------------

For further information on the **wrapt** module see:

* http://wrapt.readthedocs.org/

Quick Start
-----------

To implement your decorator you need to first define a wrapper function.
This will be called each time a decorated function is called. The wrapper
function needs to take four positional arguments:

* ``wrapped`` - The wrapped function which in turns needs to be called by your wrapper function.
* ``instance`` - The object to which the wrapped function was bound when it was called.
* ``args`` - The list of positional arguments supplied when the decorated function was called.
* ``kwargs`` - The dictionary of keyword arguments supplied when the decorated function was called.

The wrapper function would do whatever it needs to, but would usually in
turn call the wrapped function that is passed in via the ``wrapped``
argument.

The decorator ``@wrapt.decorator`` then needs to be applied to the wrapper
function to convert it into a decorator which can in turn be applied to
other functions.

::

    import wrapt
    
    @wrapt.decorator
    def pass_through(wrapped, instance, args, kwargs):
        return wrapped(*args, **kwargs)

    @pass_through
    def function():
        pass

If you wish to implement a decorator which accepts arguments, then wrap the
definition of the decorator in a function closure. Any arguments supplied
to the outer function when the decorator is applied, will be available to
the inner wrapper when the wrapped function is called.

::

    import wrapt

    def with_arguments(myarg1, myarg2):
        @wrapt.decorator
        def wrapper(wrapped, instance, args, kwargs):
            return wrapped(*args, **kwargs)
        return wrapper

    @with_arguments(1, 2)
    def function():
        pass

When applied to a normal function or static method, the wrapper function
when called will be passed ``None`` as the ``instance`` argument.

When applied to an instance method, the wrapper function when called will
be passed the instance of the class the method is being called on as the
``instance`` argument. This will be the case even when the instance method
was called explicitly via the class and the instance passed as the first
argument. That is, the instance will never be passed as part of ``args``.

When applied to a class method, the wrapper function when called will be
passed the class type as the ``instance`` argument.

When applied to a class, the wrapper function when called will be passed
``None`` as the ``instance`` argument. The ``wrapped`` argument in this
case will be the class.

The above rules can be summarised with the following example.

::

    import inspect
    
    @wrapt.decorator
    def universal(wrapped, instance, args, kwargs):
        if instance is None:
            if inspect.isclass(wrapped):
                # Decorator was applied to a class.
                return wrapped(*args, **kwargs)
            else:
                # Decorator was applied to a function or staticmethod.
                return wrapped(*args, **kwargs)
        else:
            if inspect.isclass(instance):
                # Decorator was applied to a classmethod.
                return wrapped(*args, **kwargs)
            else:
                # Decorator was applied to an instancemethod.
                return wrapped(*args, **kwargs)

Using these checks it is therefore possible to create a universal decorator
that can be applied in all situations. It is no longer necessary to create
different variants of decorators for normal functions and instance methods,
or use additional wrappers to convert a function decorator into one that
will work for instance methods.

In all cases, the wrapped function passed to the wrapper function is called
in the same way, with ``args`` and ``kwargs`` being passed. The
``instance`` argument doesn't need to be used in calling the wrapped
function.

Repository
----------

Full source code for the **wrapt** module, including documentation files
and unit tests, can be obtained from github.

* https://github.com/GrahamDumpleton/wrapt

.. |Travis| image:: https://travis-ci.org/GrahamDumpleton/wrapt.svg?branch=develop
   :target: https://travis-ci.org/GrahamDumpleton/wrapt
.. |Appveyor| image:: https://ci.appveyor.com/api/projects/status/32r7s2skrgm9ubva?svg=true
   :target: https://ci.appveyor.com/project/GrahamDumpleton/wrapt/branch/develop
.. |Coveralls| image:: https://img.shields.io/coveralls/GrahamDumpleton/wrapt/develop.svg
   :target: https://coveralls.io/github/GrahamDumpleton/wrapt?branch=develop
.. |PyPI| image:: https://img.shields.io/pypi/v/wrapt.svg
   :target: https://pypi.python.org/pypi/wrapt
  * [xattr-0.9.6](http://github.com/xattr/xattr) 
Extended attributes extend the basic attributes of files and directories
in the file system.  They are stored as name:data pairs associated with
file system objects (files, directories, symlinks, etc).

Extended attributes are currently only available on Darwin 8.0+ (Mac OS X 10.4)
and Linux 2.6+. Experimental support is included for Solaris and FreeBSD.



  * [xgboost-0.90](https://github.com/dmlc/xgboost) ======================
XGBoost Python Package
======================

|PyPI version|

Notes
=====

- Windows users: pip installation may not work on some Windows environments, and it may cause unexpected errors.

  Installation from pip on Windows is therefore currently disabled for further investigation; please `install from Github <https://xgboost.readthedocs.io/en/latest/build.html>`_ instead.
- If you want to run XGBoost process in parallel using the fork backend for joblib/multiprocessing, you must build XGBoost without support for OpenMP by ``make no_omp=1``. Otherwise, use the forkserver (in Python 3.4) or spawn backend. See the `sklearn\_parallel.py <../demo/guide-python/sklearn_parallel.py>`__ demo.

Requirements
============

Since this package contains C++ source code, ``pip`` needs a C++ compiler from the system to compile the source code on-the-fly.

macOS
-----

On macOS, ``gcc@5`` is required as later versions remove support for OpenMP. `See here <https://github.com/dmlc/xgboost/issues/1501#issuecomment-292209578>`_ for more info.

Please install ``gcc@5`` from `Homebrew <https://brew.sh/>`_::

    brew install gcc@5

After installing ``gcc@5``, set it as your compiler::

    export CC=gcc-5
    export CXX=g++-5

Linux
-----

Please install ``gcc``::

    sudo apt-get install build-essential      # Ubuntu/Debian
    sudo yum groupinstall 'Development Tools' # CentOS/RHEL

Installation
============

From `PyPI <https://pypi.python.org/pypi/xgboost>`_
---------------------------------------------------

For a stable version, install using ``pip``::

    pip install xgboost

From source
-----------

For an up-to-date version, `install from Github <https://xgboost.readthedocs.io/en/latest/build.html>`_:

-  Run ``./build.sh`` in the root of the repo.
-  Make sure you have `setuptools <https://pypi.python.org/pypi/setuptools>`_ installed: ``pip install setuptools``
-  Install with ``cd python-package; python setup.py install`` from the root of the repo
-  For Windows users, please use the Visual Studio project file under the `Windows folder <../windows/>`_. See also the `installation
   tutorial <https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13043/run-xgboost-from-windows-and-python>`_ from Kaggle Otto Forum.
-  Add MinGW to the system PATH in Windows if you are using the latest version of xgboost which requires compilation::

    python
    import os
    os.environ['PATH'] = os.environ['PATH'] + ';C:\\Program Files\\mingw-w64\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\mingw64\\bin'

Examples
========

-  Refer also to the walk through example in `demo folder <https://github.com/dmlc/xgboost/tree/master/demo/guide-python>`_.
-  See also the `example scripts <https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs>`_ for Kaggle
   Higgs Challenge, including `speedtest script <https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs/speedtest.py>`_ on this dataset.

.. |PyPI version| image:: https://badge.fury.io/py/xgboost.svg
   :target: http://badge.fury.io/py/xgboost



  * [xlrd-1.1.0](http://www.python-excel.org/) Extract data from Excel spreadsheets (.xls and .xlsx, versions 2.0 onwards) on any platform. Pure Python (2.7, 3.4+). Strong support for Excel dates. Unicode-aware.



  * [xlwt-1.3.0](http://www.python-excel.org/) |Travis|_ |Coveralls|_ |Docs|_ |PyPI|_

.. |Travis| image:: https://api.travis-ci.org/python-excel/xlwt.svg?branch=master
.. _Travis: https://travis-ci.org/python-excel/xlwt

.. |Coveralls| image:: https://coveralls.io/repos/python-excel/xlwt/badge.svg?branch=master
.. _Coveralls: https://coveralls.io/r/python-excel/xlwt?branch=master

.. |Docs| image:: https://readthedocs.org/projects/xlwt/badge/?version=latest
.. _Docs: https://xlwt.readthedocs.org/en/latest/

.. |PyPI| image:: https://badge.fury.io/py/xlwt.svg
.. _PyPI: https://badge.fury.io/py/xlwt

xlwt
====

This is a library for developers to use to generate
spreadsheet files compatible with Microsoft Excel versions 95 to 2003.

The package itself is pure Python with no dependencies on modules or packages
outside the standard Python distribution.

Please read this before using this package:
https://groups.google.com/d/msg/python-excel/P6TjJgFVjMI/g8d0eWxTBQAJ

Installation
============

Do the following in your virtualenv::

  pip install xlwt

Quick start
===========

.. code-block:: python

    import xlwt
    from datetime import datetime

    style0 = xlwt.easyxf('font: name Times New Roman, color-index red, bold on',
        num_format_str='#,##0.00')
    style1 = xlwt.easyxf(num_format_str='D-MMM-YY')

    wb = xlwt.Workbook()
    ws = wb.add_sheet('A Test Sheet')

    ws.write(0, 0, 1234.56, style0)
    ws.write(1, 0, datetime.now(), style1)
    ws.write(2, 0, 1)
    ws.write(2, 1, 1)
    ws.write(2, 2, xlwt.Formula("A3+B3"))

    wb.save('example.xls')


Documentation
=============

Documentation can be found in the ``docs`` directory of the xlwt package.
If these aren't sufficient, please consult the code in the
examples directory and the source code itself.

The latest documentation can also be found at:
https://xlwt.readthedocs.org/en/latest/

Problems?
=========
Try the following in this order:

- Read the source

- Ask a question on https://groups.google.com/group/python-excel/

Acknowledgements
================

xlwt is a fork of the pyExcelerator package, which was developed by
Roman V. Kiseliov. This product includes software developed by
Roman V. Kiseliov <roman@kiseliov.ru>.

xlwt uses ANTLR v 2.7.7 to generate its formula compiler.



  * [xmlschema-1.0.13](https://github.com/brunato/xmlschema) *********
xmlschema
*********

.. xmlschema-introduction-start

The *xmlschema* library is an implementation of `XML Schema <http://www.w3.org/2001/XMLSchema>`_
for Python (supports Python 2.7 and Python 3.5+).

This library arises from the needs of a solid Python layer for processing XML
Schema based files for
`MaX (Materials design at the Exascale) <http://www.max-centre.eu>`_  European project.
A significant problem is the encoding and the decoding of the XML data files
produced by different simulation software.
Another important requirement is the XML data validation, in order to put the
produced data under control. The lack of a suitable alternative for Python in
the schema-based decoding of XML data has led to build this library. Obviously
this library can be useful for other cases related to XML Schema based processing,
not only for the original scope.

The full `xmlschema documentation is available on "Read the Docs" <http://xmlschema.readthedocs.io/en/latest/>`_.


Features
========

This library includes the following features:

* Full XSD 1.0 and XSD 1.1 support
* Building of XML schema objects from XSD files
* Validation of XML instances against XSD schemas
* Decoding of XML data into Python data and to JSON
* Encoding of Python data and JSON to XML
* Data decoding and encoding ruled by converter classes
* An XPath based API for finding schema's elements and attributes
* Support of XSD validation modes *strict*/*lax*/*skip*
* Remote attacks protection by default using an XMLParser that forbids entities

.. note::
    Currently the XSD 1.1 validator is provided by class `XMLSchema11` and
    the default `XMLSchema` class is still an alias of the XSD 1.0 validator,
    the class `XMLSchema10`. From version 1.1 of the package the default
    validator will be linked to the XSD 1.1 validator, a version that will also
    removes support for Python 2.7.


Installation
============

You can install the library with *pip* in a Python 2.7 or Python 3.5+ environment::

    pip install xmlschema

The library uses the Python's ElementTree XML library and requires
`elementpath <https://github.com/brunato/elementpath>`_ additional package.
The base schemas of the XSD standards are included in the package for working
offline and to speed-up the building of schema instances.

.. xmlschema-introduction-end


Usage
=====

Import the library and then create a schema instance using the path of
the file containing the schema as argument:

.. code-block:: pycon

    >>> import xmlschema
    >>> my_schema = xmlschema.XMLSchema('xmlschema/tests/cases/examples/vehicles/vehicles.xsd')

.. note::
    For XSD 1.1 schemas use the class `XMLSchema11`, because the default class
    `XMLSchema` is still an alias of the XSD 1.0 validator class `XMLSchema10`.
    From next minor release (v1.1) the default class will become `XMLSchema11`.

The schema can be used to validate XML documents:

.. code-block:: pycon

    >>> my_schema.is_valid('xmlschema/tests/cases/examples/vehicles/vehicles.xml')
    True
    >>> my_schema.is_valid('xmlschema/tests/cases/examples/vehicles/vehicles-1_error.xml')
    False
    >>> my_schema.validate('xmlschema/tests/cases/examples/vehicles/vehicles-1_error.xml')
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "/home/brunato/Development/projects/xmlschema/xmlschema/validators/xsdbase.py", line 393, in validate
        raise error
    xmlschema.validators.exceptions.XMLSchemaValidationError: failed validating <Element '{http://example.com/vehicles}cars' at 0x7f8032768458> with XsdGroup(model='sequence').

    Reason: character data between child elements not allowed!

    Schema:

      <xs:sequence xmlns:xs="http://www.w3.org/2001/XMLSchema">
            <xs:element maxOccurs="unbounded" minOccurs="0" name="car" type="vh:vehicleType" />
      </xs:sequence>

    Instance:

      <vh:cars xmlns:vh="http://example.com/vehicles">
        NOT ALLOWED CHARACTER DATA
        <vh:car make="Porsche" model="911" />
        <vh:car make="Porsche" model="911" />
      </vh:cars>

Using a schema you can also decode the XML documents to nested dictionaries, with
values that match to the data types declared by the schema:

.. code-block:: pycon

    >>> import xmlschema
    >>> from pprint import pprint
    >>> xs = xmlschema.XMLSchema('xmlschema/tests/cases/examples/collection/collection.xsd')
    >>> pprint(xs.to_dict('xmlschema/tests/cases/examples/collection/collection.xml'))
    {'@xsi:schemaLocation': 'http://example.com/ns/collection collection.xsd',
     'object': [{'@available': True,
                 '@id': 'b0836217462',
                 'author': {'@id': 'PAR',
                            'born': '1841-02-25',
                            'dead': '1919-12-03',
                            'name': 'Pierre-Auguste Renoir',
                            'qualification': 'painter'},
                 'estimation': Decimal('10000.00'),
                 'position': 1,
                 'title': 'The Umbrellas',
                 'year': '1886'},
                {'@available': True,
                 '@id': 'b0836217463',
                 'author': {'@id': 'JM',
                            'born': '1893-04-20',
                            'dead': '1983-12-25',
                            'name': 'Joan Miró',
                            'qualification': 'painter, sculptor and ceramicist'},
                 'position': 2,
                 'title': None,
                 'year': '1925'}]}


Authors
=======
Davide Brunato and others who have contributed with code or with sample cases.

License
=======
This software is distributed under the terms of the MIT License.
See the file 'LICENSE' in the root directory of the present
distribution, or http://opensource.org/licenses/MIT.



  * [xopen-0.7.3](https://github.com/marcelm/xopen/) .. image:: https://travis-ci.org/marcelm/xopen.svg?branch=master
  :target: https://travis-ci.org/marcelm/xopen
  :alt: 

.. image:: https://img.shields.io/pypi/v/xopen.svg?branch=master
  :target: https://pypi.python.org/pypi/xopen

.. image:: https://img.shields.io/conda/v/conda-forge/xopen.svg
  :target: https://anaconda.org/conda-forge/xopen
  :alt:

.. image:: https://codecov.io/gh/marcelm/xopen/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/marcelm/xopen
  :alt:

=====
xopen
=====

This small Python module provides an ``xopen`` function that works like the
built-in ``open`` function, but can also deal with compressed files.
Supported compression formats are gzip, bzip2 and xz. They are automatically
recognized by their file extensions `.gz`, `.bz2` or `.xz`.

The focus is on being as efficient as possible on all supported Python versions.
For example, ``xopen`` uses ``pigz``, which is a parallel version of ``gzip``,
to open ``.gz`` files, which is faster than using the built-in ``gzip.open``
function. ``pigz`` can use multiple threads when compressing, but is also faster
when reading ``.gz`` files, so it is used both for reading and writing if it is
available.

This module has originally been developed as part of the `cutadapt
tool <https://cutadapt.readthedocs.io/>`_ that is used in bioinformatics to
manipulate sequencing data. It has been in successful use within that software
for a few years.

``xopen`` is compatible with Python versions 2.7 and 3.4 to 3.7.


Usage
-----

Open a file for reading::

    from xopen import xopen

    with xopen('file.txt.xz') as f:
        content = f.read()

Or without context manager::

    from xopen import xopen

    f = xopen('file.txt.xz')
    content = f.read()
    f.close()

Open a file in binary mode for writing::

    from xopen import xopen

    with xopen('file.txt.gz', mode='wb') as f:
        f.write(b'Hello')


Credits
-------

The name ``xopen`` was taken from the C function of the same name in the
`utils.h file which is part of
BWA <https://github.com/lh3/bwa/blob/83662032a2192d5712996f36069ab02db82acf67/utils.h>`_.

Kyle Beauchamp <https://github.com/kyleabeauchamp/> has contributed support for
appending to files.

Ruben Vorderman <https://github.com/rhpvorderman/> contributed improvements to
make reading gzipped files faster.

Some ideas were taken from the `canopener project <https://github.com/selassid/canopener>`_.
If you also want to open S3 files, you may want to use that module instead.


Changes
-------

v0.8.0
~~~~~~
* Speed improvements when iterating over gzipped files.

v0.6.0
~~~~~~
* For reading from gzipped files, xopen will now use a ``pigz`` subprocess.
  This is faster than using ``gzip.open``.
* Python 2 supported will be dropped in one of the next releases.

v0.5.0
~~~~~~
* By default, pigz is now only allowed to use at most four threads. This hopefully reduces
  problems some users had with too many threads when opening many files at the same time.
* xopen now accepts pathlib.Path objects.


Author
------

Marcel Martin <mail@marcelm.net> (`@marcelm_ on Twitter <https://twitter.com/marcelm_>`_)

Links
-----

* `Source code <https://github.com/marcelm/xopen/>`_
* `Report an issue <https://github.com/marcelm/xopen/issues>`_
* `Project page on PyPI (Python package index) <https://pypi.python.org/pypi/xopen/>`_



  * [yamlordereddictloader-0.4.0](https://github.com/fmenabe/python-yamlordereddictloader) python-yamlordereddictloader
============================

.. image:: https://img.shields.io/pypi/l/yamlordereddictloader.svg
           :target: https://opensource.org/licenses/MIT
           :alt: License

.. image:: https://img.shields.io/pypi/pyversions/yamlordereddictloader.svg
           :target: https://pypi.python.org/pypi/yamlordereddictloader
           :alt: Versions

.. image:: https://img.shields.io/pypi/v/yamlordereddictloader.svg
           :target: https://pypi.python.org/pypi/yamlordereddictloader
           :alt: PyPi

.. image:: https://img.shields.io/badge/github-repo-yellow.jpg
           :target: https://github.com/fmenabe/python-yamlordereddictloader
           :alt: Code repo

.. image:: https://landscape.io/github/fmenabe/python-yamlordereddictloader/master/landscape.svg?style=flat
           :target: https://landscape.io/github/fmenabe/python-yamlordereddictloader/master
           :alt: Code Health


This module provide a loader and a dumper for PyYAML allowing to keep items order
when loading a file (by putting them in ``OrderedDict`` objects) and to manage
``OrderedDict`` objects when dumping to a file.

The loader is based on stackoverflow topic (thanks to Eric Naeseth):
http://stackoverflow.com/questions/5121931/in-python-how-can-you-load-yaml-mappings-as-ordereddicts#answer-5121963

Self promotion: I use it a lot with `clg <https://clg.readthedocs.io>`_, which
allows to generate command-line definition from a configuration file, for keeping
order of subcommands, options and arguments in the help message!


To install it
-------------

.. code-block:: bash

    $ pip install yamlordereddictloader

Loader usage
------------

.. code-block:: python

    import yaml
    import yamlordereddictloader

    data = yaml.load(open('myfile.yml'), Loader=yamlordereddictloader.Loader)

**Note:** For using the safe loader (which want standard YAML tags and does
not construct arbitrary Python objects), replace ``yamlorderdictloader.Loader`` by
``yamlorderedictloader.SafeLoader``.

Dumper usage
------------

.. code-block:: python

    import yaml
    import yamlordereddictloader
    from collections import OrderedDict

    data = OrderedDict([
        ('key1', 'val1'),
        ('key2', OrderedDict([('key21', 'val21'), ('key22', 'val22')]))
    ])
    yaml.dump(
        data,
        open('myfile.yml', 'w'),
        Dumper=yamlordereddictloader.Dumper,
        default_flow_style=False)

**Note:** For using the safe dumper (which produce standard YAML tags and does
not represent arbitrary Python objects), replace ``yamlorderdictloader.Dumper`` by
``yamlorderedictloader.SafeDumper``.

  * [yappi-1.0](https://github.com/sumerc/yappi) ![Logo](https://i.imgur.com/xxmgGmn.png)
# Yappi
**Y**et **A**nother **P**ython **P**rof**i**ler, but this time support Multithread/CPU time profiling.

[![Build Status](https://www.travis-ci.org/sumerc/yappi.svg?branch=master)](https://www.travis-ci.org/sumerc/yappi)
![](https://img.shields.io/pypi/v/yappi.svg)
![](https://img.shields.io/pypi/dw/yappi.svg)
![](https://img.shields.io/pypi/pyversions/yappi.svg)
![](https://img.shields.io/github/last-commit/sumerc/yappi.svg)
![](https://img.shields.io/github/license/sumerc/yappi.svg)


## Motivation

CPython standard distribution comes with three profilers. `cProfile`, `Profile` and `hotshot`. 
`cProfile` is implemented as a C module based on `lsprof`, `Profile` is in pure Python and 
`hotshot` can be seen as a small subset of a cProfile. 

*The major issue is that all of these profilers lack support for multi-threaded programs and CPU time.*

If you want to profile a  multi-threaded application, you must give an entry point to these profilers and then maybe merge 
the outputs. None of these profilers are designed to work on long-running multi-threaded application.It is impossible to profile an application retrieve the statistics then stop and then start later on the fly (without affecting the profiled
application). 

## Highlights

- Profiler can be started/stopped at any time from any thread in the application.
- Profile statistics can be obtained from any thread at any time.
- Profile statistics can show actual [CPU Time](http://en.wikipedia.org/wiki/CPU_time) used instead of Wall time.
- "Profiler pollution" (effect on the application run-time) is very minimal.

## Installation

Can be installed via PyPI

```
$ pip install yappi
```

OR from the source directly.

```
$ pip install git+https://github.com/sumerc/yappi#egg=yappi
```

## Documentation

- [Introduction](doc/introduction.md)
- [Clock Types](doc/clock_types.md)
- [API](doc/api.md)
- [THANKS](THANKS.md)

## Features
- Profiler results can be saved in [callgrind](http://valgrind.org/docs/manual/cl-format.html) or [pstat](http://docs.python.org/3.4/library/profile.html#pstats.Stats) formats. (*new in 0.82*)
- Profiler results can be merged from different sessions on-the-fly. (*new in 0.82*)
- Profiler results can be easily converted to pstats. (*new in 0.82*)
- Profiling of multithreaded Python applications transparently.
- Supports profiling per-thread [CPU time](http://en.wikipedia.org/wiki/CPU_time) (*new in 0.62*)
- Profiler can be started from any thread at any time.
- Ability to get statistics at any time without even stopping the profiler.
- Various flags to arrange/sort profiler results.
- Supports Python >= 2.7.x

## Limitations:
* Threads must be derived from "threading" module's Thread object.

## Talks

- Python Performance Profiling: The Guts And The Glory

  [![Youtube link](https://img.youtube.com/vi/BOKcZjI5zME/0.jpg)](https://www.youtube.com/watch?v=BOKcZjI5zME)

## PyCharm Integration

Yappi is the default profiler in `PyCharm`. If you have Yappi installed, `PyCharm` will use it. See [the official](https://www.jetbrains.com/help/pycharm/profiler.html) documentation for more details.


  * [yarl-1.3.0](https://github.com/aio-libs/yarl/) yarl
====

.. image:: https://travis-ci.com/aio-libs/yarl.svg?branch=master
  :target:  https://travis-ci.com/aio-libs/yarl
  :align: right

.. image:: https://codecov.io/gh/aio-libs/yarl/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/aio-libs/yarl

.. image:: https://badge.fury.io/py/yarl.svg
    :target: https://badge.fury.io/py/yarl


.. image:: https://readthedocs.org/projects/yarl/badge/?version=latest
    :target: https://yarl.readthedocs.io


.. image:: https://img.shields.io/pypi/pyversions/yarl.svg
    :target: https://pypi.python.org/pypi/yarl

.. image:: https://badges.gitter.im/Join%20Chat.svg
    :target: https://gitter.im/aio-libs/Lobby
    :alt: Chat on Gitter

Introduction
------------

Url is constructed from ``str``::

   >>> from yarl import URL
   >>> url = URL('https://www.python.org/~guido?arg=1#frag')
   >>> url
   URL('https://www.python.org/~guido?arg=1#frag')

All url parts: *scheme*, *user*, *password*, *host*, *port*, *path*,
*query* and *fragment* are accessible by properties::

   >>> url.scheme
   'https'
   >>> url.host
   'www.python.org'
   >>> url.path
   '/~guido'
   >>> url.query_string
   'arg=1'
   >>> url.query
   <MultiDictProxy('arg': '1')>
   >>> url.fragment
   'frag'

All url manipulations produce a new url object::

   >>> url.parent / 'downloads/source'
   URL('https://www.python.org/downloads/source')

Strings passed to constructor and modification methods are
automatically encoded giving canonical representation as result::

   >>> url = URL('https://www.python.org/путь')
   >>> url
   URL('https://www.python.org/%D0%BF%D1%83%D1%82%D1%8C')

Regular properties are *percent-decoded*, use ``raw_`` versions for
getting *encoded* strings::

   >>> url.path
   '/путь'

   >>> url.raw_path
   '/%D0%BF%D1%83%D1%82%D1%8C'

Human readable representation of URL is available as ``.human_repr()``::

   >>> url.human_repr()
   'https://www.python.org/путь'

For full documentation please read https://yarl.readthedocs.org.


Installation
------------

::

   $ pip install yarl

The library is Python 3 only!


Dependencies
------------

YARL requires multidict_ library.


API documentation
------------------

The documentation is located at https://yarl.readthedocs.org

Comparison with other URL libraries
------------------------------------

* furl (https://pypi.python.org/pypi/furl)

  The library has rich functionality but the ``furl`` object is mutable.

  I'm afraid to pass this object into foreign code: who knows if the
  code will modify my url in a terrible way while I just want to send URL
  with handy helpers for accessing URL properties.

  ``furl`` has other non-obvious tricky things but the main objection
  is mutability.

* URLObject (https://pypi.python.org/pypi/URLObject)

  URLObject is immutable, that's pretty good.

  Every URL change generates a new URL object.

  But the library doesn't do any decode/encode transformations leaving the
  end user to cope with these gory details.


Source code
-----------

The project is hosted on GitHub_

Please file an issue on the `bug tracker
<https://github.com/aio-libs/yarl/issues>`_ if you have found a bug
or have some suggestion in order to improve the library.

The library uses `Travis <https://travis-ci.org/aio-libs/yarl>`_ for
Continuous Integration.

Discussion list
---------------

*aio-libs* google group: https://groups.google.com/forum/#!forum/aio-libs

Feel free to post your questions and ideas here.


Authors and License
-------------------

The ``yarl`` package is written by Andrew Svetlov.

It's *Apache 2* licensed and freely available.


.. _GitHub: https://github.com/aio-libs/yarl

.. _multidict: https://github.com/aio-libs/multidict


CHANGES
=======

1.3.0 (2018-12-11)
------------------

* Fix annotations for ``query`` parameter (#207)

* An incoming query sequence can have int variables (the same as for
  Mapping type) (#208)

* Add ``URL.explicit_port`` property (#218)

* Give a friendlier error when port cant be converted to int (#168)

* ``bool(URL())`` now returns ``False`` (#272)

1.2.6 (2018-06-14)
------------------

* Drop Python 3.4 trove classifier (#205)

1.2.5 (2018-05-23)
------------------

* Fix annotations for ``build`` (#199)

1.2.4 (2018-05-08)
------------------

* Fix annotations for ``cached_property`` (#195)

1.2.3 (2018-05-03)
------------------

* Accept ``str`` subclasses in ``URL`` constructor (#190)

1.2.2 (2018-05-01)
------------------

* Fix build

1.2.1 (2018-04-30)
------------------

* Pin minimal required Python to 3.5.3 (#189)

1.2.0 (2018-04-30)
------------------

* Forbid inheritance, replace ``__init__`` with ``__new__`` (#171)

* Support PEP-561 (provide type hinting marker) (#182)

1.1.1 (2018-02-17)
------------------

* Fix performance regression: don't encode enmpty netloc (#170)

1.1.0 (2018-01-21)
------------------

* Make pure Python quoter consistent with Cython version (#162)

1.0.0 (2018-01-15)
------------------

* Use fast path if quoted string does not need requoting (#154)

* Speed up quoting/unquoting by ``_Quoter`` and ``_Unquoter`` classes (#155)

* Drop ``yarl.quote`` and ``yarl.unquote`` public functions (#155)

* Add custom string writer, reuse static buffer if available (#157)
  Code is 50-80 times faster than Pure Python version (was 4-5 times faster)

* Don't recode IP zone (#144)

* Support ``encoded=True`` in ``yarl.URL.build()`` (#158)

* Fix updating query with multiple keys (#160)

0.18.0 (2018-01-10)
-------------------

* Fallback to IDNA 2003 if domain name is not IDNA 2008 compatible (#152)

0.17.0 (2017-12-30)
-------------------

* Use IDNA 2008 for domain name processing (#149)

0.16.0 (2017-12-07)
-------------------

* Fix raising ``TypeError`` by ``url.query_string()`` after
  ``url.with_query({})`` (empty mapping) (#141)

0.15.0 (2017-11-23)
-------------------

* Add ``raw_path_qs`` attribute (#137)

0.14.2 (2017-11-14)
-------------------

* Restore ``strict`` parameter as no-op in ``quote`` / ``unquote``

0.14.1 (2017-11-13)
-------------------

* Restore ``strict`` parameter as no-op for sake of compatibility with
  aiohttp 2.2

0.14.0 (2017-11-11)
-------------------

* Drop strict mode (#123)

* Fix ``"ValueError: Unallowed PCT %"`` when there's a ``"%"`` in the url (#124)

0.13.0 (2017-10-01)
-------------------

* Document ``encoded`` parameter (#102)

* Support relative urls like ``'?key=value'`` (#100)

* Unsafe encoding for QS fixed. Encode ``;`` char in value param (#104)

* Process passwords without user names (#95)

0.12.0 (2017-06-26)
-------------------

* Properly support paths without leading slash in ``URL.with_path()`` (#90)

* Enable type annotation checks

0.11.0 (2017-06-26)
-------------------

* Normalize path (#86)

* Clear query and fragment parts in ``.with_path()`` (#85)

0.10.3 (2017-06-13)
-------------------

* Prevent double URL args unquoting (#83)

0.10.2 (2017-05-05)
-------------------

* Unexpected hash behaviour (#75)


0.10.1 (2017-05-03)
-------------------

* Unexpected compare behaviour (#73)

* Do not quote or unquote + if not a query string. (#74)


0.10.0 (2017-03-14)
-------------------

* Added ``URL.build`` class method (#58)

* Added ``path_qs`` attribute (#42)


0.9.8 (2017-02-16)
------------------

* Do not quote ``:`` in path


0.9.7 (2017-02-16)
------------------

* Load from pickle without _cache (#56)

* Percent-encoded pluses in path variables become spaces (#59)


0.9.6 (2017-02-15)
------------------

* Revert backward incompatible change (BaseURL)


0.9.5 (2017-02-14)
------------------

* Fix BaseURL rich comparison support


0.9.4 (2017-02-14)
------------------

* Use BaseURL


0.9.3 (2017-02-14)
------------------

* Added BaseURL


0.9.2 (2017-02-08)
------------------

* Remove debug print


0.9.1 (2017-02-07)
------------------

* Do not lose tail chars (#45)


0.9.0 (2017-02-07)
------------------

* Allow to quote ``%`` in non strict mode (#21)

* Incorrect parsing of query parameters with %3B (;) inside (#34)

* Fix core dumps (#41)

* tmpbuf - compiling error (#43)

* Added ``URL.update_path()`` method

* Added ``URL.update_query()`` method (#47)


0.8.1 (2016-12-03)
------------------

* Fix broken aiohttp: revert back ``quote`` / ``unquote``.


0.8.0 (2016-12-03)
------------------

* Support more verbose error messages in ``.with_query()`` (#24)

* Don't percent-encode ``@`` and ``:`` in path (#32)

* Don't expose ``yarl.quote`` and ``yarl.unquote``, these functions are
  part of private API

0.7.1 (2016-11-18)
------------------

* Accept not only ``str`` but all classes inherited from ``str`` also (#25)

0.7.0 (2016-11-07)
------------------

* Accept ``int`` as value for ``.with_query()``

0.6.0 (2016-11-07)
------------------

* Explicitly use UTF8 encoding in setup.py (#20)
* Properly unquote non-UTF8 strings (#19)

0.5.3 (2016-11-02)
------------------

* Don't use namedtuple fields but indexes on URL construction

0.5.2 (2016-11-02)
------------------

* Inline ``_encode`` class method

0.5.1 (2016-11-02)
------------------

* Make URL construction faster by removing extra classmethod calls

0.5.0 (2016-11-02)
------------------

* Add cython optimization for quoting/unquoting
* Provide binary wheels

0.4.3 (2016-09-29)
------------------

* Fix typing stubs

0.4.2 (2016-09-29)
------------------

* Expose ``quote()`` and ``unquote()`` as public API

0.4.1 (2016-09-28)
------------------

* Support empty values in query (``'/path?arg'``)

0.4.0 (2016-09-27)
------------------

* Introduce ``relative()`` (#16)

0.3.2 (2016-09-27)
------------------

* Typo fixes #15

0.3.1 (2016-09-26)
------------------

* Support sequence of pairs as ``with_query()`` parameter

0.3.0 (2016-09-26)
------------------

* Introduce ``is_default_port()``

0.2.1 (2016-09-26)
------------------

* Raise ValueError for URLs like 'http://:8080/'

0.2.0 (2016-09-18)
------------------

* Avoid doubling slashes when joining paths (#13)

* Appending path starting from slash is forbidden (#12)

0.1.4 (2016-09-09)
------------------

* Add kwargs support for ``with_query()`` (#10)

0.1.3 (2016-09-07)
------------------

* Document ``with_query()``, ``with_fragment()`` and ``origin()``

* Allow ``None`` for ``with_query()`` and ``with_fragment()``

0.1.2 (2016-09-07)
------------------

* Fix links, tune docs theme.

0.1.1 (2016-09-06)
------------------

* Update README, old version used obsolete API

0.1.0 (2016-09-06)
------------------

* The library was deeply refactored, bytes are gone away but all
  accepted strings are encoded if needed.

0.0.1 (2016-08-30)
------------------

* The first release.



  * [yaspin-0.14.3](https://github.com/pavdmyt/yaspin) |Logo|

=====================================================================
``yaspin``: **Y**\ et **A**\ nother Terminal **Spin**\ ner for Python
=====================================================================

|Build Status| |Coverage| |Codacy| |pyup| |black-fmt|

|pypi| |Versions| |Wheel| |Examples| |Downloads|


``Yaspin`` provides a full-featured terminal spinner to show the progress during long-hanging operations.

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/demo.gif

It is easy to integrate into existing codebase by using it as a `context manager`_
or as a function `decorator`_:

.. code:: python

    import time
    from yaspin import yaspin

    # Context manager:
    with yaspin():
        time.sleep(3)  # time consuming code

    # Function decorator:
    @yaspin(text="Loading...")
    def some_operations():
        time.sleep(3)  # time consuming code

    some_operations()


**Yaspin** also provides an intuitive and powerful API. For example, you can easily summon a shark:

.. code:: python

    import time
    from yaspin import yaspin

    with yaspin().white.bold.shark.on_blue as sp:
        sp.text = "White bold shark in a blue sea"
        time.sleep(5)

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/shark.gif


Features
--------

- No external dependencies
- Runs at all major **CPython** versions (*2.7*, *3.4*, *3.5*, *3.6*, *3.7*), **PyPy** and **PyPy3**
- Supports all (60+) spinners from `cli-spinners`_
- Supports all *colors*, *highlights*, *attributes* and their mixes from `termcolor`_ library
- Easy to combine with other command-line libraries, e.g. `prompt-toolkit`_
- Flexible API, easy to integrate with existing code
- User-friendly API for handling POSIX `signals`_
- Safe **pipes** and **redirects**:

.. code-block:: bash

    $ python script_that_uses_yaspin.py > script.log
    $ python script_that_uses_yaspin.py | grep ERROR


Installation
------------

From `PyPI`_ using ``pip`` package manager:

.. code-block:: bash

    pip install --upgrade yaspin


Or install the latest sources from GitHub:

.. code-block:: bash

    pip install https://github.com/pavdmyt/yaspin/archive/master.zip


Usage
-----

Basic Example
/////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/basic_example.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from random import randint
    from yaspin import yaspin

    with yaspin(text="Loading", color="yellow") as spinner:
        time.sleep(2)  # time consuming code

        success = randint(0, 1)
        if success:
            spinner.ok("✅ ")
        else:
            spinner.fail("💥 ")


It is also possible to control spinner manually:

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin

    spinner = yaspin()
    spinner.start()

    time.sleep(3)  # time consuming tasks

    spinner.stop()


Run any spinner from `cli-spinners`_
////////////////////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/cli_spinners.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin
    from yaspin.spinners import Spinners

    with yaspin(Spinners.earth, text="Earth") as sp:
        time.sleep(2)                # time consuming code

        # change spinner
        sp.spinner = Spinners.moon
        sp.text = "Moon"

        time.sleep(2)                # time consuming code


Any Colour You Like `🌈`_
/////////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/basic_colors.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin

    with yaspin(text="Colors!") as sp:
        # Support all basic termcolor text colors
        colors = ("red", "green", "yellow", "blue", "magenta", "cyan", "white")

        for color in colors:
            sp.color, sp.text = color, color
            time.sleep(1)


Advanced colors usage
/////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/advanced_colors.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin
    from yaspin.spinners import Spinners

    text = "Bold blink magenta spinner on cyan color"
    with yaspin().bold.blink.magenta.bouncingBall.on_cyan as sp:
        sp.text = text
        time.sleep(3)

    # The same result can be achieved by passing arguments directly
    with yaspin(
        Spinners.bouncingBall,
        color="magenta",
        on_color="on_cyan",
        attrs=["bold", "blink"],
    ) as sp:
        sp.text = text
        time.sleep(3)


Run any spinner you want
////////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/custom_spinners.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin, Spinner

    # Compose new spinners with custom frame sequence and interval value
    sp = Spinner(["😸", "😹", "😺", "😻", "😼", "😽", "😾", "😿", "🙀"], 200)

    with yaspin(sp, text="Cat!"):
        time.sleep(3)  # cat consuming code :)


Change spinner properties on the fly
////////////////////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/sp_properties.gif

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin
    from yaspin.spinners import Spinners

    with yaspin(Spinners.noise, text="Noise spinner") as sp:
        time.sleep(2)

        sp.spinner = Spinners.arc  # spinner type
        sp.text = "Arc spinner"    # text along with spinner
        sp.color = "green"         # spinner color
        sp.side = "right"          # put spinner to the right
        sp.reversal = True         # reverse spin direction

        time.sleep(2)


Writing messages
////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/write_text.gif

You should not write any message in the terminal using ``print`` while spinner is open.
To write messages in the terminal without any collision with ``yaspin`` spinner, a ``.write()`` method is provided:

.. code:: python

    # -*- coding: utf-8 -*-
    import time
    from yaspin import yaspin

    with yaspin(text="Downloading images", color="cyan") as sp:
        # task 1
        time.sleep(1)
        sp.write("> image 1 download complete")

        # task 2
        time.sleep(2)
        sp.write("> image 2 download complete")

        # finalize
        sp.ok("✔")


Integration with other libraries
////////////////////////////////

.. image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/gifs/hide_show.gif

Utilizing ``hide`` and ``show`` methods it is possible to toggle the display of
the spinner in order to call custom methods that write to the terminal. This is
helpful for allowing easy usage in other frameworks like `prompt-toolkit`_.
Using the powerful ``print_formatted_text`` function allows you even to apply
HTML formats and CSS styles to the output:

.. code:: python

    # -*- coding: utf-8 -*-
    from __future__ import print_function

    import sys
    import time

    from yaspin import yaspin
    from prompt_toolkit import HTML, print_formatted_text
    from prompt_toolkit.styles import Style

    # override print with feature-rich ``print_formatted_text`` from prompt_toolkit
    print = print_formatted_text

    # build a basic prompt_toolkit style for styling the HTML wrapped text
    style = Style.from_dict({
        'msg': '#4caf50 bold',
        'sub-msg': '#616161 italic'
    })


    with yaspin(text='Downloading images') as sp:
        # task 1
        time.sleep(1)
        sp.hide()
        print(HTML(
            u'<b>></b> <msg>image 1</msg> <sub-msg>download complete</sub-msg>'
        ), style=style)
        sp.show()

        # task 2
        time.sleep(2)
        sp.hide()
        print(HTML(
            u'<b>></b> <msg>image 2</msg> <sub-msg>download complete</sub-msg>'
        ), style=style)
        sp.show()

        # finalize
        sp.ok()


Handling POSIX `signals`_
/////////////////////////

Handling keyboard interrupts (pressing Control-C):

.. code:: python

    # -*- coding: utf-8 -*-
    import time

    from yaspin import kbi_safe_yaspin


    with kbi_safe_yaspin(text="Press Control+C to send SIGINT (Keyboard Interrupt) signal"):
        time.sleep(5)  # time consuming code


Handling other types of signals:

.. code:: python

    # -*- coding: utf-8 -*-
    import os
    import time
    from signal import SIGTERM, SIGUSR1

    from yaspin import yaspin
    from yaspin.signal_handlers import default_handler, fancy_handler


    sigmap = {SIGUSR1: default_handler, SIGTERM: fancy_handler}
    with yaspin(sigmap=sigmap, text="Handling SIGUSR1 and SIGTERM signals") as sp:
        sp.write("Send signals using `kill` command")
        sp.write("E.g. $ kill -USR1 {0}".format(os.getpid()))
        time.sleep(20)  # time consuming code


More `examples`_.


Development
-----------

Clone the repository:

.. code-block:: bash

    git clone https://github.com/pavdmyt/yaspin.git


Install dev dependencies:

.. code-block:: bash

    pipenv install --dev


Lint code:

.. code-block:: bash

    make lint


Format code:

.. code-block:: bash

    make black-fmt


Run tests:

.. code-block:: bash

    make test


Contributing
------------

1. Fork it!
2. Create your feature branch: ``git checkout -b my-new-feature``
3. Commit your changes: ``git commit -m 'Add some feature'``
4. Push to the branch: ``git push origin my-new-feature``
5. Submit a pull request
6. Make sure tests are passing


License
-------

* MIT - Pavlo Dmytrenko; https://twitter.com/pavdmyt
* Contains `termcolor`_ package: MIT License, Copyright (c) 2008-2011 Volvox Development Team
* Contains data from `cli-spinners`_: MIT License, Copyright (c) Sindre Sorhus sindresorhus@gmail.com (sindresorhus.com)


.. |Logo| image:: https://raw.githubusercontent.com/pavdmyt/yaspin/master/static/logo_80.png
   :alt: yaspin Logo
.. |Build Status| image:: https://travis-ci.org/pavdmyt/yaspin.svg?branch=master
   :target: https://travis-ci.org/pavdmyt/yaspin
.. |Coverage| image:: https://coveralls.io/repos/github/pavdmyt/yaspin/badge.svg?branch=master
   :target: https://coveralls.io/github/pavdmyt/yaspin?branch=master
.. |Codacy| image:: https://api.codacy.com/project/badge/Grade/797c7772d0d3467c88a5e2e9dc79ec98
   :target: https://www.codacy.com/app/pavdmyt/yaspin?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=pavdmyt/yaspin&amp;utm_campaign=Badge_Grade
.. |pypi| image:: https://img.shields.io/pypi/v/yaspin.svg
   :target: https://pypi.org/project/yaspin/
.. |Versions| image:: https://img.shields.io/pypi/pyversions/yaspin.svg
   :target: https://pypi.org/project/yaspin/
.. |Wheel| image:: https://img.shields.io/pypi/wheel/yaspin.svg
   :target: https://pypi.org/project/yaspin/
.. |Examples| image:: https://img.shields.io/badge/learn%20by-examples-0077b3.svg
   :target: https://github.com/pavdmyt/yaspin/tree/master/examples
.. |pyup| image:: https://pyup.io/repos/github/pavdmyt/yaspin/shield.svg
   :target: https://pyup.io/repos/github/pavdmyt/yaspin/
.. |black-fmt| image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
.. |Downloads| image:: https://pepy.tech/badge/yaspin
   :target: https://pepy.tech/project/yaspin


.. _context manager: https://docs.python.org/3/reference/datamodel.html#context-managers
.. _decorator: https://www.thecodeship.com/patterns/guide-to-python-function-decorators/
.. _cli-spinners: https://github.com/sindresorhus/cli-spinners
.. _termcolor: https://pypi.org/project/termcolor/
.. _PyPI: https://pypi.org/
.. _🌈: https://en.wikipedia.org/wiki/Any_Colour_You_Like
.. _examples: https://github.com/pavdmyt/yaspin/tree/master/examples
.. _prompt-toolkit: https://github.com/jonathanslenders/python-prompt-toolkit/
.. _signals: https://www.computerhope.com/unix/signals.htm

  * [zappy-0.2.0](https://github.com/lasersonlab/zappy) # Zappy - distributed processing with NumPy and Zarr

Zappy is for distributed processing of chunked NumPy arrays on engines like [Pywren], Apache Spark, and Apache Beam.

[![Build Status](https://travis-ci.org/lasersonlab/zappy.svg?branch=master)](https://travis-ci.org/lasersonlab/zappy)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Coverage Status](https://coveralls.io/repos/github/lasersonlab/zappy/badge.svg)](https://coveralls.io/github/lasersonlab/zappy)
[![PyPI version shields.io](https://img.shields.io/pypi/v/zappy.svg)](https://pypi.python.org/pypi/zappy/)

The `zappy.base` module defines a `ZappyArray` class that exposes the same interface as `numpy.ndarray`, and which
is backed by distributed storage and processing. The array is broken into chunks, and is typically loaded from [Zarr],
and each chunk is processed independently.

There are a few engines provided:
* **direct** - for eager in-memory processing
* **spark** - for processing using Spark
* **beam** - for processing using Beam or Google Dataflow
* **executor** - for processing using Python's [concurrent.futures.Executor], of which [Pywren] is a notable implementation

Beam currently only runs on Python 2.

Full coverage of the `numpy.ndarray` interface is _not_ provided. Only enough has been implemented to support running
parts of [Scanpy], as demonstrated in the [Single Cell Experiments] repo.

## Installation

```
pip install zappy
```

Alternatively, zappy can be installed using [Conda](https://conda.io/docs/) (most easily obtained via the [Miniconda Python distribution](https://conda.io/miniconda.html)):

```
conda install -c conda-forge zappy
```

## Demo

Take a look at the rendered [demo Jupyter notebook](demo.ipynb), or try it out yourself as follows.

Create and activate a Python 3 virtualenv, and install the requirements:

```
python3 -m venv venv
. venv/bin/activate
pip install -r requirements.txt
pip install -e .
pip install s3fs jupyter
```

Then run the notebook with:

```
jupyter notebook demo.ipynb
```

## Testing

There is a test suite for all the engines, covering both Python 2 and 3.

Run everything in one go with tox:

```
pip install tox
tox
```

Formatting:

```
pip install black
black zappy tests/* *.py
```

Coverage:

```
pip install pytest-cov
pytest --cov-report html --cov=zappy
open htmlcov/index.html
```

## Publishing

```
pip install twine
python setup.py sdist
twine upload -r pypi dist/zappy-0.1.0.tar.gz
```

If successful, the package will be available on [PyPI].

[Scanpy]: https://scanpy.readthedocs.io/
[Single Cell Experiments]: https://github.com/lasersonlab/single-cell-experiments
[concurrent.futures.Executor]: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor
[PyPI]: https://pypi.org/project/zappy/
[Pywren]: http://pywren.io/
[Zarr]: https://zarr.readthedocs.io/
  * [zarr-2.3.2](https://github.com/zarr-developers/zarr) Zarr
====

Zarr is a Python package providing an implementation of compressed,
chunked, N-dimensional arrays, designed for use in parallel
computing. See the `documentation <http://zarr.readthedocs.io/>`_ for
more information.

.. image:: https://badges.gitter.im/zarr-developers/community.svg
    :target: https://gitter.im/zarr-developers/community

.. image:: https://readthedocs.org/projects/zarr/badge/?version=latest
    :target: http://zarr.readthedocs.io/en/latest/?badge=latest

.. image:: https://travis-ci.org/zarr-developers/zarr.svg?branch=master
    :target: https://travis-ci.org/zarr-developers/zarr

.. image:: https://ci.appveyor.com/api/projects/status/7d4iko59k8hpbbik?svg=true
    :target: https://ci.appveyor.com/project/zarr-developers/zarr

.. image:: https://coveralls.io/repos/github/zarr-developers/zarr/badge.svg?branch=master
    :target: https://coveralls.io/github/zarr-developers/zarr?branch=master
  * [zict-1.0.0](http://zict.readthedocs.io/en/latest/) Zict
====

|Build Status|

Mutable Mapping interfaces.  See documentation_.

.. _documentation: http://zict.readthedocs.io/en/latest/
.. |Build Status| image:: https://travis-ci.org/dask/zict.svg?branch=master
   :target: https://travis-ci.org/dask/zict



  * [zipp-0.5.2](https://github.com/jaraco/zipp) .. image:: https://img.shields.io/pypi/v/zipp.svg
   :target: https://pypi.org/project/zipp

.. image:: https://img.shields.io/pypi/pyversions/zipp.svg

.. image:: https://img.shields.io/travis/jaraco/zipp/master.svg
   :target: https://travis-ci.org/jaraco/zipp

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/ambv/black
   :alt: Code style: Black

.. image:: https://img.shields.io/appveyor/ci/jaraco/zipp/master.svg
   :target: https://ci.appveyor.com/project/jaraco/zipp/branch/master

.. .. image:: https://readthedocs.org/projects/zipp/badge/?version=latest
..    :target: https://zipp.readthedocs.io/en/latest/?badge=latest


A pathlib-compatible Zipfile object wrapper. A backport of the
`Path object <https://docs.python.org/3.8/library/zipfile.html#path-objects>`_.



  * [zope.component-4.5](https://github.com/zopefoundation/zope.component) ``zope.component``
==================

.. image:: https://img.shields.io/pypi/v/zope.component.svg
    :target: https://pypi.python.org/pypi/zope.component/
    :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.component.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.component
        :alt: Build Status

.. image:: https://readthedocs.org/projects/zopecomponent/badge/?version=latest
        :target: http://zopecomponent.readthedocs.org/en/latest/
        :alt: Documentation Status

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.component/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.component?branch=master
        :alt: Coverage Status


.. note::

   This package is intended to be independently reusable in any Python
   project. It is maintained by the
   `Zope Toolkit project <http://docs.zope.org/zopetoolkit/>`_.

This package represents the core of the Zope Component Architecture.
Together with the zope.interface_ package, it provides facilities for
defining, registering and looking up components.

Please see https://zopecomponent.readthedocs.io/en/latest/ for the
documentation.

.. _zope.interface: https://github.com/zopefoundation/zope.interface

Changes
=======

4.5 (2018-10-10)
----------------

- Add support for Python 3.7.

- Always install ``zope.hookable`` as a dependency (the ``hook``
  extra is now empty). ``zope.hookable`` respects the PURE_PYTHON
  environment variable, and has an optional C extension.

- Make accessing names that have been moved to ``zope.interface``
  produce a ``DeprecationWarning``.

4.4.1 (2017-09-26)
------------------

- Remove obsolete call of ``searchInterface`` from
  ``interfaceToName``. See https://github.com/zopefoundation/zope.component/issues/32


4.4.0 (2017-07-25)
------------------

- Add support for Python 3.6.

- Drop support for Python 3.3.

- Drop support for "setup.py test".

- Code coverage reports are now `produced and hosted by coveralls.io
  <https://coveralls.io/github/zopefoundation/zope.component>`_, and
  PRs must keep them at 100%.

- Internal test code in ``zope.component.testfiles`` has been adjusted
  and in some cases removed.

4.3.0 (2016-08-26)
------------------

- When testing ``PURE_PYTHON`` environments under ``tox``, avoid poisoning
  the user's global wheel cache.

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5.

4.2.2 (2015-06-04)
------------------

- Fix test cases for PyPy and PyPy3.

4.2.1 (2014-03-19)
------------------

- Add support for Python 3.4.

4.2.0 (2014-02-05)
------------------

- Update ``boostrap.py`` to version 2.2.

- Reset the cached ``adapter_hooks`` at ``zope.testing.cleanup.cleanUp``
  time (LP1100501).

- Implement ability to specify adapter and utility names in Python. Use
  the ``@zope.component.named(name)`` decorator to specify the name.

4.1.0 (2013-02-28)
------------------

- Change "ZODB3" depdendency to "persistent".

- ``tox`` now runs all tests for Python 3.2 and 3.3.

- Enable buildout for Python 3.

- Fix new failing tests.

4.0.2 (2012-12-31)
------------------

- Flesh out PyPI Trove classifiers.

4.0.1 (2012-11-21)
------------------

- Add support for Python 3.3.

4.0.0 (2012-07-02)
------------------

- Add PyPy and Python 3.2 support:

  - Security support omitted until ``zope.security`` ported.

  - Persistent registry support omitted until ``ZODB`` ported (or
    ``persistent`` factored out).

- Bring unit test coverage to 100%.

- Remove the long-deprecated ``layer`` argument to the
  ``zope.component.zcml.view`` and ``zope.component.zcml.resource``
  ZCML directives.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Got tests to run using ``setup.py test``.

- Add ``Sphinx`` documentation.

- Add ``setup.py docs`` alias (installs ``Sphinx`` and dependencies).

- Add ``setup.py dev`` alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

3.12.1 (2012-04-02)
-------------------

- Wrap ``with site(foo)`` in try/finally (LP768151).

3.12.0 (2011-11-16)
-------------------

- Add convenience function zope.component.hooks.site (a contextmanager),
  so one can write ``with site(foo): ...``.

3.11.0 (2011-09-22)
-------------------

- Move code from ``zope.component.registry`` which implements a basic
  nonperistent component registry to ``zope.interface.registry``.  This code
  was moved from ``zope.component`` into ``zope.interface`` to make porting
  systems (such as Pyramid) that rely only on a basic component registry to
  Python 3 possible without needing to port the entirety of the
  ``zope.component`` package.  Backwards compatibility import shims have been
  left behind in ``zope.component``, so this change will not break any
  existing code.

- Move interfaces from ``zope.component.interfaces`` to
  ``zope.interface.interfaces``: ``ComponentLookupError``, ``Invalid``,
  ``IObjectEvent``, ``ObjectEvent``, ``IComponentLookup``, ``IRegistration``,
  ``IUtilityRegistration``, ``IAdapterRegistration``,
  ``ISubscriptionAdapterRegistration``, ``IHandlerRegistration``,
  ``IRegistrationEvent``, ``RegistrationEvent``, ``IRegistered``,
  ``Registered``, ``IUnregistered``, ``Unregistered``,
  ``IComponentRegistry``, and ``IComponents``.  Backwards compatibility shims
  left in place.

- Depend on ``zope.interface`` >= 3.8.0.

3.10.0 (2010-09-25)
-------------------

- Remove the ``docs`` extra and the ``sphinxdoc`` recipe.

- Create a ``security`` extra to move security-related dependencies out of the
  ``test`` extra.

- Use the new ``zope.testrunner`` package for tests.

- Add a basic test for the ``configure.zcml`` file provided.

3.9.5 (2010-07-09)
------------------

- Fix test requirements specification.

3.9.4 (2010-04-30)
------------------

- Prefer the standard library ``doctest`` to the one from ``zope.testing``.

3.9.3 (2010-03-08)
------------------

- The ZCML directives provided by ``zope.component`` now register the
  components in the registry returned by ``getSiteManager`` instead of the
  global registry. This change allows the hooking of the ``getSiteManager``
  method before the load of a ZCML file to register the components in a
  custom registry.

3.9.2 (2010-01-22)
------------------

- Fix a bug introduced by recent refactoring, where passing
  ``CheckerPublic`` to ``securityAdapterFactory`` wrongly wrapped the factory
  into a ``LocatingUntrustedAdapterFactory``.

3.9.1 (2010-01-21)
------------------

- Modify the tests to avoid allowing the tested testrunner to be influenced
  by options of the outer testrunner, such a the ``-v`` option.

3.9.0 (2010-01-21)
------------------

- Add testlayer support. It is now possible to load a ZCML file within
  tests more easily. See ``src/zope/component/testlayer.py`` and
  ``src/zope/component/testlayer.txt``.

3.8.0 (2009-11-16)
------------------

- Remove the dependencies on ``zope.proxy`` and ``zope.security`` from the
  zcml extra: ``zope.component`` no longer has a hard dependency on them;
  the support for security proxied components ZCML registrations is enabled
  only if ``zope.security`` and ``zope.proxy`` are available.

- Move the ``IPossibleSite`` and ``ISite`` interfaces here from
  ``zope.location`` as they are dealing with ``zope.component``'s concept of
  a site, but not with location.

- Move the ``zope.site.hooks`` functionality to ``zope.component.hooks`` as it
  isn't actually dealing with ``zope.site``'s concept of a site.

3.7.1 (2009-07-24)
------------------

- Fix a problem, where ``queryNextUtility`` could fail if the context could
  not be adapted to a ``IComponentLookup``.

- Fix 2 related bugs:

  When a utility is registered and there was previously a utility
  registered for the same interface and name, then the old utility is
  unregistered.  The 2 bugs related to this:

  - There was no ``Unregistered`` for the implicit unregistration. Now
    there is.

  - The old utility was still held and returned by
    ``getAllUtilitiesRegisteredFor``.  In other words, it was still
    considered registered, eeven though it wasn't.  A particularly
    negative consequence of this is that the utility is held in memory
    or in the database even though it isn't used.

3.7.0 (2009-05-21)
------------------

- Ensure that ``HookableTests`` are run by the testrunner.

- Add ``zope:view`` and ``zope:resource`` implementations into
  ``zope.component.zcml`` (dependency loaded with ``zope.component [zcml]``).

3.6.0 (2009-03-12)
------------------

- IMPORTANT: the interfaces that were defined in the
  ``zope.component.bbb.interfaces`` and deprecated for years are
  now (re)moved. However, some packages, including part of zope
  framework were still using those interfaces. They will be adapted
  for this change. If you were using some of those interfaces, you
  need to adapt your code as well:

   - Move ``IView`` and ``IDefaultViewName`` to ``zope.publisher.interfaces``.

   - Move ``IResource`` to ``zope.app.publisher.interfaces``.

   - Remove ``IContextDependent``, ``IPresentation``, ``IPresentationRequest``,
     ``IResourceFactory``, and ``IViewFactory`` completely.

     If you used ``IViewFactory`` in context of ``zope.app.form``, there's now
     ``IWidgetFactory`` in the ``zope.app.form.interfaces`` instead.

- Move ``getNextUtility`` / ``queryNextUtility`` functions here from
  ``zope.site`` (they were in ``zope.app.component`` even earlier).

- Add a pure-Python ``hookable`` implementation, for use when
  ``zope.hookable`` is not present.

- Remove use of ``zope.deferredimport`` by breaking import cycles.

- Cleanup package documentation and changelog a bit. Add sphinx-based
  documentation building command to the buildout.

- Remove deprecated code.

- Change package's mailing list address to zope-dev at zope.org, because
  zope3-dev at zope.org is now retired.

3.5.1 (2008-07-25)
------------------

- Fix bug introduced in 3.5.0: ``<utility factory="...">`` no longer supported
  interfaces declared in Python and always wanted an explicit
  ``provides="..."`` attribute. https://bugs.launchpad.net/zope3/+bug/251865

3.5.0 (2008-07-25)
------------------

- Support registration of utilities via factories through the component
  registry and return factory information in the registration information.
  Fixes https://bugs.launchpad.net/zope3/+bug/240631

- Optimize ``un/registerUtility`` by storing an optimized data structure for
  efficient retrieval of already registered utilities. This avoids looping over
  all utilities when registering a new one.

3.4.0 (2007-09-29)
------------------

No further changes since 3.4.0a1.

3.4.0a1 (2007-04-22)
--------------------

Corresponds to ``zope.component`` from Zope 3.4.0a1.

- In the Zope 3.3.x series, ``zope.component`` was simplified yet once
  more.  See http://wiki.zope.org/zope3/LocalComponentManagementSimplification
  for the proposal describing the changes.

3.2.0.2 (2006-04-15)
--------------------

- Fix packaging bug:  ``package_dir`` must be a *relative* path.

3.2.0.1 (2006-04-14)
--------------------

- Packaging change: suppress inclusion of ``setup.cfg`` in ``sdist`` builds.

3.2.0 (2006-01-05)
------------------

Corresponds to the verison of the ``zope.component`` package shipped as part
of the Zope 3.2.0 release.

- Deprecated services and related APIs. The adapter and utility registries
  are now available directly via the site manager's 'adapters' and 'utilities'
  attributes, respectively.  Services are accessible, but deprecated, and
  will be removed in Zope 3.3.

- Deprecated all presentation-related APIs, including all view-related
  API functions. Use the adapter API functions instead.
  See http://dev.zope.org/Zope3/ImplementViewsAsAdapters`

- Deprecated ``contextdependent`` package:  site managers are now looked up
  via a thread global, set during URL traversal.  The ``context`` argument
  is now always optional, and should no longer be passed.

3.0.0 (2004-11-07)
------------------

Corresponds to the verison of the ``zope.component`` package shipped as part of
the Zope X3.0.0 release.



  * [zope.configuration-4.3.1](https://github.com/zopefoundation/zope.configuration) ``zope.configuration``
======================

.. image:: https://img.shields.io/pypi/v/zope.configuration.svg
    :target: https://pypi.python.org/pypi/zope.configuration/
    :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.configuration.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.configuration

.. image:: https://readthedocs.org/projects/zopeconfiguration/badge/?version=latest
        :target: http://zopeconfiguration.readthedocs.io/en/latest/
        :alt: Documentation Status

The Zope configuration system provides an extensible system for
supporting various kinds of configurations.

It is based on the idea of configuration directives. Users of the
configuration system provide configuration directives in some
language that express configuration choices. The intent is that the
language be pluggable.  An XML language is provided by default.

Please see http://zopeconfiguration.readthedocs.io/en/latest/ for the documentation.




Changes
=======

4.3.1 (2019-02-12)
------------------

- Do not break when running the tests from a wheel.


4.3.0 (2018-10-01)
------------------

- Simplify exception chaining and nested exception error messages.
  See `issue 43 <https://github.com/zopefoundation/zope.configuration/issues/43>`_.


4.2.2 (2018-09-27)
------------------

- Fix ``GlobalObject`` (and ``GlobalInterface``) no longer allowing
  multiple leading dots. See `issue 41
  <https://github.com/zopefoundation/zope.configuration/issues/41>`_.

- Add ``__all__`` to all modules listing the documented members of
  the module. Note that this is currently a broad list and may be
  reduced in the future.


4.2.1 (2018-09-26)
------------------

- Fix ``GlobalObject`` (and ``GlobalInterface``) no longer allowing
  just a single '.'. See `issue 35
  <https://github.com/zopefoundation/zope.configuration/issues/35>`_.


4.2.0 (2018-09-26)
------------------

- Reach 100% automated test coverage.

- Add support for Python 3.7.

- Drop support for Python 3.3 and remove internal compatibility
  functions needed to support it. See `issue 20
  <https://github.com/zopefoundation/zope.configuration/issues/20>`_
  and `issue 26
  <https://github.com/zopefoundation/zope.configuration/issues/26>`_.

- Drop support for ``python setup.py test``.

- Make ``zope.configuration.fields.Path`` and
  ``zope.configuration.config.ConfigurationContext`` expand
  environment variables and expand user home directories in paths. See
  `issue 3 <https://github.com/zopefoundation/zope.configuration/issues/3>`_.

- Fix resolving names from a Python 2 package whose ``__init__.py`` has
  unicode elements in ``__all__``.

- Make ``GroupingContextDecorator`` stop shadowing builtins in its
  ``__getattr__``. These were not intended as arguments to be used by
  subclasses, and the signature caused confusion.

- Fix the doctests with zope.schema 4.7 and above, and run the
  doctests on both Python 2 and Python 3. See `issue 21
  <https://github.com/zopefoundation/zope.configuration/issues/21>`_.

- Fix ``GlobalObject`` and ``GlobalInterface`` fields to only accept
  dotted names instead of names with ``/``. Previously, slash
  delimited names could result in incorrect imports. See `issue 6
  <https://github.com/zopefoundation/zope.configuration/issues/6>`_.

- Fix the schema fields to include the ``value`` and ``field`` values
  on exceptions they raise.

- Make ``zope.configuration.fields.PythonIdentifier`` subclass
  ``PythonIdentifier`` from ``zope.schema``. It now implements ``fromBytes``,
  always produces a native string, and validates the value in
  ``fromUnicode``. See `issue 28
  <https://github.com/zopefoundation/zope.configuration/issues/28>`_.

- Add ``ConfigurationMachine.pass_through_exceptions`` to allow
  customizing the exceptions that
  ``ConfigurationMachine.execute_actions`` wraps in a
  ``ConfigurationExecutionError``. See `issue 10
  <https://github.com/zopefoundation/zope.configuration/issues/10>`_.

- Stop catching ``BaseException`` and wrapping it in either
  ``ConfigurationExecutionError`` or ``ZopeXMLConfigurationError``.
  ``SystemExit`` and ``KeyboardInterrupt`` were always allowed to
  propagate; now ``GeneratorExit`` and custom subclasses of
  ``BaseException`` are also allowed te propagate.

4.1.0 (2017-04-26)
------------------

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5 and 3.6.

- Fix the ``domain`` of MessageID fields to be a native string.
  Previously on Python 3 they were bytes, which meant that they
  couldn't be used to find translation utilities registered by
  zope.i18n. See `issue 17 <https://github.com/zopefoundation/zope.configuration/issues/17>`_.

4.0.3 (2014-03-19)
------------------

- Add explicit support for Python 3.4.

4.0.2 (2012-12-31)
------------------

- Flesh out PyPI Trove classifiers.

- Remove spurious declaration of 'test' dependency on ``zope.testing``.

4.0.1 (2012-11-21)
------------------

- Add support for Python 3.3.

- Remove the deprecated 'zope.configuration.stxdocs' script.
  and made the 'zope.configuration.tests.conditions' helper module
  (used in running Sphinx doctest snippets) Py3k compatible.
  https://bugs.launchpad.net/zope.configuration/+bug/1025390

4.0.0 (2012-05-16)
------------------

- Bring unit test coverage to 100%.

- Automate build of Sphinx HTML docs and running doctest snippets via tox.

- Drop hard testing dependency on ``zope.testing``.

- Add explicit support for PyPy.

- Add explicit support for Python 3.2.

- Drop explicit support for Python 2.4 / 2.5.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Add ``Sphinx`` documentation.

- Add ``setup.py docs`` alias (installs ``Sphinx`` and dependencies).

- Add ``setup.py dev`` alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

3.8.1 (2012-05-05)
------------------

- Fix Python 2.4 backwards incompat (itemgetter used with multiple args);
  Python 2.4 now works (at least if you use zope.schema == 3.8.1).
  This is the last release which will support Python 2.4 or 2.5.

3.8.0 (2011-12-06)
------------------

- Change action structures from tuples to dictionaries to allow for action
  structure extensibility (merged chrism-dictactions branch).

3.7.4 (2011-04-03)
------------------

- Apply test fixes for Windows.

3.7.3 (2011-03-11)
------------------

- Correctly locate packages with a __path__ attribute but no
  __file__ attribute (such as namespace packages installed with setup.py
  install --single-version-externally-managed).

- Allow "info" and "includepath" to be passed optionally to context.action.

3.7.2 (2010-04-30)
------------------

- Prefer the standard libraries doctest module over zope.testing.doctest.

3.7.1 (2010-01-05)
------------------

- Jython support: use ``__builtin__`` module import rather than assuming
  ``__builtins__`` is available.

- Jython support: deal with the fact that the Jython SAX parser
  returns attribute sets that have an empty string indicating no
  namespace instead of ``None``.

- Allow ``setup.py test`` to run at least a subset of the tests that
  would be run when using the zope testrunner: ``setup.py test`` runs
  53 tests, while ``bin/test`` runs 156.

3.7.0 (2009-12-22)
------------------

- Adjust testing output to newer zope.schema.

- Prefer zope.testing.doctest over doctestunit.

3.6.0 (2009-04-01)
------------------

- Removed dependency of `zope.deprecation` package.

- Don't suppress deprecation warnings any more in 'zope.configuration'
  package level. This makes it more likely other packages will generate
  deprecation warnings now, which will allow us to remove more
  outdated ones.

- Don't fail when zope.testing is not installed.

- Added missing ``processFile`` method to ``IConfigurationContext``.
  It is already implemented in the mix-in class,
  ``zope.configuration.config.ConfigurationContext``, and used by
  implementations of ``include`` and ``exclude`` directives.

3.5.0 (2009-02-26)
------------------

- Added the ``exclude`` directive to standard directives. It was
  previously available via ``zc.configuration`` package and now it's
  merged into ``zope.configuration``.

- Changed package's mailing list address to zope-dev at zope.org,
  change "cheeseshop" to "pypi" in the package's url.

3.4.1 (2008-12-11)
------------------

- Use built-in 'set' type, rather than importin the 'sets' module,
  which is deprecated in Python 2.6.

- Added support to bootstrap on Jython.

3.4.0 (2007-10-02)
------------------

- Initial release as a standalone package.

Before 3.4.0
------------

This package was part of the Zope 3 distribution and did not have its own
CHANGES.txt. For earlier changes please refer to either our subversion log or
the CHANGES.txt of earlier Zope 3 releases.



  * [zope.copy-4.2](http://github.com/zopefoundation/zope.copy) ===============
 ``zope.copy``
===============

.. image:: https://img.shields.io/pypi/v/zope.copy.svg
        :target: https://pypi.python.org/pypi/zope.copy/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.copy.svg
        :target: https://pypi.org/project/zope.copy/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.copy.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.copy

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.copy/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.copy?branch=master

.. image:: https://readthedocs.org/projects/zopecopy/badge/?version=latest
        :target: http://zopecopy.readthedocs.org/en/latest/
        :alt: Documentation Status

This package provides a pluggable mechanism for copying persistent objects.

Documentation is hosted at https://zopecopy.readthedocs.io/en/latest/


=========
 Changes
=========

4.2 (2018-10-04)
================

- Use the latest and fastest protocol when pickling and unpickling and object
  during the clone operation

- Add support for Python 3.7.


4.1.0 (2017-07-31)
==================

- Drop support for Python 2.6, 3.2 and 3.3.

- Add support for Python 3.5 and 3.6.

- Restore ``zope.component`` as a testing requirement for running doctests.

4.0.3 (2014-12-26)
==================

- Add support for PyPy3.

4.0.2 (2014-03-19)
==================

- Add support for Python 3.3 and 3.4.

- Update ``boostrap.py`` to version 2.2.

4.0.1 (2012-12-31)
==================

- Flesh out PyPI Trove classifiers.

4.0.0 (2012-06-13)
==================

- Add support for Python 3.2.

- Drop ``zope.component`` as a testing requirement. Instead, register
  explicit (dummy) adapter hooks where needed.

- Add PyPy support.

- 100% unit test coverage.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Add Sphinx documentation:  moved doctest examples to API reference.

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose``, ``coverage``, and testing dependencies).

- Drop support for Python 2.4 and 2.5.

- Include tests of the LocationCopyHook from zope.location.

3.5.0 (2009-02-09)
==================

- Initial release. The functionality was extracted from ``zc.copy`` to
  provide a generic object copying mechanism with minimal dependencies.



  * [zope.deferredimport-4.3.1](http://github.com/zopefoundation/zope.deferredimport) =========================
 ``zope.deferredimport``
=========================

.. image:: https://img.shields.io/pypi/v/zope.deferredimport.svg
        :target: https://pypi.python.org/pypi/zope.deferredimport/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.deferredimport.svg
        :target: https://pypi.org/project/zope.deferredimport/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.deferredimport.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.deferredimport

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.deferredimport/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.deferredimport?branch=master

.. image:: https://readthedocs.org/projects/zopedeferredimport/badge/?version=latest
        :target: http://zopedeferredimport.readthedocs.io/en/latest/
        :alt: Documentation Status

Often, especially for package modules, you want to import names for
convenience, but not actually perform the imports until necessary.
The zope.deferredimport package provided facilities for defining names
in modules that will be imported from somewhere else when used.  You
can also cause deprecation warnings to be issued when a variable is
used.

Documentation is hosted at https://zopedeferredimport.readthedocs.io/


=========
 Changes
=========

4.3.1 (2019-08-05)
==================

- Avoid race condition in ``deferredmodule.ModuleProxy.__getattr__``
  `#8 <https://github.com/zopefoundation/zope.deferredimport/issues/8>`_.


4.3 (2018-10-05)
================

- Add support for Python 3.7.


4.2.1 (2017-10-24)
==================

- Preserve the docstrings of proxied modules created with
  ``deprecatedFrom``, ``deferredFrom``, etc. See `issue 5
  <https://github.com/zopefoundation/zope.deferredimport/issues/5>`_.


4.2.0 (2017-08-08)
==================

- Add support for Python 3.5 and 3.6.

- Drop support for Python 2.6 and 3.3.

- Convert doctests to Sphinx documentation, including building docs
  and running doctest snippets under ``tox``.


4.1.0 (2014-12-26)
==================

- Add support for PyPy.  PyPy3 support is blocked on release of fix for:
  https://bitbucket.org/pypy/pypy/issue/1946

- Add support for Python 3.4.

- Add support for testing on Travis.


4.0.0 (2013-02-28)
==================

- Add support for Python 3.3.

- Drop support for Python 2.4 and 2.5.


3.5.3 (2010-09-25)
==================

- Add test extra to declare test dependency on ``zope.testing``.


3.5.2 (2010-05-24)
==================

- Fix unit tests broken under Python 2.4 by the switch to the standard
  library ``doctest`` module.


3.5.1 (2010-04-30)
==================

- Prefer the standard library's ``doctest`` module to the one from
  ``zope.testing``.


3.5.0 (2009-02-04)
==================

- Add support to bootstrap on Jython.

- Add reference documentation.


3.4.0 (2007-07-19)
==================

- Finish release of ``zope.deferredimport``.


3.4.0b1 (2007-07-09)
====================

- Initial release as a separate project, corresponding to the
  ``zope.deferredimport`` from Zope 3.4.0b1.



  * [zope.deprecation-4.4.0](https://github.com/zopefoundation/zope.deprecation) ======================
 ``zope.deprecation``
======================

.. image:: https://img.shields.io/pypi/v/zope.deprecation.svg
        :target: https://pypi.python.org/pypi/zope.deprecation/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.deprecation.svg
        :target: https://pypi.org/project/zope.deprecation/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.deprecation.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.deprecation

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.deprecation/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.deprecation?branch=master

.. image:: https://readthedocs.org/projects/zopedeprecation/badge/?version=latest
        :target: https://zopedeprecation.readthedocs.io/en/latest/
        :alt: Documentation Status


This package provides a simple function called ``deprecated(names, reason)``
to mark deprecated modules, classes, functions, methods and properties.

Please see https://zopedeprecation.readthedocs.io for the documentation.


================================
 ``zope.deprecation`` Changelog
================================

4.4.0 (2018-12-03)
==================

- Add support for Python 3.7.


4.3.0 (2017-08-07)
==================

- Allow custom warning classes to be specified to override the default
  ``DeprecationWarning``.
  See https://github.com/zopefoundation/zope.deprecation/pull/7

- Add support for Python 3.6.

- Drop support for Python 3.3.

4.2.0 (2016-11-07)
==================

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5.

4.1.2 (2015-01-13)
==================

- Do not require a ``self`` parameter for deprecated functions.  See:
  https://github.com/zopefoundation/zope.deprecation/pull/1

4.1.1 (2014-03-19)
==================

- Added explicit support for Python 3.4.

4.1.0 (2013-12-20)
==================

- Added a ``Suppressor`` context manager, allowing scoped suppression of
  deprecation warnings.

- Updated ``boostrap.py`` to version 2.2.

4.0.2 (2012-12-31)
==================

- Fleshed out PyPI Trove classifiers.

4.0.1 (2012-11-21)
==================

- Added support for Python 3.3.

4.0.0 (2012-05-16)
==================

- Automated build of Sphinx HTML docs and running doctest snippets via tox.

- Added Sphinx documentation:

  - API docs moved from package-data README into ``docs/api.rst``.

  - Snippets can be tested by running 'make doctest'.

- Updated support for continuous integration using ``tox`` and ``jenkins``.

- 100% unit test coverage.

- Added ``setup.py dev`` alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Added ``setup.py docs`` alias (installs ``Sphinx`` and dependencies).

- Removed spurious dependency on ``zope.testing``.

- Dropped explicit support for Python 2.4 / 2.5 / 3.1.


3.5.1 (2012-03-15)
==================

- Revert a move of `README.txt` to unbreak ``zope.app.apidoc``.


3.5.0 (2011-09-05)
==================

- Replaced doctesting with unit testing.

- Python 3 compatibility.


3.4.1 (2011-06-07)
==================

- Removed import cycle for ``__show__`` by defining it in the
  ``zope.deprecation.deprecation`` module.

- Added support to bootstrap on Jython.

- Fix ``zope.deprecation.warn()`` to make the signature identical to
  ``warnings.warn()`` and to check for .pyc and .pyo files.


3.4.0 (2007-07-19)
==================

- Release 3.4 final, corresponding to Zope 3.4.


3.3.0 (2007-02-18)
==================

- Corresponds to the version of the ``zope.deprecation`` package shipped as
  part of the Zope 3.3.0 release.



  * [zope.event-4.4](http://github.com/zopefoundation/zope.event) =======================
 ``zope.event`` README
=======================

.. image:: https://img.shields.io/pypi/v/zope.event.svg
        :target: https://pypi.python.org/pypi/zope.event/
        :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.event.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.event

.. image:: https://readthedocs.org/projects/zopeevent/badge/?version=latest
        :target: http://zopeevent.readthedocs.org/en/latest/
        :alt: Documentation Status

The ``zope.event`` package provides a simple event system, including:

- An event publishing API, intended for use by applications which are
  unaware of any subscribers to their events.

- A very simple event-dispatching system on which more sophisticated
  event dispatching systems can be built. For example, a type-based
  event dispatching system that builds on ``zope.event`` can be found in
  ``zope.component``.

Please see http://zopeevent.readthedocs.io/ for the documentation.

==========================
 ``zope.event`` Changelog
==========================

4.4 (2018-10-05)
================

- Add support for Python 3.7


4.3.0 (2017-07-25)
==================

- Add support for Python 3.6.

- Drop support for Python 3.3.


4.2.0 (2016-02-17)
==================

- Add support for Python 3.5.

- Drop support for Python 2.6 and 3.2.


4.1.0 (2015-10-18)
==================

- Require 100% branch (as well as statement) coverage.

- Add a simple class-based handler implementation.


4.0.3 (2014-03-19)
==================

- Add support for Python 3.4.

- Update ``boostrap.py`` to version 2.2.


4.0.2 (2012-12-31)
==================

- Flesh out PyPI Trove classifiers.

- Add support for jython 2.7.


4.0.1 (2012-11-21)
==================

- Add support for Python 3.3.


4.0.0 (2012-05-16)
==================

- Automate build of Sphinx HTML docs and running doctest snippets via tox.

- Drop explicit support for Python 2.4 / 2.5 / 3.1.

- Add support for PyPy.


3.5.2 (2012-03-30)
==================

- This release is the last which will maintain support for Python 2.4 /
  Python 2.5.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).


3.5.1 (2011-08-04)
==================

- Add Sphinx documentation.


3.5.0 (2010-05-01)
==================

- Add change log to ``long-description``.

- Add support for Python 3.x.


3.4.1 (2009-03-03)
==================

- A few minor cleanups.


3.4.0 (2007-07-14)
==================

- Initial release as a separate project.



  * [zope.exceptions-4.3](https://github.com/zopefoundation/zope.exceptions) =================
 zope.exceptions
=================

.. image:: https://img.shields.io/pypi/v/zope.exceptions.svg
        :target: https://pypi.python.org/pypi/zope.exceptions/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.exceptions.svg
        :target: https://pypi.org/project/zope.exceptions/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.exceptions.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.exceptions

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.exceptions/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.exceptions?branch=master

.. image:: https://readthedocs.org/projects/zopeexceptions/badge/?version=latest
        :target: https://zopeexceptions.readthedocs.io/en/latest/
        :alt: Documentation Status

This package contains exception exceptions and implementations which are so
general purpose that they don't belong in Zope application-specific packages.

Please see https://zopeexceptions.readthedocs.io/ for the documentation.


===========================
 zope.exceptions Changelog
===========================

4.3 (2018-10-04)
================

- Add support for Python 3.7.


4.2.0 (2017-09-12)
==================

- Add support for Python 3.6.

- Drop support for Python 3.3.

- Fix handling of unicode supplemental traceback information on
  Python 2. Now such values are always encoded to UTF-8; previously
  the results were undefined and depended on system encodings and the
  values themselves. See `issue 1 <https://github.com/zopefoundation/zope.exceptions/issues/1>`_.

4.1.0 (2017-04-12)
==================

- Drop support for Python 2.6 and 3.2.

- Make ``exceptionformatter.extract_stack`` signature comply with
  ``traceback.extract_stack``

- Add support for Python 3.5.

4.0.8 (2015-08-13)
==================

- Fixes around ``TextExceptionFormatter`` ``limit``: ``formatException``
  and ``extractStack`` was cutting the traceback at the bottom,
  at the most interesting point. Now it will cut from the middle.
  Some text about the missing entries will be inserted.

- Maybe fix for ``extractStack``, it did not detect recursions in the frames.

4.0.7 (2014-03-19)
==================

- Added explicit support for Python 3.4.

- Updated ``boostrap.py`` to version 2.2.


4.0.6 (2013-02-28)
==================

- Make sure that ``setup.py`` finds all tests. Now tox runs them all as well.

- Fix failing test under Python 3.

- Made buildout work under Python 3 and Buildout 2.


4.0.5 (2012-12-31)
==================

- Fleshed out PyPI Trove classifiers.

- Fixed a test failure under Python 2.6.


4.0.4 (2012-12-13)
==================

- Release with a fixed MANIFEST.in (without ``docs/``)


4.0.3 (2012-12-10)
==================

- Fixed format_exception(..., as_html=True) not to HTML-escape the '<br />'
  it adds to the exception value.


4.0.2 (2012-11-21)
==================

- Test Python 3.3 support under tox.


4.0.1 (2012-08-20)
==================

- Fixed optional dependency code for `'zope.security`` to work under Python 3.3.


4.0.0.1 (2012-05-16)
====================

- Fixed rendering of package docs on PyPI.


4.0.0 (2012-05-16)
==================

- Automated build of Sphinx HTML docs and running doctest snippets via tox.

- Added Sphinx documentation.

- Added support for continuous integration using ``tox`` and ``jenkins``.

- Removed use of '2to3' and associated fixers when installing under Py3k.
  The code is now in a "compatible subset" which supports Python 2.6, 2.7,
  and 3.2, including PyPy 1.8 (the version compatible with the 2.7 language
  spec).

- 100% unit test coverage.

- Dropped explicit support for Python 2.4 / 2.5 / 3.1.

- Added 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Added 'setup.py docs' alias (installs ``Sphinx`` and dependencies).


3.7.1 (2012-03-28)
==================

- Fix: missed to reverse extractStack entries


3.7.0 (2012-03-28)
==================

- Added TextExceptionFormatter.extractStack and extract_stack


3.6.2 (2012-03-28)
==================

- Fallback to traceback.format_tb when the formatter is called recursively.
  i.e. Don't let errors in the formatter pass silently.

- Fix deprecated unittest functions: ``assert_`` and ``assertEquals``.

3.6.1 (2010-07-06)
==================

- Fixed tests to work under Python 2.7.

- PEP8 cleanup and removed obsolete build infrastructure files.


3.6.0 (2010-05-02)
==================

- Added support to bootstrap on Jython.

- Added Python 3 support.

- The dependency on zope.testing seemed spurious, possibly a rest of a real
  dependency that is gone now. I removed it.


3.5.2 (2008-04-30)
==================

- Updated CHANGES.txt.


3.5.1 (2008-04-28)
==================

- Reverted changes in 3.5.0.


3.5.0
=====

- Added the capability for exceptions to be formatted line-by-line.
  Unfortunately, also introduced a bug cause each line of the exception to be
  its own log message.


3.4.0 (2007-10-02)
==================

- Updated package meta-data.


3.4.0b2 (2007-08-14)
====================

- Removed superfluous dependency on ``zope.deprecation``.


3.4.0b1 (2007-07-09)
====================

- Corresponds to the version of the ``zope.exceptions`` package shipped as
  part of the Zope 3.4.0b1 release.


3.2.0 (2006-01-05)
==================

- Corresponds to the version of the ``zope.exceptions`` package shipped as part of
  the Zope 3.2.0 release.

- Deprecated the ``INotFoundError`` interface and the corresponding
  ``NotFoundError`` exception class, in favor of "standard" exceptions
  ``AttributeError``, ``KeyError``).  The deprecated items will be removed in
  Zope 3.3.


3.0.0 (2004-11-07)
==================

- Corresponds to the version of the zope.exceptions package shipped as part of
  the Zope X3.0.0 release.



  * [zope.hookable-4.2.0](http://github.com/zopefoundation/zope.hookable) ===============
 zope.hookable
===============

.. image:: https://img.shields.io/pypi/v/zope.hookable.svg
        :target: https://pypi.python.org/pypi/zope.hookable/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.hookable.svg
        :target: https://pypi.org/project/zope.hookable/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.hookable.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.hookable

.. image:: https://readthedocs.org/projects/zopehookable/badge/?version=latest
        :target: https://zopehookable.readthedocs.io/en/latest/
        :alt: Documentation Status

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.hookable/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.hookable?branch=master


This package supports the efficient creation of "hookable" objects, which
are callable objects that are meant to be optionally replaced.

The idea is that you create a function that does some default thing and make it
hookable. Later, someone can modify what it does by calling its sethook method
and changing its implementation.  All users of the function, including those
that imported it, will see the change.

Documentation is hosted at https://zopehookable.readthedocs.io


=========
 Changes
=========

4.2.0 (2017-11-07)
==================

- Expose the ``__doc__`` (and, where applicable, ``__bases__`` and
  ``__dict__``) of the hooked object. This lets Sphinx document them.
  See `issue 6 <https://github.com/zopefoundation/zope.hookable/issues/6>`_.

- Respect ``PURE_PYTHON`` at runtime. At build time, always try to
  build the C extensions on supported platforms, but allow it to fail.
  See `issue 7
  <https://github.com/zopefoundation/zope.hookable/issues/7>`_.


4.1.0 (2017-07-26)
==================

- Drop support for Python 2.6, 3.2 and 3.3.

- Add support for Python 3.5 and 3.6.

4.0.4 (2014-03-19)
==================

- Add support for Python 3.4.

4.0.3 (2014-03-17)
==================

- Update ``boostrap.py`` to version 2.2.

- Fix extension compilation on Py3k.

4.0.2 (2012-12-31)
==================

- Flesh out PyPI Trove classifiers.

4.0.1 (2012-11-21)
==================

- Add support for Python 3.3.

- Avoid building the C extension explicitly (use the "feature" indirection
  instead).  https://bugs.launchpad.net/zope.hookable/+bug/1025470

4.0.0 (2012-06-04)
==================

- Add support for PyPy.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Add a pure-Python reference implementation.

- Move doctests to Sphinx documentation.

- Bring unit test coverage to 100%.

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Drop support for Python 2.4 / 2.5.

- Remove of 'zope.testing.doctestunit' in favor of stdlib's 'doctest.

- Add Python 3 support.

3.4.1 (2009-04-05)
==================

- Update for compatibility with Python 2.6 traceback formats.

- Use Jython-compatible ``bootstrap.py``.

3.4.0 (2007-07-20)
==================

- Initial release as a separate project.



  * [zope.i18nmessageid-4.3.1](https://github.com/zopefoundation/zope.i18nmessageid) ``zope.i18nmessageid``
======================

.. image:: https://img.shields.io/pypi/v/zope.i18nmessageid.svg
    :target: https://pypi.python.org/pypi/zope.i18nmessageid/
    :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.i18nmessageid.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.i18nmessageid

.. image:: https://readthedocs.org/projects/zopei18nmessageid/badge/?version=latest
        :target: http://zopei18nmessageid.readthedocs.org/en/latest/
        :alt: Documentation Status

To translate any text, we must be able to discover the source domain
of the text.  A source domain is an identifier that identifies a
project that produces program source strings.  Source strings occur as
literals in python programs, text in templates, and some text in XML
data.  The project implies a source language and an application
context.

We can think of a source domain as a collection of messages and
associated translation strings.

We often need to create unicode strings that will be displayed by
separate views.  The view cannot translate the string without knowing
its source domain.  A string or unicode literal carries no domain
information, therefore we use messages.  Messages are unicode strings
which carry a translation source domain and possibly a default
translation.  They are created by a message factory. The message
factory is created by calling ``MessageFactory`` with the source
domain.

This package provides facilities for *declaring* such messages within
program source text;  translation of the messages is the responsiblitiy
of the 'zope.i18n' package.

Please see http://docs.zope.org/zope.i18nmessageid/ for the documentation.


=========
 Changes
=========

4.3.1 (2018-10-19)
==================

- Fix a regression copying Message objects in the Python
  implementation. See `issue 14
  <https://github.com/zopefoundation/zope.i18nmessageid/issues/14>`_.


4.3.0 (2018-10-18)
==================

- Add attributes to support pluralization on a Message and update the
  MessageFactory accordingly.


4.2.0 (2018-10-05)
==================

- Fix the possibility of a rare crash in the C extension when
  deallocating items. See `issue 7
  <https://github.com/zopefoundation/zope.i18nmessageid/issues/7>`_.

- Drop support for Python 3.3.

- Add support for Python 3.7.


4.1.0 (2017-05-02)
==================

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5 and 3.6.

- Fix the C extension not being used in Python 3. See `issue 4
  <https://github.com/zopefoundation/zope.i18nmessageid/issues/4>`_.

- Make the Python implementation of Message accept any object for the
  ``default`` argument, just as the C extension does. This should be a
  unicode or byte string. See `issue 5
  <https://github.com/zopefoundation/zope.i18nmessageid/issues/5>`_.

4.0.3 (2014-03-19)
==================

- Add support for Python 3.4.

- Update ``boostrap.py`` to version 2.2.

4.0.2 (2012-12-31)
==================

- Flesh out PyPI Trove classifiers.

4.0.1 (2012-11-21)
==================

- Add support for Python 3.3.

4.0.0 (2012-05-16)
==================

- Automate generation of Sphinx HTML docs and running doctest snippets via tox.

- Remove use of '2to3' and associated fixers when installing under Py3k.
  The code is now in a "compatible subset" which supports Python 2.6, 2.7,
  and 3.2, including PyPy 1.8 (the version compatible with the 2.7 language
  spec).

- Bring unit test coverage to 100%.

- Move doctest examples into Sphinx documentation.

- Drop explicit support for Python 2.4 / 2.5 / 3.1.

- Add explicit support for PyPy.

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).


3.6.1 (2011-07-20)
==================

- Correct metadata in this file for release date.

3.6.0 (2011-07-20)
==================

- Python 3 support.

- Suppress compiling C extensions on PyPy or Jython.

- Add a tox.ini (see http://tox.readthedocs.org/en/latest/) for easier
  automated testing.

3.5.3 (2010-08-10)
==================

- Make compilation of C extension optional again; 3.5.1 broke this
  inasmuch as this package become unusable on non-CPython platforms.
  Making the compilation of the C extension optional again implied
  removing ``setup.py`` code added in 3.5.1 which made the C extension
  a setuptools "Feature" and readding code from 3.5.0 which overrides
  the distutils ``build_ext`` command.

- Move pickle equality tests into a unittest.TestCase test to make it
  easier to condition the tests on whether the C extension has been
  compiled.  This also makes the tests pass on Jython.

3.5.2 (2010-04-30)
==================

- Remove use of 'zope.testing.doctestunit' in favor of stdlib's 'doctest.

3.5.1 (2010-04-10)
==================

- LP #257657 / 489529:  Fix memory leak in C extension.

- Fix the compilation of the C extension with python 2.6: refactored it as a
  setuptools Feature.

3.5.0 (2009-06-27)
==================

- Make compilation of C extension optional.

- Add support to bootstrap on Jython.

- Change package's mailing list address from zope3-dev at zope.org to
  zope-dev at zope.org, because zope3-dev is now retired.

- Reformat change log to common formatting style.

- Update package description and docs a little.

- Remove old .cfg files for zpkg.

3.4.3 (2007-09-26)
==================

- Make PyPI the home URL.

3.4.2 (2007-09-25)
==================

- Move the ``ZopeMessageFactory`` from ``zope.app.i18n`` to this package.

3.4.0 (2007-07-19)
==================

- Remove incorrect dependency.

- Create final release to reflect package status.

3.2.0 (2006-01-05)
==================

- Corresponds to the verison of the zope.i18nmessageid package shipped as
  part of the Zope 3.2.0 release.

- Implement 'zope.i18nmessageid.message' as a C extension.

- Deprecate 'zope.i18nmessageid.messageid' APIs ('MessageID',
  'MessageIDFactory') in favor of replacements in 'zope.i18nmessageid.message'
  ('Message', 'MessageFactory').  Deprecated items are scheduled for removal
  in Zope 3.3.

3.0.0 (2004-11-07)
==================

- Corresponds to the verison of the zope.i18nmessageid package shipped as
  part of the Zope X3.0.0 release.



  * [zope.interface-4.6.0](https://github.com/zopefoundation/zope.interface) ``zope.interface``
==================

.. image:: https://img.shields.io/pypi/v/zope.interface.svg
    :target: https://pypi.python.org/pypi/zope.interface/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/zope.interface.svg
        :target: https://pypi.org/project/zope.interface/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.interface.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.interface

.. image:: https://readthedocs.org/projects/zopeinterface/badge/?version=latest
        :target: https://zopeinterface.readthedocs.io/en/latest/
        :alt: Documentation Status

This package is intended to be independently reusable in any Python
project. It is maintained by the `Zope Toolkit project
<https://zopetoolkit.readthedocs.io/>`_.

This package provides an implementation of "object interfaces" for Python.
Interfaces are a mechanism for labeling objects as conforming to a given
API or contract. So, this package can be considered as implementation of
the `Design By Contract`_ methodology support in Python.

.. _Design By Contract: http://en.wikipedia.org/wiki/Design_by_contract

For detailed documentation, please see https://zopeinterface.readthedocs.io/en/latest/

Changes
=======

4.6.0 (2018-10-23)
------------------

- Add support for Python 3.7

- Fix ``verifyObject`` for class objects with staticmethods on
  Python 3. See `issue 126
  <https://github.com/zopefoundation/zope.interface/issues/126>`_.

4.5.0 (2018-04-19)
------------------

- Drop support for 3.3, avoid accidental dependence breakage via setup.py.
  See `PR 110 <https://github.com/zopefoundation/zope.interface/pull/110>`_.
- Allow registering and unregistering instance methods as listeners.
  See `issue 12 <https://github.com/zopefoundation/zope.interface/issues/12>`_
  and `PR 102 <https://github.com/zopefoundation/zope.interface/pull/102>`_.
- Synchronize and simplify zope/__init__.py. See `issue 114
  <https://github.com/zopefoundation/zope.interface/issues/114>`_


4.4.3 (2017-09-22)
------------------

- Avoid exceptions when the ``__annotations__`` attribute is added to
  interface definitions with Python 3.x type hints. See `issue 98
  <https://github.com/zopefoundation/zope.interface/issues/98>`_.
- Fix the possibility of a rare crash in the C extension when
  deallocating items. See `issue 100
  <https://github.com/zopefoundation/zope.interface/issues/100>`_.


4.4.2 (2017-06-14)
------------------

- Fix a regression storing
  ``zope.component.persistentregistry.PersistentRegistry`` instances.
  See `issue 85 <https://github.com/zopefoundation/zope.interface/issues/85>`_.

- Fix a regression that could lead to the utility registration cache
  of ``Components`` getting out of sync. See `issue 93
  <https://github.com/zopefoundation/zope.interface/issues/93>`_.

4.4.1 (2017-05-13)
------------------

- Simplify the caching of utility-registration data. In addition to
  simplification, avoids spurious test failures when checking for
  leaks in tests with persistent registries. See `pull 84
  <https://github.com/zopefoundation/zope.interface/pull/84>`_.

- Raise ``ValueError`` when non-text names are passed to adapter registry
  methods:  prevents corruption of lookup caches.

4.4.0 (2017-04-21)
------------------

- Avoid a warning from the C compiler.
  (https://github.com/zopefoundation/zope.interface/issues/71)

- Add support for Python 3.6.

4.3.3 (2016-12-13)
------------------

- Correct typos and ReST formatting errors in documentation.

- Add API documentation for the adapter registry.

- Ensure that the ``LICENSE.txt`` file is included in built wheels.

- Fix C optimizations broken on Py3k.  See the Python bug at:
  http://bugs.python.org/issue15657
  (https://github.com/zopefoundation/zope.interface/issues/60)


4.3.2 (2016-09-05)
------------------

- Fix equality testing of ``implementedBy`` objects and proxies.
  (https://github.com/zopefoundation/zope.interface/issues/55)


4.3.1 (2016-08-31)
------------------

- Support Components subclasses that are not hashable.
  (https://github.com/zopefoundation/zope.interface/issues/53)


4.3.0 (2016-08-31)
------------------

- Add the ability to sort the objects returned by ``implementedBy``.
  This is compatible with the way interface classes sort so they can
  be used together in ordered containers like BTrees.
  (https://github.com/zopefoundation/zope.interface/issues/42)

- Make ``setuptools`` a hard dependency of ``setup.py``.
  (https://github.com/zopefoundation/zope.interface/issues/13)

- Change a linear algorithm (O(n)) in ``Components.registerUtility`` and
  ``Components.unregisterUtility`` into a dictionary lookup (O(1)) for
  hashable components. This substantially improves the time taken to
  manipulate utilities in large registries at the cost of some
  additional memory usage. (https://github.com/zopefoundation/zope.interface/issues/46)


4.2.0 (2016-06-10)
------------------

- Add support for Python 3.5

- Drop support for Python 2.6 and 3.2.


4.1.3 (2015-10-05)
------------------

- Fix installation without a C compiler on Python 3.5
  (https://github.com/zopefoundation/zope.interface/issues/24).


4.1.2 (2014-12-27)
------------------

- Add support for PyPy3.

- Remove unittest assertions deprecated in Python3.x.

- Add ``zope.interface.document.asReStructuredText``, which formats the
  generated text for an interface using ReST double-backtick markers.


4.1.1 (2014-03-19)
------------------

- Add support for Python 3.4.


4.1.0 (2014-02-05)
------------------

- Update ``boostrap.py`` to version 2.2.

- Add ``@named(name)`` declaration, that specifies the component name, so it
  does not have to be passed in during registration.


4.0.5 (2013-02-28)
------------------

- Fix a bug where a decorated method caused false positive failures on
  ``verifyClass()``.


4.0.4 (2013-02-21)
------------------

- Fix a bug that was revealed by porting zope.traversing. During a loop, the
  loop body modified a weakref dict causing a ``RuntimeError`` error.

4.0.3 (2012-12-31)
------------------

- Fleshed out PyPI Trove classifiers.

4.0.2 (2012-11-21)
------------------

- Add support for Python 3.3.

- Restored ability to install the package in the absence of ``setuptools``.

- LP #1055223:  Fix test which depended on dictionary order and failed randomly
  in Python 3.3.

4.0.1 (2012-05-22)
------------------

- Drop explicit ``DeprecationWarnings`` for "class advice" APIS (these
  APIs are still deprecated under Python 2.x, and still raise an exception
  under Python 3.x, but no longer cause a warning to be emitted under
  Python 2.x).

4.0.0 (2012-05-16)
------------------

- Automated build of Sphinx HTML docs and running doctest snippets via tox.

- Deprecate the "class advice" APIs from ``zope.interface.declarations``:
  ``implements``, ``implementsOnly``, and ``classProvides``.  In their place,
  prefer the equivalent class decorators: ``@implementer``,
  ``@implementer_only``, and ``@provider``.  Code which uses the deprecated
  APIs will not work as expected under Py3k.

- Remove use of '2to3' and associated fixers when installing under Py3k.
  The code is now in a "compatible subset" which supports Python 2.6, 2.7,
  and 3.2, including PyPy 1.8 (the version compatible with the 2.7 language
  spec).

- Drop explicit support for Python 2.4 / 2.5 / 3.1.

- Add support for PyPy.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).

- Replace all unittest coverage previously accomplished via doctests with
  unittests.  The doctests have been moved into a ``docs`` section, managed
  as a Sphinx collection.

- LP #910987:  Ensure that the semantics of the ``lookup`` method of
  ``zope.interface.adapter.LookupBase`` are the same in both the C and
  Python implementations.

- LP #900906:  Avoid exceptions due to tne new ``__qualname__`` attribute
  added in Python 3.3 (see PEP 3155 for rationale).  Thanks to Antoine
  Pitrou for the patch.

3.8.0 (2011-09-22)
------------------

- New module ``zope.interface.registry``.  This is code moved from
  ``zope.component.registry`` which implements a basic nonperistent component
  registry as ``zope.interface.registry.Components``.  This class was moved
  from ``zope.component`` to make porting systems (such as Pyramid) that rely
  only on a basic component registry to Python 3 possible without needing to
  port the entirety of the ``zope.component`` package.  Backwards
  compatibility import shims have been left behind in ``zope.component``, so
  this change will not break any existing code.

- New ``tests_require`` dependency: ``zope.event`` to test events sent by
  Components implementation.  The ``zope.interface`` package does not have a
  hard dependency on ``zope.event``, but if ``zope.event`` is importable, it
  will send component registration events when methods of an instance of
  ``zope.interface.registry.Components`` are called.

- New interfaces added to support ``zope.interface.registry.Components``
  addition: ``ComponentLookupError``, ``Invalid``, ``IObjectEvent``,
  ``ObjectEvent``, ``IComponentLookup``, ``IRegistration``,
  ``IUtilityRegistration``, ``IAdapterRegistration``,
  ``ISubscriptionAdapterRegistration``, ``IHandlerRegistration``,
  ``IRegistrationEvent``, ``RegistrationEvent``, ``IRegistered``,
  ``Registered``, ``IUnregistered``, ``Unregistered``,
  ``IComponentRegistry``, and ``IComponents``.

- No longer Python 2.4 compatible (tested under 2.5, 2.6, 2.7, and 3.2).

3.7.0 (2011-08-13)
------------------

- Move changes from 3.6.2 - 3.6.5 to a new 3.7.x release line.

3.6.7 (2011-08-20)
------------------

- Fix sporadic failures on x86-64 platforms in tests of rich comparisons
  of interfaces.

3.6.6 (2011-08-13)
------------------

- LP #570942:  Now correctly compare interfaces  from different modules but
  with the same names.

  N.B.: This is a less intrusive / destabilizing fix than the one applied in
  3.6.3:  we only fix the underlying cmp-alike function, rather than adding
  the other "rich comparison" functions.

- Revert to software as released with 3.6.1 for "stable" 3.6 release branch.

3.6.5 (2011-08-11)
------------------

- LP #811792:  work around buggy behavior in some subclasses of
  ``zope.interface.interface.InterfaceClass``, which invoke ``__hash__``
  before initializing ``__module__`` and ``__name__``.  The workaround
  returns a fixed constant hash in such cases, and issues a ``UserWarning``.

- LP #804832:  Under PyPy, ``zope.interface`` should not build its C
  extension.  Also, prevent attempting to build it under Jython.

- Add a tox.ini for easier xplatform testing.

- Fix testing deprecation warnings issued when tested under Py3K.

3.6.4 (2011-07-04)
------------------

- LP 804951:  InterfaceClass instances were unhashable under Python 3.x.

3.6.3 (2011-05-26)
------------------

- LP #570942:  Now correctly compare interfaces  from different modules but
  with the same names.

3.6.2 (2011-05-17)
------------------

- Moved detailed documentation out-of-line from PyPI page, linking instead to
  http://docs.zope.org/zope.interface .

- Fixes for small issues when running tests under Python 3.2 using
  ``zope.testrunner``.

- LP # 675064:  Specify return value type for C optimizations module init
  under Python 3:  undeclared value caused warnings, and segfaults on some
  64 bit architectures.

- setup.py now raises RuntimeError if you don't have Distutils installed when
  running under Python 3.

3.6.1 (2010-05-03)
------------------

- A non-ASCII character in the changelog made 3.6.0 uninstallable on
  Python 3 systems with another default encoding than UTF-8.

- Fix compiler warnings under GCC 4.3.3.

3.6.0 (2010-04-29)
------------------

- LP #185974:  Clear the cache used by ``Specificaton.get`` inside
  ``Specification.changed``.  Thanks to Jacob Holm for the patch.

- Add support for Python 3.1. Contributors:

    Lennart Regebro
    Martin v Loewis
    Thomas Lotze
    Wolfgang Schnerring

  The 3.1 support is completely backwards compatible. However, the implements
  syntax used under Python 2.X does not work under 3.X, since it depends on
  how metaclasses are implemented and this has changed. Instead it now supports
  a decorator syntax (also under Python 2.X)::

    class Foo:
        implements(IFoo)
        ...

  can now also be written::

    @implementer(IFoo):
    class Foo:
        ...

  There are 2to3 fixers available to do this change automatically in the
  zope.fixers package.

- Python 2.3 is no longer supported.


3.5.4 (2009-12-23)
------------------

- Use the standard Python doctest module instead of zope.testing.doctest, which
  has been deprecated.


3.5.3 (2009-12-08)
------------------

- Fix an edge case: make providedBy() work when a class has '__provides__' in
  its __slots__ (see http://thread.gmane.org/gmane.comp.web.zope.devel/22490)


3.5.2 (2009-07-01)
------------------

- BaseAdapterRegistry.unregister, unsubscribe: Remove empty portions of
  the data structures when something is removed.  This avoids leaving
  references to global objects (interfaces) that may be slated for
  removal from the calling application.


3.5.1 (2009-03-18)
------------------

- verifyObject: use getattr instead of hasattr to test for object attributes
  in order to let exceptions other than AttributeError raised by properties
  propagate to the caller

- Add Sphinx-based documentation building to the package buildout
  configuration. Use the ``bin/docs`` command after buildout.

- Improve package description a bit. Unify changelog entries formatting.

- Change package's mailing list address to zope-dev at zope.org as
  zope3-dev at zope.org is now retired.


3.5.0 (2008-10-26)
------------------

- Fix declaration of _zope_interface_coptimizations, it's not a top level
  package.

- Add a DocTestSuite for odd.py module, so their tests are run.

- Allow to bootstrap on Jython.

- Fix https://bugs.launchpad.net/zope3/3.3/+bug/98388: ISpecification
  was missing a declaration for __iro__.

- Add optional code optimizations support, which allows the building
  of C code optimizations to fail (Jython).

- Replace `_flatten` with a non-recursive implementation, effectively making
  it 3x faster.


3.4.1 (2007-10-02)
------------------

- Fix a setup bug that prevented installation from source on systems
  without setuptools.


3.4.0 (2007-07-19)
------------------

- Final release for 3.4.0.


3.4.0b3 (2007-05-22)
--------------------


- When checking whether an object is already registered, use identity
  comparison, to allow adding registering with picky custom comparison methods.


3.3.0.1 (2007-01-03)
--------------------

- Made a reference to OverflowWarning, which disappeared in Python
  2.5, conditional.


3.3.0 (2007/01/03)
------------------

New Features
++++++++++++

- Refactor the adapter-lookup algorithim to make it much simpler and faster.

  Also, implement more of the adapter-lookup logic in C, making
  debugging of application code easier, since there is less
  infrastructre code to step through.

- Treat objects without interface declarations as if they
  declared that they provide ``zope.interface.Interface``.

- Add a number of richer new adapter-registration interfaces
  that provide greater control and introspection.

- Add a new interface decorator to zope.interface that allows the
  setting of tagged values on an interface at definition time (see
  zope.interface.taggedValue).

Bug Fixes
+++++++++

- A bug in multi-adapter lookup sometimes caused incorrect adapters to
  be returned.


3.2.0.2 (2006-04-15)
--------------------

- Fix packaging bug:  'package_dir' must be a *relative* path.


3.2.0.1 (2006-04-14)
--------------------

- Packaging change:  suppress inclusion of 'setup.cfg' in 'sdist' builds.


3.2.0 (2006-01-05)
------------------

- Corresponds to the verison of the zope.interface package shipped as part of
  the Zope 3.2.0 release.


3.1.0 (2005-10-03)
------------------

- Corresponds to the verison of the zope.interface package shipped as part of
  the Zope 3.1.0 release.

- Made attribute resolution order consistent with component lookup order,
  i.e. new-style class MRO semantics.

- Deprecate 'isImplementedBy' and 'isImplementedByInstancesOf' APIs in
  favor of 'implementedBy' and 'providedBy'.


3.0.1 (2005-07-27)
------------------

- Corresponds to the verison of the zope.interface package shipped as part of
  the Zope X3.0.1 release.

- Fix a bug reported by James Knight, which caused adapter registries
  to fail occasionally to reflect declaration changes.


3.0.0 (2004-11-07)
------------------

- Corresponds to the verison of the zope.interface package shipped as part of
  the Zope X3.0.0 release.



  * [zope.location-4.2](http://github.com/zopefoundation/zope.location/) ===================
 ``zope.location``
===================

.. image:: https://img.shields.io/pypi/v/zope.location.svg
        :target: https://pypi.python.org/pypi/zope.location/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.location.svg
        :target: https://pypi.org/project/zope.location/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.location.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.location

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.location/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.location?branch=master

.. image:: https://readthedocs.org/projects/zopelocation/badge/?version=latest
        :target: http://zopelocation.readthedocs.org/en/latest/
        :alt: Documentation Status

In Zope3, "locations" are special objects that have a structural
location, indicated with ``__name__`` and ``__parent__`` attributes.

See `zope.container <https://zopecontainer.readthedocs.io/en/latest>`_
for a useful extension of this concept to "containers."

Documentation is hosted at https://zopelocation.readthedocs.io/en/latest/


=========
 Changes
=========

4.2 (2018-10-09)
================

- Add support for Python 3.7.


4.1.0 (2017-08-03)
==================

- Drop support for Python 2.6, 3.2 and 3.3.

- Add a page to the docs on hacking ``zope.location``.

- Note additional documentation dependencies.

- Add support for Python 3.5 and 3.6.

- Remove internal ``_compat`` implementation module.

4.0.3 (2014-03-19)
==================

- Add Python 3.4 support.

- Update ``boostrap.py`` to version 2.2.


4.0.2 (2013-03-11)
==================

- Change the behavior of ``LocationProxy``'s ``__setattr__()`` to correctly
  behave when dealing with the pure Python version of the ``ProxyBase``
  class. Also added a test suite that fully tests the pure Python proxy
  version of the ``LocationProxy`` class.


4.0.1 (2013-02-19)
==================

- Add Python 3.3 support.

4.0.0 (2012-06-07)
==================

- Remove backward-compatibility imports:

  - ``zope.copy.clone`` (aliased as ``zope.location.pickling.locationCopy``)

  - ``zope.copy.CopyPersistent`` (aliased as
    ``zope.location.pickling.CopyPersistent``).

  - ``zope.site.interfaces.IPossibleSite`` (aliased as
    ``zope.location.interfaces.IPossibleSite``).

- Add Python 3.2 support.

- Make ``zope.component`` dependency optional.  Use the ``component`` extra
  to force its installation (or just require it directly).  If
  ``zope.component`` is not present, this package defines the ``ISite``
  interface itself, and omits adapter registrations from its ZCML.

- Add support for PyPy.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Bring unit test coverage to 100%.

- Add Sphinx documentation:  moved doctest examples to API reference.

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Replace deprecated ``zope.component.adapts`` usage with equivalent
  ``zope.component.adapter`` decorator.

- Replace deprecated ``zope.interface.implements`` usage with equivalent
  ``zope.interface.implementer`` decorator.

- Drop support for Python 2.4 and 2.5.


3.9.1 (2011-08-22)
==================

- Add zcml extra as well as a test for configure.zcml.


3.9.0 (2009-12-29)
==================

- Move LocationCopyHook related tests to zope.copy and remove a test
  dependency on that package.

3.8.2 (2009-12-23)
==================

- Fix a typo in the configure.zcml.

3.8.1 (2009-12-23)
==================

- Remove dependency on zope.copy: the LocationCopyHook adapter is registered
  only if zope.copy is available.

- Use the standard Python doctest module instead of zope.testing.doctest, which
  has been deprecated.

3.8.0 (2009-12-22)
==================

- Adjust to testing output caused by new zope.schema.

3.7.1 (2009-11-18)
==================

- Move the IPossibleSite and ISite interfaces to zope.component as they are
  dealing with zope.component's concept of a site, but not with location.

3.7.0 (2009-09-29)
==================

- Add getParent() to ILocationInfo and moved the actual implementation here
  from zope.traversal.api, analogous to getParents().

- Actually remove deprecated PathPersistent class from
  zope.location.pickling.

- Move ITraverser back to zope.traversing where it belongs conceptually. The
  interface had been moved to zope.location to invert the package
  interdependency but is no longer used here.

3.6.0 (2009-08-27)
==================

- New feature release: deprecate locationCopy, CopyPersistent and
  PathPersistent from zope.location.pickling. These changes were already part
  of the 3.5.3 release, which was erroneously numbered as a bugfix relese.

- Remove dependency on zope.deferredimport, directly import deprecated modules
  without using it.

3.5.5 (2009-08-15)
==================

- Add zope.deferredimport as a dependency as it's used directly by
  zope.location.pickling.

3.5.4 (2009-05-17)
==================

- Add ``IContained`` interface to ``zope.location.interfaces`` module.
  This interface was moved from ``zope.container`` (after
  ``zope.container`` 3.8.2); consumers of ``IContained`` may now
  depend on zope.location rather than zope.container to reduce
  dependency cycles.

3.5.3 (2009-02-09)
==================

- Use new zope.copy package for implementing location copying. Thus
  there's changes in the ``zope.locaton.pickling`` module:

   * The ``locationCopy`` and ``CopyPersistent`` was removed in prefer
     to their equivalents in zope.copy. Deprecated backward-compatibility
     imports provided.

   * The module now provides a ``zope.copy.interfaces.ICopyHook`` adapter
     for ``ILocation`` objects that replaces the old CopyPersistent
     functionality of checking for the need to clone objects based on
     their location.

3.5.2 (2009-02-04)
==================

- Split RootPhysicallyLocatable adapter back from LocationPhysicallyLocatable,
  because the IRoot object may not always provide ILocation and the code
  for the root object is also simplier. It's basically a copy of the
  RootPhysicallyLocatable adapter from zope.traversing version 3.5.0 and
  below with ``getParents`` method added (returns an empty list).

3.5.1 (2009-02-02)
==================

- Improve test coverage.

- The new ``getParents`` method was extracted from ``zope.traversing``
  and added to ILocationInfo interface in the previous release. Custom
  ILocationInfo implementations should make sure they have this method
  as well. That method is already used in ``zope.traversing.api.getParents``
  function.

- Make ``getName`` of LocationPhysicallyLocatable always return empty
  string for the IRoot object, like RootPhysicallyLocatable from
  ``zope.traversing`` did. So, now LocationPhysicallyLocatable is
  fully compatible with RootPhysicallyLocatable, making the latter one
  obsolete.

- Change package mailing list address to zope-dev at zope.org instead
  of retired zope3-dev at zope.org.

3.5.0 (2009-01-31)
==================

- Reverse the dependency between zope.location and zope.traversing. This
  also causes the dependency to various other packages go away.

3.4.0 (2007-10-02)
==================

- Initial release independent of the main Zope tree.



  * [zope.proxy-4.3.2](http://github.com/zopefoundation/zope.proxy) ``zope.proxy``
==============

.. image:: https://img.shields.io/pypi/v/zope.proxy.svg
    :target: https://pypi.python.org/pypi/zope.proxy/
    :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.proxy.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.proxy

.. image:: https://readthedocs.org/projects/zopeproxy/badge/?version=latest
        :target: http://zopeproxy.readthedocs.org/en/latest/
        :alt: Documentation Status

Proxies are special objects which serve as mostly-transparent
wrappers around another object, intervening in the apparent behavior of
the wrapped object only when necessary to apply the policy (e.g., access
checking, location brokering, etc.) for which the proxy is responsible.

zope.proxy is implemented via a C extension module, which lets it do things
like lie about its own ``__class__`` that are difficult in pure Python (and
were completely impossible before metaclasses).  It also proxies all the
internal slots (such as ``__int__``/``__str__``/``__add__``).

Complete documentation is at https://zopeproxy.readthedocs.io


Changes
=======

4.3.2 (2019-07-12)
------------------

- Fix error handling in ``ProxyBase.__setattr__``: any the exception raised by
  ``PyString_AsString``/``PyUnicode_AsUTF8`` would be silently swallowed up
  and ignored.  See `issue 31
  <https://github.com/zopefoundation/zope.proxy/issues/31>`_.


4.3.1 (2018-08-09)
------------------

- Simplify the internal C handling of attribute names in
  ``__getattribute__`` and ``__setattr__``.

- Make building the C extension optional. We still attempt to build it
  on supported platforms, but we allow it to fail in case of a missing
  compiler or headers. See `issue 26
  <https://github.com/zopefoundation/zope.proxy/issues/26>`_.

- Test the PURE_PYTHON environment and PyPy3 on Travis CI.

- Add support for Python 3.7.

4.3.0 (2017-09-13)
------------------

- Fix a potential rare crash when deallocating proxies. See `issue 20
  <https://github.com/zopefoundation/zope.proxy/issues/20>`_.

- Drop support for Python 3.3.

- Drop support for "python setup.py test".

- 100% test coverage.

- Fix indexing pure-Python proxies with slices under Python 3, and
  restore the use of ``__getslice__`` (if implemented by the target's
  type) under Python 2. Previously, pure-Python proxies would fail
  with an AttributeError when given a slice on Python 3, and on Python
  2, a custom ``__getslice__`` was ignored. See `issue 21
  <https://github.com/zopefoundation/zope.proxy/issues/21>`_.

4.2.1 (2017-04-23)
------------------

- Make the pure-Python implementation of ``sameProxiedObjects`` handle
  ``zope.security`` proxies. See `issue 15 <https://github.com/zopefoundation/zope.proxy/issues/15>`_.

- Add support for Python 3.6.

4.2.0 (2016-05-05)
------------------

- Correctly strip ``zope.security`` proxies in ``removeAllProxies``.
  See `issue 13 <https://github.com/zopefoundation/zope.proxy/pull/13>`_.

- Avoid poisoning the user's global wheel cache when testing ``PURE_PYTHON``
  environments under ``tox``,

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5.

4.1.6 (2015-06-02)
------------------

- Make subclasses of ProxyBase properly delegate ``__module__`` to the
  wrapped object. This fixes some ``zope.interface`` lookups under
  PyPy.

- Make the pure-Python implementation of ProxyBase properly report the
  ``zope.interface`` interfaces implemented by builtin types like
  ``list``. This fixes some ``zope.interface`` lookups under PyPy.

4.1.5 (2015-05-19)
------------------

- Make the C implementation proxy ``__unicode__`` correctly.

- Make the C implementation use the standard methods to proxy ``int`` and
  ``float``.

- Make the pure Python implementation handle descriptors defined in
  subclasses like the C version. See
  https://github.com/zopefoundation/zope.proxy/issues/5.

4.1.4 (2014-03-19)
------------------

- Add support for Python 3.4.

- Update ``bootstrap.py`` to version 2.2.

4.1.3 (2013-03-12)
------------------

- Fix interface object introspection in PyPy. For some reason PyPy makes
  attributes available despite the restrictive ``__slots__`` declaration.

- Add a bunch of tests surrounding interface lookup and adaptation.

4.1.2 (2013-03-11)
------------------

- Make ``PyProxyBase.__iter__()`` return the result of
  ``PyProxyBase._wrapped.__iter__`` if available, otherwise falling back to
  Python internals. The previous implementation always created a generator.

- In ``PyProxyBase.__setattr__()``, allow setting of properties on the
  proxy itself. This is needed to properly allow proxy extensions as was
  evidenced int he ``zope.security.decorator`` module.

4.1.1 (2012-12-31)
------------------

- Fleshed out PyPI Trove classifiers.

4.1.0 (2012-12-19)
------------------

- Enable compilation of dependent modules under Py3k.

- Replace use of ``PyCObject`` APIs with equivalent ``PyCapsule`` APIs,
  except under Python 2.6.

  N.B.  This change is an ABI incompatibility under Python 2.7:
        extensions built under Python 2.7 against 4.0.x versions of
        ``zope.proxy`` must be rebuilt.

4.0.1 (2012-11-21)
------------------

- Add support for Python 3.3.

4.0.0 (2012-06-06)
------------------

- Add support for PyPy.

  N.B.:  the C extension is *not* built under PyPy.

- Add a pure-Python reference / fallback implementations of
  ``zope.proxy.ProxyBase`` and the proxy module API functions.

  N.B.:  the pure-Python proxy implements all regular features of
  ``ProxyBase``;  however, it does not exclude access to the wrapped object
  in the same way that the C version does.  If you need that information
  hiding (e.g., to implement security sandboxing), you still need to use
  the C version.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- 100% unit test coverage.

- Add Sphinx documentation:  moved doctest examples to API reference.

- Add 'setup.py docs' alias (installs ``Sphinx`` and dependencies).

- Add 'setup.py dev' alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Replace deprecated ``zope.interface.implements`` usage with equivalent
  ``zope.interface.implementer`` decorator.

- Drop support for Python 2.4 and 2.5.

- Add Python 3.2 support.

3.6.1 (2010-07-06)
------------------

- Make tests compatible with Python 2.7.

3.6.0 (2010-04-30)
------------------

- Remove test extra and the remaining dependency on zope.testing.

- Remove use of 'zope.testing.doctestunit' in favor of stdlib's 'doctest.

3.5.0 (2009/01/31)
------------------

- Add support to bootstrap on Jython.

- Use ``zope.container`` instead of ``zope.app.container``.

3.4.2 (2008/07/27)
------------------

- Make C code compatible with Python 2.5 on 64bit architectures.

3.4.1 (2008/06/24)
------------------

- Bug: Update ``setup.py`` script to conform to common layout. Also updated
  some of the fields.

- Bug: Honor pre-cooked indices for tuples and lists in the ``__getslice__()``
  and ``__setslice__()`` methods. See
  http://docs.python.org/ref/sequence-methods.html.

3.4.0 (2007/07/12)
------------------

- Feature: Add a ``decorator`` module that supports declaring interfaces on
  proxies that get blended with the interfaces of the things they proxy.

3.3.0 (2006/12/20)
------------------

- Corresponds to the verison of the ``zope.proxy`` package shipped as part of
  the Zope 3.3.0 release.


3.2.0 (2006/01/05)
------------------

- Corresponds to the verison of the ``zope.proxy`` package shipped as part of
  the Zope 3.2.0 release.


3.0.0 (2004/11/07)
------------------

- Corresponds to the verison of the ``zope.proxy`` package shipped as part of
  the Zope X3.0.0 release.
  * [zope.schema-4.9.3](https://github.com/zopefoundation/zope.schema) =============
 zope.schema
=============

.. image:: https://img.shields.io/pypi/v/zope.schema.svg
   :target: https://pypi.org/project/zope.schema/
   :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/zope.schema.svg
   :target: https://pypi.org/project/zope.schema/
   :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.schema.svg?branch=master
   :target: https://travis-ci.org/zopefoundation/zope.schema
   :alt: Build Status

.. image:: https://readthedocs.org/projects/zopeschema/badge/?version=latest
   :target: https://zopeschema.readthedocs.org/en/latest/
   :alt: Documentation Status

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.schema/badge.svg
   :target: https://coveralls.io/github/zopefoundation/zope.schema
   :alt: Code Coverage

Schemas extend the notion of interfaces to detailed descriptions of
Attributes (but not methods).  Every schema is an interface and
specifies the public fields of an object.  A *field* roughly
corresponds to an attribute of a Python object.  But a Field provides
space for at least a title and a description.  It can also constrain
its value and provide a validation method.  Besides you can optionally
specify characteristics such as its value being read-only or not
required.

See https://zopeschema.readthedocs.io/ for more information.


=========
 Changes
=========

4.9.3 (2018-10-12)
==================

- Fixed a ReST error in getDoc() results when having "subfields" 
  with titles. 


4.9.2 (2018-10-11)
==================

- Make sure that the title for ``IObject.validate_invariants`` is a unicode
  string.


4.9.1 (2018-10-05)
==================

- Fix ``SimpleTerm`` token for non-ASCII bytes values.


4.9.0 (2018-09-24)
==================

- Make ``NativeString`` and ``NativeStringLine`` distinct types that
  implement the newly-distinct interfaces ``INativeString`` and
  ``INativeStringLine``. Previously these were just aliases for either
  ``Text`` (on Python 3) or ``Bytes`` (on Python 2).

- Fix ``Field.getDoc()`` when ``value_type`` or ``key_type`` is
  present. Previously it could produce ReST that generated Sphinx
  warnings. See `issue 76 <https://github.com/zopefoundation/zope.schema/issues/76>`_.

- Make ``DottedName`` accept leading underscores for each segment.

- Add ``PythonIdentifier``, which accepts one segment of a dotted
  name, e.g., a python variable or class.

4.8.0 (2018-09-19)
==================

- Add the interface ``IFromBytes``, which is implemented by the
  numeric and bytes fields, as well as ``URI``, ``DottedName``, and
  ``Id``.

- Fix passing ``None`` as the description to a field constructor. See
  `issue 69 <https://github.com/zopefoundation/zope.schema/issues/69>`_.

4.7.0 (2018-09-11)
==================

- Make ``WrongType`` have an ``expected_type`` field.

- Add ``NotAnInterface``, an exception derived from ``WrongType`` and
  ``SchemaNotProvided`` and raised by the constructor of ``Object``
  and when validation fails for ``InterfaceField``.

- Give ``SchemaNotProvided`` a ``schema`` field.

- Give ``WrongContainedType`` an ``errors`` list.

- Give ``TooShort``, ``TooLong``, ``TooBig`` and ``TooSmall`` a
  ``bound`` field and the common superclasses ``LenOutOfBounds``,
  ``OrderableOutOfBounds``, respectively, both of which inherit from
  ``OutOfBounds``.

4.6.2 (2018-09-10)
==================

- Fix checking a field's constraint to set the ``field`` and ``value``
  properties if the constraint raises a ``ValidationError``. See
  `issue 66
  <https://github.com/zopefoundation/zope.schema/issues/66>`_.


4.6.1 (2018-09-10)
==================

- Fix the ``Field`` constructor to again allow ``MessageID`` values
  for the ``description``. This was a regression introduced with the
  fix for `issue 60
  <https://github.com/zopefoundation/zope.schema/issues/60>`_. See
  `issue 63
  <https://github.com/zopefoundation/zope.schema/issues/63>`_.


4.6.0 (2018-09-07)
==================

- Add support for Python 3.7.

- ``Object`` instances call their schema's ``validateInvariants``
  method by default to collect errors from functions decorated with
  ``@invariant`` when validating. This can be disabled by passing
  ``validate_invariants=False`` to the ``Object`` constructor. See
  `issue 10 <https://github.com/zopefoundation/zope.schema/issues/10>`_.

- ``ValidationError`` can be sorted on Python 3.

- ``DottedName`` and ``Id`` consistently handle non-ASCII unicode
  values on Python 2 and 3 by raising ``InvalidDottedName`` and
  ``InvalidId`` in ``fromUnicode`` respectively. Previously, a
  ``UnicodeEncodeError`` would be raised on Python 2 while Python 3
  would raise the descriptive exception.

- ``Field`` instances are hashable on Python 3, and use a defined
  hashing algorithm that matches what equality does on all versions of
  Python. Previously, on Python 2, fields were hashed based on their
  identity. This violated the rule that equal objects should have
  equal hashes, and now they do. Since having equal hashes does not
  imply that the objects are equal, this is not expected to be a
  compatibility problem. See `issue 36
  <https://github.com/zopefoundation/zope.schema/issues/36>`_.

- ``Field`` instances are only equal when their ``.interface`` is
  equal. In practice, this means that two otherwise identical fields
  of separate schemas are not equal, do not hash the same, and can
  both be members of the same ``dict`` or ``set``. Prior to this
  release, when hashing was identity based but only worked on Python
  2, that was the typical behaviour. (Field objects that are *not*
  members of a schema continue to compare and hash equal if they have
  the same attributes and interfaces.) See `issue 40
  <https://github.com/zopefoundation/zope.schema/issues/40>`_.

- Orderable fields, including ``Int``, ``Float``, ``Decimal``,
  ``Timedelta``, ``Date`` and ``Time``, can now have a
  ``missing_value`` without needing to specify concrete ``min`` and
  ``max`` values (they must still specify a ``default`` value). See
  `issue 9 <https://github.com/zopefoundation/zope.schema/issues/9>`_.

- ``Choice``, ``SimpleVocabulary`` and  ``SimpleTerm`` all gracefully
  handle using Unicode token values with non-ASCII characters by encoding
  them with the ``backslashreplace`` error handler. See `issue 15
  <https://github.com/zopefoundation/zope.schema/issues/15>`_ and `PR
  6 <https://github.com/zopefoundation/zope.schema/pull/6>`_.

- All instances of ``ValidationError`` have a ``field`` and ``value``
  attribute that is set to the field that raised the exception and the
  value that failed validation.

- ``Float``, ``Int`` and ``Decimal`` fields raise ``ValidationError``
  subclasses for literals that cannot be parsed. These subclasses also
  subclass ``ValueError`` for backwards compatibility.

- Add a new exception ``SchemaNotCorrectlyImplemented``, a subclass of
  ``WrongContainedType`` that is raised by the ``Object`` field. It
  has a dictionary (``schema_errors``) mapping invalid schema
  attributes to their corresponding exception, and a list
  (``invariant_errors``) containing the exceptions raised by
  validating invariants. See `issue 16
  <https://github.com/zopefoundation/zope.schema/issues/16>`_.

- Add new fields ``Mapping`` and ``MutableMapping``, corresponding to
  the collections ABCs of the same name; ``Dict`` now extends and
  specializes ``MutableMapping`` to only accept instances of ``dict``.

- Add new fields ``Sequence`` and ``MutableSequence``, corresponding
  to the collections ABCs of the same name; ``Tuple`` now extends
  ``Sequence`` and ``List`` now extends ``MutableSequence``.

- Add new field ``Collection``, implementing ``ICollection``. This is
  the base class of ``Sequence``. Previously this was known as
  ``AbstractCollection`` and was not public. It can be subclassed to
  add ``value_type``, ``_type`` and ``unique`` attributes at the class
  level, enabling a simpler constructor call. See `issue 23
  <https://github.com/zopefoundation/zope.schema/issues/23>`_.

- Make ``Object`` respect a ``schema`` attribute defined by a
  subclass, enabling a simpler constructor call. See `issue 23
  <https://github.com/zopefoundation/zope.schema/issues/23>`_.

- Add fields and interfaces representing Python's numeric tower. In
  descending order of generality these are ``Number``, ``Complex``,
  ``Real``, ``Rational`` and ``Integral``. The ``Int`` class extends
  ``Integral``, the ``Float`` class extends ``Real``, and the
  ``Decimal`` class extends ``Number``. See `issue 49
  <https://github.com/zopefoundation/zope.schema/issues/49>`_.

- Make ``Iterable`` and ``Container`` properly implement ``IIterable``
  and ``IContainer``, respectively.

- Make ``SimpleVocabulary.fromItems`` accept triples to allow
  specifying the title of terms. See `issue 18
  <https://github.com/zopefoundation/zope.schema/issues/18>`_.

- Make ``TreeVocabulary.fromDict`` only create
  ``ITitledTokenizedTerms`` when a title is actually provided.

- Make ``Choice`` fields reliably raise a ``ValidationError`` when a
  named vocabulary cannot be found; for backwards compatibility this
  is also a ``ValueError``. Previously this only worked when the
  default ``VocabularyRegistry`` was in use, not when it was replaced
  with `zope.vocabularyregistry
  <https://pypi.org/project/zope.vocabularyregistry/>`_. See `issue 55
  <https://github.com/zopefoundation/zope.schema/issues/55>`_.

- Make ``SimpleVocabulary`` and ``SimpleTerm`` have value-based
  equality and hashing methods.

- All fields of the schema of an ``Object`` field are bound to the
  top-level value being validated before attempting validation of
  their particular attribute. Previously only ``IChoice`` fields were
  bound. See `issue 17
  <https://github.com/zopefoundation/zope.schema/issues/17>`_.

- Share the internal logic of ``Object`` field validation and
  ``zope.schema.getValidationErrors``. See `issue 57
  <https://github.com/zopefoundation/zope.schema/issues/57>`_.


- Make ``Field.getDoc()`` return more information about the properties
  of the field, such as its required and readonly status. Subclasses
  can add more information using the new method
  ``Field.getExtraDocLines()``. This is used to generate Sphinx
  documentation when using `repoze.sphinx.autointerface
  <https://pypi.org/project/repoze.sphinx.autointerface/>`_. See
  `issue 60
  <https://github.com/zopefoundation/zope.schema/issues/60>`_.


4.5.0 (2017-07-10)
==================

- Drop support for Python 2.6, 3.2, and 3.3.

- Add support for Python 3.5 and 3.6.

- Drop support for 'setup.py test'. Use zope.testrunner instead.


4.4.2 (2014-09-04)
==================

- Fix description of min max field: max value is included, not excluded.


4.4.1 (2014-03-19)
==================

- Add support for Python 3.4.


4.4.0 (2014-01-22)
==================

- Add an event on field properties to notify that a field has been updated.
  This event enables definition of subscribers based on an event, a context
  and a field. The event contains also the old value and the new value.
  (also see package ``zope.schemaevent`` that define a field event handler)


4.3.3 (2014-01-06)
==================

- PEP 8 cleanup.

- Don't raise RequiredMissing if a field's defaultFactory returns the field's
  missing_value.

- Update ``boostrap.py`` to version 2.2.

- Add the ability to swallow ValueErrors when rendering a SimpleVocabulary,
  allowing for cases where vocabulary items may be duplicated (e.g., due to
  user input).

- Include the field name in ``ConstraintNotSatisfied``.


4.3.2 (2013-02-24)
==================

- Fix Python 2.6 support. (Forgot to run tox with all environments before last
  release.)


4.3.1 (2013-02-24)
==================

- Make sure that we do not fail during bytes decoding of term token when
  generated from a bytes value by ignoring all errors. (Another option would
  have been to hexlify the value, but that would break way too many tests.)


4.3.0 (2013-02-24)
==================

- Fix a bug where bytes values were turned into tokens inproperly in
  Python 3.

- Add ``zope.schema.fieldproperty.createFieldProperties()`` function which
  maps schema fields into ``FieldProperty`` instances.

4.2.2 (2012-11-21)
==================

- Add support for Python 3.3.

4.2.1 (2012-11-09)
==================

- Fix the default property of fields that have no defaultFactory attribute.


4.2.0 (2012-05-12)
==================

- Automate build of Sphinx HTML docs and running doctest snippets via tox.

- Drop explicit support for Python 3.1.

- Introduce NativeString and NativeStringLine which are equal to Bytes and
  BytesLine on Python 2 and Text and TextLine on Python 3.

- Change IURI from a Bytes string to a "native" string. This is a backwards
  incompatibility which only affects Python 3.

- Bring unit test coverage to 100%.

- Move doctests from the package and wired up as normal Sphinx documentation.

- Add explicit support for PyPy.

- Add support for continuous integration using ``tox`` and ``jenkins``.

- Drop the external ``six`` dependency in favor of a much-trimmed
  ``zope.schema._compat`` module.

- Ensure tests pass when run under ``nose``.

- Add ``setup.py dev`` alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Add ``setup.py docs`` alias (installs ``Sphinx`` and dependencies).


4.1.1 (2012-03-23)
==================

- Remove trailing slash in MANIFEST.in, it causes Winbot to crash.


4.1.0 (2012-03-23)
==================

- Add TreeVocabulary for nested tree-like vocabularies.

- Fix broken Object field validation where the schema contains a Choice with
  ICountextSourceBinder source. In this case the vocabulary was not iterable
  because the field was not bound and the source binder didn't return the
  real vocabulary. Added simple test for IContextSourceBinder validation. But a
  test with an Object field with a schema using a Choice with
  IContextSourceBinder is still missing.

4.0.1 (2011-11-14)
==================

- Fix bug in ``fromUnicode`` method of ``DottedName`` which would fail
  validation on being given unicode. Introduced in 4.0.0.

4.0.0 (2011-11-09)
==================

- Fix deprecated unittest methods.

- Port to Python 3. This adds a dependency on six and removes support for
  Python 2.5.

3.8.1 (2011-09-23)
==================

- Fix broken Object field validation. Previous version was using a volatile
  property on object field values which ends in a ForbiddenAttribute error
  on security proxied objects.

3.8.0 (2011-03-18)
==================

- Implement a ``defaultFactory`` attribute for all fields. It is a callable
  that can be used to compute default values. The simplest case is::

    Date(defaultFactory=datetime.date.today)

  If the factory needs a context to compute a sensible default value, then it
  must provide ``IContextAwareDefaultFactory``, which can be used as follows::

    @provider(IContextAwareDefaultFactory)
    def today(context):
        return context.today()

    Date(defaultFactory=today)

3.7.1 (2010-12-25)
==================

- Rename the validation token, used in the validation of schema with Object
  Field to avoid infinite recursion:
  ``__schema_being_validated`` became ``_v_schema_being_validated``,
  a volatile attribute, to avoid persistency and therefore,
  read/write conflicts.

- Don't allow "[\]^`" in DottedName.
  https://bugs.launchpad.net/zope.schema/+bug/191236

3.7.0 (2010-09-12)
==================

- Improve error messages when term tokens or values are duplicates.

- Fix the buildout so the tests run.

3.6.4 (2010-06-08)
==================

- fix validation of schema with Object Field that specify Interface schema.

3.6.3 (2010-04-30)
==================

- Prefer the standard libraries doctest module to the one from zope.testing.

3.6.2 (2010-04-30)
==================

- Avoid maximum recursion when validating Object field that points to cycles

- Make the dependency on ``zope.i18nmessageid`` optional.

3.6.1 (2010-01-05)
==================

- Allow "setup.py test" to run at least a subset of the tests runnable
  via ``bin/test`` (227 for ``setup.py test`` vs. 258. for
  ``bin/test``)

- Make ``zope.schema._bootstrapfields.ValidatedProperty`` descriptor
  work under Jython.

- Make "setup.py test" tests pass on Jython.

3.6.0 (2009-12-22)
==================

- Prefer zope.testing.doctest over doctestunit.

- Extend validation error to hold the field name.

- Add FieldProperty class that uses Field.get and Field.set methods
  instead of storing directly on the instance __dict__.

3.5.4 (2009-03-25)
==================

- Don't fail trying to validate default value for Choice fields with
  IContextSourceBinder object given as a source. See
  https://bugs.launchpad.net/zope3/+bug/340416.

- Add an interface for ``DottedName`` field.

- Add ``vocabularyName`` attribute to the ``IChoice`` interface, change
  "vocabulary" attribute description to be more sensible, making it
  ``zope.schema.Field`` instead of plain ``zope.interface.Attribute``.

- Make IBool interface of Bool more important than IFromUnicode so adapters
  registered for IBool take precendence over adapters registered for
  IFromUnicode.


3.5.3 (2009-03-10)
==================

- Make Choice and Bool fields implement IFromUnicode interface, because
  they do provide the ``fromUnicode`` method.

- Change package's mailing list address to zope-dev at zope.org, as
  zope3-dev at zope.org is now retired.

- Fix package's documentation formatting. Change package's description.

- Add buildout part that builds Sphinx-generated documentation.

- Remove zpkg-related file.

3.5.2 (2009-02-04)
==================

- Made validation tests compatible with Python 2.5 again (hopefully not
  breaking Python 2.4)

- Add an __all__ package attribute to expose documentation.

3.5.1 (2009-01-31)
==================

- Stop using the old old set type.

- Make tests compatible and silent with Python 2.4.

- Fix __cmp__ method in ValidationError. Show some side effects based on the
  existing __cmp__ implementation. See validation.txt

- Make 'repr' of the ValidationError and its subclasses more sensible. This
  may require you to adapt your doctests for the new style, but now it makes
  much more sense for debugging for developers.

3.5.0a2 (2008-12-11)
====================

- Move zope.testing to "test" extras_require, as it is not needed
  for zope.schema itself.

- Change the order of classes in SET_TYPES tuple, introduced in
  previous release to one that was in 3.4 (SetType, set), because
  third-party code could be dependent on that order. The one
  example is z3c.form's converter.

3.5.0a1 (2008-10-10)
====================

- Add the doctests to the long description.

- Remove use of deprecated 'sets' module when running under Python 2.6.

- Remove spurious doctest failure when running under Python 2.6.

- Add support to bootstrap on Jython.

- Add helper methods for schema validation: ``getValidationErrors``
  and ``getSchemaValidationErrors``.

- zope.schema now works on Python2.5

3.4.0 (2007-09-28)
==================

Add BeforeObjectAssignedEvent that is triggered before the object
field sets a value.

3.3.0 (2007-03-15)
==================

Corresponds to the version of the zope.schema package shipped as part of
the Zope 3.3.0 release.

3.2.1 (2006-03-26)
==================

Corresponds to the version of the zope.schema package shipped as part of
the Zope 3.2.1 release.

Fix missing import of 'VocabularyRegistryError'.  See
http://www.zope.org/Collectors/Zope3-dev/544 .

3.2.0 (2006-01-05)
==================

Corresponds to the version of the zope.schema package shipped as part of
the Zope 3.2.0 release.

Add "iterable" sources to replace vocabularies, which are now deprecated
and scheduled for removal in Zope 3.3.

3.1.0 (2005-10-03)
==================

Corresponds to the version of the zope.schema package shipped as part of
the Zope 3.1.0 release.

Allow 'Choice' fields to take either a 'vocabulary' or a 'source'
argument (sources are a simpler implementation).

Add 'TimeDelta' and 'ASCIILine' field types.

3.0.0 (2004-11-07)
==================

Corresponds to the version of the zope.schema package shipped as part of
the Zope X3.0.0 release.



  * [zope.security-4.3.1](http://github.com/zopefoundation/zope.security) ===============
 zope.security
===============

.. image:: https://img.shields.io/pypi/v/zope.security.svg
        :target: https://pypi.python.org/pypi/zope.security/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.security.svg
        :target: https://pypi.org/project/zope.security/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.security.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.security

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.security/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.security?branch=master

.. image:: https://readthedocs.org/projects/zopesecurity/badge/?version=latest
        :target: https://zopesecurity.readthedocs.io/en/latest/
        :alt: Documentation Status


The Security framework provides a generic mechanism to implement security
policies on Python objects.

Documentation is available at https://zopesecurity.readthedocs.io/


=========
 Changes
=========

4.3.1 (2019-01-03)
==================

- Fix the decimal.Decimal checker, ``__truediv__`` was missing causing
  ``ForbiddenAttribute`` on a ``ProxyFactory(Decimal('1')) / 1`` operation


4.3.0 (2018-08-24)
==================

- Add the interface ``ISystemPrincipal`` and make
  ``zope.security.management.system_user`` a regular object that
  implements this interface. This facilitates providing adapter
  registrations specifically for the ``system_user``.


4.2.3 (2018-08-09)
==================

- Add support for Python 3.7.


4.2.2 (2018-01-11)
==================

- Make the pure-Python proxy on Python 2 *not* check permissions for
  ``__unicode__`` just like the C implementation. Note that
  ``__str__`` is checked for both implementations on both Python 2 and
  3, but if there is no ``__unicode__`` method defined, Python 2's
  automatic fallback to ``__str__`` is **not** checked when
  ``unicode`` is called. See `issue 10
  <https://github.com/zopefoundation/zope.security/issues/10>`_.


4.2.1 (2017-11-30)
==================

- Fix the default values for ``Permission`` fields ``title`` and
  ``description`` under Python 2. See `issue 48
  <https://github.com/zopefoundation/zope.security/issues/48>`_.

- Change the ``IPermission.id`` from ``Text`` (unicode) to a
  ``NativeStringLine``. This matches what ZCML creates and what is
  usually written in source code.


4.2.0 (2017-09-20)
==================

- Fix the extremely rare potential for a crash when the C extensions
  are in use. See `issue 35 <https://github.com/zopefoundation/zope.security/issues/35>`_.

- Fix `issue 7
  <https://github.com/zopefoundation/zope.security/issues/7>`_: The
  pure-Python proxy didn't propagate ``TypeError`` from ``__repr__``
  and ``__str__`` like the C implementation did.

- Fix `issue 27 <https://github.com/zopefoundation/zope.security/issues/27>`_:
  iteration of ``zope.interface.providedBy()`` is now allowed by
  default on all versions of Python. Previously it only worked on
  Python 2. Note that ``providedBy`` returns unproxied objects for backwards
  compatibility.

- Fix ``__length_hint__`` of proxied iterator objects. Previously it
  was ignored.

- Drop support for Python 3.3.

- Enable coveralls.io for coverage measurement and run doctests on all
  supported Python versions.

- Fix `issue 9
  <https://github.com/zopefoundation/zope.security/issues/9>`_:
  iteration of ``itertools.groupby`` objects is now allowed by
  default. In addition, iteration of all the custom iterator types
  defined in itertools are also allowed by default.

- Simplify the internal ``_compat.py`` module now that we only run on
  newer Python versions. See `PR 32 <https://github.com/zopefoundation/zope.security/pull/32>`_.

- Respect ``PURE_PYTHON`` at runtime. At build time, always try to
  build the C extensions on supported platforms, ignoring
  ``PURE_PYTHON``. See `issue 33
  <https://github.com/zopefoundation/zope.security/issues/33>`_.

- Fix watching checkers (``ZOPE_WATCH_CHECKERS=1``) in pure-Python
  mode. See `issue 8 <https://github.com/zopefoundation/zope.security/issues/8>`_.

- Remove unused internal files from ``tests/``.

- Remove ``zope.security.setup``. It was unused and did not work
  anyway.

- Fix the pure-Python proxy on Python 2 letting ``__getslice__`` and
  ``__setslice__`` fall through to ``__getitem__`` or ``__setitem__``,
  respectively, if it raised an error.

- Fix the pure-Python proxy calling a wrapped ``__getattr__`` or
  ``__getattribute__`` more than once in situations where the C
  implementation only called it one time (when it raised an
  AttributeError).

- Reach 100% test coverage and maintain it via automated checks.

4.1.1 (2017-05-17)
==================

- Fix `issue 23 <https://github.com/zopefoundation/zope.security/issues/23>`_:
  iteration of ``collections.OrderedDict`` and its various views is
  now allowed by default on all versions of Python.

- As a further fix for issue 20, iteration of ``BTree`` itself is now
  allowed by default.

4.1.0 (2017-04-24)
==================

- When testing ``PURE_PYTHON`` environments under ``tox``, avoid poisoning
  the user's global wheel cache.

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5 and 3.6.

- Fix `issue 20 <https://github.com/zopefoundation/zope.security/issues/20>`_:
  iteration of pure-Python ``BTrees.items()``, and also creating a list from
  ``BTrees.items()`` on Python 3. The same applies for ``keys()`` and ``values()``.

4.0.3 (2015-06-02)
==================

- Fix iteration over security proxies in Python 3 using the pure-Python
  implementation.

4.0.2 (2015-06-02)
==================

- Fix compatibility with ``zope.proxy`` 4.1.5 under PyPy.

- Fix the very first call to ``removeSecurityProxy`` returning
  incorrect results if given a proxy under PyPy.

4.0.1 (2014-03-19)
==================

- Add support for Python 3.4.

4.0.0 (2013-07-09)
==================

- Update ``boostrap.py`` to version 2.2.

- Bugfix: ZOPE_WATCH_CHECKERS=2 used to incorrectly suppress
  unauthorized/forbidden warnings.

- Bugfix: ZOPE_WATCH_CHECKERS=1 used to miss most of the checks.


4.0.0b1 (2013-03-11)
====================

- Add support for PyPy.

- Fix extension compilation on windows python 3.x


4.0.0a5 (2013-02-28)
====================

- Undo changes from 4.0.0a4. Instead, ``zope.untrustedpython`` is only
  included during Python 2 installs.


4.0.0a4 (2013-02-28)
====================

- Remove ``untrustedpython`` extra again, since we do not want to support
  ``zope.untrustedpython`` in ZTK 2.0. If BBB is really needed, we will create
  a 3.10.0 release.

4.0.0a3 (2013-02-15)
====================

- Fix test breakage in 4.0.0a2 due to deprecation strategy.

4.0.0a2 (2013-02-15)
====================

- Add back the ``untrustedpython`` extra:  now pulls in
  ``zope.untrustedpython``.  Restored deprecated backward-compatible imports
  for ``zope.security.untrustedpython.{builtins,interpreter,rcompile}``
  (the extra and the imports are to be removed in version 4.1).


4.0.0a1 (2013-02-14)
====================

- Add support for Python 3.2 and 3.3.

- Bring unit test coverage to 100%.

- ``zope.security.untrustedpython`` moved to separate project:
  ``zope.untrustedpython``

- Convert use of ``assert`` in non-test code to apprpriate error types:

  - Non-dict's passed to ``Checker.__init__``.

- Remove dprecattion of ``zope.security.adapter.TrustedAdapterFactory``.
  Although it has been marked as deprectaed since before Zope3 3.2, current
  versions of ``zope.compoent`` still rely on it.

- Convert doctests to Sphinx documentation in 'docs'.

- Add ``setup.py docs`` alias (installs ``Sphinx`` and dependencies).

- Add ``setup.py dev`` alias (runs ``setup.py develop`` plus installs
  ``nose`` and ``coverage``).

- Make non-doctest tests fully independent of ``zope.testing``.

  Two modules, ``zope.security.checker`` and ``zope.security.management``,
  register cleanups with ``zope.testing`` IFF it is importable, but the
  tests no longer rely on it.

- Enable building extensions without the ``svn:external`` of the ``zope.proxy``
  headers into our ``include`` dir.

- Bump ``zope.proxy`` dependency to ">= 4.1.0" to enable compilation
  on Py3k.

- Replace deprecated ``zope.component.adapts`` usage with equivalent
  ``zope.component.adapter`` decorator.

- Replace deprecated ``zope.interface.classProvides`` usage with equivalent
  ``zope.interface.provider`` decorator.

- Replace deprecated ``zope.interface.implements`` usage with equivalent
  ``zope.interface.implementer`` decorator.

- Drop support for Python 2.4 and 2.5.

- Add test convenience helper ``create_interaction`` and
  ``with interaction()``.

3.9.0 (2012-12-21)
==================

- Pin ``zope.proxy >= 4.1.0``

- Ship with an included ``proxy.h`` header which is compatible with the
  4.1.x version ov ``zope.proxy``.

3.8.5 (2012-12-21)
==================

- Ship with an included ``proxy.h`` header which is compatible with the
  supported versions of ``zope.proxy``.

3.8.4 (2012-12-20)
==================

- Pin ``zope.proxy >= 3.4.2, <4.1dev``

3.8.3 (2011-09-24)
==================

- Fix a regression introduced in 3.8.1: ``zope.location``\'s LocationProxy
  did not get a security checker if ``zope.security.decorator`` was not
  imported manually. Now ``zope.security.decorator`` is imported in
  ``zope.security.proxy`` without re-introducing the circular import fixed in
  3.8.1.

3.8.2 (2011-05-24)
==================

- Fix a test that failed on Python 2.7.


3.8.1 (2011-05-03)
==================

- Fix circular import beween ``zope.security.decorator`` and
  ``zope.security.proxy`` which led to an ``ImportError`` when only
  importing ``zope.security.decorator``.


3.8.0 (2010-12-14)
==================

- Add tests for our own ``configure.zcml``.

- Add ``zcml`` extra dependencies;  run related tests only if
  ``zope.configuration`` is available.

- Run tests related to the ``untrustedpython`` functionality only if
  ``RestrictedPython`` is available.


3.7.3 (2010-04-30)
==================

- Prefer the standard library's ``doctest`` module to the one from
  ``zope.testing``.

- Ensure ``PermissionIdsVocabulary`` directly provides ``IVocabularyFactory``,
  even though it might be unnecessary because ``IVocabularyFactory`` is
  provided in ZCML.

- Remove the dependency on the zope.exceptions package: zope.security.checker
  now imports ``DuplicationError`` from zope.exceptions if available, otherwise
  it defines a package-specific ``DuplicationError`` class which inherits from
  Exception.


3.7.2 (2009-11-10)
==================

- Add compatibility with Python 2.6 abstract base classes.


3.7.1 (2009-08-13)
==================

- Fix for LP bug 181833 (from Gustavo Niemeyer). Before "visiting" a
  sub-object, a check should be made to ensure the object is still valid.
  Because garbage collection may involve loops, if you garbage collect an
  object, it is possible that the actions done on this object may modify the
  state of other objects. This may cause another round of garbage collection,
  eventually generating a segfault (see LP bug). The Py_VISIT macro does the
  necessary checks, so it is used instead of the previous code.


3.7.0 (2009-05-13)
==================

- Make ``pytz`` a soft dependency:  the checker for ``pytz.UTC`` is
  created / tested only if the package is already present.  Run
  ``bin/test_pytz`` to run the tests with ``pytz`` on the path.


3.6.3 (2009-03-23)
==================

- Ensure that simple zope.schema's ``VocabularyRegistry`` is used for
  ``PermissionVocabulary`` tests, because it's replaced implicitly in
  environments with ``zope.app.schema`` installed that makes that tests
  fail.

- Fix a bug in ``DecoratedSecurityCheckerDescriptor`` which made
  security-wrapping location proxied exception instances throw
  exceptions on Python 2.5.
  See https://bugs.launchpad.net/zope3/+bug/251848


3.6.2 (2009-03-14)
==================

- Add ``zope.i18nmessageid.Message`` to non-proxied basic types. It's okay,
  because messages are immutable. Done previously by ``zope.app.security``.

- Add ``__name__`` and ``__parent__`` attributes to list of available by
  default.  Done previously by ``zope.app.security``.

- Move ``PermissionsVocabulary`` and ``PermissionIdsVocabulary`` vocabularies
  to the ``zope.security.permission`` module from the ``zope.app.security``
  package.

- Add zcml permission definitions for most common and useful permissions,
  like ``zope.View`` and ``zope.ManageContent``, as well as for the special
  ``zope.Public`` permission. They are placed in a separate
  ``permissions.zcml`` file, so it can be easily excluded/redefined. They are
  selected part of permissions moved from ``zope.app.security`` and used by
  many ``zope.*`` packages.

- Add ``addCheckerPublic`` helper function in ``zope.security.testing`` module
  that registers the "zope.Public" permission as an IPermission utility.

- Add security declarations for the ``zope.security.permisson.Permission``
  class.

- Improve test coverage.


3.6.1 (2009-03-10)
==================

- Use ``from`` imports instead of ``zope.deferred`` to avoid circular
  import problems, thus drop dependency on ``zope.deferredimport``.

- Raise ``NoInteraction`` when ``zope.security.checkPermission`` is called
  without interaction being active (LP #301565).

- Don't define security checkers for deprecated set types from the
  "sets" module on Python 2.6. It's discouraged to use them and
  ``set`` and ``frozenset`` built-in types should be used instead.

- Change package's mailng list address to zope-dev at zope.org as
  zope3-dev at zope.org is now retired.

- Remove old zpkg-related files.


3.6.0 (2009-01-31)
==================

- Install decorated security checker support on ``LocationProxy`` from the
  outside.

- Add support to bootstrap on Jython.

- Move the ``protectclass`` module from ``zope.app.security`` to this
  package to reduce the number of dependencies on ``zope.app.security``.

- Move the ``<module>`` directive implementation from ``zope.app.security``
  to this package.

- Move the ``<class>`` directive implementation from ``zope.app.component``
  to this package.


3.5.2 (2008-07-27)
==================

- Make C code compatible with Python 2.5 on 64bit architectures.


3.5.1 (2008-06-04)
==================

- Add ``frozenset``, ``set``, ``reversed``, and ``sorted`` to the list of
  safe builtins.


3.5.0 (2008-03-05)
==================

- Changed title for ``zope.security.management.system_user`` to be more
  presentable.


3.4.3 - (2009/11/26)
====================

- Backport a fix made by Gary Poster to the 3.4 branch:
  Fix for LP bug 181833 (from Gustavo Niemeyer). Before "visiting" a
  sub-object, a check should be made to ensure the object is still valid.
  Because garbage collection may involve loops, if you garbage collect an
  object, it is possible that the actions done on this object may modify the
  state of other objects. This may cause another round of garbage collection,
  eventually generating a segfault (see LP bug). The ``Py_VISIT`` macro does
  the necessary checks, so it is used instead of the previous code.


3.4.2 - (2009/03/23)
====================

- Add dependency on ``zope.thread`` to setup.py; without it, the tests were
  failing.

- Backport a fix made by Albertas Agejevas to the 3.4 branch. He
  fixed a bug in DecoratedSecurityCheckerDescriptor which made
  security-wrapping location proxied exception instances throw
  exceptions on Python 2.5.  See
  https://bugs.launchpad.net/zope3/+bug/251848


3.4.1 - 2008/07/27
==================

- Make C code compatible with Python 2.5 on 64bit architectures.


3.4.0 (2007-10-02)
==================

- Update meta-data.


3.4.0b5 (2007-08-15)
====================

- Fix a circular import in the C implementation.


3.4.0b4 (2007-08-14)
====================

- Improve ugly/brittle ID of ``zope.security.management.system_user``.


3.4.0b3 (2007-08-14)
====================

- Add support for Python 2.5.

- Bug: ``zope.security.management.system_user`` wasn't a valid principal
  (didn't provide IPrincipal).

- Bug: Fix inclusion of doctest to use the doctest module from
  ``zope.testing``. Now tests can be run multiple times without
  breaking. (#98250)


3.4.0b2 (2007-06-15)
====================

- Bug: Remove stack extraction in ``newInteraction``. When using eggs this is
  an extremly expensive function. The publisher is now more than 10 times
  faster when using eggs and about twice as fast with a zope trunk checkout.


3.4.0b1
=======

- Temporarily fixed the hidden (and accidental) dependency on zope.testing to
  become optional.

Note: The releases between 3.2.0 and 3.4.0b1 where not tracked as an
individual package and have been documented in the Zope 3 changelog.


3.2.0 (2006-01-05)
==================

- Corresponds to the verison of the ``zope.security`` package shipped as part
  of the Zope 3.2.0 release.

- Remove deprecated helper functions, ``proxy.trustedRemoveSecurityProxy``
  and ``proxy.getProxiedObject``.

- Make handling of ``management.{end,restore}Interaction`` more careful w.r.t.
  edge cases.

- Make behavior of ``canWrite`` consistent with ``canAccess``:
  if ``canAccess`` does not raise ``ForbiddenAttribute``, then neither will
  ``canWrite``.  See: http://www.zope.org/Collectors/Zope3-dev/506

- Code style / documentation / test fixes.


3.1.0 (2005-10-03)
==================

- Add support for use of the new Python 2.4 datatypes, ``set`` and
  ``frozenset``, within checked code.

- Make the C security proxy depend on the ``proxy.h`` header from the
  ``zope.proxy`` package.

- XXX: the spelling of the ``#include`` is bizarre!  It seems to be related to
  ``zpkg``-based builds, and should likely be revisited.  For the moment, I have
  linked in the ``zope.proxy`` package into our own ``include`` directory.  See
  the subversion checkin: http://svn.zope.org/Zope3/?rev=37882&view=rev

- Update checker to avoid re-proxying objects which have and explicit
  ``__Security_checker__`` assigned.

- Corresponds to the verison of the ``zope.security`` package shipped as part of
  the Zope 3.1.0 release.

- Clarify contract of ``IChecker`` to indicate that its ``check*`` methods may
  raise only ``Forbidden`` or ``Unauthorized`` exceptions.

- Add interfaces, (``IPrincipal``, ``IGroupAwarePrincipal``, ``IGroup``, and
  ``IPermission``) specifying contracts of components in the security framework.

- Code style / documentation / test fixes.


3.0.0 (2004-11-07)
==================

- Corresponds to the version of the ``zope.security`` package shipped as part
  of the Zope X3.0.0 release.
  * [zope.testing-4.7](https://github.com/zopefoundation/zope.testing) =================
``zope.testing``
=================

.. image:: https://img.shields.io/pypi/v/zope.testing.svg
    :target: https://pypi.python.org/pypi/zope.testing/
    :alt: Latest Version

.. image:: https://travis-ci.org/zopefoundation/zope.testing.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.testing

.. image:: https://readthedocs.org/projects/zopetesting/badge/?version=latest
        :target: http://zopetesting.readthedocs.org/en/latest/
        :alt: Documentation Status

This package provides a number of testing frameworks.

cleanup
  Provides a mixin class for cleaning up after tests that
  make global changes.

formparser
  An HTML parser that extracts form information.

  **Python 2 only**

  This is intended to support functional tests that need to extract
  information from HTML forms returned by the publisher.

  See formparser.txt.

loggingsupport
  Support for testing logging code

  If you want to test that your code generates proper log output, you
  can create and install a handler that collects output.

loghandler
  Logging handler for tests that check logging output.

module
  Lets a doctest pretend to be a Python module.

  See module.txt.

renormalizing
  Regular expression pattern normalizing output checker.
  Useful for doctests.

server
  Provides a simple HTTP server compatible with the zope.app.testing
  functional testing API.  Lets you interactively play with the system
  under test.  Helpful in debugging functional doctest failures.

  **Python 2 only**

setupstack
  A simple framework for automating doctest set-up and tear-down.
  See setupstack.txt.

wait
  A small utility for dealing with timing non-determinism
  See wait.txt.

doctestcase
  Support for defining doctests as methods of ``unittest.TestCase``
  classes so that they can be more easily found by test runners, like
  nose, that ignore test suites.

.. contents::

Getting started developing zope.testing
=======================================

zope.testing uses buildout.  To start, run ``python bootstrap.py``.  It will
create a number of directories and the ``bin/buildout`` script.  Next, run
``bin/buildout``.  It will create a test script for you.  Now, run ``bin/test``
to run the zope.testing test suite.


Parsing HTML Forms
==================

Sometimes in functional tests, information from a generated form must
be extracted in order to re-submit it as part of a subsequent request.
The `zope.testing.formparser` module can be used for this purpose.

NOTE
   formparser doesn't support Python 3.

The scanner is implemented using the `FormParser` class.  The
constructor arguments are the page data containing the form and
(optionally) the URL from which the page was retrieved:

  >>> import zope.testing.formparser

  >>> page_text = '''\
  ... <html><body>
  ...   <form name="form1" action="/cgi-bin/foobar.py" method="POST">
  ...     <input type="hidden" name="f1" value="today" />
  ...     <input type="submit" name="do-it-now" value="Go for it!" />
  ...     <input type="IMAGE" name="not-really" value="Don't."
  ...            src="dont.png" />
  ...     <select name="pick-two" size="3" multiple>
  ...       <option value="one" selected>First</option>
  ...       <option value="two" label="Second">Another</option>
  ...       <optgroup>
  ...         <option value="three">Third</option>
  ...         <option selected="selected">Fourth</option>
  ...       </optgroup>
  ...     </select>
  ...   </form>
  ...
  ...   Just for fun, a second form, after specifying a base:
  ...   <base href="http://www.example.com/base/" />
  ...   <form action = 'sproing/sprung.html' enctype="multipart/form">
  ...     <textarea name="sometext" rows="5">Some text.</textarea>
  ...     <input type="Image" name="action" value="Do something."
  ...            src="else.png" />
  ...     <input type="text" value="" name="multi" size="2" />
  ...     <input type="text" value="" name="multi" size="3" />
  ...   </form>
  ... </body></html>
  ... '''

  >>> parser = zope.testing.formparser.FormParser(page_text)
  >>> forms = parser.parse()

  >>> len(forms)
  2
  >>> forms.form1 is forms[0]
  True
  >>> forms.form1 is forms[1]
  False

More often, the `parse()` convenience function is all that's needed:

  >>> forms = zope.testing.formparser.parse(
  ...     page_text, "http://cgi.example.com/somewhere/form.html")

  >>> len(forms)
  2
  >>> forms.form1 is forms[0]
  True
  >>> forms.form1 is forms[1]
  False

Once we have the form we're interested in, we can check form
attributes and individual field values:

  >>> form = forms.form1
  >>> form.enctype
  'application/x-www-form-urlencoded'
  >>> form.method
  'post'

  >>> keys = form.keys()
  >>> keys.sort()
  >>> keys
  ['do-it-now', 'f1', 'not-really', 'pick-two']

  >>> not_really = form["not-really"]
  >>> not_really.type
  'image'
  >>> not_really.value
  "Don't."
  >>> not_really.readonly
  False
  >>> not_really.disabled
  False

Note that relative URLs are converted to absolute URLs based on the
``<base>`` element (if present) or using the base passed in to the
constructor.

  >>> form.action
  'http://cgi.example.com/cgi-bin/foobar.py'
  >>> not_really.src
  'http://cgi.example.com/somewhere/dont.png'

  >>> forms[1].action
  'http://www.example.com/base/sproing/sprung.html'
  >>> forms[1]["action"].src
  'http://www.example.com/base/else.png'

Fields which are repeated are reported as lists of objects that
represent each instance of the field::

  >>> field = forms[1]["multi"]
  >>> isinstance(field, list)
  True
  >>> [o.value for o in field]
  ['', '']
  >>> [o.size for o in field]
  [2, 3]

The ``<textarea>`` element provides some additional attributes:

  >>> ta = forms[1]["sometext"]
  >>> print ta.rows
  5
  >>> print ta.cols
  None
  >>> ta.value
  'Some text.'

The ``<select>`` element provides access to the options as well:

  >>> select = form["pick-two"]
  >>> select.multiple
  True
  >>> select.size
  3
  >>> select.type
  'select'
  >>> select.value
  ['one', 'Fourth']

  >>> options = select.options
  >>> len(options)
  4
  >>> [opt.label for opt in options]
  ['First', 'Second', 'Third', 'Fourth']
  >>> [opt.value for opt in options]
  ['one', 'two', 'three', 'Fourth']


Support for testing logging code
================================

If you want to test that your code generates proper log output, you
can create and install a handler that collects output:

  >>> from zope.testing.loggingsupport import InstalledHandler
  >>> handler = InstalledHandler('foo.bar')

The handler is installed into loggers for all of the names passed. In
addition, the logger level is set to 1, which means, log
everything. If you want to log less than everything, you can provide a
level keyword argument.  The level setting effects only the named
loggers.

  >>> import logging
  >>> handler_with_levels = InstalledHandler('baz', level=logging.WARNING)

Then, any log output is collected in the handler:

  >>> logging.getLogger('foo.bar').exception('eek')
  >>> logging.getLogger('foo.bar').info('blah blah')

  >>> for record in handler.records:
  ...     print_(record.name, record.levelname)
  ...     print_(' ', record.getMessage())
  foo.bar ERROR
    eek
  foo.bar INFO
    blah blah

A similar effect can be gotten by just printing the handler:

  >>> print_(handler)
  foo.bar ERROR
    eek
  foo.bar INFO
    blah blah

After checking the log output, you need to uninstall the handler:

  >>> handler.uninstall()
  >>> handler_with_levels.uninstall()

At which point, the handler won't get any more log output.
Let's clear the handler:

  >>> handler.clear()
  >>> handler.records
  []

And then log something:

  >>> logging.getLogger('foo.bar').info('blah')

and, sure enough, we still have no output:

  >>> handler.records
  []


Regular expression pattern normalizing output checker
=====================================================

The pattern-normalizing output checker extends the default output checker with
an option to normalize expected and actual output.

You specify a sequence of patterns and replacements.  The replacements are
applied to the expected and actual outputs before calling the default outputs
checker.  Let's look at an example.  In this example, we have some times and
addresses:

    >>> want = '''\
    ... <object object at 0xb7f14438>
    ... completed in 1.234 seconds.
    ... <BLANKLINE>
    ... <object object at 0xb7f14440>
    ... completed in 123.234 seconds.
    ... <BLANKLINE>
    ... <object object at 0xb7f14448>
    ... completed in .234 seconds.
    ... <BLANKLINE>
    ... <object object at 0xb7f14450>
    ... completed in 1.234 seconds.
    ... <BLANKLINE>
    ... '''

    >>> got = '''\
    ... <object object at 0xb7f14458>
    ... completed in 1.235 seconds.
    ...
    ... <object object at 0xb7f14460>
    ... completed in 123.233 seconds.
    ...
    ... <object object at 0xb7f14468>
    ... completed in .231 seconds.
    ...
    ... <object object at 0xb7f14470>
    ... completed in 1.23 seconds.
    ...
    ... '''

We may wish to consider these two strings to match, even though they differ in
actual addresses and times.  The default output checker will consider them
different:

    >>> import doctest
    >>> doctest.OutputChecker().check_output(want, got, 0)
    False

We'll use the zope.testing.renormalizing.OutputChecker to normalize both the
wanted and gotten strings to ignore differences in times and
addresses:

    >>> import re
    >>> from zope.testing.renormalizing import OutputChecker
    >>> checker = OutputChecker([
    ...    (re.compile('[0-9]*[.][0-9]* seconds'), '<SOME NUMBER OF> seconds'),
    ...    (re.compile('at 0x[0-9a-f]+'), 'at <SOME ADDRESS>'),
    ...    ])

    >>> checker.check_output(want, got, 0)
    True

Usual OutputChecker options work as expected:

    >>> want_ellided = '''\
    ... <object object at 0xb7f14438>
    ... completed in 1.234 seconds.
    ... ...
    ... <object object at 0xb7f14450>
    ... completed in 1.234 seconds.
    ... <BLANKLINE>
    ... '''

    >>> checker.check_output(want_ellided, got, 0)
    False

    >>> checker.check_output(want_ellided, got, doctest.ELLIPSIS)
    True

When we get differencs, we output them with normalized text:

    >>> source = '''\
    ... >>> do_something()
    ... <object object at 0xb7f14438>
    ... completed in 1.234 seconds.
    ... ...
    ... <object object at 0xb7f14450>
    ... completed in 1.234 seconds.
    ... <BLANKLINE>
    ... '''

    >>> example = doctest.Example(source, want_ellided)

    >>> print_(checker.output_difference(example, got, 0))
    Expected:
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        ...
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        <BLANKLINE>
    Got:
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        <BLANKLINE>
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        <BLANKLINE>
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        <BLANKLINE>
        <object object at <SOME ADDRESS>>
        completed in <SOME NUMBER OF> seconds.
        <BLANKLINE>
    <BLANKLINE>

    >>> print_(checker.output_difference(example, got,
    ...                                 doctest.REPORT_NDIFF))
    Differences (ndiff with -expected +actual):
        - <object object at <SOME ADDRESS>>
        - completed in <SOME NUMBER OF> seconds.
        - ...
          <object object at <SOME ADDRESS>>
          completed in <SOME NUMBER OF> seconds.
          <BLANKLINE>
        + <object object at <SOME ADDRESS>>
        + completed in <SOME NUMBER OF> seconds.
        + <BLANKLINE>
        + <object object at <SOME ADDRESS>>
        + completed in <SOME NUMBER OF> seconds.
        + <BLANKLINE>
        + <object object at <SOME ADDRESS>>
        + completed in <SOME NUMBER OF> seconds.
        + <BLANKLINE>
    <BLANKLINE>

    If the wanted text is empty, however, we don't transform the actual output.
    This is usful when writing tests.  We leave the expected output empty, run
    the test, and use the actual output as expected, after reviewing it.

    >>> source = '''\
    ... >>> do_something()
    ... '''

    >>> example = doctest.Example(source, '\n')
    >>> print_(checker.output_difference(example, got, 0))
    Expected:
    <BLANKLINE>
    Got:
        <object object at 0xb7f14458>
        completed in 1.235 seconds.
        <BLANKLINE>
        <object object at 0xb7f14460>
        completed in 123.233 seconds.
        <BLANKLINE>
        <object object at 0xb7f14468>
        completed in .231 seconds.
        <BLANKLINE>
        <object object at 0xb7f14470>
        completed in 1.23 seconds.
        <BLANKLINE>
    <BLANKLINE>

If regular expressions aren't expressive enough, you can use arbitrary Python
callables to transform the text.  For example, suppose you want to ignore
case during comparison:

    >>> checker = OutputChecker([
    ...    lambda s: s.lower(),
    ...    lambda s: s.replace('<blankline>', '<BLANKLINE>'),
    ...    ])

    >>> want = '''\
    ... Usage: thundermonkey [options] [url]
    ... <BLANKLINE>
    ... Options:
    ...     -h    display this help message
    ... '''

    >>> got = '''\
    ... usage: thundermonkey [options] [URL]
    ...
    ... options:
    ...     -h    Display this help message
    ... '''

    >>> checker.check_output(want, got, 0)
    True

Suppose we forgot that <BLANKLINE> must be in upper case:

    >>> checker = OutputChecker([
    ...    lambda s: s.lower(),
    ...    ])

    >>> checker.check_output(want, got, 0)
    False

The difference would show us that:

    >>> source = '''\
    ... >>> print_help_message()
    ... ''' + want
    >>> example = doctest.Example(source, want)
    >>> print_(checker.output_difference(example, got,
    ...                                 doctest.REPORT_NDIFF))
    Differences (ndiff with -expected +actual):
          usage: thundermonkey [options] [url]
        - <blankline>
        + <BLANKLINE>
          options:
              -h    display this help message
    <BLANKLINE>


It is possible to combine OutputChecker checkers for easy reuse:

    >>> address_and_time_checker = OutputChecker([
    ...    (re.compile('[0-9]*[.][0-9]* seconds'), '<SOME NUMBER OF> seconds'),
    ...    (re.compile('at 0x[0-9a-f]+'), 'at <SOME ADDRESS>'),
    ...    ])
    >>> lowercase_checker = OutputChecker([
    ...    lambda s: s.lower(),
    ...    ])
    >>> combined_checker = address_and_time_checker + lowercase_checker
    >>> len(combined_checker.transformers)
    3

Combining a checker with something else does not work:

    >>> lowercase_checker + 5 #doctest: +ELLIPSIS
    Traceback (most recent call last):
        ...
    TypeError: unsupported operand type(s) for +: ...

Using the 2to3 exception normalization:

    >>> from zope.testing.renormalizing import (
    ...     IGNORE_EXCEPTION_MODULE_IN_PYTHON2)
    >>> checker = OutputChecker()
    >>> want = """\
    ... Traceback (most recent call last):
    ... foo.bar.FooBarError: requires at least one argument."""
    >>> got = """\
    ... Traceback (most recent call last):
    ... FooBarError: requires at least one argument."""
    >>> result = checker.check_output(
    ...     want, got, IGNORE_EXCEPTION_MODULE_IN_PYTHON2)
    >>> import sys
    >>> if sys.version_info[0] < 3:
    ...     expected = True
    ... else:
    ...     expected = False
    >>> result == expected
    True

When reporting a failing test and running in Python 2, the normalizer tries
to be helpful by explaining how to test for exceptions in the traceback output.

    >>> want = """\
    ... Traceback (most recent call last):
    ... foo.bar.FooBarErrorXX: requires at least one argument.
    ... """
    >>> got = """\
    ... Traceback (most recent call last):
    ... FooBarError: requires at least one argument.
    ... """
    >>> checker.check_output(want, got, IGNORE_EXCEPTION_MODULE_IN_PYTHON2)
    False
    >>> from doctest import Example
    >>> example = Example('dummy', want)
    >>> result = checker.output_difference(
    ...     example, got, IGNORE_EXCEPTION_MODULE_IN_PYTHON2)
    >>> output = """\
    ... Expected:
    ...     Traceback (most recent call last):
    ...     foo.bar.FooBarErrorXX: requires at least one argument.
    ... Got:
    ...     Traceback (most recent call last):
    ...     FooBarError: requires at least one argument.
    ... """
    >>> hint = """\
    ...     ===============================================================
    ...     HINT:
    ...       The optionflag IGNORE_EXCEPTION_MODULE_IN_PYTHON2 is set.
    ...       You seem to test traceback output.
    ...       If you are indeed, make sure to use the full dotted name of
    ...       the exception class like Python 3 displays,
    ...       even though you are running the tests in Python 2.
    ...       The exception message needs to be last line (and thus not
    ...       split over multiple lines).
    ...     ==============================================================="""
    >>> if sys.version_info[0] < 3:
    ...     expected = output + hint
    ... else:
    ...     expected = output
    >>> result == expected
    True


Stack-based test setUp and tearDown
===================================

Writing doctest setUp and tearDown functions can be a bit tedious,
especially when setUp/tearDown functions are combined.

the zope.testing.setupstack module provides a small framework for
automating test tear down.  It provides a generic setUp function that
sets up a stack. Normal test setUp functions call this function to set
up the stack and then use the register function to register tear-down
functions.

To see how this works we'll create a faux test:

    >>> class Test:
    ...     def __init__(self):
    ...         self.globs = {}
    >>> test = Test()

We'll register some tearDown functions that just print something:

    >>> import sys
    >>> import zope.testing.setupstack
    >>> zope.testing.setupstack.register(
    ...     test, lambda : sys.stdout.write('td 1\n'))
    >>> zope.testing.setupstack.register(
    ...     test, lambda : sys.stdout.write('td 2\n'))

Now, when we call the tearDown function:

    >>> zope.testing.setupstack.tearDown(test)
    td 2
    td 1

The registered tearDown functions are run. Note that they are run in
the reverse order that they were registered.


Extra positional arguments can be passed to register:

    >>> zope.testing.setupstack.register(
    ...    test, lambda x, y, z: sys.stdout.write('%s %s %s\n' % (x, y, z)),
    ...    1, 2, z=9)
    >>> zope.testing.setupstack.tearDown(test)
    1 2 9


Temporary Test Directory
------------------------

Often, tests create files as they demonstrate functionality.  They
need to arrange for the removeal of these files when the test is
cleaned up.

The setUpDirectory function automates this.  We'll get the current
directory first:

    >>> import os
    >>> here = os.getcwd()

We'll also create a new test:

    >>> test = Test()

Now we'll call the setUpDirectory function:

    >>> zope.testing.setupstack.setUpDirectory(test)

We don't have to call zope.testing.setupstack.setUp, because
setUpDirectory calls it for us.

Now the current working directory has changed:

    >>> here == os.getcwd()
    False
    >>> setupstack_cwd = os.getcwd()

We can create files to out heart's content:

    >>> with open('Data.fs', 'w') as f:
    ...     foo = f.write('xxx')
    >>> os.path.exists(os.path.join(setupstack_cwd, 'Data.fs'))
    True

We'll make the file read-only. This can cause problems on Windows, but
setupstack takes care of that by making files writable before trying
to remove them.

    >>> import stat
    >>> os.chmod('Data.fs', stat.S_IREAD)

On Unix systems, broken symlinks can cause problems because the chmod
attempt by the teardown hook will fail; let's set up a broken symlink as
well, and verify the teardown doesn't break because of that:

    >>> if sys.platform != 'win32':
    ...     os.symlink('NotThere', 'BrokenLink')

When tearDown is called:

    >>> zope.testing.setupstack.tearDown(test)

We'll be back where we started:

    >>> here == os.getcwd()
    True

and the files we created will be gone (along with the temporary
directory that was created:

    >>> os.path.exists(os.path.join(setupstack_cwd, 'Data.fs'))
    False

Context-manager support
-----------------------

You can leverage context managers using the ``contextmanager`` method.
The result of calling the content manager's __enter__ method will be
returned. The context-manager's __exit__ method will be called as part
of test tear down:

    >>> class Manager(object):
    ...     def __init__(self, *args, **kw):
    ...         if kw:
    ...             args += (kw, )
    ...         self.args = args
    ...     def __enter__(self):
    ...         print_('enter', *self.args)
    ...         return 42
    ...     def __exit__(self, *args):
    ...         print_('exit', args, *self.args)

    >>> manager = Manager()
    >>> test = Test()

    >>> zope.testing.setupstack.context_manager(test, manager)
    enter
    42

    >>> zope.testing.setupstack.tearDown(test)
    exit (None, None, None)

.. faux mock

    >>> old_mock = sys.modules.get('mock')
    >>> class FauxMock:
    ...     @classmethod
    ...     def patch(self, *args, **kw):
    ...         return Manager(*args, **kw)

    >>> sys.modules['mock'] = FauxMock

By far the most commonly called context manager is ``mock.patch``, so
there's a convenience function to make that simpler:

    >>> zope.testing.setupstack.mock(test, 'time.time', return_value=42)
    enter time.time {'return_value': 42}
    42

    >>> zope.testing.setupstack.tearDown(test)
    exit (None, None, None) time.time {'return_value': 42}

globs
-----

Doctests have ``globs`` attributes used to hold test globals.
``setupstack`` was originally designed to work with doctests, but can
now work with either doctests, or other test objects, as long as the
test objects have either a ``globs`` attribute or a ``__dict__``
attribute.  The ``zope.testing.setupstack.globs`` function is used to
get the globals for a test object:

    >>> zope.testing.setupstack.globs(test) is test.globs
    True

Here, because the test object had a ``globs`` attribute, it was
returned. Because we used the test object above, it has a setupstack:

    >>> '__zope.testing.setupstack' in test.globs
    True

If we remove the ``globs`` attribute, the object's instance dictionary
will be used:

    >>> del test.globs
    >>> zope.testing.setupstack.globs(test) is test.__dict__
    True
    >>> zope.testing.setupstack.context_manager(test, manager)
    enter
    42

    >>> '__zope.testing.setupstack' in test.__dict__
    True

The ``globs`` function is used internally, but can also be used by
setup code to support either doctests or other test objects.

TestCase
--------

A TestCase class is provided that:

- Makes it easier to call setupstack apis, and

- provides an inheritable tearDown method.

In addition to a tearDown method, the class provides methods:

``setupDirectory()``
    Creates a temporary directory, runs the test, and cleans it up.

``register(func)``
    Register a tear-down function.

``context_manager(manager)``
    Enters a context manager and exits it on tearDown.

``mock(*args, **kw)``
    Enters  ``mock.patch`` with the given arguments.

    This is syntactic sugur for::

        context_manager(mock.patch(*args, **kw))

Here's an example:

    >>> open('t', 'w').close()

    >>> class MyTests(zope.testing.setupstack.TestCase):
    ...
    ...     def setUp(self):
    ...         self.setUpDirectory()
    ...         self.context_manager(manager)
    ...         self.mock("time.time", return_value=42)
    ...
    ...         @self.register
    ...         def _():
    ...             print('done w test')
    ...
    ...     def test(self):
    ...         print(os.listdir('.'))

.. let's try it

    >>> import unittest
    >>> loader = unittest.TestLoader()
    >>> suite = loader.loadTestsFromTestCase(MyTests)
    >>> result = suite.run(unittest.TestResult())
    enter
    enter time.time {'return_value': 42}
    []
    done w test
    exit (None, None, None) time.time {'return_value': 42}
    exit (None, None, None)

.. cleanup

    >>> if old_mock:
    ...     sys.modules['mock'] = old_mock
    ... else:
    ...     del sys.modules['mock']
    >>> os.remove('t')



Wait until a condition holds (or until a time out)
==================================================

Often, in tests, you need to wait until some condition holds.  This
may be because you're testing interaction with an external system or
testing threaded (threads, processes, greenlet's, etc.) interactions.

You can add sleeps to your tests, but it's often hard to know how
long to sleep.

``zope.testing.wait`` provides a convenient way to wait until
some condition holds.  It will test a condition and, when true,
return.  It will sleep a short time between tests.

Here's a silly example, that illustrates it's use:

    >>> from zope.testing.wait import wait
    >>> wait(lambda : True)

Since the condition we passed is always True, it returned
immediately.  If the condition doesn't hold, then we'll get a timeout:

    >>> wait((lambda : False), timeout=.01)
    Traceback (most recent call last):
    ...
    TimeOutWaitingFor: <lambda>

``wait`` has some keyword options:

timeout
   How long, in seconds, to wait for the condition to hold

   Defaults to 9 seconds.

wait
   How long to wait between calls.

   Defaults to .01 seconds.

message
   A message (or other data) to pass to the timeout exception.

   This defaults to ``None``.  If this is false, then the callable's
   doc string or ``__name__`` is used.

``wait`` can be used as a decorator:

    >>> @wait
    ... def ok():
    ...     return True

    >>> @wait(timeout=.01)
    ... def no_way():
    ...     pass
    Traceback (most recent call last):
    ...
    TimeOutWaitingFor: no_way

    >>> @wait(timeout=.01)
    ... def no_way():
    ...     "never true"
    Traceback (most recent call last):
    ...
    TimeOutWaitingFor: never true

.. more tests

    >>> import time
    >>> now = time.time()
    >>> @wait(timeout=.01, message='dang')
    ... def no_way():
    ...     "never true"
    Traceback (most recent call last):
    ...
    TimeOutWaitingFor: dang

    >>> .01 < (time.time() - now) < .03
    True


Customization
-------------

``wait`` is an instance of ``Wait``.  With ``Wait``,
you can create you're own custom ``wait`` utilities.  For
example, if you're testing something that uses getevent, you'd want to
use gevent's sleep function:

    >>> import zope.testing.wait
    >>> wait = zope.testing.wait.Wait(getsleep=lambda : gevent.sleep)

Wait takes a number of customization parameters:

exception
  Timeout exception class

getnow
  Function used to get a function for getting the current time.

  Default: lambda : time.time

getsleep
  Function used to get a sleep function.

  Default: lambda : time.sleep

timeout
  Default timeout

  Default: 9

wait
  Default time to wait between attempts

  Default: .01


.. more tests

    >>> def mysleep(t):
    ...     print_('mysleep', t)
    ...     time.sleep(t)

    >>> def mynow():
    ...     print_('mynow')
    ...     return time.time()

    >>> wait = zope.testing.wait.Wait(
    ...    getnow=(lambda : mynow), getsleep=(lambda : mysleep),
    ...    exception=ValueError, timeout=.1, wait=.02)

    >>> @wait
    ... def _(state=[]):
    ...     if len(state) > 1:
    ...        return True
    ...     state.append(0)
    mynow
    mysleep 0.02
    mynow
    mysleep 0.02

    >>> @wait(wait=.002)
    ... def _(state=[]):
    ...     if len(state) > 1:
    ...        return True
    ...     state.append(0)
    mynow
    mysleep 0.002
    mynow
    mysleep 0.002

    >>> @wait(timeout=0)
    ... def _(state=[]):
    ...     if len(state) > 1:
    ...        return True
    ...     state.append(0)
    Traceback (most recent call last):
    ...
    ValueError: _

    >>> wait = zope.testing.wait.Wait(timeout=0)
    >>> @wait(timeout=0)
    ... def _(state=[]):
    ...     if len(state) > 1:
    ...        return True
    ...     state.append(0)
    Traceback (most recent call last):
    ...
    TimeOutWaitingFor: _


Doctests in TestCase classes
============================

The original ``doctest`` unittest integration was based on
``unittest`` test suites, which have fallen out of favor. This module
provides a way to define doctests inside of unittest ``TestCase``
classes. It provides better integration with unittest test fixtures,
because doctests use setup provided by the containing test case
class. It provides access to unittest assertion methods.

You can define doctests in multiple ways:

- references to named files

- strings

- decorated functions with docstrings

- reference to named files decorating test-specific setup functions

- reference to named files decorating a test class

.. some setup

   >>> __name__ = 'tests'

Here are some examples::

    >>> from zope.testing import doctestcase
    >>> import doctest
    >>> import unittest

    >>> g = 'global'

    >>> class MyTest(unittest.TestCase):
    ...
    ...     def setUp(self):
    ...         self.a = 1
    ...         self.globs = dict(c=9)
    ...
    ...     test1 = doctestcase.file('test-1.txt', optionflags=doctest.ELLIPSIS)
    ...
    ...     test2 = doctestcase.docteststring('''
    ...       >>> self.a, g, c
    ...       (1, 'global', 9)
    ...     ''')
    ...
    ...     @doctestcase.doctestmethod(optionflags=doctest.ELLIPSIS)
    ...     def test3(self):
    ...         '''
    ...         >>> self.a, self.x, g, c
    ...         (1, 3, 'global', 9)
    ...         '''
    ...         self.x = 3
    ...
    ...     @doctestcase.doctestfile('test4.txt')
    ...     def test4(self):
    ...         self.x = 5

    >>> import sys

    >>> @doctestcase.doctestfiles('loggingsupport.txt', 'renormalizing.txt')
    ... class MoreTests(unittest.TestCase):
    ...
    ...    def setUp(self):
    ...        def print_(*args):
    ...            sys.stdout.write(' '.join(map(str, args))+'\n')
    ...        self.globs = dict(print_=print_)


.. We can run these tests with the ``unittest`` test runner.

    >>> loader = unittest.TestLoader()
    >>> sys.stdout.writeln = lambda s: sys.stdout.write(s+'\n')
    >>> suite = loader.loadTestsFromTestCase(MyTest)
    >>> result = suite.run(unittest.TextTestResult(sys.stdout, True, 3))
    test1 (tests.MyTest) ... ok
    test2 (tests.MyTest) ... ok
    test3 (tests.MyTest) ... ok
    test4 (tests.MyTest) ... ok

    >>> suite = loader.loadTestsFromTestCase(MoreTests)
    >>> result = suite.run(unittest.TextTestResult(sys.stdout, True, 3))
    test_loggingsupport (tests.MoreTests) ... ok
    test_renormalizing (tests.MoreTests) ... ok

    >>> for _, e in result.errors:
    ...     print(e); print

    Check meta data:

    >>> MyTest.test1.__name__
    'test_1'
    >>> import os, zope.testing
    >>> (MyTest.test1.filepath ==
    ...  os.path.join(os.path.dirname(zope.testing.__file__), 'test-1.txt'))
    True
    >>> MyTest.test1.filename
    'test-1.txt'

    >>> MyTest.test3.__name__
    'test3'
    >>> MyTest.test4.__name__
    'test4'

    >>> (MyTest.test4.filepath ==
    ...  os.path.join(os.path.dirname(zope.testing.__file__), 'test4.txt'))
    True
    >>> MyTest.test4.filename
    'test4.txt'

    >>> MoreTests.test_loggingsupport.__name__
    'test_loggingsupport'
    >>> MoreTests.test_loggingsupport.filename
    'loggingsupport.txt'
    >>> (MoreTests.test_loggingsupport.filepath ==
    ...  os.path.join(os.path.dirname(zope.testing.__file__),
    ...               'loggingsupport.txt'))
    True

In these examples, 4 constructors were used:

doctestfile (alias: file)
  doctestfile makes a file-based test case.

  This can be used as a decorator, in which case, the decorated
  function is called before the test is run, to provide test-specific
  setup.

doctestfiles (alias: files)
  doctestfiles makes file-based test cases and assigns them to the
  decorated class.

  Multiple files can be specified and the resulting doctests are added
  as members of the decorated class.

docteststring (alias string)
  docteststring constructs a doctest from a string.

doctestmethod (alias method)
  doctestmethod constructs a doctest from a method.

  The method's docstring provides the test. The method's body provides
  optional test-specific setup.

Note that short aliases are provided, which maye be useful in certain
import styles.

Tests have access to the following data:

- Tests created with the ``docteststring`` and ``doctestmethod``
  constructors have access to the module globals of the defining
  module.

- In tests created with the ``docteststring`` and ``doctestmethod``
  constructors, the test case instance is available as the ``self``
  variable.

- In tests created with the ``doctestfile`` and ``doctestfiles``
  constructor, the test case instance is available as the ``test``
  variable.

- If a test case defines a globs attribute, it must be a dictionary
  and it's contents are added to the test globals.

The constructors accept standard doctest ``optionflags`` and
``checker`` arguments.

Note that the doctest IGNORE_EXCEPTION_DETAIL option flag is
added to optionflags.

When using ``doctestfile`` and ``doctestfile``, ``filename`` and
``filepath`` attributes are available that contain the test file name
and full path.

``__name__`` attributes of class members
----------------------------------------

Class members have ``__name__`` attributes set as follows:

- When using ``doctestmethod`` or ``doctestfile`` with a setup
  function, ``__name__`` attribute is set to the name of the function.
  A ``test_`` prefix is added, if the name doesn't start with ``test``.

- When ``doctestfile`` is used without a setup function or when
  ``doctestfiles`` is used, ``__name__`` is set to the last part of the
  file path with the extension removed and non-word characters
  converted to underscores. For example, with a test path of
  ``'/foo/bar/test-it.rst'``, the ``__name__`` attribute is set to
  ``'test_it'``.  A ``test_`` prefix is added, if the name doesn't
  start with ``test``.

- when using ``docteststring``, a ``name`` option can be passed in to
  set ``__name__``.  A ``test_`` prefix is added, if the name doesn't
  start with ``test``.

The ``__name__`` attribute is important when using nose, because nose
discovers tests as class members using their ``__name__`` attributes,
whereas the unittest and py.test test runners use class dictionary keys.

.. Let's look at some failure cases:

    >>> class MyTest(unittest.TestCase):
    ...
    ...     test2 = doctestcase.string('''
    ...     >>> 1
    ...     1
    ...     >>> 1 + 1
    ...     1
    ...     ''', name='test2')
    ...
    ...     @doctestcase.method
    ...     def test3(self):
    ...         '''
    ...         >>> self.x
    ...         3
    ...         >>> 1 + 1
    ...         1
    ...         '''
    ...         self.x = 3
    ...
    ...     @doctestcase.file('test4f.txt')
    ...     def test4(self):
    ...         self.x = 5

    >>> suite = loader.loadTestsFromTestCase(MyTest)
    >>> result = suite.run(unittest.TextTestResult(sys.stdout, True, 1))
    FFF
    >>> for c, e in result.failures:
    ...     print(e) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
      ...
    ...: Failed doctest test for <string>
      File "<string>", line 0, in <string>
    <BLANKLINE>
    ----------------------------------------------------------------------
    File "<string>", line 4, in <string>
    Failed example:
        1 + 1
    Expected:
        1
    Got:
        2
    <BLANKLINE>
    <BLANKLINE>
    Traceback (most recent call last):
      ...
    ...: Failed doctest test for test3
      File "None", line 10, in test3
    <BLANKLINE>
    ----------------------------------------------------------------------
    Line 4, in test3
    Failed example:
        1 + 1
    Expected:
        1
    Got:
        2
    <BLANKLINE>
    <BLANKLINE>
    Traceback (most recent call last):
    ...
    ...: Failed doctest test for test4f.txt
      File "...test4f.txt", line 0, in txt
    <BLANKLINE>
    ----------------------------------------------------------------------
    File "...test4f.txt", line 3, in test4f.txt
    Failed example:
        1 + 1
    Expected:
        1
    Got:
        2
    <BLANKLINE>
    <BLANKLINE>

    Check string meta data:

    >>> MyTest.test2.__name__
    'test2'

.. Verify setting optionflags and checker

    >>> class EasyChecker:
    ...     def check_output(self, want, got, optionflags):
    ...         return True
    ...     def output_difference(self, example, got, optionflags):
    ...         return ''

    >>> class MyTest(unittest.TestCase):
    ...
    ...     test2 = doctestcase.string('''
    ...     >>> 1
    ...     2
    ...     ''', checker=EasyChecker())
    ...
    ...     @doctestcase.method(optionflags=doctest.ELLIPSIS)
    ...     def test3(self):
    ...         '''
    ...         >>> 'Hello'
    ...         '...'
    ...         '''
    ...
    ...     @doctestcase.file('test4e.txt', optionflags=doctest.ELLIPSIS)
    ...     def test4(self):
    ...         self.x = 5

    >>> suite = loader.loadTestsFromTestCase(MyTest)
    >>> result = suite.run(unittest.TextTestResult(sys.stdout, True, 2))
    test2 (tests.MyTest) ... ok
    test3 (tests.MyTest) ... ok
    test4 (tests.MyTest) ... ok

.. test __name__ variations

    >>> class MyTest(unittest.TestCase):
    ...
    ...     foo = doctestcase.string('''>>> 1''', name='foo')
    ...
    ...     @doctestcase.method
    ...     def bar(self):
    ...         '''
    ...         >>> self.x
    ...         3
    ...         '''
    ...     @doctestcase.file('test4f.txt')
    ...     def baz(self):
    ...         pass
    ...     wait = doctestcase.file('wait.txt')

    >>> MyTest.foo.__name__
    'test_foo'
    >>> MyTest.bar.__name__
    'test_bar'
    >>> MyTest.baz.__name__
    'test_baz'
    >>> MyTest.wait.__name__
    'test_wait'


Changes
=======

4.7 (2018-10-04)
----------------

- Added support for Python 3.7.


4.6.2 (2017-06-12)
------------------

- Remove dependencies on ``zope.interface`` and ``zope.exceptions``;
  they're not used here.

- Remove use of 2to3 for outdated versions of PyPy3, letting us build
  universal wheels.


4.6.1 (2017-01-04)
------------------

- Add support for Python 3.6.


4.6.0 (2016-10-20)
------------------

- Introduce option flag ``IGNORE_EXCEPTION_MODULE_IN_PYTHON2`` to normalize
  exception class names in traceback output. In Python 3 they are displayed as
  the full dotted name. In Python 2 they are displayed as "just" the class
  name.  When running doctests in Python 3, the option flag will not have any
  effect, however when running the same test in Python 2, the segments in the
  full dotted name leading up to the class name are stripped away from the
  "expected" string.

- Drop support for Python 2.6 and 3.2.

- Add support for Python 3.5.

- Cleaned up useless 2to3 conversion.

4.5.0 (2015-09-02)
------------------

- Added meta data for test case methods created with
  ``zope.testing.doctestcase``.

  - Reasonable values for ``__name__``, making sure that ``__name__``
    starts with ``test``.

  - For ``doctestfile`` methods, provide ``filename`` and ``filepath``
    attributes.

  The meta data us useful, for example, for selecting tests with the
  nose attribute mechanism.

- Added ``doctestcase.doctestfiles``

  - Define multiple doctest files at once.

  - Automatically assign test class members.  So rather than::

      class MYTests(unittest.TestCase):
          ...
          test_foo = doctestcase.doctestfile('foo.txt')

    You can use::

      @doctestcase.doctestfiles('foo.txt', 'bar.txt', ...)
      class MYTests(unittest.TestCase):
          ...

4.4.0 (2015-07-16)
------------------

- Added ``zope.testing.setupstack.mock`` as a convenience function for
  setting up mocks in tests.  (The Python ``mock`` package must be in
  the path for this to work. The excellent ``mock`` package isn't a
  dependency of ``zope.testing``.)

- Added the base class ``zope.testing.setupstack.TestCase`` to make it
  much easier to use ``zope.testing.setupstack`` in ``unittest`` test
  cases.


4.3.0 (2015-07-15)
------------------

- Added support for creating doctests as methods of
  ``unittest.TestCase`` classes so that they can found automatically
  by test runners, like *nose* that ignore test suites.

4.2.0 (2015-06-01)
------------------

- **Actually** remove long-deprecated ``zope.testing.doctest`` (announced as
  removed in 4.0.0) and ``zope.testing.doctestunit``.

- Add support for PyPy and PyPy3.

4.1.3 (2014-03-19)
------------------

- Add support for Python 3.4.

- Update ``boostrap.py`` to version 2.2.

4.1.2 (2013-02-19)
------------------

- Adjust Trove classifiers to reflect the currently supported Python
  versions. Officially drop Python 2.4 and 2.5. Add Python 3.3.

- LP: #1055720: Fix failing test on Python 3.3 due to changed exception
  messaging.

4.1.1 (2012-02-01)
------------------

- Fix: Windows test failure.

4.1.0 (2012-01-29)
------------------

- Add context-manager support to ``zope.testing.setupstack``

- Make ``zope.testing.setupstack`` usable with all tests, not just
  doctests and added ``zope.testing.setupstack.globs``, which makes it
  easier to write test setup code that workes with doctests and other
  kinds of tests.

- Add the ``wait`` module, which makes it easier to deal with
  non-deterministic timing issues.

- Rename ``zope.testing.renormalizing.RENormalizing`` to
  ``zope.testing.renormalizing.OutputChecker``. The old name is an
  alias.

- Update tests to run with Python 3.

- Label more clearly which features are supported by Python 3.

- Reorganize documentation.

4.0.0 (2011-11-09)
------------------

- Remove the deprecated ``zope.testing.doctest``.

- Add Python 3 support.

- Fix test which fails if there is a file named `Data.fs` in the current
  working directory.


3.10.2 (2010-11-30)
-------------------

- Fix test of broken symlink handling to not break on Windows.


3.10.1 (2010-11-29)
-------------------

- Fix removal of broken symlinks on Unix.


3.10.0 (2010-07-21)
-------------------

- Remove ``zope.testing.testrunner``, which now is moved to zope.testrunner.

- Update fix for LP #221151 to a spelling compatible with Python 2.4.

3.9.5 (2010-05-19)
------------------

- LP #579019: When layers are run in parallel, ensure that each ``tearDown``
  is called, including the first layer which is run in the main
  thread.

- Deprecate ``zope.testing.testrunner`` and ``zope.testing.exceptions``.
  They have been moved to a separate zope.testrunner module, and will be
  removed from zope.testing in 4.0.0, together with ``zope.testing.doctest``.

3.9.4 (2010-04-13)
------------------

- LP #560259: Fix subunit output formatter to handle layer setup
  errors.

- LP #399394:  Add a ``--stop-on-error`` / ``--stop`` / ``-x`` option to
  the testrunner.

- LP #498162:  Add a ``--pdb`` alias for the existing ``--post-mortem``
  / ``-D`` option to the testrunner.

- LP #547023:  Add a ``--version`` option to the testrunner.

- Add tests for LP #144569 and #69988.

  https://bugs.launchpad.net/bugs/69988

  https://bugs.launchpad.net/zope3/+bug/144569


3.9.3 (2010-03-26)
------------------

- Remove import of ``zope.testing.doctest`` from ``zope.testing.renormalizer``.

- Suppress output to ``sys.stderr`` in ``testrunner-layers-ntd.txt``.

- Suppress ``zope.testing.doctest`` deprecation warning when running
  our own test suite.


3.9.2 (2010-03-15)
------------------

- Fix broken ``from zope.testing.doctest import *``

3.9.1 (2010-03-15)
------------------

- No changes; reupload to fix broken 3.9.0 release on PyPI.

3.9.0 (2010-03-12)
------------------

- Modify the testrunner to use the standard Python ``doctest`` module instead
  of the deprecated ``zope.testing.doctest``.

- Fix ``testrunner-leaks.txt`` to use the ``run_internal`` helper, so that
  ``sys.exit`` isn't triggered during the test run.

- Add support for conditionally using a subunit-based output
  formatter upon request if subunit and testtools are available. Patch
  contributed by Jonathan Lange.

3.8.7 (2010-01-26)
------------------

- Downgrade the ``zope.testing.doctest`` deprecation warning into a
  PendingDeprecationWarning.

3.8.6 (2009-12-23)
------------------

- Add ``MANIFEST.in`` and reupload to fix broken 3.8.5 release on PyPI.


3.8.5 (2009-12-23)
------------------

- Add back ``DocFileSuite``, ``DocTestSuite``, ``debug_src`` and ``debug``
  BBB imports back into ``zope.testing.doctestunit``; apparently many packages
  still import them from there!

- Deprecate ``zope.testing.doctest`` and ``zope.testing.doctestunit``
  in favor of the stdlib ``doctest`` module.


3.8.4 (2009-12-18)
------------------

- Fix missing imports and undefined variables reported by pyflakes,
  adding tests to exercise the blind spots.

- Cleaned up unused imports reported by pyflakes.

- Add two new options to generate randomly ordered list of tests and to
  select a specific order of tests.

- Allow combining RENormalizing checkers via ``+`` now:
  ``checker1 + checker2`` creates a checker with the transformations of both
  checkers.

- Fix tests under Python 2.7.

3.8.3 (2009-09-21)
------------------

- Fix test failures due to using ``split()`` on filenames when running from a
  directory with spaces in it.

- Fix testrunner behavior on Windows for ``-j2`` (or greater) combined with
  ``-v`` (or greater).

3.8.2 (2009-09-15)
------------------

- Remove hotshot profiler when using Python 2.6. That makes zope.testing
  compatible with Python 2.6


3.8.1 (2009-08-12)
------------------

- Avoid hardcoding ``sys.argv[0]`` as script;
  allow, for instance, Zope 2's `bin/instance test` (LP#407916).

- Produce a clear error message when a subprocess doesn't follow the
  ``zope.testing.testrunner`` protocol (LP#407916).

- Avoid unnecessarily squelching verbose output in a subprocess when there are
  not multiple subprocesses.

- Avoid unnecessarily batching subprocess output, which can stymie automated
  and human processes for identifying hung tests.

- Include incremental output when there are multiple subprocesses and a
  verbosity of ``-vv`` or greater is requested.  This again is not batched,
  supporting automated processes and humans looking for hung tests.


3.8.0 (2009-07-24)
------------------

- Allow testrunner to include descendants of ``unittest.TestCase`` in test
  modules, which no longer need to provide ``test_suite()``.


3.7.7 (2009-07-15)
------------------

- Clean up support for displaying tracebacks with supplements by turning it
  into an always-enabled feature and making the dependency on
  ``zope.exceptions`` explicit.

- Fix #251759: prevent the testrunner descending into directories that
  aren't Python packages.

- Code cleanups.


3.7.6 (2009-07-02)
------------------

- Add zope-testrunner ``console_scripts`` entry point. This exposes a
  ``zope-testrunner`` script with default installs allowing the testrunner
  to be run from the command line.

3.7.5 (2009-06-08)
------------------

- Fix bug when running subprocesses on Windows.

- The option ``REPORT_ONLY_FIRST_FAILURE`` (command line option "-1") is now
  respected even when a doctest declares its own ``REPORTING_FLAGS``, such as
  ``REPORT_NDIFF``.

- Fix bug that broke readline with pdb when using doctest
  (see http://bugs.python.org/issue5727).

- Make tests pass on Windows and Linux at the same time.


3.7.4 (2009-05-01)
------------------

- Filenames of doctest examples now contain the line number and not
  only the example number. So a stack trace in pdb tells the exact
  line number of the current example. This fixes
  https://bugs.launchpad.net/bugs/339813

- Colorization of doctest output correctly handles blank lines.


3.7.3 (2009-04-22)
------------------

- Improve handling of rogue threads:  always exit with status so even
  spinning daemon threads won't block the runner from exiting. This deprecated
  the ``--with-exit-status`` option.


3.7.2 (2009-04-13)
------------------

- Fix test failure on Python 2.4 due to slight difference in the way
  coverage is reported (__init__ files with only a single comment line are now
  not reported)

- Fix bug that caused the test runner to hang when running subprocesses (as a
  result Python 2.3 is no longer supported).

- Work around a bug in Python 2.6 (related to
  http://bugs.python.org/issue1303673) that causes the profile tests to fail.

- Add explanitory notes to ``buildout.cfg`` about how to run the tests with
  multiple versions of Python


3.7.1 (2008-10-17)
------------------

- The ``setupstack`` temporary directory support now properly handles
  read-only files by making them writable before removing them.


3.7.0 (2008-09-22)
------------------

- Add alterate setuptools / distutils commands for running all tests
  using our testrunner.  See 'zope.testing.testrunner.eggsupport:ftest'.

- Add a setuptools-compatible test loader which skips tests with layers:
  the testrunner used by ``setup.py test`` doesn't know about them, and those
  tests then fail.  See ``zope.testing.testrunner.eggsupport:SkipLayers``.

- Add support for Jython, when a garbage collector call is sent.

- Add support to bootstrap on Jython.

- Fix NameError in StartUpFailure.

- Open doctest files in universal mode, so that packages released on Windows
  can be tested on Linux, for example.


3.6.0 (2008-07-10)
------------------

- Add ``-j`` option to parallel tests run in subprocesses.

- RENormalizer accepts plain Python callables.

- Add ``--slow-test`` option.

- Add ``--no-progress`` and ``--auto-progress`` options.

- Complete refactoring of the test runner into multiple code files and a more
  modular (pipeline-like) architecture.

- Unify unit tests with the layer support by introducing a real unit test
  layer.

- Add a doctest for ``zope.testing.module``. There were several bugs
  that were fixed:

  * ``README.txt`` was a really bad default argument for the module
    name, as it is not a proper dotted name. The code would
    immediately fail as it would look for the ``txt`` module in the
    ``README`` package. The default is now ``__main__``.

  * The ``tearDown`` function did not clean up the ``__name__`` entry in the
    global dictionary.

- Fix a bug that caused a SubprocessError to be generated if a subprocess
  sent any output to stderr.

- Fix a bug that caused the unit tests to be skipped if run in a subprocess.


3.5.1 (2007-08-14)
------------------

- Invoke post-mortem debugging for layer-setup failures.

3.5.0 (2007-07-19)
------------------

- Ensure that the test runner works on Python 2.5.

- Add support for ``cProfile``.

- Add output colorizing (``-c`` option).

- Add ``--hide-secondary-failures`` and ``--show-secondary-failures`` options
  (https://bugs.launchpad.net/zope3/+bug/115454).

- Fix some problems with Unicode in doctests.

- Fix "Error reading from subprocess" errors on Unix-like systems.

3.4 (2007-03-29)
----------------

- Add ``exit-with-status`` support (supports use with buildbot and
  ``zc.recipe.testing``)

- Add a small framework for automating set up and tear down of
  doctest tests. See ``setupstack.txt``.

- Allow ``testrunner-wo-source.txt`` and ``testrunner-errors.txt`` to run
  within a read-only source tree.

3.0 (2006-09-20)
----------------

- Update the doctest copy with text-file encoding support.

- Add logging-level support to the ``loggingsuppport`` module.

- At verbosity-level 1, dots are not output continuously, without any
  line breaks.

- Improve output when the inability to tear down a layer causes tests
  to be run in a subprocess.

- Make ``zope.exception`` required only if the ``zope_tracebacks`` extra is
  requested.

- Fix the test coverage. If a module, for example `interfaces`, was in an
  ignored directory/package, then if a module of the same name existed in a
  covered directory/package, then it was also ignored there, because the
  ignore cache stored the result by module name and not the filename of the
  module.

2.0 (2006-01-05)
----------------

- Release a separate project corresponding to the version of ``zope.testing``
  shipped as part of the Zope 3.2.0 release.



  * [zope.testrunner-5.0](https://github.com/zopefoundation/zope.testrunner) =================
 zope.testrunner
=================

.. image:: https://img.shields.io/pypi/v/zope.testrunner.svg
        :target: https://pypi.org/project/zope.testrunner/
        :alt: Latest release

.. image:: https://img.shields.io/pypi/pyversions/zope.testrunner.svg
        :target: https://pypi.org/project/zope.testrunner/
        :alt: Supported Python versions

.. image:: https://travis-ci.org/zopefoundation/zope.testrunner.svg?branch=master
        :target: https://travis-ci.org/zopefoundation/zope.testrunner

.. image:: https://coveralls.io/repos/github/zopefoundation/zope.testrunner/badge.svg?branch=master
        :target: https://coveralls.io/github/zopefoundation/zope.testrunner?branch=master

.. image:: https://readthedocs.org/projects/zopetestrunner/badge/?version=latest
        :target: https://zopetestrunner.readthedocs.io/en/latest/?badge=latest
        :alt: Documentation Status

.. contents::

This package provides a flexible test runner with layer support.

Detailed documentation is hosted at https://zopetestrunner.readthedocs.io


=======================
 Using zope.testrunner
=======================

.. IMPORTANT: This document is included in the long_description for
   rendering on PyPI, so it cannot use Sphinx-only features.

Installation
============

Buildout-based projects
-----------------------

zope.testrunner is often used for projects that use buildout_::

    [buildout]
    develop = .
    parts = ... test ...

    [test]
    recipe = zc.recipe.testrunner
    eggs = mypackage

The usual buildout process ::

    python bootstrap.py
    bin/buildout

creates a ``bin/test`` script that will run the tests for *mypackage*.

.. tip::

    zc.recipe.testrunner_ takes care to specify the right
    ``--test-path`` option in the generated script.  You can add
    other options (such as ``--tests-pattern``) too; check
    zc.recipe.testrunner_'s documentation for details.


Virtualenv-based projects
-------------------------

``pip install zope.testrunner`` and you'll get a ``zope-testrunner``
script.  Run your tests with ::

    zope-testrunner --test-path=path/to/your/source/tree

Your source code needs to be available for the testrunner to import,
so you need to run ``python setup.py install`` or ``pip install -e
.`` into the same virtualenv_.


Some useful command-line options to get you started
===================================================

-p              show a progress indicator
-v              increase verbosity
-c              colorize the output
-t test         specify test names (one or more regexes)
-m module       specify test modules (one or more regexes)
-s package      specify test packages (one or more regexes)
--list-tests    show names of tests instead of running them
-x              stop on first error or failure
-D, --pdb       enable post-mortem debugging of test failures
--help          show *all* command-line options (there are many more!)

For example ::

    bin/test -pvc -m test_foo -t TestBar

runs all TestBar tests from a module called test_foo.py.

Writing tests
=============

``zope.testrunner`` expects to find your tests inside your package
directory, in a subpackage or module named ``tests``.  Test modules
in a test subpackage should be named ``test*.py``.

.. tip::

    You can change these assumptions with ``--tests-pattern`` and
    ``--test-file-pattern`` test runner options.

Tests themselves should be classes inheriting from
``unittest.TestCase``, and if you wish to use doctests, please tell
the test runner where to find them and what options to use for them
in by supplying a function named ``test_suite``.

Example::

    import unittest
    import doctest

    class TestArithmetic(unittest.TestCase):

        def test_two_plus_two(self):
            self.assertEqual(2 + 2, 4)


    def doctest_string_formatting():
        """Test Python string formatting

            >>> print('{} + {}'.format(2, 2))
            2 + 2

        """

    def test_suite():
        return unittest.TestSuite([
            unittest.defaultTestLoader.loadTestsFromName(__name__),
            doctest.DocTestSuite(),
            doctest.DocFileSuite('../README.txt',
                                 optionflags=doctest.ELLIPSIS),
        ])



Test grouping
=============

In addition to per-package and per-module filtering, zope.testrunner
has other mechanisms for grouping tests:

* **layers** allow you to have shared setup/teardown code to be used
  by a group of tests, that is executed only once, and not for each
  test.  Layers are orthogonal to the usual package/module structure
  and are specified by setting the ``layer`` attribute on test
  suites.

* **levels** allow you to group slow-running tests and not run them
  by default.  They're specified by setting the ``level`` attribute
  on test suites to an int.


Other features
==============

zope.testrunner can profile your tests, measure test coverage,
check for memory leaks, integrate with subunit_, shuffle the
test execution order, and run multiple tests in parallel.

.. _buildout: https://buildout.readthedocs.io
.. _virtualenv: https://virtualenv.pypa.io/
.. _zc.recipe.testrunner: https://pypi.python.org/pypi/zc.recipe.testrunner
.. _subunit: https://pypi.python.org/pypi/python-subunit


===========================
 zope.testrunner Changelog
===========================

5.0 (2019-03-19)
================

- Fix test failures and deprecation warnings occurring when using Python 3.8a1.
  (`#89 <https://github.com/zopefoundation/zope.testrunner/pull/89>`_)

- Drop support for Python 3.4.

4.9.2 (2018-11-24)
==================

- Fix ``TypeError: a bytes-like object is required, not 'str'``
  running tests in parallel on Python 3. See `issue 80
  <https://github.com/zopefoundation/zope.testrunner/issues/80>`_.


4.9.1 (2018-11-21)
==================

- Fix AssertionError in _DummyThread.isAlive on Python 3 (`#81
  <https://github.com/zopefoundation/zope.testrunner/issues/81>`_).


4.9 (2018-10-05)
================

- Drop support for Python 3.3.

- Add support for Python 3.7.

- Enable test coverage reporting on coveralls.io and in tox.ini.

- Host documentation at https://zopetestrunner.readthedocs.io

- Remove untested support for the ``--pychecker`` option. See
  `issue 63 <https://github.com/zopefoundation/zope.testrunner/issues/63>`_.

- Update the command line interface to use ``argparse`` instead of
  ``optparse``. See `issue 61
  <https://github.com/zopefoundation/zope.testrunner/issues/61>`_.

- Use ipdb instead of pdb for post-mortem debugging if available
  (`#10 <https://github.com/zopefoundation/zope.testrunner/issues/10>`_).

- Add a --require-unique option to check for duplicate test IDs. See
  `LP #682771
  <https://bugs.launchpad.net/launchpad/+bug/682771>`_.

- Reintroduce optional support for ``subunit``, now with support for both
  version 1 and version 2 of its protocol.

- Handle string in exception values when formatting chained exceptions.
  (`#74 <https://github.com/zopefoundation/zope.testrunner/pull/74>`_)


4.8.1 (2017-11-12)
==================

- Enable ``DeprecationWarning`` earlier, when discovering test
  modules. This lets warnings that are raised on import (such as those
  produced by ``zope.deprecation.moved``) be reported. See `issue 57
  <https://github.com/zopefoundation/zope.testrunner/issues/57>`_.


4.8.0 (2017-11-10)
==================

- Automatically enable ``DeprecationWarning`` when running tests. This
  is recommended by the Python core developers and matches the
  behaviour of the ``unittest`` module. This can be overridden with
  Python command-line options (``-W``) or environment variables
  (``PYTHONWARNINGS``). See `issue 54
  <https://github.com/zopefoundation/zope.testrunner/issues/54>`_.

4.7.0 (2017-05-30)
==================

- Drop all support for ``subunit``.


4.6.0 (2016-12-28)
==================

- Make the ``subunit`` support purely optional: applications which have
  been getting the dependencies via ``zope.testrunner`` should either add
  ``zope.testrunner[subunit]`` to their ``install_requires`` or else
  depend directly on ``python-subunit``.

- New option ``--ignore-new-thread=<regexp>`` to suppress "New thread(s)"
  warnings.

- Support Python 3.6.


4.5.1 (2016-06-20)
==================

- Fixed: Using the ``-j`` option to run tests in multiple processes
  caused tests that used the ``multiprocessing`` package to hang
  (because the testrunner replaced ``sys.stdin`` with an unclosable
  object).

- Drop conditional dependency on ``unittest2`` (redundant after dropping
  support for Python 2.6).


4.5.0 (2016-05-02)
==================

- Stop tests for all layers when test fails/errors when started with
  -x/--stop-on-error
  (`#37 <https://github.com/zopefoundation/zope.testrunner/pull/37>`_).

- Drop support for Python 2.6 and 3.2.


4.4.10 (2015-11-10)
===================

- Add support for Python 3.5
  (`#31 <https://github.com/zopefoundation/zope.testrunner/pull/31>`_).

- Insert extra paths (from ``--path``) to the front of sys.argv
  (`#32 <https://github.com/zopefoundation/zope.testrunner/issues/32>`_).


4.4.9 (2015-05-21)
==================

- When using ``-j``, parallelize all the tests, including the first test layer
  (`#28 <https://github.com/zopefoundation/zope.testrunner/issues/28>`_).


4.4.8 (2015-05-01)
==================

- Support skipped tests in subunit output
  (`#25 <https://github.com/zopefoundation/zope.testrunner/pull/25>`_).

- More efficient test filtering
  (`#26 <https://github.com/zopefoundation/zope.testrunner/pull/26>`_).


4.4.7 (2015-04-02)
==================

- Work around a bug in PyPy3's curses module
  (`#24 <https://github.com/zopefoundation/zope.testrunner/issues/24>`_).


4.4.6 (2015-01-21)
==================

- Restore support for instance-based test layers that regressed in 4.4.5
  (`#20 <https://github.com/zopefoundation/zope.testrunner/pull/20>`_).


4.4.5 (2015-01-06)
==================

- Sort related layers close to each other to reduce the number of unnecessary
  teardowns (fixes `#14
  <https://github.com/zopefoundation/zope.testrunner/issues/14>`_).

- Run the unit test layer first (fixes `LP #497871
  <https://bugs.launchpad.net/zope.testrunner/+bug/497871>`__).


4.4.4 (2014-12-27)
==================

- When looking for the right location of test code, start with longest
  location paths first. This fixes problems with nested code locations.


4.4.3 (2014-03-19)
==================

- Added support for Python 3.4.


4.4.2 (2014-02-22)
==================

- Drop support for Python 3.1.

- Fix post-mortem debugging when a non-printable exception happens
  (https://github.com/zopefoundation/zope.testrunner/issues/8).


4.4.1 (2013-07-10)
==================

- Updated ``boostrap.py`` to version 2.2.

- Fix nondeterministic test failures on Python 3.3

- Tear down layers after ``post_mortem`` debugging is finished.

- Fix tests that write to source directory, it might be read-only.


4.4.0 (2013-06-06)
==================

- Fix tests selection when the negative "!" pattern is used several times
  (LP #1160965)

- Moved tests into a 'tests' subpackage.

- Made ``python -m zope.testrunner`` work again.

- Support 'skip' feature of unittest2 (which became the new unittest in Python
  2.7).

- Better diagnostics when communication with subprocess fails
  (https://github.com/zopefoundation/zope.testrunner/issues/5).

- Do not break subprocess execution when the test suite changes the working
  directory (https://github.com/zopefoundation/zope.testrunner/issues/6).

- Count test module import errors as errors (LP #1026576).


4.3.3 (2013-03-03)
==================

- Running layers in sub-processes did not use to work when run via
  ``python setup.py ftest`` since it tried to run setup.py with all the
  command line options. It now detects ``setup.py`` runs and we run the test
  runner directly.


4.3.2 (2013-03-03)
==================

- Fix ``SkipLayers`` class in cases where the distribution specifies a
  ``test_suite`` value.


4.3.1 (2013-03-02)
==================

- Fixed a bug in the `ftest` command and added a test.

- Fixed a trivial test failure with Python 3 of the previous release.


4.3.0 (2013-03-02)
==================

- Expose `ftest` distutils command via an entry point.

- Added tests for ``zope.testrunner.eggsupport``.


4.2.0 (2013-02-12)
==================

- Dropped use of 2to3, rewrote source code to be compatible with all Python
  versions.  Introduced a dependency on `six`_.


4.1.1 (2013-02-08)
==================

- Dropped use of zope.fixers (LP: #1118877).

- Fixed tox test error reporting; fixed tests on Pythons 2.6, 3.1, 3.2, 3.3 and
  PyPy 1.9.

- Fix --shuffle ordering on Python 3.2 to be the same as it was on older Python
  versions.

- Fix --shuffle nondeterminism when multiple test layers are present.
  Note: this will likely change the order of tests for the same --shuffle-seed.

- New option: --profile-directory.  Use it in the test suite so that tests
  executed by detox in parallel don't conflict.

- Use a temporary coverage directory in the test suite so that tests
  executed by detox in parallel don't conflict.

- Fix --post-mortem (aka -D, --pdb) when a test module cannot be imported
  or is invalid (LP #1119363).


4.1.0 (2013-02-07)
==================

- Replaced deprecated ``zope.interface.implements`` usage with equivalent
  ``zope.interface.implementer`` decorator.

- Dropped support for Python 2.4 and 2.5.

- Made StartUpFailure compatible with unittest.TextTestRunner() (LP #1118344).


4.0.4 (2011-10-25)
==================

- Work around sporadic timing-related issues in the subprocess buffering
  tests.  Thanks to Jonathan Ballet for the patch!


4.0.3 (2011-03-17)
==================

- Added back support for Python <= 2.6 which was broken in 4.0.2.


4.0.2 (2011-03-16)
==================

- Added back Python 3 support which was broken in 4.0.1.

- Fixed `Unexpected success`_ support by implementing the whole concept.

- Added support for the new __pycache__ directories in Python 3.2.


4.0.1 (2011-02-21)
==================

- LP #719369: An `Unexpected success`_ (concept introduced in Python 2.7) is
  no longer handled as success but as failure. This is a workaround. The
  whole unexpected success concept might be implemented later.

.. _`Unexpected success`: http://www.voidspace.org.uk/python/articles/unittest2.shtml#more-skipping


4.0.0 (2010-10-19)
==================

- Show more information about layers whose setup fails (LP #638153).


4.0.0b5 (2010-07-20)
====================

- Update fix for LP #221151 to a spelling compatible with Python 2.4.

- Timestamps are now always included in subunit output (r114849).

- LP #591309: fix a crash when subunit reports test failures containing
  UTF8-encoded data.


4.0.0b4 (2010-06-23)
====================

- Package as a zipfile to work around Python 2.4 distutils bug (no
  feature changes or bugfixes in ``zope.testrunner`` itself).


4.0.0b3 (2010-06-16)
====================

- LP #221151: keep ``unittest.TestCase.shortDescription`` happy by supplying
  a ``_testMethodDoc`` attribute.

- LP #595052: keep the distribution installable under Python 2.4:  its
  distutils appears to munge the empty ``__init__.py`` file in the
  ``foo.bar`` egg used for testing into a directory.

- LP #580083: fix the ``bin/test`` script to run only tests from
  ``zope.testrunner``.

- LP #579019: When layers were run in parallel, their tearDown was
  not called. Additionally, the first layer which was run in the main
  thread did not have its tearDown called either.


4.0.0b2 (2010-05-03)
====================

- Having 'sampletests' in the MANIFEST.in gave warnings, but doesn't actually
  seem to include any more files, so I removed it.

- Moved zope.testing.exceptions to zope.testrunner.exceptions. Now
  zope.testrunner no longer requires zope.testing except for when running
  its own tests.


4.0.0b1 (2010-04-29)
====================

- Initial release of the testrunner from zope.testrunner as its own module.
  (Previously it was part of zope.testing.)


.. _six: http://pypi.python.org/pypi/six



  * [zstandard-0.11.1](https://github.com/indygreg/python-zstandard) ================
python-zstandard
================

This project provides Python bindings for interfacing with the
`Zstandard <http://www.zstd.net>`_ compression library. A C extension
and CFFI interface are provided.

The primary goal of the project is to provide a rich interface to the
underlying C API through a Pythonic interface while not sacrificing
performance. This means exposing most of the features and flexibility
of the C API while not sacrificing usability or safety that Python provides.

The canonical home for this project lives in a Mercurial repository run by
the author. For convenience, that repository is frequently synchronized to
https://github.com/indygreg/python-zstandard.

|  |ci-status|

Requirements
============

This extension is designed to run with Python 2.7, 3.4, 3.5, 3.6, and 3.7
on common platforms (Linux, Windows, and OS X). On PyPy (both PyPy2 and PyPy3) we support version 6.0.0 and above. 
x86 and x86_64 are well-tested on Windows. Only x86_64 is well-tested on Linux and macOS.

Installing
==========

This package is uploaded to PyPI at https://pypi.python.org/pypi/zstandard.
So, to install this package::

   $ pip install zstandard

Binary wheels are made available for some platforms. If you need to
install from a source distribution, all you should need is a working C
compiler and the Python development headers/libraries. On many Linux
distributions, you can install a ``python-dev`` or ``python-devel``
package to provide these dependencies.

Packages are also uploaded to Anaconda Cloud at
https://anaconda.org/indygreg/zstandard. See that URL for how to install
this package with ``conda``.

Performance
===========

zstandard is a highly tunable compression algorithm. In its default settings
(compression level 3), it will be faster at compression and decompression and
will have better compression ratios than zlib on most data sets. When tuned
for speed, it approaches lz4's speed and ratios. When tuned for compression
ratio, it approaches lzma ratios and compression speed, but decompression
speed is much faster. See the official zstandard documentation for more.

zstandard and this library support multi-threaded compression. There is a
mechanism to compress large inputs using multiple threads.

The performance of this library is usually very similar to what the zstandard
C API can deliver. Overhead in this library is due to general Python overhead
and can't easily be avoided by *any* zstandard Python binding. This library
exposes multiple APIs for performing compression and decompression so callers
can pick an API suitable for their need. Contrast with the compression
modules in Python's standard library (like ``zlib``), which only offer limited
mechanisms for performing operations. The API flexibility means consumers can
choose to use APIs that facilitate zero copying or minimize Python object
creation and garbage collection overhead.

This library is capable of single-threaded throughputs well over 1 GB/s. For
exact numbers, measure yourself. The source code repository has a ``bench.py``
script that can be used to measure things.

API
===

To interface with Zstandard, simply import the ``zstandard`` module::

   import zstandard

It is a popular convention to alias the module as a different name for
brevity::

   import zstandard as zstd

This module attempts to import and use either the C extension or CFFI
implementation. On Python platforms known to support C extensions (like
CPython), it raises an ImportError if the C extension cannot be imported.
On Python platforms known to not support C extensions (like PyPy), it only
attempts to import the CFFI implementation and raises ImportError if that
can't be done. On other platforms, it first tries to import the C extension
then falls back to CFFI if that fails and raises ImportError if CFFI fails.

To change the module import behavior, a ``PYTHON_ZSTANDARD_IMPORT_POLICY``
environment variable can be set. The following values are accepted:

default
   The behavior described above.
cffi_fallback
   Always try to import the C extension then fall back to CFFI if that
   fails.
cext
   Only attempt to import the C extension.
cffi
   Only attempt to import the CFFI implementation.

In addition, the ``zstandard`` module exports a ``backend`` attribute
containing the string name of the backend being used. It will be one
of ``cext`` or ``cffi`` (for *C extension* and *cffi*, respectively).

The types, functions, and attributes exposed by the ``zstandard`` module
are documented in the sections below.

.. note::

   The documentation in this section makes references to various zstd
   concepts and functionality. The source repository contains a
   ``docs/concepts.rst`` file explaining these in more detail.

ZstdCompressor
--------------

The ``ZstdCompressor`` class provides an interface for performing
compression operations. Each instance is essentially a wrapper around a
``ZSTD_CCtx`` from the C API.

Each instance is associated with parameters that control compression
behavior. These come from the following named arguments (all optional):

level
   Integer compression level. Valid values are between 1 and 22.
dict_data
   Compression dictionary to use.

   Note: When using dictionary data and ``compress()`` is called multiple
   times, the ``ZstdCompressionParameters`` derived from an integer
   compression ``level`` and the first compressed data's size will be reused
   for all subsequent operations. This may not be desirable if source data
   size varies significantly.
compression_params
   A ``ZstdCompressionParameters`` instance defining compression settings.
write_checksum
   Whether a 4 byte checksum should be written with the compressed data.
   Defaults to False. If True, the decompressor can verify that decompressed
   data matches the original input data.
write_content_size
   Whether the size of the uncompressed data will be written into the
   header of compressed data. Defaults to True. The data will only be
   written if the compressor knows the size of the input data. This is
   often not true for streaming compression.
write_dict_id
   Whether to write the dictionary ID into the compressed data.
   Defaults to True. The dictionary ID is only written if a dictionary
   is being used.
threads
   Enables and sets the number of threads to use for multi-threaded compression
   operations. Defaults to 0, which means to use single-threaded compression.
   Negative values will resolve to the number of logical CPUs in the system.
   Read below for more info on multi-threaded compression. This argument only
   controls thread count for operations that operate on individual pieces of
   data. APIs that spawn multiple threads for working on multiple pieces of
   data have their own ``threads`` argument.

``compression_params`` is mutually exclusive with ``level``, ``write_checksum``,
``write_content_size``, ``write_dict_id``, and ``threads``.

Unless specified otherwise, assume that no two methods of ``ZstdCompressor``
instances can be called from multiple Python threads simultaneously. In other
words, assume instances are not thread safe unless stated otherwise.

Utility Methods
^^^^^^^^^^^^^^^

``frame_progression()`` returns a 3-tuple containing the number of bytes
ingested, consumed, and produced by the current compression operation.

``memory_size()`` obtains the memory utilization of the underlying zstd
compression context, in bytes.::

    cctx = zstd.ZstdCompressor()
    memory = cctx.memory_size()

Simple API
^^^^^^^^^^

``compress(data)`` compresses and returns data as a one-shot operation.::

   cctx = zstd.ZstdCompressor()
   compressed = cctx.compress(b'data to compress')

The ``data`` argument can be any object that implements the *buffer protocol*.

Stream Reader API
^^^^^^^^^^^^^^^^^

``stream_reader(source)`` can be used to obtain an object conforming to the
``io.RawIOBase`` interface for reading compressed output as a stream::

   with open(path, 'rb') as fh:
       cctx = zstd.ZstdCompressor()
       reader = cctx.stream_reader(fh)
       while True:
           chunk = reader.read(16384)
           if not chunk:
               break

           # Do something with compressed chunk.

Instances can also be used as context managers::

   with open(path, 'rb') as fh:
       with cctx.stream_reader(fh) as reader:
           while True:
               chunk = reader.read(16384)
               if not chunk:
                   break

               # Do something with compressed chunk.

When the context manager exits or ``close()`` is called, the stream is closed,
underlying resources are released, and future operations against the compression
stream will fail.

The ``source`` argument to ``stream_reader()`` can be any object with a
``read(size)`` method or any object implementing the *buffer protocol*.

``stream_reader()`` accepts a ``size`` argument specifying how large the input
stream is. This is used to adjust compression parameters so they are
tailored to the source size.::

   with open(path, 'rb') as fh:
       cctx = zstd.ZstdCompressor()
       with cctx.stream_reader(fh, size=os.stat(path).st_size) as reader:
           ...

If the ``source`` is a stream, you can specify how large ``read()`` requests
to that stream should be via the ``read_size`` argument. It defaults to
``zstandard.COMPRESSION_RECOMMENDED_INPUT_SIZE``.::

   with open(path, 'rb') as fh:
       cctx = zstd.ZstdCompressor()
       # Will perform fh.read(8192) when obtaining data to feed into the
       # compressor.
       with cctx.stream_reader(fh, read_size=8192) as reader:
           ...

The stream returned by ``stream_reader()`` is neither writable nor seekable
(even if the underlying source is seekable). ``readline()`` and
``readlines()`` are not implemented because they don't make sense for
compressed data. ``tell()`` returns the number of compressed bytes
emitted so far.

Streaming Input API
^^^^^^^^^^^^^^^^^^^

``stream_writer(fh)`` allows you to *stream* data into a compressor.

Returned instances implement the ``io.RawIOBase`` interface. Only methods
that involve writing will do useful things.

The argument to ``stream_writer()`` must have a ``write(data)`` method. As
compressed data is available, ``write()`` will be called with the compressed
data as its argument. Many common Python types implement ``write()``, including
open file handles and ``io.BytesIO``.

The ``write(data)`` method is used to feed data into the compressor.

The ``flush([flush_mode=FLUSH_BLOCK])`` method can be called to evict whatever
data remains within the compressor's internal state into the output object. This
may result in 0 or more ``write()`` calls to the output object. This method
accepts an optional ``flush_mode`` argument to control the flushing behavior.
Its value can be any of the ``FLUSH_*`` constants.

Both ``write()`` and ``flush()`` return the number of bytes written to the
object's ``write()``. In many cases, small inputs do not accumulate enough
data to cause a write and ``write()`` will return ``0``.

Calling ``close()`` will mark the stream as closed and subsequent I/O
operations will raise ``ValueError`` (per the documented behavior of
``io.RawIOBase``). ``close()`` will also call ``close()`` on the underlying
stream if such a method exists.

Typically usage is as follows::

   cctx = zstd.ZstdCompressor(level=10)
   compressor = cctx.stream_writer(fh)

   compressor.write(b'chunk 0\n')
   compressor.write(b'chunk 1\n')
   compressor.flush()
   # Receiver will be able to decode ``chunk 0\nchunk 1\n`` at this point.
   # Receiver is also expecting more data in the zstd *frame*.

   compressor.write(b'chunk 2\n')
   compressor.flush(zstd.FLUSH_FRAME)
   # Receiver will be able to decode ``chunk 0\nchunk 1\nchunk 2``.
   # Receiver is expecting no more data, as the zstd frame is closed.
   # Any future calls to ``write()`` at this point will construct a new
   # zstd frame.

Instances can be used as context managers. Exiting the context manager is
the equivalent of calling ``close()``, which is equivalent to calling
``flush(zstd.FLUSH_FRAME)``::

   cctx = zstd.ZstdCompressor(level=10)
   with cctx.stream_writer(fh) as compressor:
       compressor.write(b'chunk 0')
       compressor.write(b'chunk 1')
       ...

.. important::

   If ``flush(FLUSH_FRAME)`` is not called, emitted data doesn't constitute
   a full zstd *frame* and consumers of this data may complain about malformed
   input. It is recommended to use instances as a context manager to ensure
   *frames* are properly finished.

If the size of the data being fed to this streaming compressor is known,
you can declare it before compression begins::

   cctx = zstd.ZstdCompressor()
   with cctx.stream_writer(fh, size=data_len) as compressor:
       compressor.write(chunk0)
       compressor.write(chunk1)
       ...

Declaring the size of the source data allows compression parameters to
be tuned. And if ``write_content_size`` is used, it also results in the
content size being written into the frame header of the output data.

The size of chunks being ``write()`` to the destination can be specified::

    cctx = zstd.ZstdCompressor()
    with cctx.stream_writer(fh, write_size=32768) as compressor:
        ...

To see how much memory is being used by the streaming compressor::

    cctx = zstd.ZstdCompressor()
    with cctx.stream_writer(fh) as compressor:
        ...
        byte_size = compressor.memory_size()

Thte total number of bytes written so far are exposed via ``tell()``::

    cctx = zstd.ZstdCompressor()
    with cctx.stream_writer(fh) as compressor:
        ...
        total_written = compressor.tell()

``stream_writer()`` accepts a ``write_return_read`` boolean argument to control
the return value of ``write()``. When ``False`` (the default), ``write()`` returns
the number of bytes that were ``write()``en to the underlying object. When
``True``, ``write()`` returns the number of bytes read from the input that
were subsequently written to the compressor. ``True`` is the *proper* behavior
for ``write()`` as specified by the ``io.RawIOBase`` interface and will become
the default value in a future release.

Streaming Output API
^^^^^^^^^^^^^^^^^^^^

``read_to_iter(reader)`` provides a mechanism to stream data out of a
compressor as an iterator of data chunks.::

   cctx = zstd.ZstdCompressor()
   for chunk in cctx.read_to_iter(fh):
        # Do something with emitted data.

``read_to_iter()`` accepts an object that has a ``read(size)`` method or
conforms to the buffer protocol.

Uncompressed data is fetched from the source either by calling ``read(size)``
or by fetching a slice of data from the object directly (in the case where
the buffer protocol is being used). The returned iterator consists of chunks
of compressed data.

If reading from the source via ``read()``, ``read()`` will be called until
it raises or returns an empty bytes (``b''``). It is perfectly valid for
the source to deliver fewer bytes than were what requested by ``read(size)``.

Like ``stream_writer()``, ``read_to_iter()`` also accepts a ``size`` argument
declaring the size of the input stream::

    cctx = zstd.ZstdCompressor()
    for chunk in cctx.read_to_iter(fh, size=some_int):
        pass

You can also control the size that data is ``read()`` from the source and
the ideal size of output chunks::

    cctx = zstd.ZstdCompressor()
    for chunk in cctx.read_to_iter(fh, read_size=16384, write_size=8192):
        pass

Unlike ``stream_writer()``, ``read_to_iter()`` does not give direct control
over the sizes of chunks fed into the compressor. Instead, chunk sizes will
be whatever the object being read from delivers. These will often be of a
uniform size.

Stream Copying API
^^^^^^^^^^^^^^^^^^

``copy_stream(ifh, ofh)`` can be used to copy data between 2 streams while
compressing it.::

   cctx = zstd.ZstdCompressor()
   cctx.copy_stream(ifh, ofh)

For example, say you wish to compress a file::

   cctx = zstd.ZstdCompressor()
   with open(input_path, 'rb') as ifh, open(output_path, 'wb') as ofh:
       cctx.copy_stream(ifh, ofh)

It is also possible to declare the size of the source stream::

   cctx = zstd.ZstdCompressor()
   cctx.copy_stream(ifh, ofh, size=len_of_input)

You can also specify how large the chunks that are ``read()`` and ``write()``
from and to the streams::

   cctx = zstd.ZstdCompressor()
   cctx.copy_stream(ifh, ofh, read_size=32768, write_size=16384)

The stream copier returns a 2-tuple of bytes read and written::

   cctx = zstd.ZstdCompressor()
   read_count, write_count = cctx.copy_stream(ifh, ofh)

Compressor API
^^^^^^^^^^^^^^

``compressobj()`` returns an object that exposes ``compress(data)`` and
``flush()`` methods. Each returns compressed data or an empty bytes.

The purpose of ``compressobj()`` is to provide an API-compatible interface
with ``zlib.compressobj``, ``bz2.BZ2Compressor``, etc. This allows callers to
swap in different compressor objects while using the same API.

``flush()`` accepts an optional argument indicating how to end the stream.
``zstd.COMPRESSOBJ_FLUSH_FINISH`` (the default) ends the compression stream.
Once this type of flush is performed, ``compress()`` and ``flush()`` can
no longer be called. This type of flush **must** be called to end the
compression context. If not called, returned data may be incomplete.

A ``zstd.COMPRESSOBJ_FLUSH_BLOCK`` argument to ``flush()`` will flush a
zstd block. Flushes of this type can be performed multiple times. The next
call to ``compress()`` will begin a new zstd block.

Here is how this API should be used::

   cctx = zstd.ZstdCompressor()
   cobj = cctx.compressobj()
   data = cobj.compress(b'raw input 0')
   data = cobj.compress(b'raw input 1')
   data = cobj.flush()

Or to flush blocks::

   cctx.zstd.ZstdCompressor()
   cobj = cctx.compressobj()
   data = cobj.compress(b'chunk in first block')
   data = cobj.flush(zstd.COMPRESSOBJ_FLUSH_BLOCK)
   data = cobj.compress(b'chunk in second block')
   data = cobj.flush()

For best performance results, keep input chunks under 256KB. This avoids
extra allocations for a large output object.

It is possible to declare the input size of the data that will be fed into
the compressor::

   cctx = zstd.ZstdCompressor()
   cobj = cctx.compressobj(size=6)
   data = cobj.compress(b'foobar')
   data = cobj.flush()

Chunker API
^^^^^^^^^^^

``chunker(size=None, chunk_size=COMPRESSION_RECOMMENDED_OUTPUT_SIZE)`` returns
an object that can be used to iteratively feed chunks of data into a compressor
and produce output chunks of a uniform size.

The object returned by ``chunker()`` exposes the following methods:

``compress(data)``
   Feeds new input data into the compressor.

``flush()``
   Flushes all data currently in the compressor.

``finish()``
   Signals the end of input data. No new data can be compressed after this
   method is called.

``compress()``, ``flush()``, and ``finish()`` all return an iterator of
``bytes`` instances holding compressed data. The iterator may be empty. Callers
MUST iterate through all elements of the returned iterator before performing
another operation on the object.

All chunks emitted by ``compress()`` will have a length of ``chunk_size``.

``flush()`` and ``finish()`` may return a final chunk smaller than
``chunk_size``.

Here is how the API should be used::

   cctx = zstd.ZstdCompressor()
   chunker = cctx.chunker(chunk_size=32768)

   with open(path, 'rb') as fh:
       while True:
           in_chunk = fh.read(32768)
           if not in_chunk:
               break

           for out_chunk in chunker.compress(in_chunk):
               # Do something with output chunk of size 32768.

       for out_chunk in chunker.finish():
           # Do something with output chunks that finalize the zstd frame.

The ``chunker()`` API is often a better alternative to ``compressobj()``.

``compressobj()`` will emit output data as it is available. This results in a
*stream* of output chunks of varying sizes. The consistency of the output chunk
size with ``chunker()`` is more appropriate for many usages, such as sending
compressed data to a socket.

``compressobj()`` may also perform extra memory reallocations in order to
dynamically adjust the sizes of the output chunks. Since ``chunker()`` output
chunks are all the same size (except for flushed or final chunks), there is
less memory allocation overhead.

Batch Compression API
^^^^^^^^^^^^^^^^^^^^^

(Experimental. Not yet supported in CFFI bindings.)

``multi_compress_to_buffer(data, [threads=0])`` performs compression of multiple
inputs as a single operation.

Data to be compressed can be passed as a ``BufferWithSegmentsCollection``, a
``BufferWithSegments``, or a list containing byte like objects. Each element of
the container will be compressed individually using the configured parameters
on the ``ZstdCompressor`` instance.

The ``threads`` argument controls how many threads to use for compression. The
default is ``0`` which means to use a single thread. Negative values use the
number of logical CPUs in the machine.

The function returns a ``BufferWithSegmentsCollection``. This type represents
N discrete memory allocations, eaching holding 1 or more compressed frames.

Output data is written to shared memory buffers. This means that unlike
regular Python objects, a reference to *any* object within the collection
keeps the shared buffer and therefore memory backing it alive. This can have
undesirable effects on process memory usage.

The API and behavior of this function is experimental and will likely change.
Known deficiencies include:

* If asked to use multiple threads, it will always spawn that many threads,
  even if the input is too small to use them. It should automatically lower
  the thread count when the extra threads would just add overhead.
* The buffer allocation strategy is fixed. There is room to make it dynamic,
  perhaps even to allow one output buffer per input, facilitating a variation
  of the API to return a list without the adverse effects of shared memory
  buffers.

ZstdDecompressor
----------------

The ``ZstdDecompressor`` class provides an interface for performing
decompression. It is effectively a wrapper around the ``ZSTD_DCtx`` type from
the C API.

Each instance is associated with parameters that control decompression. These
come from the following named arguments (all optional):

dict_data
   Compression dictionary to use.
max_window_size
   Sets an uppet limit on the window size for decompression operations in
   kibibytes. This setting can be used to prevent large memory allocations
   for inputs using large compression windows.
format
   Set the format of data for the decoder. By default, this is
   ``zstd.FORMAT_ZSTD1``. It can be set to ``zstd.FORMAT_ZSTD1_MAGICLESS`` to
   allow decoding frames without the 4 byte magic header. Not all decompression
   APIs support this mode.

The interface of this class is very similar to ``ZstdCompressor`` (by design).

Unless specified otherwise, assume that no two methods of ``ZstdDecompressor``
instances can be called from multiple Python threads simultaneously. In other
words, assume instances are not thread safe unless stated otherwise.

Utility Methods
^^^^^^^^^^^^^^^

``memory_size()`` obtains the size of the underlying zstd decompression context,
in bytes.::

    dctx = zstd.ZstdDecompressor()
    size = dctx.memory_size()

Simple API
^^^^^^^^^^

``decompress(data)`` can be used to decompress an entire compressed zstd
frame in a single operation.::

    dctx = zstd.ZstdDecompressor()
    decompressed = dctx.decompress(data)

By default, ``decompress(data)`` will only work on data written with the content
size encoded in its header (this is the default behavior of
``ZstdCompressor().compress()`` but may not be true for streaming compression). If
compressed data without an embedded content size is seen, ``zstd.ZstdError`` will
be raised.

If the compressed data doesn't have its content size embedded within it,
decompression can be attempted by specifying the ``max_output_size``
argument.::

    dctx = zstd.ZstdDecompressor()
    uncompressed = dctx.decompress(data, max_output_size=1048576)

Ideally, ``max_output_size`` will be identical to the decompressed output
size.

If ``max_output_size`` is too small to hold the decompressed data,
``zstd.ZstdError`` will be raised.

If ``max_output_size`` is larger than the decompressed data, the allocated
output buffer will be resized to only use the space required.

Please note that an allocation of the requested ``max_output_size`` will be
performed every time the method is called. Setting to a very large value could
result in a lot of work for the memory allocator and may result in
``MemoryError`` being raised if the allocation fails.

.. important::

   If the exact size of decompressed data is unknown (not passed in explicitly
   and not stored in the zstandard frame), for performance reasons it is
   encouraged to use a streaming API.

Stream Reader API
^^^^^^^^^^^^^^^^^

``stream_reader(source)`` can be used to obtain an object conforming to the
``io.RawIOBase`` interface for reading decompressed output as a stream::

   with open(path, 'rb') as fh:
       dctx = zstd.ZstdDecompressor()
       reader = dctx.stream_reader(fh)
       while True:
           chunk = reader.read(16384)
            if not chunk:
                break

            # Do something with decompressed chunk.

The stream can also be used as a context manager::

   with open(path, 'rb') as fh:
       dctx = zstd.ZstdDecompressor()
       with dctx.stream_reader(fh) as reader:
           ...

When used as a context manager, the stream is closed and the underlying
resources are released when the context manager exits. Future operations against
the stream will fail.

The ``source`` argument to ``stream_reader()`` can be any object with a
``read(size)`` method or any object implementing the *buffer protocol*.

If the ``source`` is a stream, you can specify how large ``read()`` requests
to that stream should be via the ``read_size`` argument. It defaults to
``zstandard.DECOMPRESSION_RECOMMENDED_INPUT_SIZE``.::

   with open(path, 'rb') as fh:
       dctx = zstd.ZstdDecompressor()
       # Will perform fh.read(8192) when obtaining data for the decompressor.
       with dctx.stream_reader(fh, read_size=8192) as reader:
           ...

The stream returned by ``stream_reader()`` is not writable.

The stream returned by ``stream_reader()`` is *partially* seekable.
Absolute and relative positions (``SEEK_SET`` and ``SEEK_CUR``) forward
of the current position are allowed. Offsets behind the current read
position and offsets relative to the end of stream are not allowed and
will raise ``ValueError`` if attempted.

``tell()`` returns the number of decompressed bytes read so far.

Not all I/O methods are implemented. Notably missing is support for
``readline()``, ``readlines()``, and linewise iteration support. This is
because streams operate on binary data - not text data. If you want to
convert decompressed output to text, you can chain an ``io.TextIOWrapper``
to the stream::

   with open(path, 'rb') as fh:
       dctx = zstd.ZstdDecompressor()
       stream_reader = dctx.stream_reader(fh)
       text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')

       for line in text_stream:
           ...

The ``read_across_frames`` argument to ``stream_reader()`` controls the
behavior of read operations when the end of a zstd *frame* is encountered.
When ``False`` (the default), a read will complete when the end of a
zstd *frame* is encountered. When ``True``, a read can potentially
return data spanning multiple zstd *frames*.

Streaming Input API
^^^^^^^^^^^^^^^^^^^

``stream_writer(fh)`` allows you to *stream* data into a decompressor.

Returned instances implement the ``io.RawIOBase`` interface. Only methods
that involve writing will do useful things.

The argument to ``stream_writer()`` is typically an object that also implements
``io.RawIOBase``. But any object with a ``write(data)`` method will work. Many
common Python types conform to this interface, including open file handles
and ``io.BytesIO``.

Behavior is similar to ``ZstdCompressor.stream_writer()``: compressed data
is sent to the decompressor by calling ``write(data)`` and decompressed
output is written to the underlying stream by calling its ``write(data)``
method.::

    dctx = zstd.ZstdDecompressor()
    decompressor = dctx.stream_writer(fh)

    decompressor.write(compressed_data)
    ...


Calls to ``write()`` will return the number of bytes written to the output
object. Not all inputs will result in bytes being written, so return values
of ``0`` are possible.

Like the ``stream_writer()`` compressor, instances can be used as context
managers. However, context managers add no extra special behavior and offer
little to no benefit to being used.

Calling ``close()`` will mark the stream as closed and subsequent I/O operations
will raise ``ValueError`` (per the documented behavior of ``io.RawIOBase``).
``close()`` will also call ``close()`` on the underlying stream if such a
method exists.

The size of chunks being ``write()`` to the destination can be specified::

    dctx = zstd.ZstdDecompressor()
    with dctx.stream_writer(fh, write_size=16384) as decompressor:
        pass

You can see how much memory is being used by the decompressor::

    dctx = zstd.ZstdDecompressor()
    with dctx.stream_writer(fh) as decompressor:
        byte_size = decompressor.memory_size()

``stream_writer()`` accepts a ``write_return_read`` boolean argument to control
the return value of ``write()``. When ``False`` (the default)``, ``write()``
returns the number of bytes that were ``write()``en to the underlying stream.
When ``True``, ``write()`` returns the number of bytes read from the input.
``True`` is the *proper* behavior for ``write()`` as specified by the
``io.RawIOBase`` interface and will become the default in a future release.

Streaming Output API
^^^^^^^^^^^^^^^^^^^^

``read_to_iter(fh)`` provides a mechanism to stream decompressed data out of a
compressed source as an iterator of data chunks.:: 

    dctx = zstd.ZstdDecompressor()
    for chunk in dctx.read_to_iter(fh):
        # Do something with original data.

``read_to_iter()`` accepts an object with a ``read(size)`` method that will
return  compressed bytes or an object conforming to the buffer protocol that
can expose its data as a contiguous range of bytes.

``read_to_iter()`` returns an iterator whose elements are chunks of the
decompressed data.

The size of requested ``read()`` from the source can be specified::

    dctx = zstd.ZstdDecompressor()
    for chunk in dctx.read_to_iter(fh, read_size=16384):
        pass

It is also possible to skip leading bytes in the input data::

    dctx = zstd.ZstdDecompressor()
    for chunk in dctx.read_to_iter(fh, skip_bytes=1):
        pass

.. tip::

   Skipping leading bytes is useful if the source data contains extra
   *header* data. Traditionally, you would need to create a slice or
   ``memoryview`` of the data you want to decompress. This would create
   overhead. It is more efficient to pass the offset into this API.

Similarly to ``ZstdCompressor.read_to_iter()``, the consumer of the iterator
controls when data is decompressed. If the iterator isn't consumed,
decompression is put on hold.

When ``read_to_iter()`` is passed an object conforming to the buffer protocol,
the behavior may seem similar to what occurs when the simple decompression
API is used. However, this API works when the decompressed size is unknown.
Furthermore, if feeding large inputs, the decompressor will work in chunks
instead of performing a single operation.

Stream Copying API
^^^^^^^^^^^^^^^^^^

``copy_stream(ifh, ofh)`` can be used to copy data across 2 streams while
performing decompression.::

    dctx = zstd.ZstdDecompressor()
    dctx.copy_stream(ifh, ofh)

e.g. to decompress a file to another file::

    dctx = zstd.ZstdDecompressor()
    with open(input_path, 'rb') as ifh, open(output_path, 'wb') as ofh:
        dctx.copy_stream(ifh, ofh)

The size of chunks being ``read()`` and ``write()`` from and to the streams
can be specified::

    dctx = zstd.ZstdDecompressor()
    dctx.copy_stream(ifh, ofh, read_size=8192, write_size=16384)

Decompressor API
^^^^^^^^^^^^^^^^

``decompressobj()`` returns an object that exposes a ``decompress(data)``
method. Compressed data chunks are fed into ``decompress(data)`` and
uncompressed output (or an empty bytes) is returned. Output from subsequent
calls needs to be concatenated to reassemble the full decompressed byte
sequence.

The purpose of ``decompressobj()`` is to provide an API-compatible interface
with ``zlib.decompressobj`` and ``bz2.BZ2Decompressor``. This allows callers
to swap in different decompressor objects while using the same API.

Each object is single use: once an input frame is decoded, ``decompress()``
can no longer be called.

Here is how this API should be used::

   dctx = zstd.ZstdDecompressor()
   dobj = dctx.decompressobj()
   data = dobj.decompress(compressed_chunk_0)
   data = dobj.decompress(compressed_chunk_1)

By default, calls to ``decompress()`` write output data in chunks of size
``DECOMPRESSION_RECOMMENDED_OUTPUT_SIZE``. These chunks are concatenated
before being returned to the caller. It is possible to define the size of
these temporary chunks by passing ``write_size`` to ``decompressobj()``::

   dctx = zstd.ZstdDecompressor()
   dobj = dctx.decompressobj(write_size=1048576)

.. note::

   Because calls to ``decompress()`` may need to perform multiple
   memory (re)allocations, this streaming decompression API isn't as
   efficient as other APIs.

For compatibility with the standard library APIs, instances expose a
``flush([length=None])`` method. This method no-ops and has no meaningful
side-effects, making it safe to call any time.

Batch Decompression API
^^^^^^^^^^^^^^^^^^^^^^^

(Experimental. Not yet supported in CFFI bindings.)

``multi_decompress_to_buffer()`` performs decompression of multiple
frames as a single operation and returns a ``BufferWithSegmentsCollection``
containing decompressed data for all inputs.

Compressed frames can be passed to the function as a ``BufferWithSegments``,
a ``BufferWithSegmentsCollection``, or as a list containing objects that
conform to the buffer protocol. For best performance, pass a
``BufferWithSegmentsCollection`` or a ``BufferWithSegments``, as
minimal input validation will be done for that type. If calling from
Python (as opposed to C), constructing one of these instances may add
overhead cancelling out the performance overhead of validation for list
inputs.::

    dctx = zstd.ZstdDecompressor()
    results = dctx.multi_decompress_to_buffer([b'...', b'...'])

The decompressed size of each frame MUST be discoverable. It can either be
embedded within the zstd frame (``write_content_size=True`` argument to
``ZstdCompressor``) or passed in via the ``decompressed_sizes`` argument.

The ``decompressed_sizes`` argument is an object conforming to the buffer
protocol which holds an array of 64-bit unsigned integers in the machine's
native format defining the decompressed sizes of each frame. If this argument
is passed, it avoids having to scan each frame for its decompressed size.
This frame scanning can add noticeable overhead in some scenarios.::

    frames = [...]
    sizes = struct.pack('=QQQQ', len0, len1, len2, len3)

    dctx = zstd.ZstdDecompressor()
    results = dctx.multi_decompress_to_buffer(frames, decompressed_sizes=sizes)

The ``threads`` argument controls the number of threads to use to perform
decompression operations. The default (``0``) or the value ``1`` means to
use a single thread. Negative values use the number of logical CPUs in the
machine.

.. note::

   It is possible to pass a ``mmap.mmap()`` instance into this function by
   wrapping it with a ``BufferWithSegments`` instance (which will define the
   offsets of frames within the memory mapped region).

This function is logically equivalent to performing ``dctx.decompress()``
on each input frame and returning the result.

This function exists to perform decompression on multiple frames as fast
as possible by having as little overhead as possible. Since decompression is
performed as a single operation and since the decompressed output is stored in
a single buffer, extra memory allocations, Python objects, and Python function
calls are avoided. This is ideal for scenarios where callers know up front that
they need to access data for multiple frames, such as when  *delta chains* are
being used.

Currently, the implementation always spawns multiple threads when requested,
even if the amount of work to do is small. In the future, it will be smarter
about avoiding threads and their associated overhead when the amount of
work to do is small.

Prefix Dictionary Chain Decompression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``decompress_content_dict_chain(frames)`` performs decompression of a list of
zstd frames produced using chained *prefix* dictionary compression. Such
a list of frames is produced by compressing discrete inputs where each
non-initial input is compressed with a *prefix* dictionary consisting of the
content of the previous input.

For example, say you have the following inputs::

   inputs = [b'input 1', b'input 2', b'input 3']

The zstd frame chain consists of:

1. ``b'input 1'`` compressed in standalone/discrete mode
2. ``b'input 2'`` compressed using ``b'input 1'`` as a *prefix* dictionary
3. ``b'input 3'`` compressed using ``b'input 2'`` as a *prefix* dictionary

Each zstd frame **must** have the content size written.

The following Python code can be used to produce a *prefix dictionary chain*::

    def make_chain(inputs):
        frames = []

        # First frame is compressed in standalone/discrete mode.
        zctx = zstd.ZstdCompressor()
        frames.append(zctx.compress(inputs[0]))

        # Subsequent frames use the previous fulltext as a prefix dictionary
        for i, raw in enumerate(inputs[1:]):
            dict_data = zstd.ZstdCompressionDict(
                inputs[i], dict_type=zstd.DICT_TYPE_RAWCONTENT)
            zctx = zstd.ZstdCompressor(dict_data=dict_data)
            frames.append(zctx.compress(raw))

        return frames

``decompress_content_dict_chain()`` returns the uncompressed data of the last
element in the input chain.


.. note::

   It is possible to implement *prefix dictionary chain* decompression
   on top of other APIs. However, this function will likely be faster -
   especially for long input chains - as it avoids the overhead of instantiating
   and passing around intermediate objects between C and Python.

Multi-Threaded Compression
--------------------------

``ZstdCompressor`` accepts a ``threads`` argument that controls the number
of threads to use for compression. The way this works is that input is split
into segments and each segment is fed into a worker pool for compression. Once
a segment is compressed, it is flushed/appended to the output.

.. note::

   These threads are created at the C layer and are not Python threads. So they
   work outside the GIL. It is therefore possible to CPU saturate multiple cores
   from Python.

The segment size for multi-threaded compression is chosen from the window size
of the compressor. This is derived from the ``window_log`` attribute of a
``ZstdCompressionParameters`` instance. By default, segment sizes are in the 1+MB
range.

If multi-threaded compression is requested and the input is smaller than the
configured segment size, only a single compression thread will be used. If the
input is smaller than the segment size multiplied by the thread pool size or
if data cannot be delivered to the compressor fast enough, not all requested
compressor threads may be active simultaneously.

Compared to non-multi-threaded compression, multi-threaded compression has
higher per-operation overhead. This includes extra memory operations,
thread creation, lock acquisition, etc.

Due to the nature of multi-threaded compression using *N* compression
*states*, the output from multi-threaded compression will likely be larger
than non-multi-threaded compression. The difference is usually small. But
there is a CPU/wall time versus size trade off that may warrant investigation.

Output from multi-threaded compression does not require any special handling
on the decompression side. To the decompressor, data generated with single
threaded compressor looks the same as data generated by a multi-threaded
compressor and does not require any special handling or additional resource
requirements.

Dictionary Creation and Management
----------------------------------

Compression dictionaries are represented with the ``ZstdCompressionDict`` type.

Instances can be constructed from bytes::

   dict_data = zstd.ZstdCompressionDict(data)

It is possible to construct a dictionary from *any* data. If the data doesn't
begin with a magic header, it will be treated as a *prefix* dictionary.
*Prefix* dictionaries allow compression operations to reference raw data
within the dictionary.

It is possible to force the use of *prefix* dictionaries or to require a
dictionary header:

   dict_data = zstd.ZstdCompressionDict(data,
                                        dict_type=zstd.DICT_TYPE_RAWCONTENT)

   dict_data = zstd.ZstdCompressionDict(data,
                                        dict_type=zstd.DICT_TYPE_FULLDICT)

You can see how many bytes are in the dictionary by calling ``len()``::

   dict_data = zstd.train_dictionary(size, samples)
   dict_size = len(dict_data)  # will not be larger than ``size``

Once you have a dictionary, you can pass it to the objects performing
compression and decompression::

   dict_data = zstd.train_dictionary(131072, samples)

   cctx = zstd.ZstdCompressor(dict_data=dict_data)
   for source_data in input_data:
       compressed = cctx.compress(source_data)
       # Do something with compressed data.

   dctx = zstd.ZstdDecompressor(dict_data=dict_data)
   for compressed_data in input_data:
       buffer = io.BytesIO()
       with dctx.stream_writer(buffer) as decompressor:
           decompressor.write(compressed_data)
       # Do something with raw data in ``buffer``.

Dictionaries have unique integer IDs. You can retrieve this ID via::

   dict_id = zstd.dictionary_id(dict_data)

You can obtain the raw data in the dict (useful for persisting and constructing
a ``ZstdCompressionDict`` later) via ``as_bytes()``::

   dict_data = zstd.train_dictionary(size, samples)
   raw_data = dict_data.as_bytes()

By default, when a ``ZstdCompressionDict`` is *attached* to a
``ZstdCompressor``, each ``ZstdCompressor`` performs work to prepare the
dictionary for use. This is fine if only 1 compression operation is being
performed or if the ``ZstdCompressor`` is being reused for multiple operations.
But if multiple ``ZstdCompressor`` instances are being used with the dictionary,
this can add overhead.

It is possible to *precompute* the dictionary so it can readily be consumed
by multiple ``ZstdCompressor`` instances::

    d = zstd.ZstdCompressionDict(data)

    # Precompute for compression level 3.
    d.precompute_compress(level=3)

    # Precompute with specific compression parameters.
    params = zstd.ZstdCompressionParameters(...)
    d.precompute_compress(compression_params=params)

.. note::

   When a dictionary is precomputed, the compression parameters used to
   precompute the dictionary overwrite some of the compression parameters
   specified to ``ZstdCompressor.__init__``.

Training Dictionaries
^^^^^^^^^^^^^^^^^^^^^

Unless using *prefix* dictionaries, dictionary data is produced by *training*
on existing data::

   dict_data = zstd.train_dictionary(size, samples)

This takes a target dictionary size and list of bytes instances and creates and
returns a ``ZstdCompressionDict``.

The dictionary training mechanism is known as *cover*. More details about it are
available in the paper *Effective Construction of Relative Lempel-Ziv
Dictionaries* (authors: Liao, Petri, Moffat, Wirth).

The cover algorithm takes parameters ``k` and ``d``. These are the
*segment size* and *dmer size*, respectively. The returned dictionary
instance created by this function has ``k`` and ``d`` attributes
containing the values for these parameters. If a ``ZstdCompressionDict``
is constructed from raw bytes data (a content-only dictionary), the
``k`` and ``d`` attributes will be ``0``.

The segment and dmer size parameters to the cover algorithm can either be
specified manually or ``train_dictionary()`` can try multiple values
and pick the best one, where *best* means the smallest compressed data size.
This later mode is called *optimization* mode.

If none of ``k``, ``d``, ``steps``, ``threads``, ``level``, ``notifications``,
or ``dict_id`` (basically anything from the underlying ``ZDICT_cover_params_t``
struct) are defined, *optimization* mode is used with default parameter
values.

If ``steps`` or ``threads`` are defined, then *optimization* mode is engaged
with explicit control over those parameters. Specifying ``threads=0`` or
``threads=1`` can be used to engage *optimization* mode if other parameters
are not defined.

Otherwise, non-*optimization* mode is used with the parameters specified.

This function takes the following arguments:

dict_size
   Target size in bytes of the dictionary to generate.
samples
   A list of bytes holding samples the dictionary will be trained from.
k
   Parameter to cover algorithm defining the segment size. A reasonable range
   is [16, 2048+].
d
   Parameter to cover algorithm defining the dmer size. A reasonable range is
   [6, 16]. ``d`` must be less than or equal to ``k``.
dict_id
   Integer dictionary ID for the produced dictionary. Default is 0, which uses
   a random value.
steps
   Number of steps through ``k`` values to perform when trying parameter
   variations.
threads
   Number of threads to use when trying parameter variations. Default is 0,
   which means to use a single thread. A negative value can be specified to
   use as many threads as there are detected logical CPUs.
level
   Integer target compression level when trying parameter variations.
notifications
   Controls writing of informational messages to ``stderr``. ``0`` (the
   default) means to write nothing. ``1`` writes errors. ``2`` writes
   progression info. ``3`` writes more details. And ``4`` writes all info.

Explicit Compression Parameters
-------------------------------

Zstandard offers a high-level *compression level* that maps to lower-level
compression parameters. For many consumers, this numeric level is the only
compression setting you'll need to touch.

But for advanced use cases, it might be desirable to tweak these lower-level
settings.

The ``ZstdCompressionParameters`` type represents these low-level compression
settings.

Instances of this type can be constructed from a myriad of keyword arguments
(defined below) for complete low-level control over each adjustable
compression setting.

From a higher level, one can construct a ``ZstdCompressionParameters`` instance
given a desired compression level and target input and dictionary size
using ``ZstdCompressionParameters.from_level()``. e.g.::

    # Derive compression settings for compression level 7.
    params = zstd.ZstdCompressionParameters.from_level(7)

    # With an input size of 1MB
    params = zstd.ZstdCompressionParameters.from_level(7, source_size=1048576)

Using ``from_level()``, it is also possible to override individual compression
parameters or to define additional settings that aren't automatically derived.
e.g.::

    params = zstd.ZstdCompressionParameters.from_level(4, window_log=10)
    params = zstd.ZstdCompressionParameters.from_level(5, threads=4)

Or you can define low-level compression settings directly::

    params = zstd.ZstdCompressionParameters(window_log=12, enable_ldm=True)

Once a ``ZstdCompressionParameters`` instance is obtained, it can be used to
configure a compressor::

    cctx = zstd.ZstdCompressor(compression_params=params)

The named arguments and attributes of ``ZstdCompressionParameters`` are as
follows:

* format
* compression_level
* window_log
* hash_log
* chain_log
* search_log
* min_match
* target_length
* strategy
* compression_strategy (deprecated: same as ``strategy``)
* write_content_size
* write_checksum
* write_dict_id
* job_size
* overlap_log
* overlap_size_log (deprecated: same as ``overlap_log``)
* force_max_window
* enable_ldm
* ldm_hash_log
* ldm_min_match
* ldm_bucket_size_log
* ldm_hash_rate_log
* ldm_hash_every_log (deprecated: same as ``ldm_hash_rate_log``)
* threads

Some of these are very low-level settings. It may help to consult the official
zstandard documentation for their behavior. Look for the ``ZSTD_p_*`` constants
in ``zstd.h`` (https://github.com/facebook/zstd/blob/dev/lib/zstd.h).

Frame Inspection
----------------

Data emitted from zstd compression is encapsulated in a *frame*. This frame
begins with a 4 byte *magic number* header followed by 2 to 14 bytes describing
the frame in more detail. For more info, see
https://github.com/facebook/zstd/blob/master/doc/zstd_compression_format.md.

``zstd.get_frame_parameters(data)`` parses a zstd *frame* header from a bytes
instance and return a ``FrameParameters`` object describing the frame.

Depending on which fields are present in the frame and their values, the
length of the frame parameters varies. If insufficient bytes are passed
in to fully parse the frame parameters, ``ZstdError`` is raised. To ensure
frame parameters can be parsed, pass in at least 18 bytes.

``FrameParameters`` instances have the following attributes:

content_size
   Integer size of original, uncompressed content. This will be ``0`` if the
   original content size isn't written to the frame (controlled with the
   ``write_content_size`` argument to ``ZstdCompressor``) or if the input
   content size was ``0``.

window_size
   Integer size of maximum back-reference distance in compressed data.

dict_id
   Integer of dictionary ID used for compression. ``0`` if no dictionary
   ID was used or if the dictionary ID was ``0``.

has_checksum
   Bool indicating whether a 4 byte content checksum is stored at the end
   of the frame.

``zstd.frame_header_size(data)`` returns the size of the zstandard frame
header.

``zstd.frame_content_size(data)`` returns the content size as parsed from
the frame header. ``-1`` means the content size is unknown. ``0`` means
an empty frame. The content size is usually correct. However, it may not
be accurate.

Misc Functionality
------------------

estimate_decompression_context_size()
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Estimate the memory size requirements for a decompressor instance.

Constants
---------

The following module constants/attributes are exposed:

ZSTD_VERSION
    This module attribute exposes a 3-tuple of the Zstandard version. e.g.
    ``(1, 0, 0)``
MAX_COMPRESSION_LEVEL
    Integer max compression level accepted by compression functions
COMPRESSION_RECOMMENDED_INPUT_SIZE
    Recommended chunk size to feed to compressor functions
COMPRESSION_RECOMMENDED_OUTPUT_SIZE
    Recommended chunk size for compression output
DECOMPRESSION_RECOMMENDED_INPUT_SIZE
    Recommended chunk size to feed into decompresor functions
DECOMPRESSION_RECOMMENDED_OUTPUT_SIZE
    Recommended chunk size for decompression output

FRAME_HEADER
    bytes containing header of the Zstandard frame
MAGIC_NUMBER
    Frame header as an integer

FLUSH_BLOCK
    Flushing behavior that denotes to flush a zstd block. A decompressor will
    be able to decode all data fed into the compressor so far.
FLUSH_FRAME
    Flushing behavior that denotes to end a zstd frame. Any new data fed
    to the compressor will start a new frame.

CONTENTSIZE_UNKNOWN
    Value for content size when the content size is unknown.
CONTENTSIZE_ERROR
    Value for content size when content size couldn't be determined.

WINDOWLOG_MIN
    Minimum value for compression parameter
WINDOWLOG_MAX
    Maximum value for compression parameter
CHAINLOG_MIN
    Minimum value for compression parameter
CHAINLOG_MAX
    Maximum value for compression parameter
HASHLOG_MIN
    Minimum value for compression parameter
HASHLOG_MAX
    Maximum value for compression parameter
SEARCHLOG_MIN
    Minimum value for compression parameter
SEARCHLOG_MAX
    Maximum value for compression parameter
MINMATCH_MIN
    Minimum value for compression parameter
MINMATCH_MAX
    Maximum value for compression parameter
SEARCHLENGTH_MIN
    Minimum value for compression parameter

    Deprecated: use ``MINMATCH_MIN``
SEARCHLENGTH_MAX
    Maximum value for compression parameter

    Deprecated: use ``MINMATCH_MAX``
TARGETLENGTH_MIN
    Minimum value for compression parameter
STRATEGY_FAST
    Compression strategy
STRATEGY_DFAST
    Compression strategy
STRATEGY_GREEDY
    Compression strategy
STRATEGY_LAZY
    Compression strategy
STRATEGY_LAZY2
    Compression strategy
STRATEGY_BTLAZY2
    Compression strategy
STRATEGY_BTOPT
    Compression strategy
STRATEGY_BTULTRA
    Compression strategy
STRATEGY_BTULTRA2
    Compression strategy

FORMAT_ZSTD1
    Zstandard frame format
FORMAT_ZSTD1_MAGICLESS
    Zstandard frame format without magic header

Performance Considerations
--------------------------

The ``ZstdCompressor`` and ``ZstdDecompressor`` types maintain state to a
persistent compression or decompression *context*. Reusing a ``ZstdCompressor``
or ``ZstdDecompressor`` instance for multiple operations is faster than
instantiating a new ``ZstdCompressor`` or ``ZstdDecompressor`` for each
operation. The differences are magnified as the size of data decreases. For
example, the difference between *context* reuse and non-reuse for 100,000
100 byte inputs will be significant (possiby over 10x faster to reuse contexts)
whereas 10 100,000,000 byte inputs will be more similar in speed (because the
time spent doing compression dwarfs time spent creating new *contexts*).

Buffer Types
------------

The API exposes a handful of custom types for interfacing with memory buffers.
The primary goal of these types is to facilitate efficient multi-object
operations.

The essential idea is to have a single memory allocation provide backing
storage for multiple logical objects. This has 2 main advantages: fewer
allocations and optimal memory access patterns. This avoids having to allocate
a Python object for each logical object and furthermore ensures that access of
data for objects can be sequential (read: fast) in memory.

BufferWithSegments
^^^^^^^^^^^^^^^^^^

The ``BufferWithSegments`` type represents a memory buffer containing N
discrete items of known lengths (segments). It is essentially a fixed size
memory address and an array of 2-tuples of ``(offset, length)`` 64-bit
unsigned native endian integers defining the byte offset and length of each
segment within the buffer.

Instances behave like containers.

``len()`` returns the number of segments within the instance.

``o[index]`` or ``__getitem__`` obtains a ``BufferSegment`` representing an
individual segment within the backing buffer. That returned object references
(not copies) memory. This means that iterating all objects doesn't copy
data within the buffer.

The ``.size`` attribute contains the total size in bytes of the backing
buffer.

Instances conform to the buffer protocol. So a reference to the backing bytes
can be obtained via ``memoryview(o)``. A *copy* of the backing bytes can also
be obtained via ``.tobytes()``.

The ``.segments`` attribute exposes the array of ``(offset, length)`` for
segments within the buffer. It is a ``BufferSegments`` type.

BufferSegment
^^^^^^^^^^^^^

The ``BufferSegment`` type represents a segment within a ``BufferWithSegments``.
It is essentially a reference to N bytes within a ``BufferWithSegments``.

``len()`` returns the length of the segment in bytes.

``.offset`` contains the byte offset of this segment within its parent
``BufferWithSegments`` instance.

The object conforms to the buffer protocol. ``.tobytes()`` can be called to
obtain a ``bytes`` instance with a copy of the backing bytes.

BufferSegments
^^^^^^^^^^^^^^

This type represents an array of ``(offset, length)`` integers defining segments
within a ``BufferWithSegments``.

The array members are 64-bit unsigned integers using host/native bit order.

Instances conform to the buffer protocol.

BufferWithSegmentsCollection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``BufferWithSegmentsCollection`` type represents a virtual spanning view
of multiple ``BufferWithSegments`` instances.

Instances are constructed from 1 or more ``BufferWithSegments`` instances. The
resulting object behaves like an ordered sequence whose members are the
segments within each ``BufferWithSegments``.

``len()`` returns the number of segments within all ``BufferWithSegments``
instances.

``o[index]`` and ``__getitem__(index)`` return the ``BufferSegment`` at
that offset as if all ``BufferWithSegments`` instances were a single
entity.

If the object is composed of 2 ``BufferWithSegments`` instances with the
first having 2 segments and the second have 3 segments, then ``b[0]``
and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
and ``b[4]`` access segments from the second.

Choosing an API
===============

There are multiple APIs for performing compression and decompression. This is
because different applications have different needs and the library wants to
facilitate optimal use in as many use cases as possible.

From a high-level, APIs are divided into *one-shot* and *streaming*: either you
are operating on all data at once or you operate on it piecemeal.

The *one-shot* APIs are useful for small data, where the input or output
size is known. (The size can come from a buffer length, file size, or
stored in the zstd frame header.) A limitation of the *one-shot* APIs is that
input and output must fit in memory simultaneously. For say a 4 GB input,
this is often not feasible.

The *one-shot* APIs also perform all work as a single operation. So, if you
feed it large input, it could take a long time for the function to return.

The streaming APIs do not have the limitations of the simple API. But the
price you pay for this flexibility is that they are more complex than a
single function call.

The streaming APIs put the caller in control of compression and decompression
behavior by allowing them to directly control either the input or output side
of the operation.

With the *streaming input*, *compressor*, and *decompressor* APIs, the caller
has full control over the input to the compression or decompression stream.
They can directly choose when new data is operated on.

With the *streaming ouput* APIs, the caller has full control over the output
of the compression or decompression stream. It can choose when to receive
new data.

When using the *streaming* APIs that operate on file-like or stream objects,
it is important to consider what happens in that object when I/O is requested.
There is potential for long pauses as data is read or written from the
underlying stream (say from interacting with a filesystem or network). This
could add considerable overhead.

Thread Safety
=============

``ZstdCompressor`` and ``ZstdDecompressor`` instances have no guarantees
about thread safety. Do not operate on the same ``ZstdCompressor`` and
``ZstdDecompressor`` instance simultaneously from different threads. It is
fine to have different threads call into a single instance, just not at the
same time.

Some operations require multiple function calls to complete. e.g. streaming
operations. A single ``ZstdCompressor`` or ``ZstdDecompressor`` cannot be used
for simultaneously active operations. e.g. you must not start a streaming
operation when another streaming operation is already active.

The C extension releases the GIL during non-trivial calls into the zstd C
API. Non-trivial calls are notably compression and decompression. Trivial
calls are things like parsing frame parameters. Where the GIL is released
is considered an implementation detail and can change in any release.

APIs that accept bytes-like objects don't enforce that the underlying object
is read-only. However, it is assumed that the passed object is read-only for
the duration of the function call. It is possible to pass a mutable object
(like a ``bytearray``) to e.g. ``ZstdCompressor.compress()``, have the GIL
released, and mutate the object from another thread. Such a race condition
is a bug in the consumer of python-zstandard. Most Python data types are
immutable, so unless you are doing something fancy, you don't need to
worry about this.

Note on Zstandard's *Experimental* API
======================================

Many of the Zstandard APIs used by this module are marked as *experimental*
within the Zstandard project.

It is unclear how Zstandard's C API will evolve over time, especially with
regards to this *experimental* functionality. We will try to maintain
backwards compatibility at the Python API level. However, we cannot
guarantee this for things not under our control.

Since a copy of the Zstandard source code is distributed with this
module and since we compile against it, the behavior of a specific
version of this module should be constant for all of time. So if you
pin the version of this module used in your projects (which is a Python
best practice), you should be shielded from unwanted future changes.

Donate
======

A lot of time has been invested into this project by the author.

If you find this project useful and would like to thank the author for
their work, consider donating some money. Any amount is appreciated.

.. image:: https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif
    :target: https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=gregory%2eszorc%40gmail%2ecom&lc=US&item_name=python%2dzstandard&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donate_LG%2egif%3aNonHosted
    :alt: Donate via PayPal

.. |ci-status| image:: https://dev.azure.com/gregoryszorc/python-zstandard/_apis/build/status/indygreg.python-zstandard?branchName=master
    :target: https://dev.azure.com/gregoryszorc/python-zstandard/_apis/build/status/indygreg.python-zstandard?branchName=master



