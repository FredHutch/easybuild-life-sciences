name = 'PyTorch'
version = '2.0.1'
versionsuffix = 'a-CUDA-%(cudaver)s'

homepage = 'https://pytorch.org/'
description = """Tensors and Dynamic neural networks in Python with strong GPU acceleration.
PyTorch is a deep learning framework that puts Python first."""

toolchain = {'name': 'foss', 'version': '2022b'}

sources = [{
    'filename': '%(name)s-%(version)s.tar.gz',
    'git_config': {
        'url': 'https://github.com/pytorch',
        'repo_name': 'pytorch',
        'tag': 'v%(version)s',
        'recursive': True,
    },
}]

patches = []
checksums = []

osdependencies = [OS_PKG_IBVERBS_DEV]

builddependencies = [
    ('CMake', '3.24.3'),
    ('hypothesis', '6.68.2'),
]

dependencies = [
    ('CUDA', '11.7.0', '', SYSTEM),
    ('Ninja', '1.11.1'),  # Required for JIT compilation of C++ extensions
    ('Python', '3.10.8'),
    ('protobuf', '22.4'),
    ('protobuf-python', '4.21.9'),
    ('pybind11', '2.10.3'),
    ('SciPy-bundle', '2023.02'),
    # ('typing-extensions', '3.10.0.2'),
    ('PyYAML', '6.0'),
    ('MPFR', '4.2.0'),
    ('GMP', '6.2.1'),
    ('numactl', '2.0.16'),
    ('FFmpeg', '5.1.2'),
    ('Pillow', '9.4.0'),
    ('cuDNN', '8.4.1.50', '-CUDA-%(cudaver)s', SYSTEM),
    ('magma', '2.7.0', '-CUDA-%(cudaver)s'),
    ('NCCL', '2.10.3', '-CUDA-%(cudaver)s'),
    ('expecttest', '0.1.3'),
]

# default CUDA compute capabilities to use (override via --cuda-compute-capabilities)
cuda_compute_capabilities = ['3.5', '3.7', '5.2', '6.0', '6.1', '7.0', '7.2', '7.5', '8.0', '8.6']

custom_opts = ["USE_CUPTI_SO=1"]

excluded_tests = {
    '': [
        # Bad tests: https://github.com/pytorch/pytorch/issues/60260
        'distributed/elastic/utils/distributed_test',
        'distributed/elastic/multiprocessing/api_test',
        # These tests fail on A10s at the very least, they time out forever no matter how long the timeout is.
        # Possibly related to NCCL 2.8.3: https://docs.nvidia.com/deeplearning/nccl/release-notes/rel_2-8-3.html
        # 'distributed/test_distributed_fork',
        # 'distributed/test_distributed_spawn',
        # Fails on A10s: https://github.com/pytorch/pytorch/issues/63079
        'test_optim',
        # Test from this suite timeout often. The process group backend is deprecated anyway
        # 'distributed/rpc/test_process_group_agent',
        # This test fails constently when run as part of the test suite, but succeeds when run interactively
        # 'test_model_dump',
        # These tests appear flaky, possibly related to number of GPUs that are used
        'distributed/fsdp/test_fsdp_memory',
        'distributed/fsdp/test_fsdp_overlap',
    ]
}

runtest = 'cd test && PYTHONUNBUFFERED=1 %(python)s run_test.py --continue-through-error  --verbose %(excluded_tests)s'

# several tests are known to be flaky, and fail in some contexts (like having multiple GPUs available),
# so we allow up to 10 (out of ~90k) tests to fail before treating the installation to be faulty
# For the RTX 6000 on Skylake, that number might be up to 24
max_failed_tests = 10

# The readelf sanity check command can be taken out once the TestRPATH test from 
# https://github.com/pytorch/pytorch/pull/68912 is accepted, since it is then checked as part of the PyTorch test suite
local_libcaffe2 = "$EBROOTPYTORCH/lib/python%%(pyshortver)s/site-packages/torch/lib/libcaffe2_nvrtc.%s" % SHLIB_EXT
sanity_check_commands = [
    "readelf -d %s | egrep 'RPATH|RUNPATH' | grep -v stubs" % local_libcaffe2,
]
tests = ['PyTorch-check-cpp-extension.py']

moduleclass = 'ai'
